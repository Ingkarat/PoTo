{
 "1": {
  "name": "value",
  "type": "NoneType",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/context.py",
  "lineno": "48",
  "column": "8",
  "context": "` being replaced\n    \"\"\"\n    def inner():\n        value = CLIARGS.get(key, default=default)\n        if not shallowcopy:\n            return val",
  "context_lines": "    This function is not directly bound to ``CliArgs`` so that it works with\n    ``CLIARGS`` being replaced\n    \"\"\"\n    def inner():\n        value = CLIARGS.get(key, default=default)\n        if not shallowcopy:\n            return value\n        elif is_sequence(value):\n            return value[:]\n",
  "slicing": [
   "CLIARGS = CLIArgs({})\n",
   "    CLIARGS = GlobalCLIArgs.from_options(cli_args)\n",
   "        value = CLIARGS.get(key, default=default)\n",
   "            return value\n",
   "        elif is_sequence(value):\n",
   "            return value[:]\n",
   "        elif isinstance(value, (Mapping, Set)):\n",
   "            return value.copy()\n",
   "        return value\n"
  ]
 },
 "2": {
  "name": "aobj",
  "type": "Block",
  "class": "customized",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/vars/reserved.py",
  "lineno": "42",
  "column": "8",
  "context": "ock, Task]\n\n    for aclass in class_list:\n        aobj = aclass()\n\n        # build ordered list to loop over and dic",
  "context_lines": "    result = set()\n\n    # FIXME: find a way to 'not hardcode', possibly need role deps/includes\n    class_list = [Play, Role, Block, Task]\n\n    for aclass in class_list:\n        aobj = aclass()\n\n        # build ordered list to loop over and dict with attributes\n        for attribute in aobj.__dict__['_attributes']:\n            if 'private' in attribute:\n",
  "slicing": [
   "    class_list = [Play, Role, Block, Task]\n",
   "    for aclass in class_list:\n",
   "        aobj = aclass()\n",
   "        for attribute in aobj.__dict__['_attributes']:\n",
   "            if 'private' in attribute:\n",
   "                private.add(attribute)\n",
   "                public.add(attribute)\n"
  ]
 },
 "3": {
  "name": "result",
  "type": "set",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/vars/reserved.py",
  "lineno": "61",
  "column": "8",
  "context": "lic.add('with_')\n\n    if include_private:\n        result = public.union(private)\n    else:\n        result = public\n\n    return resu",
  "context_lines": "    # FIXME: remove after with_ is not only deprecated but removed\n    if 'loop' in private or 'loop' in public:\n        public.add('with_')\n\n    if include_private:\n        result = public.union(private)\n    else:\n        result = public\n\n    return result\n\n\ndef warn_if_reserved(myvars):\n",
  "slicing": [
   "    public = set()\n",
   "    private = set()\n",
   "    class_list = [Play, Role, Block, Task]\n",
   "    for aclass in class_list:\n",
   "        aobj = aclass()\n",
   "        for attribute in aobj.__dict__['_attributes']:\n",
   "            if 'private' in attribute:\n",
   "                private.add(attribute)\n",
   "                public.add(attribute)\n",
   "    if 'action' in public:\n",
   "        public.add('local_action')\n",
   "    if 'loop' in private or 'loop' in public:\n",
   "        public.add('with_')\n",
   "        result = public.union(private)\n",
   "        result = public\n",
   "    return result\n"
  ]
 },
 "4": {
  "name": "_ALLOWED",
  "type": "frozenset",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/vars/manager.py",
  "lineno": "77",
  "column": "4",
  "context": "))\n\n    return data\n\n\nclass VariableManager:\n\n    _ALLOWED = frozenset(['plugins_by_group', 'groups_plugins_play', 'groups_plugins_inventory', 'groups_inventory',\n                          'all_plugins_play', 'all",
  "context_lines": "        if not isinstance(item, MutableMapping):\n            raise AnsibleError(\"variable files must contain either a dictionary of variables, or a list of dictionaries. Got: %s (%s)\" % (a, type(a)))\n\n    return data\n\n\nclass VariableManager:\n\n    _ALLOWED = frozenset(['plugins_by_group', 'groups_plugins_play', 'groups_plugins_inventory', 'groups_inventory',\n                          'all_plugins_play', 'all_plugins_inventory', 'all_inventory'])\n\n    def __init__(self, loader=None, inventory=None, version_info=None):\n        self._nonpersistent_fact_cache = defaultdict(dict)\n        self._vars_cache = defaultdict(dict)\n",
  "slicing": [
   "display = Display()\n",
   "def preprocess_vars(a):\n",
   "        data = [a]\n",
   "        data = a\n",
   "    for item in data:\n",
   "        if not isinstance(item, MutableMapping):\n",
   "            raise AnsibleError(\"variable files must contain either a dictionary of variables, or a list of dictionaries. Got: %s (%s)\" % (a, type(a)))\n",
   "    return data\n",
   "    _ALLOWED = frozenset(['plugins_by_group', 'groups_plugins_play', 'groups_plugins_inventory', 'groups_inventory',\n",
   "        basedir = self._options_vars.get('basedir', False)\n",
   "        self.safe_basedir = bool(basedir is False or basedir)\n",
   "            display.warning(to_text(e))\n",
   "        data = dict(\n",
   "        return data\n",
   "        self._fact_cache = data.get('fact_cache', defaultdict(dict))\n",
   "        self._nonpersistent_fact_cache = data.get('np_fact_cache', defaultdict(dict))\n",
   "        self._vars_cache = data.get('vars_cache', defaultdict(dict))\n",
   "        self._extra_vars = data.get('extra_vars', dict())\n",
   "        self._host_vars_files = data.get('host_vars_files', defaultdict(dict))\n",
   "        self._group_vars_files = data.get('group_vars_files', defaultdict(dict))\n",
   "        self._omit_token = data.get('omit_token', '__omit_place_holder__%s' % sha1(os.urandom(64)).hexdigest())\n",
   "        self._inventory = data.get('inventory', None)\n",
   "        self._options_vars = data.get('options_vars', dict())\n",
   "        self.safe_basedir = data.get('safe_basedir', False)\n",
   "        display.debug(\"in VariableManager get_vars()\")\n",
   "        all_vars = dict()\n",
   "        magic_variables = self._get_magic_variables(\n",
   "        _vars_sources = {}\n",
   "        def _combine_and_track(data, new_data, source):\n",
   "                for key in new_data:\n",
   "                    _vars_sources[key] = source\n",
   "            return combine_vars(data, new_data)\n",
   "        basedirs = []\n",
   "            basedirs = [self._loader.get_basedir()]\n",
   "            for role in play.get_roles():\n",
   "                all_vars = _combine_and_track(all_vars, role.get_default_vars(), \"role '%s' defaults\" % role.name)\n",
   "                basedirs = task.get_search_path()\n",
   "                basedirs = [task.get_search_path()[0]]\n",
   "                all_vars = _combine_and_track(all_vars, task._role.get_default_vars(dep_chain=task.get_dep_chain()),\n",
   "            all_group = self._inventory.groups.get('all')\n",
   "            host_groups = sort_groups([g for g in host.get_groups() if g.name not in ['all']])\n",
   "                data = {}\n",
   "                    data = plugin.get_vars(self._loader, path, entities)\n",
   "                        for entity in entities:\n",
   "                            if isinstance(entity, Host):\n",
   "                                data.update(plugin.get_host_vars(entity.name))\n",
   "                                data.update(plugin.get_group_vars(entity.name))\n",
   "                return data\n",
   "            def _plugins_inventory(entities):\n",
   "            def _plugins_play(entities):\n",
   "                data = {}\n",
   "                for path in basedirs:\n",
   "                    data = _combine_and_track(data, get_vars_from_path(self._loader, path, entities, stage), \"path '%s'\" % path)\n",
   "                return data\n",
   "                return all_group.get_vars()\n",
   "                return _plugins_inventory([all_group])\n",
   "                return _plugins_play([all_group])\n",
   "                return get_group_vars(host_groups)\n",
   "                return _plugins_inventory(host_groups)\n",
   "                return _plugins_play(host_groups)\n",
   "                data = {}\n",
   "                for group in host_groups:\n",
   "                    data[group] = _combine_and_track(data[group], _plugins_inventory(group), \"inventory group_vars for '%s'\" % group)\n",
   "                    data[group] = _combine_and_track(data[group], _plugins_play(group), \"playbook group_vars for '%s'\" % group)\n",
   "                return data\n",
   "            for entry in C.VARIABLE_PRECEDENCE:\n",
   "                if entry in self._ALLOWED:\n",
   "                    display.debug('Calling %s to load vars for %s' % (entry, host.name))\n",
   "                    all_vars = _combine_and_track(all_vars, locals()[entry](), \"group vars, precedence entry '%s'\" % entry)\n",
   "                    display.warning('Ignoring unknown variable precedence entry: %s' % (entry))\n",
   "            all_vars = _combine_and_track(all_vars, host.get_vars(), \"host vars for '%s'\" % host)\n",
   "            all_vars = _combine_and_track(all_vars, _plugins_inventory([host]), \"inventory host_vars for '%s'\" % host)\n",
   "            all_vars = _combine_and_track(all_vars, _plugins_play([host]), \"playbook host_vars for '%s'\" % host)\n",
   "                facts = wrap_var(self._fact_cache.get(host.name, {}))\n",
   "                all_vars.update(namespace_facts(facts))\n",
   "                    all_vars = _combine_and_track(all_vars, wrap_var(clean_facts(facts)), \"facts\")\n",
   "                    all_vars = _combine_and_track(all_vars, wrap_var({'ansible_local': facts.get('ansible_local', {})}), \"facts\")\n",
   "            all_vars = _combine_and_track(all_vars, play.get_vars(), \"play vars\")\n",
   "            vars_files = play.get_vars_files()\n",
   "                for vars_file_item in vars_files:\n",
   "                    temp_vars = combine_vars(all_vars, self._extra_vars)\n",
   "                    temp_vars = combine_vars(temp_vars, magic_variables)\n",
   "                    templar = Templar(loader=self._loader, variables=temp_vars)\n",
   "                    vars_file_list = vars_file_item\n",
   "                    if not isinstance(vars_file_list, list):\n",
   "                        vars_file_list = [vars_file_list]\n",
   "                        for vars_file in vars_file_list:\n",
   "                            vars_file = templar.template(vars_file)\n",
   "                            if not (isinstance(vars_file, Sequence)):\n",
   "                                    \"a list of string types after template expansion\" % vars_file\n",
   "                                data = preprocess_vars(self._loader.load_from_file(vars_file, unsafe=True))\n",
   "                                if data is not None:\n",
   "                                    for item in data:\n",
   "                                        all_vars = _combine_and_track(all_vars, item, \"play vars_files from '%s'\" % vars_file)\n",
   "                                raise AnsibleFileNotFound(\"vars file %s was not found\" % vars_file_item)\n",
   "                                                           % vars_file_item, obj=vars_file_item)\n",
   "                            display.vvv(\"skipping vars_file '%s' due to an undefined variable\" % vars_file_item)\n",
   "                    display.vvv(\"Read vars_file '%s'\" % vars_file_item)\n",
   "                                         \"Got '%s' of type %s\" % (vars_files, type(vars_files)))\n",
   "                for role in play.get_roles():\n",
   "                    all_vars = _combine_and_track(all_vars, role.get_vars(include_params=False), \"role '%s' vars\" % role.name)\n",
   "                all_vars = _combine_and_track(all_vars, task._role.get_vars(task.get_dep_chain(), include_params=False),\n",
   "            all_vars = _combine_and_track(all_vars, task.get_vars(), \"task vars\")\n",
   "            all_vars = _combine_and_track(all_vars, self._vars_cache.get(host.get_name(), dict()), \"include_vars\")\n",
   "            all_vars = _combine_and_track(all_vars, self._nonpersistent_fact_cache.get(host.name, dict()), \"set_fact\")\n",
   "                all_vars = _combine_and_track(all_vars, task._role.get_role_params(task.get_dep_chain()), \"role '%s' params\" % task._role.name)\n",
   "            all_vars = _combine_and_track(all_vars, task.get_include_params(), \"include params\")\n",
   "        all_vars = _combine_and_track(all_vars, self._extra_vars, \"extra vars\")\n",
   "        all_vars = _combine_and_track(all_vars, magic_variables, \"magic vars\")\n",
   "            all_vars['environment'] = task.environment\n",
   "            all_vars['ansible_delegated_vars'], all_vars['_ansible_loop_cache'] = self._get_delegated_vars(play, task, all_vars)\n",
   "            all_vars['vars'] = all_vars.copy()\n",
   "        display.debug(\"done with get_vars()\")\n",
   "            return VarsWithSources.new_vars_with_sources(all_vars, _vars_sources)\n",
   "            return all_vars\n",
   "        variables = {}\n",
   "        variables['playbook_dir'] = os.path.abspath(self._loader.get_basedir())\n",
   "        variables['ansible_playbook_python'] = sys.executable\n",
   "        variables['ansible_config_file'] = C.CONFIG_FILE\n",
   "            dependency_role_names = list(set([d.get_name() for r in play.roles for d in r.get_all_dependencies()]))\n",
   "            play_role_names = [r.get_name() for r in play.roles]\n",
   "            variables['ansible_role_names'] = list(set(dependency_role_names + play_role_names))\n",
   "            variables['ansible_play_role_names'] = play_role_names\n",
   "            variables['ansible_dependent_role_names'] = dependency_role_names\n",
   "            variables['role_names'] = variables['ansible_play_role_names']\n",
   "            variables['ansible_play_name'] = play.get_name()\n",
   "                variables['role_name'] = task._role.get_name(include_role_fqcn=False)\n",
   "                variables['role_path'] = task._role._role_path\n",
   "                variables['role_uuid'] = text_type(task._role._uuid)\n",
   "                variables['ansible_collection_name'] = task._role._role_collection\n",
   "                variables['ansible_role_name'] = task._role.get_name()\n",
   "            variables['groups'] = self._inventory.get_groups_dict()\n",
   "                templar = Templar(loader=self._loader)\n",
   "                if templar.is_template(play.hosts):\n",
   "                    pattern = 'all'\n",
   "                    pattern = play.hosts or 'all'\n",
   "                    _hosts_all = [h.name for h in self._inventory.get_hosts(pattern=pattern, ignore_restrictions=True)]\n",
   "                    _hosts = [h.name for h in self._inventory.get_hosts()]\n",
   "                variables['ansible_play_hosts_all'] = _hosts_all[:]\n",
   "                variables['ansible_play_hosts'] = [x for x in variables['ansible_play_hosts_all'] if x not in play._removed_hosts]\n",
   "                variables['ansible_play_batch'] = [x for x in _hosts if x not in play._removed_hosts]\n",
   "                variables['play_hosts'] = variables['ansible_play_batch']\n",
   "        variables['omit'] = self._omit_token\n",
   "        for option, option_value in iteritems(self._options_vars):\n",
   "            variables[option] = option_value\n",
   "            variables['hostvars'] = self._hostvars\n",
   "        return variables\n",
   "        vars_copy = existing_variables.copy()\n",
   "        templar = Templar(loader=self._loader, variables=vars_copy)\n",
   "        items = []\n",
   "        has_loop = True\n",
   "                    loop_terms = listify_lookup_plugin_terms(terms=task.loop, templar=templar,\n",
   "                    items = wrap_var(lookup_loader.get(task.loop_with, loader=self._loader, templar=templar).run(terms=loop_terms, variables=vars_copy))\n",
   "                    items = [None]\n",
   "                items = templar.template(task.loop)\n",
   "                items = [None]\n",
   "            has_loop = False\n",
   "            items = [None]\n",
   "        delegated_host_vars = dict()\n",
   "        item_var = getattr(task.loop_control, 'loop_var', 'item')\n",
   "        cache_items = False\n",
   "        for item in items:\n",
   "            if item is not None:\n",
   "                vars_copy[item_var] = item\n",
   "            templar.available_variables = vars_copy\n",
   "            delegated_host_name = templar.template(task.delegate_to, fail_on_undefined=False)\n",
   "            if delegated_host_name != task.delegate_to:\n",
   "                cache_items = True\n",
   "            if delegated_host_name is None:\n",
   "            if not isinstance(delegated_host_name, string_types):\n",
   "                                           \" converted to a string type.\" % type(delegated_host_name), obj=task._ds)\n",
   "            if delegated_host_name in delegated_host_vars:\n",
   "            delegated_host = None\n",
   "                delegated_host = self._inventory.get_host(delegated_host_name)\n",
   "                if delegated_host is None:\n",
   "                    for h in self._inventory.get_hosts(ignore_limits=True, ignore_restrictions=True):\n",
   "                        if h.address == delegated_host_name:\n",
   "                            delegated_host = h\n",
   "                        delegated_host = Host(name=delegated_host_name)\n",
   "                delegated_host = Host(name=delegated_host_name)\n",
   "            delegated_host_vars[delegated_host_name] = self.get_vars(\n",
   "                host=delegated_host,\n",
   "            delegated_host_vars[delegated_host_name]['inventory_hostname'] = vars_copy.get('inventory_hostname')\n",
   "        _ansible_loop_cache = None\n",
   "        if has_loop and cache_items:\n",
   "            _ansible_loop_cache = items\n",
   "        return delegated_host_vars, _ansible_loop_cache\n",
   "        if not isinstance(facts, Mapping):\n",
   "            raise AnsibleAssertionError(\"the type of 'facts' to set for host_facts should be a Mapping but is a %s\" % type(facts))\n",
   "            host_cache = self._fact_cache[host]\n",
   "            host_cache = facts\n",
   "            if not isinstance(host_cache, MutableMapping):\n",
   "                                ' a {1}'.format(host, type(host_cache)))\n",
   "            host_cache.update(facts)\n",
   "        self._fact_cache[host] = host_cache\n",
   "        if not isinstance(facts, Mapping):\n",
   "            raise AnsibleAssertionError(\"the type of 'facts' to set for nonpersistent_facts should be a Mapping but is a %s\" % type(facts))\n",
   "            self._nonpersistent_fact_cache[host].update(facts)\n",
   "            self._nonpersistent_fact_cache[host] = facts\n",
   "        v = cls(data)\n",
   "        v.sources = sources\n",
   "        return v\n",
   "        return self.sources.get(key, None)\n",
   "        val = self.data[key]\n",
   "        display.debug(\"variable '%s' from source: %s\" % (key, self.sources.get(key, \"unknown\")))\n",
   "        return val\n",
   "        self.data[key] = value\n",
   "        del self.data[key]\n",
   "        return self.data.__contains__(key)\n"
  ]
 },
 "5": {
  "name": "FROM_STDIN",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/cli/vault.py",
  "lineno": "35",
  "column": "4",
  "context": "vidual task file entirely encrypted.\n    '''\n\n    FROM_STDIN = \"stdin\"\n    FROM_ARGS = \"the command line args\"\n    FROM_P",
  "context_lines": "    Role variables and defaults are also included!\n\n    Because Ansible tasks, handlers, and other objects are data, these can also be encrypted with vault.\n    If you'd like to not expose what variables you are using, you can keep an individual task file entirely encrypted.\n    '''\n\n    FROM_STDIN = \"stdin\"\n    FROM_ARGS = \"the command line args\"\n    FROM_PROMPT = \"the interactive prompt\"\n\n    def __init__(self, args):\n\n        self.b_vault_pass = None\n",
  "slicing": "    FROM_STDIN = \"stdin\"\n"
 },
 "6": {
  "name": "FROM_ARGS",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/cli/vault.py",
  "lineno": "36",
  "column": "4",
  "context": " encrypted.\n    '''\n\n    FROM_STDIN = \"stdin\"\n    FROM_ARGS = \"the command line args\"\n    FROM_PROMPT = \"the interactive prompt\"\n\n    de",
  "context_lines": "    Because Ansible tasks, handlers, and other objects are data, these can also be encrypted with vault.\n    If you'd like to not expose what variables you are using, you can keep an individual task file entirely encrypted.\n    '''\n\n    FROM_STDIN = \"stdin\"\n    FROM_ARGS = \"the command line args\"\n    FROM_PROMPT = \"the interactive prompt\"\n\n    def __init__(self, args):\n\n        self.b_vault_pass = None\n        self.b_new_vault_pass = None\n",
  "slicing": "    FROM_ARGS = \"the command line args\"\n"
 },
 "7": {
  "name": "FROM_PROMPT",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/cli/vault.py",
  "lineno": "37",
  "column": "4",
  "context": "tdin\"\n    FROM_ARGS = \"the command line args\"\n    FROM_PROMPT = \"the interactive prompt\"\n\n    def __init__(self, args):\n\n        self.b_vau",
  "context_lines": "    If you'd like to not expose what variables you are using, you can keep an individual task file entirely encrypted.\n    '''\n\n    FROM_STDIN = \"stdin\"\n    FROM_ARGS = \"the command line args\"\n    FROM_PROMPT = \"the interactive prompt\"\n\n    def __init__(self, args):\n\n        self.b_vault_pass = None\n        self.b_new_vault_pass = None\n",
  "slicing": "    FROM_PROMPT = \"the interactive prompt\"\n"
 },
 "8": {
  "name": "_ITALIC",
  "type": "_sre.SRE_Pattern",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/cli/__init__.py",
  "lineno": "49",
  "column": "4",
  "context": "   ''' code behind bin/ansible* programs '''\n\n    _ITALIC = re.compile(r\"I\\(([^)]+)\\)\")\n    _BOLD = re.compile(r\"B\\(([^)]+)\\)\")\n    _MODUL",
  "context_lines": "    HAS_ARGCOMPLETE = False\n\n\ndisplay = Display()\n\n\nclass CLI(with_metaclass(ABCMeta, object)):\n    ''' code behind bin/ansible* programs '''\n\n    _ITALIC = re.compile(r\"I\\(([^)]+)\\)\")\n    _BOLD = re.compile(r\"B\\(([^)]+)\\)\")\n    _MODULE = re.compile(r\"M\\(([^)]+)\\)\")\n    _URL = re.compile(r\"U\\(([^)]+)\\)\")\n    _CONST = re.compile(r\"C\\(([^)]+)\\)\")\n\n",
  "slicing": [
   "    _ITALIC = re.compile(r\"I\\(([^)]+)\\)\")\n"
  ]
 },
 "9": {
  "name": "_BOLD",
  "type": "_sre.SRE_Pattern",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/cli/__init__.py",
  "lineno": "50",
  "column": "4",
  "context": "''\n\n    _ITALIC = re.compile(r\"I\\(([^)]+)\\)\")\n    _BOLD = re.compile(r\"B\\(([^)]+)\\)\")\n    _MODULE = re.compile(r\"M\\(([^)]+)\\)\")\n    _URL",
  "context_lines": "display = Display()\n\n\nclass CLI(with_metaclass(ABCMeta, object)):\n    ''' code behind bin/ansible* programs '''\n\n    _ITALIC = re.compile(r\"I\\(([^)]+)\\)\")\n    _BOLD = re.compile(r\"B\\(([^)]+)\\)\")\n    _MODULE = re.compile(r\"M\\(([^)]+)\\)\")\n    _URL = re.compile(r\"U\\(([^)]+)\\)\")\n    _CONST = re.compile(r\"C\\(([^)]+)\\)\")\n\n    PAGER = 'less'\n\n",
  "slicing": [
   "    _BOLD = re.compile(r\"B\\(([^)]+)\\)\")\n"
  ]
 },
 "10": {
  "name": "_MODULE",
  "type": "_sre.SRE_Pattern",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/cli/__init__.py",
  "lineno": "51",
  "column": "4",
  "context": ")\\)\")\n    _BOLD = re.compile(r\"B\\(([^)]+)\\)\")\n    _MODULE = re.compile(r\"M\\(([^)]+)\\)\")\n    _URL = re.compile(r\"U\\(([^)]+)\\)\")\n    _CONST ",
  "context_lines": "class CLI(with_metaclass(ABCMeta, object)):\n    ''' code behind bin/ansible* programs '''\n\n    _ITALIC = re.compile(r\"I\\(([^)]+)\\)\")\n    _BOLD = re.compile(r\"B\\(([^)]+)\\)\")\n    _MODULE = re.compile(r\"M\\(([^)]+)\\)\")\n    _URL = re.compile(r\"U\\(([^)]+)\\)\")\n    _CONST = re.compile(r\"C\\(([^)]+)\\)\")\n\n    PAGER = 'less'\n\n    # -F (quit-if-one-screen) -R (allow raw ansi control chars)\n",
  "slicing": [
   "    _MODULE = re.compile(r\"M\\(([^)]+)\\)\")\n"
  ]
 },
 "11": {
  "name": "_URL",
  "type": "_sre.SRE_Pattern",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/cli/__init__.py",
  "lineno": "52",
  "column": "4",
  "context": ")\")\n    _MODULE = re.compile(r\"M\\(([^)]+)\\)\")\n    _URL = re.compile(r\"U\\(([^)]+)\\)\")\n    _CONST = re.compile(r\"C\\(([^)]+)\\)\")\n\n    PAGE",
  "context_lines": "    ''' code behind bin/ansible* programs '''\n\n    _ITALIC = re.compile(r\"I\\(([^)]+)\\)\")\n    _BOLD = re.compile(r\"B\\(([^)]+)\\)\")\n    _MODULE = re.compile(r\"M\\(([^)]+)\\)\")\n    _URL = re.compile(r\"U\\(([^)]+)\\)\")\n    _CONST = re.compile(r\"C\\(([^)]+)\\)\")\n\n    PAGER = 'less'\n\n    # -F (quit-if-one-screen) -R (allow raw ansi control chars)\n    # -S (chop long lines) -X (disable termcap init and de-init)\n",
  "slicing": [
   "    _URL = re.compile(r\"U\\(([^)]+)\\)\")\n"
  ]
 },
 "12": {
  "name": "_CONST",
  "type": "_sre.SRE_Pattern",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/cli/__init__.py",
  "lineno": "53",
  "column": "4",
  "context": "+)\\)\")\n    _URL = re.compile(r\"U\\(([^)]+)\\)\")\n    _CONST = re.compile(r\"C\\(([^)]+)\\)\")\n\n    PAGER = 'less'\n\n    # -F (quit-if-one-screen)",
  "context_lines": "    _ITALIC = re.compile(r\"I\\(([^)]+)\\)\")\n    _BOLD = re.compile(r\"B\\(([^)]+)\\)\")\n    _MODULE = re.compile(r\"M\\(([^)]+)\\)\")\n    _URL = re.compile(r\"U\\(([^)]+)\\)\")\n    _CONST = re.compile(r\"C\\(([^)]+)\\)\")\n\n    PAGER = 'less'\n\n    # -F (quit-if-one-screen) -R (allow raw ansi control chars)\n    # -S (chop long lines) -X (disable termcap init and de-init)\n",
  "slicing": [
   "    _CONST = re.compile(r\"C\\(([^)]+)\\)\")\n"
  ]
 },
 "13": {
  "name": "PAGER",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/cli/__init__.py",
  "lineno": "55",
  "column": "4",
  "context": ")\")\n    _CONST = re.compile(r\"C\\(([^)]+)\\)\")\n\n    PAGER = 'less'\n\n    # -F (quit-if-one-screen) -R (allow raw ansi ",
  "context_lines": "    _BOLD = re.compile(r\"B\\(([^)]+)\\)\")\n    _MODULE = re.compile(r\"M\\(([^)]+)\\)\")\n    _URL = re.compile(r\"U\\(([^)]+)\\)\")\n    _CONST = re.compile(r\"C\\(([^)]+)\\)\")\n\n    PAGER = 'less'\n\n    # -F (quit-if-one-screen) -R (allow raw ansi control chars)\n    # -S (chop long lines) -X (disable termcap init and de-init)\n    LESS_OPTS = 'FRSX'\n",
  "slicing": "    PAGER = 'less'\n"
 },
 "14": {
  "name": "LESS_OPTS",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/cli/__init__.py",
  "lineno": "59",
  "column": "4",
  "context": " lines) -X (disable termcap init and de-init)\n    LESS_OPTS = 'FRSX'\n    SKIP_INVENTORY_DEFAULTS = False\n\n    def __ini",
  "context_lines": "    _CONST = re.compile(r\"C\\(([^)]+)\\)\")\n\n    PAGER = 'less'\n\n    # -F (quit-if-one-screen) -R (allow raw ansi control chars)\n    # -S (chop long lines) -X (disable termcap init and de-init)\n    LESS_OPTS = 'FRSX'\n    SKIP_INVENTORY_DEFAULTS = False\n\n    def __init__(self, args, callback=None):\n        \"\"\"\n        Base init method for all command line programs\n",
  "slicing": "    LESS_OPTS = 'FRSX'\n"
 },
 "15": {
  "name": "do_serial",
  "type": "function",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/cli/console.py",
  "lineno": "268",
  "column": "4",
  "context": "self.forks = forks\n        self.set_prompt()\n\n    do_serial = do_forks\n\n    def do_verbosity(self, arg):\n        \"\"\"Set v",
  "context_lines": "            display.display('forks must be greater than or equal to 1')\n            return\n\n        self.forks = forks\n        self.set_prompt()\n\n    do_serial = do_forks\n\n    def do_verbosity(self, arg):\n        \"\"\"Set verbosity level\"\"\"\n        if not arg:\n",
  "slicing": [
   "display = Display()\n",
   "    modules = []\n",
   "        options = super(ConsoleCLI, self).post_process_args(options)\n",
   "        display.verbosity = options.verbosity\n",
   "        self.validate_conflicts(options, runas_opts=True, fork_opts=True)\n",
   "        return options\n",
   "        login_user = self.remote_user or getpass.getuser()\n",
   "        prompt = \"%s@%s (%d)[f:%s]\" % (login_user, self.cwd, len(self.selected), self.forks)\n",
   "            prompt += \"# \"\n",
   "            color = C.COLOR_ERROR\n",
   "            prompt += \"$ \"\n",
   "            color = self.NORMAL_PROMPT\n",
   "        self.prompt = stringc(prompt, color, wrap_nonvisible_chars=True)\n",
   "        modules = set()\n",
   "            for path in context.CLIARGS['module_path']:\n",
   "                if path:\n",
   "                    module_loader.add_directory(path)\n",
   "        module_paths = module_loader._get_paths()\n",
   "        for path in module_paths:\n",
   "            if path is not None:\n",
   "                modules.update(self._find_modules_in_path(path))\n",
   "        return modules\n",
   "        if os.path.isdir(path):\n",
   "            for module in os.listdir(path):\n",
   "                if module.startswith('.'):\n",
   "                elif os.path.isdir(module):\n",
   "                    self._find_modules_in_path(module)\n",
   "                elif module.startswith('__'):\n",
   "                elif any(module.endswith(x) for x in C.BLACKLIST_EXTS):\n",
   "                elif module in C.IGNORE_FILES:\n",
   "                elif module.startswith('_'):\n",
   "                    fullpath = '/'.join([path, module])\n",
   "                    if os.path.islink(fullpath):  # avoids aliases\n",
   "                    module = module.replace('_', '', 1)\n",
   "                module = os.path.splitext(module)[0]  # removes the extension\n",
   "                yield module\n",
   "            display.error(\"No host found\")\n",
   "            module = arg.split()[0]\n",
   "            module_args = ' '.join(arg.split()[1:])\n",
   "            module = 'shell'\n",
   "            module_args = arg\n",
   "            module = 'shell'\n",
   "            module_args = arg\n",
   "        result = None\n",
   "            check_raw = module in ('command', 'shell', 'script', 'raw')\n",
   "            play_ds = dict(\n",
   "                tasks=[dict(action=dict(module=module, args=parse_kv(module_args, check_raw=check_raw)))],\n",
   "            play = Play().load(play_ds, variable_manager=self.variable_manager, loader=self.loader)\n",
   "            display.error(u\"Unable to build command: %s\" % to_text(e))\n",
   "            cb = 'minimal'  # FIXME: make callbacks configurable\n",
   "                    stdout_callback=cb,\n",
   "                result = self._tqm.run(play)\n",
   "            if result is None:\n",
   "                display.error(\"No hosts found\")\n",
   "            display.error('User interrupted execution')\n",
   "            display.error(to_text(e))\n",
   "            display.display('Usage: forks <number>')\n",
   "        forks = int(arg)\n",
   "        if forks <= 0:\n",
   "            display.display('forks must be greater than or equal to 1')\n",
   "        self.forks = forks\n",
   "    do_serial = do_forks\n",
   "            display.display('Usage: verbosity <number>')\n",
   "            display.verbosity = int(arg)\n",
   "            display.v('verbosity level set to %s' % arg)\n",
   "            display.display(\"no host matched\")\n",
   "            for group in self.groups:\n",
   "                display.display(group)\n",
   "            for host in self.selected:\n",
   "                display.display(host.name)\n",
   "            display.v(\"become changed to %s\" % self.become)\n",
   "            display.display(\"Please specify become value, e.g. `become yes`\")\n",
   "            display.display(\"Please specify a remote user, e.g. `remote_user root`\")\n",
   "            display.display(\"Please specify a user, e.g. `become_user jenkins`\")\n",
   "            display.v(\"Current user is %s\" % self.become_user)\n",
   "            display.v(\"become_method changed to %s\" % self.become_method)\n",
   "            display.display(\"Please specify a become_method, e.g. `become_method su`\")\n",
   "            display.v(\"check mode changed to %s\" % self.check_mode)\n",
   "            display.display(\"Please specify check mode value, e.g. `check yes`\")\n",
   "            display.v(\"diff mode changed to %s\" % self.diff)\n",
   "            display.display(\"Please specify a diff value , e.g. `diff yes`\")\n",
   "            in_path = module_loader.find_plugin(module_name)\n",
   "            if in_path:\n",
   "                oc, a, _, _ = plugin_docs.get_docstring(in_path, fragment_loader)\n",
   "                if oc:\n",
   "                    display.display(oc['short_description'])\n",
   "                    display.display('Parameters:')\n",
   "                    for opt in oc['options'].keys():\n",
   "                        display.display('  ' + stringc(opt, self.NORMAL_PROMPT) + ' ' + oc['options'][opt]['description'][0])\n",
   "                    display.error('No documentation found for %s.' % module_name)\n",
   "                display.error('%s is not a valid command, use ? to list all valid commands.' % module_name)\n",
   "        mline = line.partition(' ')[2]\n",
   "        offs = len(mline) - len(text)\n",
   "            completions = self.hosts + self.groups\n",
   "            completions = [x.name for x in self.inventory.list_hosts(self.cwd)]\n",
   "        return [to_native(s)[offs:] for s in completions if to_native(s).startswith(to_native(mline))]\n",
   "            mline = line.split(' ')[-1]\n",
   "            offs = len(mline) - len(text)\n",
   "            return [s[offs:] + '=' for s in completions if s.startswith(mline)]\n",
   "        oc, a, _, _ = plugin_docs.get_docstring(in_path, fragment_loader, is_module=True)\n",
   "        return list(oc['options'].keys())\n",
   "            setattr(self, 'do_' + module, lambda arg, module=module: self.default(module + ' ' + arg))\n",
   "            setattr(self, 'help_' + module, lambda module=module: self.helpdefault(module))\n",
   "        self.hosts = [x.name for x in hosts]\n"
  ]
 },
 "16": {
  "name": "do_EOF",
  "type": "function",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/cli/console.py",
  "lineno": "362",
  "column": "4",
  "context": "    sys.stdout.write('\\n')\n        return -1\n\n    do_EOF = do_exit\n\n    def helpdefault(self, module_name):\n        i",
  "context_lines": "    def do_exit(self, args):\n        \"\"\"Exits from the console\"\"\"\n        sys.stdout.write('\\n')\n        return -1\n\n    do_EOF = do_exit\n\n    def helpdefault(self, module_name):\n        if module_name in self.modules:\n            in_path = module_loader.find_plugin(module_name)\n",
  "slicing": [
   "display = Display()\n",
   "    modules = []\n",
   "        options = super(ConsoleCLI, self).post_process_args(options)\n",
   "        display.verbosity = options.verbosity\n",
   "        self.validate_conflicts(options, runas_opts=True, fork_opts=True)\n",
   "        return options\n",
   "        login_user = self.remote_user or getpass.getuser()\n",
   "        prompt = \"%s@%s (%d)[f:%s]\" % (login_user, self.cwd, len(self.selected), self.forks)\n",
   "            prompt += \"# \"\n",
   "            color = C.COLOR_ERROR\n",
   "            prompt += \"$ \"\n",
   "            color = self.NORMAL_PROMPT\n",
   "        self.prompt = stringc(prompt, color, wrap_nonvisible_chars=True)\n",
   "        modules = set()\n",
   "            for path in context.CLIARGS['module_path']:\n",
   "                if path:\n",
   "                    module_loader.add_directory(path)\n",
   "        module_paths = module_loader._get_paths()\n",
   "        for path in module_paths:\n",
   "            if path is not None:\n",
   "                modules.update(self._find_modules_in_path(path))\n",
   "        return modules\n",
   "        if os.path.isdir(path):\n",
   "            for module in os.listdir(path):\n",
   "                if module.startswith('.'):\n",
   "                elif os.path.isdir(module):\n",
   "                    self._find_modules_in_path(module)\n",
   "                elif module.startswith('__'):\n",
   "                elif any(module.endswith(x) for x in C.BLACKLIST_EXTS):\n",
   "                elif module in C.IGNORE_FILES:\n",
   "                elif module.startswith('_'):\n",
   "                    fullpath = '/'.join([path, module])\n",
   "                    if os.path.islink(fullpath):  # avoids aliases\n",
   "                    module = module.replace('_', '', 1)\n",
   "                module = os.path.splitext(module)[0]  # removes the extension\n",
   "                yield module\n",
   "            display.error(\"No host found\")\n",
   "            module = arg.split()[0]\n",
   "            module_args = ' '.join(arg.split()[1:])\n",
   "            module = 'shell'\n",
   "            module_args = arg\n",
   "            module = 'shell'\n",
   "            module_args = arg\n",
   "        result = None\n",
   "            check_raw = module in ('command', 'shell', 'script', 'raw')\n",
   "            play_ds = dict(\n",
   "                tasks=[dict(action=dict(module=module, args=parse_kv(module_args, check_raw=check_raw)))],\n",
   "            play = Play().load(play_ds, variable_manager=self.variable_manager, loader=self.loader)\n",
   "            display.error(u\"Unable to build command: %s\" % to_text(e))\n",
   "            cb = 'minimal'  # FIXME: make callbacks configurable\n",
   "                    stdout_callback=cb,\n",
   "                result = self._tqm.run(play)\n",
   "            if result is None:\n",
   "                display.error(\"No hosts found\")\n",
   "            display.error('User interrupted execution')\n",
   "            display.error(to_text(e))\n",
   "            display.display('Usage: forks <number>')\n",
   "        forks = int(arg)\n",
   "        if forks <= 0:\n",
   "            display.display('forks must be greater than or equal to 1')\n",
   "        self.forks = forks\n",
   "            display.display('Usage: verbosity <number>')\n",
   "            display.verbosity = int(arg)\n",
   "            display.v('verbosity level set to %s' % arg)\n",
   "            display.display(\"no host matched\")\n",
   "            for group in self.groups:\n",
   "                display.display(group)\n",
   "            for host in self.selected:\n",
   "                display.display(host.name)\n",
   "            display.v(\"become changed to %s\" % self.become)\n",
   "            display.display(\"Please specify become value, e.g. `become yes`\")\n",
   "            display.display(\"Please specify a remote user, e.g. `remote_user root`\")\n",
   "            display.display(\"Please specify a user, e.g. `become_user jenkins`\")\n",
   "            display.v(\"Current user is %s\" % self.become_user)\n",
   "            display.v(\"become_method changed to %s\" % self.become_method)\n",
   "            display.display(\"Please specify a become_method, e.g. `become_method su`\")\n",
   "            display.v(\"check mode changed to %s\" % self.check_mode)\n",
   "            display.display(\"Please specify check mode value, e.g. `check yes`\")\n",
   "            display.v(\"diff mode changed to %s\" % self.diff)\n",
   "            display.display(\"Please specify a diff value , e.g. `diff yes`\")\n",
   "    do_EOF = do_exit\n",
   "            in_path = module_loader.find_plugin(module_name)\n",
   "            if in_path:\n",
   "                oc, a, _, _ = plugin_docs.get_docstring(in_path, fragment_loader)\n",
   "                if oc:\n",
   "                    display.display(oc['short_description'])\n",
   "                    display.display('Parameters:')\n",
   "                    for opt in oc['options'].keys():\n",
   "                        display.display('  ' + stringc(opt, self.NORMAL_PROMPT) + ' ' + oc['options'][opt]['description'][0])\n",
   "                    display.error('No documentation found for %s.' % module_name)\n",
   "                display.error('%s is not a valid command, use ? to list all valid commands.' % module_name)\n",
   "        mline = line.partition(' ')[2]\n",
   "        offs = len(mline) - len(text)\n",
   "            completions = self.hosts + self.groups\n",
   "            completions = [x.name for x in self.inventory.list_hosts(self.cwd)]\n",
   "        return [to_native(s)[offs:] for s in completions if to_native(s).startswith(to_native(mline))]\n",
   "            mline = line.split(' ')[-1]\n",
   "            offs = len(mline) - len(text)\n",
   "            return [s[offs:] + '=' for s in completions if s.startswith(mline)]\n",
   "        oc, a, _, _ = plugin_docs.get_docstring(in_path, fragment_loader, is_module=True)\n",
   "        return list(oc['options'].keys())\n",
   "            setattr(self, 'do_' + module, lambda arg, module=module: self.default(module + ' ' + arg))\n",
   "            setattr(self, 'help_' + module, lambda module=module: self.helpdefault(module))\n",
   "        self.hosts = [x.name for x in hosts]\n"
  ]
 },
 "17": {
  "name": "options_list",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/modules/apt.py",
  "lineno": "532",
  "column": "4",
  "context": "expand_dpkg_options(dpkg_options_compressed):\n    options_list = dpkg_options_compressed.split(',')\n    dpkg_options = \"\"\n    for dpkg_option in optio",
  "context_lines": "            # assume older version of python-apt is installed\n            package_is_upgradable = pkg.isUpgradable\n\n    return package_is_installed, version_is_installed, package_is_upgradable, has_files\n\n\ndef expand_dpkg_options(dpkg_options_compressed):\n    options_list = dpkg_options_compressed.split(',')\n    dpkg_options = \"\"\n    for dpkg_option in options_list:\n        dpkg_options = '%s -o \"Dpkg::Options::=--%s\"' \\\n                       % (dpkg_options, dpkg_option)\n",
  "slicing": "    options_list = dpkg_options_compressed.split(',')\n"
 },
 "18": {
  "name": "dpkg_options",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/modules/apt.py",
  "lineno": "533",
  "column": "4",
  "context": "ons_list = dpkg_options_compressed.split(',')\n    dpkg_options = \"\"\n    for dpkg_option in options_list:\n        dpkg_",
  "context_lines": "            package_is_upgradable = pkg.isUpgradable\n\n    return package_is_installed, version_is_installed, package_is_upgradable, has_files\n\n\ndef expand_dpkg_options(dpkg_options_compressed):\n    options_list = dpkg_options_compressed.split(',')\n    dpkg_options = \"\"\n    for dpkg_option in options_list:\n        dpkg_options = '%s -o \"Dpkg::Options::=--%s\"' \\\n                       % (dpkg_options, dpkg_option)\n    return dpkg_options.strip()\n\n\n",
  "slicing": "    dpkg_options = \"\"\n"
 },
 "19": {
  "name": "dpkg_options",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/modules/apt.py",
  "lineno": "535",
  "column": "8",
  "context": "= \"\"\n    for dpkg_option in options_list:\n        dpkg_options = '%s -o \"Dpkg::Options::=--%s\"' \\\n                       % (dpkg_options, dpkg_optio",
  "context_lines": "def expand_dpkg_options(dpkg_options_compressed):\n    options_list = dpkg_options_compressed.split(',')\n    dpkg_options = \"\"\n    for dpkg_option in options_list:\n        dpkg_options = '%s -o \"Dpkg::Options::=--%s\"' \\\n                       % (dpkg_options, dpkg_option)\n    return dpkg_options.strip()\n\n\ndef expand_pkgspec_from_fnmatches(m, pkgspec, cache):\n    # Note: apt-get does implicit regex matching when an exact package name\n",
  "slicing": "        dpkg_options = '%s -o \"Dpkg::Options::=--%s\"' \\\n"
 },
 "20": {
  "name": "_CANONICALIZE_RE",
  "type": "_sre.SRE_Pattern",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/modules/pip.py",
  "lineno": "531",
  "column": "4",
  "context": "ther a package is already satisfied.\n    \"\"\"\n\n    _CANONICALIZE_RE = re.compile(r'[-_.]+')\n\n    def __init__(self, name_string, version_strin",
  "context_lines": "    A wrapper class for Requirement, which provides\n    API to parse package name, version specifier,\n    test whether a package is already satisfied.\n    \"\"\"\n\n    _CANONICALIZE_RE = re.compile(r'[-_.]+')\n\n    def __init__(self, name_string, version_string=None):\n        self._plain_package = False\n        self.package_name = name_string\n",
  "slicing": [
   "SETUPTOOLS_IMP_ERR = None\n",
   "    HAS_SETUPTOOLS = True\n",
   "    HAS_SETUPTOOLS = False\n",
   "    SETUPTOOLS_IMP_ERR = traceback.format_exc()\n",
   "_SPECIAL_PACKAGE_CHECKERS = {'setuptools': 'import setuptools; print(setuptools.__version__)',\n",
   "_VCS_RE = re.compile(r'(svn|git|hg|bzr)\\+')\n",
   "op_dict = {\">=\": operator.ge, \"<=\": operator.le, \">\": operator.gt,\n",
   "def _is_vcs_url(name):\n",
   "    return re.match(_VCS_RE, name)\n",
   "def _is_package_name(name):\n",
   "    return not name.lstrip().startswith(tuple(op_dict.keys()))\n",
   "def _recover_package_name(names):\n",
   "    tmp = []\n",
   "    for one_line in names:\n",
   "        tmp.extend(one_line.split(\",\"))\n",
   "    names = tmp\n",
   "    name_parts = []\n",
   "    package_names = []\n",
   "    in_brackets = False\n",
   "    for name in names:\n",
   "        if _is_package_name(name) and not in_brackets:\n",
   "            if name_parts:\n",
   "                package_names.append(\",\".join(name_parts))\n",
   "            name_parts = []\n",
   "        if \"[\" in name:\n",
   "            in_brackets = True\n",
   "        if in_brackets and \"]\" in name:\n",
   "            in_brackets = False\n",
   "        name_parts.append(name)\n",
   "    package_names.append(\",\".join(name_parts))\n",
   "    return package_names\n",
   "def _get_cmd_options(module, cmd):\n",
   "    thiscmd = cmd + \" --help\"\n",
   "    rc, stdout, stderr = module.run_command(thiscmd)\n",
   "    if rc != 0:\n",
   "        module.fail_json(msg=\"Could not get output from %s: %s\" % (thiscmd, stdout + stderr))\n",
   "    words = stdout.strip().split()\n",
   "    cmd_options = [x for x in words if x.startswith('--')]\n",
   "    return cmd_options\n",
   "def _get_packages(module, pip, chdir):\n",
   "    command = '%s list --format=freeze' % pip\n",
   "    lang_env = {'LANG': 'C', 'LC_ALL': 'C', 'LC_MESSAGES': 'C'}\n",
   "    rc, out, err = module.run_command(command, cwd=chdir, environ_update=lang_env)\n",
   "    if rc != 0:\n",
   "        command = '%s freeze' % pip\n",
   "        rc, out, err = module.run_command(command, cwd=chdir)\n",
   "        if rc != 0:\n",
   "            _fail(module, command, out, err)\n",
   "    return command, out, err\n",
   "def _is_present(module, req, installed_pkgs, pkg_command):\n",
   "    for pkg in installed_pkgs:\n",
   "        if '==' in pkg:\n",
   "            pkg_name, pkg_version = pkg.split('==')\n",
   "            pkg_name = Package.canonicalize_name(pkg_name)\n",
   "        if pkg_name == req.package_name and req.is_satisfied_by(pkg_version):\n",
   "def _get_pip(module, env=None, executable=None):\n",
   "    candidate_pip_basenames = ('pip2', 'pip')\n",
   "        candidate_pip_basenames = ('pip3',)\n",
   "    pip = None\n",
   "            pip = executable\n",
   "            candidate_pip_basenames = (executable,)\n",
   "    if pip is None:\n",
   "            opt_dirs = []\n",
   "            for basename in candidate_pip_basenames:\n",
   "                pip = module.get_bin_path(basename, False, opt_dirs)\n",
   "                if pip is not None:\n",
   "                                     ' needs to be installed.' % ', '.join(candidate_pip_basenames))\n",
   "            venv_dir = os.path.join(env, 'bin')\n",
   "            candidate_pip_basenames = (candidate_pip_basenames[0], 'pip')\n",
   "            for basename in candidate_pip_basenames:\n",
   "                candidate = os.path.join(venv_dir, basename)\n",
   "                if os.path.exists(candidate) and is_executable(candidate):\n",
   "                    pip = candidate\n",
   "                                     'under any of these names: %s. ' % (', '.join(candidate_pip_basenames)) +\n",
   "    return pip\n",
   "def _fail(module, cmd, out, err):\n",
   "    msg = ''\n",
   "    if out:\n",
   "        msg += \"stdout: %s\" % (out, )\n",
   "    if err:\n",
   "        msg += \"\\n:stderr: %s\" % (err, )\n",
   "    module.fail_json(cmd=cmd, msg=msg)\n",
   "def _get_package_info(module, package, env=None):\n",
   "        opt_dirs = ['%s/bin' % env]\n",
   "        opt_dirs = []\n",
   "    python_bin = module.get_bin_path('python', False, opt_dirs)\n",
   "    if python_bin is None:\n",
   "        formatted_dep = None\n",
   "        rc, out, err = module.run_command([python_bin, '-c', _SPECIAL_PACKAGE_CHECKERS[package]])\n",
   "        if rc:\n",
   "            formatted_dep = None\n",
   "            formatted_dep = '%s==%s' % (package, out.strip())\n",
   "    return formatted_dep\n",
   "def setup_virtualenv(module, env, chdir, out, err):\n",
   "    cmd = shlex.split(module.params['virtualenv_command'])\n",
   "    if os.path.basename(cmd[0]) == cmd[0]:\n",
   "        cmd[0] = module.get_bin_path(cmd[0], True)\n",
   "        cmd.append('--system-site-packages')\n",
   "        cmd_opts = _get_cmd_options(module, cmd[0])\n",
   "        if '--no-site-packages' in cmd_opts:\n",
   "            cmd.append('--no-site-packages')\n",
   "    virtualenv_python = module.params['virtualenv_python']\n",
   "    if not any(ex in module.params['virtualenv_command'] for ex in ('pyvenv', '-m venv')):\n",
   "        if virtualenv_python:\n",
   "            cmd.append('-p%s' % virtualenv_python)\n",
   "            cmd.append('-p%s' % sys.executable)\n",
   "    elif module.params['virtualenv_python']:\n",
   "        module.fail_json(\n",
   "    cmd.append(env)\n",
   "    rc, out_venv, err_venv = module.run_command(cmd, cwd=chdir)\n",
   "    out += out_venv\n",
   "    err += err_venv\n",
   "    if rc != 0:\n",
   "        _fail(module, cmd, out, err)\n",
   "    return out, err\n",
   "    _CANONICALIZE_RE = re.compile(r'[-_.]+')\n",
   "            version_string = version_string.lstrip()\n",
   "            separator = '==' if version_string[0].isdigit() else ' '\n",
   "            name_string = separator.join((name_string, version_string))\n",
   "            self._requirement = Requirement.parse(name_string)\n",
   "            if self._requirement.project_name == \"distribute\" and \"setuptools\" in name_string:\n",
   "            version_to_test = LooseVersion(version_to_test)\n",
   "                op_dict[op](version_to_test, LooseVersion(ver))\n",
   "                for op, ver in self._requirement.specs\n",
   "        return Package._CANONICALIZE_RE.sub(\"-\", name).lower()\n",
   "    state_map = dict(\n",
   "    module = AnsibleModule(\n",
   "            state=dict(type='str', default='present', choices=state_map.keys()),\n",
   "    if not HAS_SETUPTOOLS:\n",
   "        module.fail_json(msg=missing_required_lib(\"setuptools\"),\n",
   "                         exception=SETUPTOOLS_IMP_ERR)\n",
   "    state = module.params['state']\n",
   "    name = module.params['name']\n",
   "    version = module.params['version']\n",
   "    requirements = module.params['requirements']\n",
   "    extra_args = module.params['extra_args']\n",
   "    chdir = module.params['chdir']\n",
   "    umask = module.params['umask']\n",
   "    env = module.params['virtualenv']\n",
   "    venv_created = False\n",
   "    if env and chdir:\n",
   "        env = os.path.join(chdir, env)\n",
   "    if umask and not isinstance(umask, int):\n",
   "            umask = int(umask, 8)\n",
   "            module.fail_json(msg=\"umask must be an octal integer\",\n",
   "    if umask is not None:\n",
   "        old_umask = os.umask(umask)\n",
   "        if state == 'latest' and version is not None:\n",
   "            module.fail_json(msg='version is incompatible with state=latest')\n",
   "        if chdir is None:\n",
   "            chdir = tempfile.gettempdir()\n",
   "        err = ''\n",
   "        out = ''\n",
   "        if env:\n",
   "            if not os.path.exists(os.path.join(env, 'bin', 'activate')):\n",
   "                venv_created = True\n",
   "                out, err = setup_virtualenv(module, env, chdir, out, err)\n",
   "        pip = _get_pip(module, env, module.params['executable'])\n",
   "        cmd = [pip] + state_map[state]\n",
   "        path_prefix = None\n",
   "        if env:\n",
   "            path_prefix = \"/\".join(pip.split('/')[:-1])\n",
   "        has_vcs = False\n",
   "        if name:\n",
   "            for pkg in name:\n",
   "                if pkg and _is_vcs_url(pkg):\n",
   "                    has_vcs = True\n",
   "            packages = [Package(pkg) for pkg in _recover_package_name(name)]\n",
   "            if version is not None:\n",
   "                if len(packages) > 1:\n",
   "                    module.fail_json(\n",
   "                if packages[0].has_version_specifier:\n",
   "                    module.fail_json(\n",
   "                packages[0] = Package(to_native(packages[0]), version)\n",
   "        if module.params['editable']:\n",
   "            args_list = []  # used if extra_args is not used at all\n",
   "            if extra_args:\n",
   "                args_list = extra_args.split(' ')\n",
   "            if '-e' not in args_list:\n",
   "                args_list.append('-e')\n",
   "                extra_args = ' '.join(args_list)\n",
   "        if extra_args:\n",
   "            cmd.extend(shlex.split(extra_args))\n",
   "        if name:\n",
   "            cmd.extend(to_native(p) for p in packages)\n",
   "        elif requirements:\n",
   "            cmd.extend(['-r', requirements])\n",
   "            module.exit_json(\n",
   "        if module.check_mode:\n",
   "            if extra_args or requirements or state == 'latest' or not name:\n",
   "                module.exit_json(changed=True)\n",
   "            pkg_cmd, out_pip, err_pip = _get_packages(module, pip, chdir)\n",
   "            out += out_pip\n",
   "            err += err_pip\n",
   "            changed = False\n",
   "            if name:\n",
   "                pkg_list = [p for p in out.split('\\n') if not p.startswith('You are using') and not p.startswith('You should consider') and p]\n",
   "                if pkg_cmd.endswith(' freeze') and ('pip' in name or 'setuptools' in name):\n",
   "                    for pkg in ('setuptools', 'pip'):\n",
   "                        if pkg in name:\n",
   "                            formatted_dep = _get_package_info(module, pkg, env)\n",
   "                            if formatted_dep is not None:\n",
   "                                pkg_list.append(formatted_dep)\n",
   "                                out += '%s\\n' % formatted_dep\n",
   "                for package in packages:\n",
   "                    is_present = _is_present(module, package, pkg_list, pkg_cmd)\n",
   "                    if (state == 'present' and not is_present) or (state == 'absent' and is_present):\n",
   "                        changed = True\n",
   "            module.exit_json(changed=changed, cmd=pkg_cmd, stdout=out, stderr=err)\n",
   "        out_freeze_before = None\n",
   "        if requirements or has_vcs:\n",
   "            _, out_freeze_before, _ = _get_packages(module, pip, chdir)\n",
   "        rc, out_pip, err_pip = module.run_command(cmd, path_prefix=path_prefix, cwd=chdir)\n",
   "        out += out_pip\n",
   "        err += err_pip\n",
   "        if rc == 1 and state == 'absent' and \\\n",
   "           ('not installed' in out_pip or 'not installed' in err_pip):\n",
   "        elif rc != 0:\n",
   "            _fail(module, cmd, out, err)\n",
   "        if state == 'absent':\n",
   "            changed = 'Successfully uninstalled' in out_pip\n",
   "            if out_freeze_before is None:\n",
   "                changed = 'Successfully installed' in out_pip\n",
   "                _, out_freeze_after, _ = _get_packages(module, pip, chdir)\n",
   "                changed = out_freeze_before != out_freeze_after\n",
   "        changed = changed or venv_created\n",
   "        module.exit_json(changed=changed, cmd=cmd, name=name, version=version,\n",
   "                         state=state, requirements=requirements, virtualenv=env,\n",
   "                         stdout=out, stderr=err)\n",
   "        if old_umask is not None:\n",
   "            os.umask(old_umask)\n"
  ]
 },
 "21": {
  "name": "errmsg",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "81",
  "column": "4",
  "context": "ypes.\n        :string: Same as 'str'\n    '''\n\n    errmsg = ''\n    basedir = None\n    if origin and os.path.isabs",
  "context_lines": "            tildes's in the value.\n        :str: Sets the value to string types.\n        :string: Same as 'str'\n    '''\n\n    errmsg = ''\n    basedir = None\n    if origin and os.path.isabs(origin) and os.path.exists(to_bytes(origin)):\n        basedir = origin\n\n    if value_type:\n",
  "slicing": [
   "    errmsg = ''\n",
   "        if errmsg:\n",
   "            raise ValueError('Invalid type provided for \"%s\": %s' % (errmsg, to_native(value)))\n"
  ]
 },
 "22": {
  "name": "basedir",
  "type": "NoneType",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "82",
  "column": "4",
  "context": "tring: Same as 'str'\n    '''\n\n    errmsg = ''\n    basedir = None\n    if origin and os.path.isabs(origin) and os.pat",
  "context_lines": "        :str: Sets the value to string types.\n        :string: Same as 'str'\n    '''\n\n    errmsg = ''\n    basedir = None\n    if origin and os.path.isabs(origin) and os.path.exists(to_bytes(origin)):\n        basedir = origin\n\n    if value_type:\n        value_type = value_type.lower()\n\n",
  "slicing": [
   "INTERNAL_DEFS = {'lookup': ('_terms',)}\n",
   "def _get_entry(plugin_type, plugin_name, config):\n",
   "    entry = ''\n",
   "        entry += 'plugin_type: %s ' % plugin_type\n",
   "            entry += 'plugin: %s ' % plugin_name\n",
   "    entry += 'setting: %s ' % config\n",
   "    return entry\n",
   "def ensure_type(value, value_type, origin=None):\n",
   "    errmsg = ''\n",
   "    basedir = None\n",
   "        basedir = origin\n",
   "        value_type = value_type.lower()\n",
   "        if value_type in ('boolean', 'bool'):\n",
   "            value = boolean(value, strict=False)\n",
   "        elif value_type in ('integer', 'int'):\n",
   "            value = int(value)\n",
   "        elif value_type == 'float':\n",
   "            value = float(value)\n",
   "        elif value_type == 'list':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            elif not isinstance(value, Sequence):\n",
   "                errmsg = 'list'\n",
   "        elif value_type == 'none':\n",
   "            if value == \"None\":\n",
   "                value = None\n",
   "            if value is not None:\n",
   "                errmsg = 'None'\n",
   "        elif value_type == 'path':\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                errmsg = 'path'\n",
   "        elif value_type in ('tmp', 'temppath', 'tmppath'):\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                if not os.path.exists(value):\n",
   "                    makedirs_safe(value, 0o700)\n",
   "                prefix = 'ansible-local-%s' % os.getpid()\n",
   "                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n",
   "                atexit.register(cleanup_tmp_file, value, warn=True)\n",
   "                errmsg = 'temppath'\n",
   "        elif value_type == 'pathspec':\n",
   "            if isinstance(value, string_types):\n",
   "                value = value.split(os.pathsep)\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathspec'\n",
   "        elif value_type == 'pathlist':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathlist'\n",
   "        elif value_type in ('str', 'string'):\n",
   "            if isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "                value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "                errmsg = 'string'\n",
   "        elif isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "            value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "        if errmsg:\n",
   "            raise ValueError('Invalid type provided for \"%s\": %s' % (errmsg, to_native(value)))\n",
   "    return to_text(value, errors='surrogate_or_strict', nonstring='passthru')\n",
   "        path = path.replace('{{CWD}}', os.getcwd())\n",
   "    return unfrackpath(path, follow=False, basedir=basedir)\n",
   "def get_config_type(cfile):\n",
   "    ftype = None\n",
   "        ext = os.path.splitext(cfile)[-1]\n",
   "        if ext in ('.ini', '.cfg'):\n",
   "            ftype = 'ini'\n",
   "        elif ext in ('.yaml', '.yml'):\n",
   "            ftype = 'yaml'\n",
   "            raise AnsibleOptionsError(\"Unsupported configuration file extension for %s: %s\" % (cfile, to_native(ext)))\n",
   "    return ftype\n",
   "def get_ini_config_value(p, entry):\n",
   "    value = None\n",
   "            value = p.get(entry.get('section', 'defaults'), entry.get('key', ''), raw=True)\n",
   "    return value\n",
   "def find_ini_config_file(warnings=None):\n",
   "        warnings = set()\n",
   "    SENTINEL = object\n",
   "    potential_paths = []\n",
   "    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n",
   "    if path_from_env is not SENTINEL:\n",
   "        path_from_env = unfrackpath(path_from_env, follow=False)\n",
   "        if os.path.isdir(to_bytes(path_from_env)):\n",
   "            path_from_env = os.path.join(path_from_env, \"ansible.cfg\")\n",
   "        potential_paths.append(path_from_env)\n",
   "    warn_cmd_public = False\n",
   "        cwd = os.getcwd()\n",
   "        perms = os.stat(cwd)\n",
   "        cwd_cfg = os.path.join(cwd, \"ansible.cfg\")\n",
   "        if perms.st_mode & stat.S_IWOTH:\n",
   "            if os.path.exists(cwd_cfg):\n",
   "                warn_cmd_public = True\n",
   "            potential_paths.append(to_text(cwd_cfg, errors='surrogate_or_strict'))\n",
   "    potential_paths.append(unfrackpath(\"~/.ansible.cfg\", follow=False))\n",
   "    potential_paths.append(\"/etc/ansible/ansible.cfg\")\n",
   "    for path in potential_paths:\n",
   "        b_path = to_bytes(path)\n",
   "        if os.path.exists(b_path) and os.access(b_path, os.R_OK):\n",
   "        path = None\n",
   "    if path_from_env != path and warn_cmd_public:\n",
   "        warnings.add(u\"Ansible is being run in a world writable directory (%s),\"\n",
   "                     % to_text(cwd))\n",
   "    return path\n",
   "        yml_file = to_bytes(yml_file)\n",
   "        if os.path.exists(yml_file):\n",
   "            with open(yml_file, 'rb') as config_def:\n",
   "                return yaml_load(config_def, Loader=SafeLoader) or {}\n",
   "            \"Missing base YAML definition file (bad install?): %s\" % to_native(yml_file))\n",
   "            cfile = self._config_file\n",
   "        ftype = get_config_type(cfile)\n",
   "        if cfile is not None:\n",
   "            if ftype == 'ini':\n",
   "                self._parsers[cfile] = configparser.ConfigParser()\n",
   "                with open(to_bytes(cfile), 'rb') as f:\n",
   "                        cfg_text = to_text(f.read(), errors='surrogate_or_strict')\n",
   "                        raise AnsibleOptionsError(\"Error reading config file(%s) because the config file was not utf8 encoded: %s\" % (cfile, to_native(e)))\n",
   "                        self._parsers[cfile].read_string(cfg_text)\n",
   "                        cfg_file = io.StringIO(cfg_text)\n",
   "                        self._parsers[cfile].readfp(cfg_file)\n",
   "                    raise AnsibleOptionsError(\"Error reading config file (%s): %s\" % (cfile, to_native(e)))\n",
   "                raise AnsibleOptionsError(\"Unsupported configuration file type: %s\" % to_native(ftype))\n",
   "        options = {}\n",
   "        defs = self.get_configuration_definitions(plugin_type, name)\n",
   "        for option in defs:\n",
   "            options[option] = self.get_config_value(option, plugin_type=plugin_type, plugin_name=name, keys=keys, variables=variables, direct=direct)\n",
   "        return options\n",
   "        pvars = []\n",
   "        for pdef in self.get_configuration_definitions(plugin_type, name).values():\n",
   "            if 'vars' in pdef and pdef['vars']:\n",
   "                for var_entry in pdef['vars']:\n",
   "                    pvars.append(var_entry['name'])\n",
   "        return pvars\n",
   "        ret = {}\n",
   "            ret = self._base_defs.get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(plugin_name, {}).get(name, None)\n",
   "        return ret\n",
   "        ret = {}\n",
   "            ret = self._base_defs\n",
   "            ret = self._plugins.get(plugin_type, {})\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, {})\n",
   "        return ret\n",
   "        value = None\n",
   "        origin = None\n",
   "        for entry in entry_list:\n",
   "            name = entry.get('name')\n",
   "                temp_value = container.get(name, None)\n",
   "                self.WARNINGS.add(u'value for config entry {0} contains invalid characters, ignoring...'.format(to_text(name)))\n",
   "            if temp_value is not None:  # only set if entry is defined in container\n",
   "                if isinstance(temp_value, AnsibleVaultEncryptedUnicode):\n",
   "                    temp_value = to_text(temp_value, errors='surrogate_or_strict')\n",
   "                value = temp_value\n",
   "                origin = name\n",
   "                if 'deprecated' in entry:\n",
   "                    self.DEPRECATED.append((entry['name'], entry['deprecated']))\n",
   "        return value, origin\n",
   "            value, _drop = self.get_config_value_and_origin(config, cfile=cfile, plugin_type=plugin_type, plugin_name=plugin_name,\n",
   "        return value\n",
   "        if cfile is None:\n",
   "            cfile = self._config_file\n",
   "        value = None\n",
   "        origin = None\n",
   "        defs = self.get_configuration_definitions(plugin_type, plugin_name)\n",
   "        if config in defs:\n",
   "            direct_aliases = []\n",
   "                direct_aliases = [direct[alias] for alias in defs[config].get('aliases', []) if alias in direct]\n",
   "                value = direct[config]\n",
   "                origin = 'Direct'\n",
   "            elif direct and direct_aliases:\n",
   "                value = direct_aliases[0]\n",
   "                origin = 'Direct'\n",
   "                if variables and defs[config].get('vars'):\n",
   "                    value, origin = self._loop_entries(variables, defs[config]['vars'])\n",
   "                    origin = 'var: %s' % origin\n",
   "                if value is None and keys and config in keys:\n",
   "                    value, origin = keys[config], 'keyword'\n",
   "                    origin = 'keyword: %s' % origin\n",
   "                if value is None and defs[config].get('env'):\n",
   "                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n",
   "                    origin = 'env: %s' % origin\n",
   "                if self._parsers.get(cfile, None) is None:\n",
   "                    self._parse_config_file(cfile)\n",
   "                if value is None and cfile is not None:\n",
   "                    ftype = get_config_type(cfile)\n",
   "                    if ftype and defs[config].get(ftype):\n",
   "                        if ftype == 'ini':\n",
   "                                for ini_entry in defs[config]['ini']:\n",
   "                                    temp_value = get_ini_config_value(self._parsers[cfile], ini_entry)\n",
   "                                    if temp_value is not None:\n",
   "                                        value = temp_value\n",
   "                                        origin = cfile\n",
   "                                        if 'deprecated' in ini_entry:\n",
   "                                            self.DEPRECATED.append(('[%s]%s' % (ini_entry['section'], ini_entry['key']), ini_entry['deprecated']))\n",
   "                                sys.stderr.write(\"Error while loading ini config %s: %s\" % (cfile, to_native(e)))\n",
   "                        elif ftype == 'yaml':\n",
   "                            origin = cfile\n",
   "                if value is None:\n",
   "                    if defs[config].get('required', False):\n",
   "                        if not plugin_type or config not in INTERNAL_DEFS.get(plugin_type, {}):\n",
   "                                               to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "                        value = defs[config].get('default')\n",
   "                        origin = 'default'\n",
   "                        if plugin_type is None and isinstance(value, string_types) and (value.startswith('{{') and value.endswith('}}')):\n",
   "                            return value, origin\n",
   "                value = ensure_type(value, defs[config].get('type'), origin=origin)\n",
   "                if origin.startswith('env:') and value == '':\n",
   "                    origin = 'default'\n",
   "                    value = ensure_type(defs[config].get('default'), defs[config].get('type'), origin=origin)\n",
   "                                              (to_native(_get_entry(plugin_type, plugin_name, config)), to_native(e)))\n",
   "            if 'deprecated' in defs[config] and origin != 'default':\n",
   "                self.DEPRECATED.append((config, defs[config].get('deprecated')))\n",
   "            raise AnsibleError('Requested entry (%s) was not defined in configuration.' % to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "        return value, origin\n",
   "        if plugin_type not in self._plugins:\n",
   "            self._plugins[plugin_type] = {}\n",
   "        self._plugins[plugin_type][name] = defs\n",
   "        if defs is None:\n",
   "        if not isinstance(defs, dict):\n",
   "            raise AnsibleOptionsError(\"Invalid configuration definition type: %s for %s\" % (type(defs), defs))\n",
   "        for config in defs:\n",
   "            if not isinstance(defs[config], dict):\n",
   "                raise AnsibleOptionsError(\"Invalid configuration definition '%s': type is %s\" % (to_native(config), type(defs[config])))\n",
   "                value, origin = self.get_config_value_and_origin(config, configfile)\n",
   "                raise AnsibleError(\"Invalid settings supplied for %s: %s\\n\" % (config, to_native(e)), orig_exc=e)\n",
   "            self.data.update_setting(Setting(config, value, origin, defs[config].get('type', 'string')))\n"
  ]
 },
 "23": {
  "name": "value_type",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "87",
  "column": "8",
  "context": "     basedir = origin\n\n    if value_type:\n        value_type = value_type.lower()\n\n    if value is not None:\n        if value_type i",
  "context_lines": "    basedir = None\n    if origin and os.path.isabs(origin) and os.path.exists(to_bytes(origin)):\n        basedir = origin\n\n    if value_type:\n        value_type = value_type.lower()\n\n    if value is not None:\n        if value_type in ('boolean', 'bool'):\n            value = boolean(value, strict=False)\n\n",
  "slicing": [
   "INTERNAL_DEFS = {'lookup': ('_terms',)}\n",
   "def _get_entry(plugin_type, plugin_name, config):\n",
   "    entry = ''\n",
   "        entry += 'plugin_type: %s ' % plugin_type\n",
   "            entry += 'plugin: %s ' % plugin_name\n",
   "    entry += 'setting: %s ' % config\n",
   "    return entry\n",
   "def ensure_type(value, value_type, origin=None):\n",
   "    errmsg = ''\n",
   "    basedir = None\n",
   "        basedir = origin\n",
   "        value_type = value_type.lower()\n",
   "        if value_type in ('boolean', 'bool'):\n",
   "            value = boolean(value, strict=False)\n",
   "        elif value_type in ('integer', 'int'):\n",
   "            value = int(value)\n",
   "        elif value_type == 'float':\n",
   "            value = float(value)\n",
   "        elif value_type == 'list':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            elif not isinstance(value, Sequence):\n",
   "                errmsg = 'list'\n",
   "        elif value_type == 'none':\n",
   "            if value == \"None\":\n",
   "                value = None\n",
   "            if value is not None:\n",
   "                errmsg = 'None'\n",
   "        elif value_type == 'path':\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                errmsg = 'path'\n",
   "        elif value_type in ('tmp', 'temppath', 'tmppath'):\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                if not os.path.exists(value):\n",
   "                    makedirs_safe(value, 0o700)\n",
   "                prefix = 'ansible-local-%s' % os.getpid()\n",
   "                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n",
   "                atexit.register(cleanup_tmp_file, value, warn=True)\n",
   "                errmsg = 'temppath'\n",
   "        elif value_type == 'pathspec':\n",
   "            if isinstance(value, string_types):\n",
   "                value = value.split(os.pathsep)\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathspec'\n",
   "        elif value_type == 'pathlist':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathlist'\n",
   "        elif value_type in ('str', 'string'):\n",
   "            if isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "                value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "                errmsg = 'string'\n",
   "        elif isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "            value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "        if errmsg:\n",
   "            raise ValueError('Invalid type provided for \"%s\": %s' % (errmsg, to_native(value)))\n",
   "    return to_text(value, errors='surrogate_or_strict', nonstring='passthru')\n",
   "        path = path.replace('{{CWD}}', os.getcwd())\n",
   "    return unfrackpath(path, follow=False, basedir=basedir)\n",
   "def get_config_type(cfile):\n",
   "    ftype = None\n",
   "        ext = os.path.splitext(cfile)[-1]\n",
   "        if ext in ('.ini', '.cfg'):\n",
   "            ftype = 'ini'\n",
   "        elif ext in ('.yaml', '.yml'):\n",
   "            ftype = 'yaml'\n",
   "            raise AnsibleOptionsError(\"Unsupported configuration file extension for %s: %s\" % (cfile, to_native(ext)))\n",
   "    return ftype\n",
   "def get_ini_config_value(p, entry):\n",
   "    value = None\n",
   "            value = p.get(entry.get('section', 'defaults'), entry.get('key', ''), raw=True)\n",
   "    return value\n",
   "def find_ini_config_file(warnings=None):\n",
   "        warnings = set()\n",
   "    SENTINEL = object\n",
   "    potential_paths = []\n",
   "    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n",
   "    if path_from_env is not SENTINEL:\n",
   "        path_from_env = unfrackpath(path_from_env, follow=False)\n",
   "        if os.path.isdir(to_bytes(path_from_env)):\n",
   "            path_from_env = os.path.join(path_from_env, \"ansible.cfg\")\n",
   "        potential_paths.append(path_from_env)\n",
   "    warn_cmd_public = False\n",
   "        cwd = os.getcwd()\n",
   "        perms = os.stat(cwd)\n",
   "        cwd_cfg = os.path.join(cwd, \"ansible.cfg\")\n",
   "        if perms.st_mode & stat.S_IWOTH:\n",
   "            if os.path.exists(cwd_cfg):\n",
   "                warn_cmd_public = True\n",
   "            potential_paths.append(to_text(cwd_cfg, errors='surrogate_or_strict'))\n",
   "    potential_paths.append(unfrackpath(\"~/.ansible.cfg\", follow=False))\n",
   "    potential_paths.append(\"/etc/ansible/ansible.cfg\")\n",
   "    for path in potential_paths:\n",
   "        b_path = to_bytes(path)\n",
   "        if os.path.exists(b_path) and os.access(b_path, os.R_OK):\n",
   "        path = None\n",
   "    if path_from_env != path and warn_cmd_public:\n",
   "        warnings.add(u\"Ansible is being run in a world writable directory (%s),\"\n",
   "                     % to_text(cwd))\n",
   "    return path\n",
   "        yml_file = to_bytes(yml_file)\n",
   "        if os.path.exists(yml_file):\n",
   "            with open(yml_file, 'rb') as config_def:\n",
   "                return yaml_load(config_def, Loader=SafeLoader) or {}\n",
   "            \"Missing base YAML definition file (bad install?): %s\" % to_native(yml_file))\n",
   "            cfile = self._config_file\n",
   "        ftype = get_config_type(cfile)\n",
   "        if cfile is not None:\n",
   "            if ftype == 'ini':\n",
   "                self._parsers[cfile] = configparser.ConfigParser()\n",
   "                with open(to_bytes(cfile), 'rb') as f:\n",
   "                        cfg_text = to_text(f.read(), errors='surrogate_or_strict')\n",
   "                        raise AnsibleOptionsError(\"Error reading config file(%s) because the config file was not utf8 encoded: %s\" % (cfile, to_native(e)))\n",
   "                        self._parsers[cfile].read_string(cfg_text)\n",
   "                        cfg_file = io.StringIO(cfg_text)\n",
   "                        self._parsers[cfile].readfp(cfg_file)\n",
   "                    raise AnsibleOptionsError(\"Error reading config file (%s): %s\" % (cfile, to_native(e)))\n",
   "                raise AnsibleOptionsError(\"Unsupported configuration file type: %s\" % to_native(ftype))\n",
   "        options = {}\n",
   "        defs = self.get_configuration_definitions(plugin_type, name)\n",
   "        for option in defs:\n",
   "            options[option] = self.get_config_value(option, plugin_type=plugin_type, plugin_name=name, keys=keys, variables=variables, direct=direct)\n",
   "        return options\n",
   "        pvars = []\n",
   "        for pdef in self.get_configuration_definitions(plugin_type, name).values():\n",
   "            if 'vars' in pdef and pdef['vars']:\n",
   "                for var_entry in pdef['vars']:\n",
   "                    pvars.append(var_entry['name'])\n",
   "        return pvars\n",
   "        ret = {}\n",
   "            ret = self._base_defs.get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(plugin_name, {}).get(name, None)\n",
   "        return ret\n",
   "        ret = {}\n",
   "            ret = self._base_defs\n",
   "            ret = self._plugins.get(plugin_type, {})\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, {})\n",
   "        return ret\n",
   "        value = None\n",
   "        origin = None\n",
   "        for entry in entry_list:\n",
   "            name = entry.get('name')\n",
   "                temp_value = container.get(name, None)\n",
   "                self.WARNINGS.add(u'value for config entry {0} contains invalid characters, ignoring...'.format(to_text(name)))\n",
   "            if temp_value is not None:  # only set if entry is defined in container\n",
   "                if isinstance(temp_value, AnsibleVaultEncryptedUnicode):\n",
   "                    temp_value = to_text(temp_value, errors='surrogate_or_strict')\n",
   "                value = temp_value\n",
   "                origin = name\n",
   "                if 'deprecated' in entry:\n",
   "                    self.DEPRECATED.append((entry['name'], entry['deprecated']))\n",
   "        return value, origin\n",
   "            value, _drop = self.get_config_value_and_origin(config, cfile=cfile, plugin_type=plugin_type, plugin_name=plugin_name,\n",
   "        return value\n",
   "        if cfile is None:\n",
   "            cfile = self._config_file\n",
   "        value = None\n",
   "        origin = None\n",
   "        defs = self.get_configuration_definitions(plugin_type, plugin_name)\n",
   "        if config in defs:\n",
   "            direct_aliases = []\n",
   "                direct_aliases = [direct[alias] for alias in defs[config].get('aliases', []) if alias in direct]\n",
   "                value = direct[config]\n",
   "                origin = 'Direct'\n",
   "            elif direct and direct_aliases:\n",
   "                value = direct_aliases[0]\n",
   "                origin = 'Direct'\n",
   "                if variables and defs[config].get('vars'):\n",
   "                    value, origin = self._loop_entries(variables, defs[config]['vars'])\n",
   "                    origin = 'var: %s' % origin\n",
   "                if value is None and keys and config in keys:\n",
   "                    value, origin = keys[config], 'keyword'\n",
   "                    origin = 'keyword: %s' % origin\n",
   "                if value is None and defs[config].get('env'):\n",
   "                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n",
   "                    origin = 'env: %s' % origin\n",
   "                if self._parsers.get(cfile, None) is None:\n",
   "                    self._parse_config_file(cfile)\n",
   "                if value is None and cfile is not None:\n",
   "                    ftype = get_config_type(cfile)\n",
   "                    if ftype and defs[config].get(ftype):\n",
   "                        if ftype == 'ini':\n",
   "                                for ini_entry in defs[config]['ini']:\n",
   "                                    temp_value = get_ini_config_value(self._parsers[cfile], ini_entry)\n",
   "                                    if temp_value is not None:\n",
   "                                        value = temp_value\n",
   "                                        origin = cfile\n",
   "                                        if 'deprecated' in ini_entry:\n",
   "                                            self.DEPRECATED.append(('[%s]%s' % (ini_entry['section'], ini_entry['key']), ini_entry['deprecated']))\n",
   "                                sys.stderr.write(\"Error while loading ini config %s: %s\" % (cfile, to_native(e)))\n",
   "                        elif ftype == 'yaml':\n",
   "                            origin = cfile\n",
   "                if value is None:\n",
   "                    if defs[config].get('required', False):\n",
   "                        if not plugin_type or config not in INTERNAL_DEFS.get(plugin_type, {}):\n",
   "                                               to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "                        value = defs[config].get('default')\n",
   "                        origin = 'default'\n",
   "                        if plugin_type is None and isinstance(value, string_types) and (value.startswith('{{') and value.endswith('}}')):\n",
   "                            return value, origin\n",
   "                value = ensure_type(value, defs[config].get('type'), origin=origin)\n",
   "                if origin.startswith('env:') and value == '':\n",
   "                    origin = 'default'\n",
   "                    value = ensure_type(defs[config].get('default'), defs[config].get('type'), origin=origin)\n",
   "                                              (to_native(_get_entry(plugin_type, plugin_name, config)), to_native(e)))\n",
   "            if 'deprecated' in defs[config] and origin != 'default':\n",
   "                self.DEPRECATED.append((config, defs[config].get('deprecated')))\n",
   "            raise AnsibleError('Requested entry (%s) was not defined in configuration.' % to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "        return value, origin\n",
   "        if plugin_type not in self._plugins:\n",
   "            self._plugins[plugin_type] = {}\n",
   "        self._plugins[plugin_type][name] = defs\n",
   "        if defs is None:\n",
   "        if not isinstance(defs, dict):\n",
   "            raise AnsibleOptionsError(\"Invalid configuration definition type: %s for %s\" % (type(defs), defs))\n",
   "        for config in defs:\n",
   "            if not isinstance(defs[config], dict):\n",
   "                raise AnsibleOptionsError(\"Invalid configuration definition '%s': type is %s\" % (to_native(config), type(defs[config])))\n",
   "                value, origin = self.get_config_value_and_origin(config, configfile)\n",
   "                raise AnsibleError(\"Invalid settings supplied for %s: %s\\n\" % (config, to_native(e)), orig_exc=e)\n",
   "            self.data.update_setting(Setting(config, value, origin, defs[config].get('type', 'string')))\n"
  ]
 },
 "24": {
  "name": "prefix",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "123",
  "column": "16",
  "context": "      makedirs_safe(value, 0o700)\n                prefix = 'ansible-local-%s' % os.getpid()\n                value = tempfile.mkdtemp(prefix=pr",
  "context_lines": "            if isinstance(value, string_types):\n                value = resolve_path(value, basedir=basedir)\n                if not os.path.exists(value):\n                    makedirs_safe(value, 0o700)\n                prefix = 'ansible-local-%s' % os.getpid()\n                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n                atexit.register(cleanup_tmp_file, value, warn=True)\n            else:\n                errmsg = 'temppath'\n\n",
  "slicing": [
   "INTERNAL_DEFS = {'lookup': ('_terms',)}\n",
   "def _get_entry(plugin_type, plugin_name, config):\n",
   "    entry = ''\n",
   "        entry += 'plugin_type: %s ' % plugin_type\n",
   "            entry += 'plugin: %s ' % plugin_name\n",
   "    entry += 'setting: %s ' % config\n",
   "    return entry\n",
   "def ensure_type(value, value_type, origin=None):\n",
   "    errmsg = ''\n",
   "    basedir = None\n",
   "        basedir = origin\n",
   "        value_type = value_type.lower()\n",
   "        if value_type in ('boolean', 'bool'):\n",
   "            value = boolean(value, strict=False)\n",
   "        elif value_type in ('integer', 'int'):\n",
   "            value = int(value)\n",
   "        elif value_type == 'float':\n",
   "            value = float(value)\n",
   "        elif value_type == 'list':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            elif not isinstance(value, Sequence):\n",
   "                errmsg = 'list'\n",
   "        elif value_type == 'none':\n",
   "            if value == \"None\":\n",
   "                value = None\n",
   "            if value is not None:\n",
   "                errmsg = 'None'\n",
   "        elif value_type == 'path':\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                errmsg = 'path'\n",
   "        elif value_type in ('tmp', 'temppath', 'tmppath'):\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                if not os.path.exists(value):\n",
   "                    makedirs_safe(value, 0o700)\n",
   "                prefix = 'ansible-local-%s' % os.getpid()\n",
   "                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n",
   "                atexit.register(cleanup_tmp_file, value, warn=True)\n",
   "                errmsg = 'temppath'\n",
   "        elif value_type == 'pathspec':\n",
   "            if isinstance(value, string_types):\n",
   "                value = value.split(os.pathsep)\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathspec'\n",
   "        elif value_type == 'pathlist':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathlist'\n",
   "        elif value_type in ('str', 'string'):\n",
   "            if isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "                value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "                errmsg = 'string'\n",
   "        elif isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "            value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "        if errmsg:\n",
   "            raise ValueError('Invalid type provided for \"%s\": %s' % (errmsg, to_native(value)))\n",
   "    return to_text(value, errors='surrogate_or_strict', nonstring='passthru')\n",
   "        path = path.replace('{{CWD}}', os.getcwd())\n",
   "    return unfrackpath(path, follow=False, basedir=basedir)\n",
   "def get_config_type(cfile):\n",
   "    ftype = None\n",
   "        ext = os.path.splitext(cfile)[-1]\n",
   "        if ext in ('.ini', '.cfg'):\n",
   "            ftype = 'ini'\n",
   "        elif ext in ('.yaml', '.yml'):\n",
   "            ftype = 'yaml'\n",
   "            raise AnsibleOptionsError(\"Unsupported configuration file extension for %s: %s\" % (cfile, to_native(ext)))\n",
   "    return ftype\n",
   "def get_ini_config_value(p, entry):\n",
   "    value = None\n",
   "            value = p.get(entry.get('section', 'defaults'), entry.get('key', ''), raw=True)\n",
   "    return value\n",
   "def find_ini_config_file(warnings=None):\n",
   "        warnings = set()\n",
   "    SENTINEL = object\n",
   "    potential_paths = []\n",
   "    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n",
   "    if path_from_env is not SENTINEL:\n",
   "        path_from_env = unfrackpath(path_from_env, follow=False)\n",
   "        if os.path.isdir(to_bytes(path_from_env)):\n",
   "            path_from_env = os.path.join(path_from_env, \"ansible.cfg\")\n",
   "        potential_paths.append(path_from_env)\n",
   "    warn_cmd_public = False\n",
   "        cwd = os.getcwd()\n",
   "        perms = os.stat(cwd)\n",
   "        cwd_cfg = os.path.join(cwd, \"ansible.cfg\")\n",
   "        if perms.st_mode & stat.S_IWOTH:\n",
   "            if os.path.exists(cwd_cfg):\n",
   "                warn_cmd_public = True\n",
   "            potential_paths.append(to_text(cwd_cfg, errors='surrogate_or_strict'))\n",
   "    potential_paths.append(unfrackpath(\"~/.ansible.cfg\", follow=False))\n",
   "    potential_paths.append(\"/etc/ansible/ansible.cfg\")\n",
   "    for path in potential_paths:\n",
   "        b_path = to_bytes(path)\n",
   "        if os.path.exists(b_path) and os.access(b_path, os.R_OK):\n",
   "        path = None\n",
   "    if path_from_env != path and warn_cmd_public:\n",
   "        warnings.add(u\"Ansible is being run in a world writable directory (%s),\"\n",
   "                     % to_text(cwd))\n",
   "    return path\n",
   "        yml_file = to_bytes(yml_file)\n",
   "        if os.path.exists(yml_file):\n",
   "            with open(yml_file, 'rb') as config_def:\n",
   "                return yaml_load(config_def, Loader=SafeLoader) or {}\n",
   "            \"Missing base YAML definition file (bad install?): %s\" % to_native(yml_file))\n",
   "            cfile = self._config_file\n",
   "        ftype = get_config_type(cfile)\n",
   "        if cfile is not None:\n",
   "            if ftype == 'ini':\n",
   "                self._parsers[cfile] = configparser.ConfigParser()\n",
   "                with open(to_bytes(cfile), 'rb') as f:\n",
   "                        cfg_text = to_text(f.read(), errors='surrogate_or_strict')\n",
   "                        raise AnsibleOptionsError(\"Error reading config file(%s) because the config file was not utf8 encoded: %s\" % (cfile, to_native(e)))\n",
   "                        self._parsers[cfile].read_string(cfg_text)\n",
   "                        cfg_file = io.StringIO(cfg_text)\n",
   "                        self._parsers[cfile].readfp(cfg_file)\n",
   "                    raise AnsibleOptionsError(\"Error reading config file (%s): %s\" % (cfile, to_native(e)))\n",
   "                raise AnsibleOptionsError(\"Unsupported configuration file type: %s\" % to_native(ftype))\n",
   "        options = {}\n",
   "        defs = self.get_configuration_definitions(plugin_type, name)\n",
   "        for option in defs:\n",
   "            options[option] = self.get_config_value(option, plugin_type=plugin_type, plugin_name=name, keys=keys, variables=variables, direct=direct)\n",
   "        return options\n",
   "        pvars = []\n",
   "        for pdef in self.get_configuration_definitions(plugin_type, name).values():\n",
   "            if 'vars' in pdef and pdef['vars']:\n",
   "                for var_entry in pdef['vars']:\n",
   "                    pvars.append(var_entry['name'])\n",
   "        return pvars\n",
   "        ret = {}\n",
   "            ret = self._base_defs.get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(plugin_name, {}).get(name, None)\n",
   "        return ret\n",
   "        ret = {}\n",
   "            ret = self._base_defs\n",
   "            ret = self._plugins.get(plugin_type, {})\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, {})\n",
   "        return ret\n",
   "        value = None\n",
   "        origin = None\n",
   "        for entry in entry_list:\n",
   "            name = entry.get('name')\n",
   "                temp_value = container.get(name, None)\n",
   "                self.WARNINGS.add(u'value for config entry {0} contains invalid characters, ignoring...'.format(to_text(name)))\n",
   "            if temp_value is not None:  # only set if entry is defined in container\n",
   "                if isinstance(temp_value, AnsibleVaultEncryptedUnicode):\n",
   "                    temp_value = to_text(temp_value, errors='surrogate_or_strict')\n",
   "                value = temp_value\n",
   "                origin = name\n",
   "                if 'deprecated' in entry:\n",
   "                    self.DEPRECATED.append((entry['name'], entry['deprecated']))\n",
   "        return value, origin\n",
   "            value, _drop = self.get_config_value_and_origin(config, cfile=cfile, plugin_type=plugin_type, plugin_name=plugin_name,\n",
   "        return value\n",
   "        if cfile is None:\n",
   "            cfile = self._config_file\n",
   "        value = None\n",
   "        origin = None\n",
   "        defs = self.get_configuration_definitions(plugin_type, plugin_name)\n",
   "        if config in defs:\n",
   "            direct_aliases = []\n",
   "                direct_aliases = [direct[alias] for alias in defs[config].get('aliases', []) if alias in direct]\n",
   "                value = direct[config]\n",
   "                origin = 'Direct'\n",
   "            elif direct and direct_aliases:\n",
   "                value = direct_aliases[0]\n",
   "                origin = 'Direct'\n",
   "                if variables and defs[config].get('vars'):\n",
   "                    value, origin = self._loop_entries(variables, defs[config]['vars'])\n",
   "                    origin = 'var: %s' % origin\n",
   "                if value is None and keys and config in keys:\n",
   "                    value, origin = keys[config], 'keyword'\n",
   "                    origin = 'keyword: %s' % origin\n",
   "                if value is None and defs[config].get('env'):\n",
   "                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n",
   "                    origin = 'env: %s' % origin\n",
   "                if self._parsers.get(cfile, None) is None:\n",
   "                    self._parse_config_file(cfile)\n",
   "                if value is None and cfile is not None:\n",
   "                    ftype = get_config_type(cfile)\n",
   "                    if ftype and defs[config].get(ftype):\n",
   "                        if ftype == 'ini':\n",
   "                                for ini_entry in defs[config]['ini']:\n",
   "                                    temp_value = get_ini_config_value(self._parsers[cfile], ini_entry)\n",
   "                                    if temp_value is not None:\n",
   "                                        value = temp_value\n",
   "                                        origin = cfile\n",
   "                                        if 'deprecated' in ini_entry:\n",
   "                                            self.DEPRECATED.append(('[%s]%s' % (ini_entry['section'], ini_entry['key']), ini_entry['deprecated']))\n",
   "                                sys.stderr.write(\"Error while loading ini config %s: %s\" % (cfile, to_native(e)))\n",
   "                        elif ftype == 'yaml':\n",
   "                            origin = cfile\n",
   "                if value is None:\n",
   "                    if defs[config].get('required', False):\n",
   "                        if not plugin_type or config not in INTERNAL_DEFS.get(plugin_type, {}):\n",
   "                                               to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "                        value = defs[config].get('default')\n",
   "                        origin = 'default'\n",
   "                        if plugin_type is None and isinstance(value, string_types) and (value.startswith('{{') and value.endswith('}}')):\n",
   "                            return value, origin\n",
   "                value = ensure_type(value, defs[config].get('type'), origin=origin)\n",
   "                if origin.startswith('env:') and value == '':\n",
   "                    origin = 'default'\n",
   "                    value = ensure_type(defs[config].get('default'), defs[config].get('type'), origin=origin)\n",
   "                                              (to_native(_get_entry(plugin_type, plugin_name, config)), to_native(e)))\n",
   "            if 'deprecated' in defs[config] and origin != 'default':\n",
   "                self.DEPRECATED.append((config, defs[config].get('deprecated')))\n",
   "            raise AnsibleError('Requested entry (%s) was not defined in configuration.' % to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "        return value, origin\n",
   "        if plugin_type not in self._plugins:\n",
   "            self._plugins[plugin_type] = {}\n",
   "        self._plugins[plugin_type][name] = defs\n",
   "        if defs is None:\n",
   "        if not isinstance(defs, dict):\n",
   "            raise AnsibleOptionsError(\"Invalid configuration definition type: %s for %s\" % (type(defs), defs))\n",
   "        for config in defs:\n",
   "            if not isinstance(defs[config], dict):\n",
   "                raise AnsibleOptionsError(\"Invalid configuration definition '%s': type is %s\" % (to_native(config), type(defs[config])))\n",
   "                value, origin = self.get_config_value_and_origin(config, configfile)\n",
   "                raise AnsibleError(\"Invalid settings supplied for %s: %s\\n\" % (config, to_native(e)), orig_exc=e)\n",
   "            self.data.update_setting(Setting(config, value, origin, defs[config].get('type', 'string')))\n"
  ]
 },
 "25": {
  "name": "value",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "124",
  "column": "16",
  "context": " 'ansible-local-%s' % os.getpid()\n                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n                atexit.register(cleanup_tmp_file, ",
  "context_lines": "                value = resolve_path(value, basedir=basedir)\n                if not os.path.exists(value):\n                    makedirs_safe(value, 0o700)\n                prefix = 'ansible-local-%s' % os.getpid()\n                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n                atexit.register(cleanup_tmp_file, value, warn=True)\n            else:\n                errmsg = 'temppath'\n\n        elif value_type == 'pathspec':\n",
  "slicing": [
   "INTERNAL_DEFS = {'lookup': ('_terms',)}\n",
   "def _get_entry(plugin_type, plugin_name, config):\n",
   "    entry = ''\n",
   "        entry += 'plugin_type: %s ' % plugin_type\n",
   "            entry += 'plugin: %s ' % plugin_name\n",
   "    entry += 'setting: %s ' % config\n",
   "    return entry\n",
   "def ensure_type(value, value_type, origin=None):\n",
   "    errmsg = ''\n",
   "    basedir = None\n",
   "        basedir = origin\n",
   "        value_type = value_type.lower()\n",
   "        if value_type in ('boolean', 'bool'):\n",
   "            value = boolean(value, strict=False)\n",
   "        elif value_type in ('integer', 'int'):\n",
   "            value = int(value)\n",
   "        elif value_type == 'float':\n",
   "            value = float(value)\n",
   "        elif value_type == 'list':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            elif not isinstance(value, Sequence):\n",
   "                errmsg = 'list'\n",
   "        elif value_type == 'none':\n",
   "            if value == \"None\":\n",
   "                value = None\n",
   "            if value is not None:\n",
   "                errmsg = 'None'\n",
   "        elif value_type == 'path':\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                errmsg = 'path'\n",
   "        elif value_type in ('tmp', 'temppath', 'tmppath'):\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                if not os.path.exists(value):\n",
   "                    makedirs_safe(value, 0o700)\n",
   "                prefix = 'ansible-local-%s' % os.getpid()\n",
   "                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n",
   "                atexit.register(cleanup_tmp_file, value, warn=True)\n",
   "                errmsg = 'temppath'\n",
   "        elif value_type == 'pathspec':\n",
   "            if isinstance(value, string_types):\n",
   "                value = value.split(os.pathsep)\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathspec'\n",
   "        elif value_type == 'pathlist':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathlist'\n",
   "        elif value_type in ('str', 'string'):\n",
   "            if isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "                value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "                errmsg = 'string'\n",
   "        elif isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "            value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "        if errmsg:\n",
   "            raise ValueError('Invalid type provided for \"%s\": %s' % (errmsg, to_native(value)))\n",
   "    return to_text(value, errors='surrogate_or_strict', nonstring='passthru')\n",
   "        path = path.replace('{{CWD}}', os.getcwd())\n",
   "    return unfrackpath(path, follow=False, basedir=basedir)\n",
   "def get_config_type(cfile):\n",
   "    ftype = None\n",
   "        ext = os.path.splitext(cfile)[-1]\n",
   "        if ext in ('.ini', '.cfg'):\n",
   "            ftype = 'ini'\n",
   "        elif ext in ('.yaml', '.yml'):\n",
   "            ftype = 'yaml'\n",
   "            raise AnsibleOptionsError(\"Unsupported configuration file extension for %s: %s\" % (cfile, to_native(ext)))\n",
   "    return ftype\n",
   "def get_ini_config_value(p, entry):\n",
   "    value = None\n",
   "            value = p.get(entry.get('section', 'defaults'), entry.get('key', ''), raw=True)\n",
   "    return value\n",
   "def find_ini_config_file(warnings=None):\n",
   "        warnings = set()\n",
   "    SENTINEL = object\n",
   "    potential_paths = []\n",
   "    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n",
   "    if path_from_env is not SENTINEL:\n",
   "        path_from_env = unfrackpath(path_from_env, follow=False)\n",
   "        if os.path.isdir(to_bytes(path_from_env)):\n",
   "            path_from_env = os.path.join(path_from_env, \"ansible.cfg\")\n",
   "        potential_paths.append(path_from_env)\n",
   "    warn_cmd_public = False\n",
   "        cwd = os.getcwd()\n",
   "        perms = os.stat(cwd)\n",
   "        cwd_cfg = os.path.join(cwd, \"ansible.cfg\")\n",
   "        if perms.st_mode & stat.S_IWOTH:\n",
   "            if os.path.exists(cwd_cfg):\n",
   "                warn_cmd_public = True\n",
   "            potential_paths.append(to_text(cwd_cfg, errors='surrogate_or_strict'))\n",
   "    potential_paths.append(unfrackpath(\"~/.ansible.cfg\", follow=False))\n",
   "    potential_paths.append(\"/etc/ansible/ansible.cfg\")\n",
   "    for path in potential_paths:\n",
   "        b_path = to_bytes(path)\n",
   "        if os.path.exists(b_path) and os.access(b_path, os.R_OK):\n",
   "        path = None\n",
   "    if path_from_env != path and warn_cmd_public:\n",
   "        warnings.add(u\"Ansible is being run in a world writable directory (%s),\"\n",
   "                     % to_text(cwd))\n",
   "    return path\n",
   "        yml_file = to_bytes(yml_file)\n",
   "        if os.path.exists(yml_file):\n",
   "            with open(yml_file, 'rb') as config_def:\n",
   "                return yaml_load(config_def, Loader=SafeLoader) or {}\n",
   "            \"Missing base YAML definition file (bad install?): %s\" % to_native(yml_file))\n",
   "            cfile = self._config_file\n",
   "        ftype = get_config_type(cfile)\n",
   "        if cfile is not None:\n",
   "            if ftype == 'ini':\n",
   "                self._parsers[cfile] = configparser.ConfigParser()\n",
   "                with open(to_bytes(cfile), 'rb') as f:\n",
   "                        cfg_text = to_text(f.read(), errors='surrogate_or_strict')\n",
   "                        raise AnsibleOptionsError(\"Error reading config file(%s) because the config file was not utf8 encoded: %s\" % (cfile, to_native(e)))\n",
   "                        self._parsers[cfile].read_string(cfg_text)\n",
   "                        cfg_file = io.StringIO(cfg_text)\n",
   "                        self._parsers[cfile].readfp(cfg_file)\n",
   "                    raise AnsibleOptionsError(\"Error reading config file (%s): %s\" % (cfile, to_native(e)))\n",
   "                raise AnsibleOptionsError(\"Unsupported configuration file type: %s\" % to_native(ftype))\n",
   "        options = {}\n",
   "        defs = self.get_configuration_definitions(plugin_type, name)\n",
   "        for option in defs:\n",
   "            options[option] = self.get_config_value(option, plugin_type=plugin_type, plugin_name=name, keys=keys, variables=variables, direct=direct)\n",
   "        return options\n",
   "        pvars = []\n",
   "        for pdef in self.get_configuration_definitions(plugin_type, name).values():\n",
   "            if 'vars' in pdef and pdef['vars']:\n",
   "                for var_entry in pdef['vars']:\n",
   "                    pvars.append(var_entry['name'])\n",
   "        return pvars\n",
   "        ret = {}\n",
   "            ret = self._base_defs.get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(plugin_name, {}).get(name, None)\n",
   "        return ret\n",
   "        ret = {}\n",
   "            ret = self._base_defs\n",
   "            ret = self._plugins.get(plugin_type, {})\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, {})\n",
   "        return ret\n",
   "        value = None\n",
   "        origin = None\n",
   "        for entry in entry_list:\n",
   "            name = entry.get('name')\n",
   "                temp_value = container.get(name, None)\n",
   "                self.WARNINGS.add(u'value for config entry {0} contains invalid characters, ignoring...'.format(to_text(name)))\n",
   "            if temp_value is not None:  # only set if entry is defined in container\n",
   "                if isinstance(temp_value, AnsibleVaultEncryptedUnicode):\n",
   "                    temp_value = to_text(temp_value, errors='surrogate_or_strict')\n",
   "                value = temp_value\n",
   "                origin = name\n",
   "                if 'deprecated' in entry:\n",
   "                    self.DEPRECATED.append((entry['name'], entry['deprecated']))\n",
   "        return value, origin\n",
   "            value, _drop = self.get_config_value_and_origin(config, cfile=cfile, plugin_type=plugin_type, plugin_name=plugin_name,\n",
   "        return value\n",
   "        if cfile is None:\n",
   "            cfile = self._config_file\n",
   "        value = None\n",
   "        origin = None\n",
   "        defs = self.get_configuration_definitions(plugin_type, plugin_name)\n",
   "        if config in defs:\n",
   "            direct_aliases = []\n",
   "                direct_aliases = [direct[alias] for alias in defs[config].get('aliases', []) if alias in direct]\n",
   "                value = direct[config]\n",
   "                origin = 'Direct'\n",
   "            elif direct and direct_aliases:\n",
   "                value = direct_aliases[0]\n",
   "                origin = 'Direct'\n",
   "                if variables and defs[config].get('vars'):\n",
   "                    value, origin = self._loop_entries(variables, defs[config]['vars'])\n",
   "                    origin = 'var: %s' % origin\n",
   "                if value is None and keys and config in keys:\n",
   "                    value, origin = keys[config], 'keyword'\n",
   "                    origin = 'keyword: %s' % origin\n",
   "                if value is None and defs[config].get('env'):\n",
   "                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n",
   "                    origin = 'env: %s' % origin\n",
   "                if self._parsers.get(cfile, None) is None:\n",
   "                    self._parse_config_file(cfile)\n",
   "                if value is None and cfile is not None:\n",
   "                    ftype = get_config_type(cfile)\n",
   "                    if ftype and defs[config].get(ftype):\n",
   "                        if ftype == 'ini':\n",
   "                                for ini_entry in defs[config]['ini']:\n",
   "                                    temp_value = get_ini_config_value(self._parsers[cfile], ini_entry)\n",
   "                                    if temp_value is not None:\n",
   "                                        value = temp_value\n",
   "                                        origin = cfile\n",
   "                                        if 'deprecated' in ini_entry:\n",
   "                                            self.DEPRECATED.append(('[%s]%s' % (ini_entry['section'], ini_entry['key']), ini_entry['deprecated']))\n",
   "                                sys.stderr.write(\"Error while loading ini config %s: %s\" % (cfile, to_native(e)))\n",
   "                        elif ftype == 'yaml':\n",
   "                            origin = cfile\n",
   "                if value is None:\n",
   "                    if defs[config].get('required', False):\n",
   "                        if not plugin_type or config not in INTERNAL_DEFS.get(plugin_type, {}):\n",
   "                                               to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "                        value = defs[config].get('default')\n",
   "                        origin = 'default'\n",
   "                        if plugin_type is None and isinstance(value, string_types) and (value.startswith('{{') and value.endswith('}}')):\n",
   "                            return value, origin\n",
   "                value = ensure_type(value, defs[config].get('type'), origin=origin)\n",
   "                if origin.startswith('env:') and value == '':\n",
   "                    origin = 'default'\n",
   "                    value = ensure_type(defs[config].get('default'), defs[config].get('type'), origin=origin)\n",
   "                                              (to_native(_get_entry(plugin_type, plugin_name, config)), to_native(e)))\n",
   "            if 'deprecated' in defs[config] and origin != 'default':\n",
   "                self.DEPRECATED.append((config, defs[config].get('deprecated')))\n",
   "            raise AnsibleError('Requested entry (%s) was not defined in configuration.' % to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "        return value, origin\n",
   "        if plugin_type not in self._plugins:\n",
   "            self._plugins[plugin_type] = {}\n",
   "        self._plugins[plugin_type][name] = defs\n",
   "        if defs is None:\n",
   "        if not isinstance(defs, dict):\n",
   "            raise AnsibleOptionsError(\"Invalid configuration definition type: %s for %s\" % (type(defs), defs))\n",
   "        for config in defs:\n",
   "            if not isinstance(defs[config], dict):\n",
   "                raise AnsibleOptionsError(\"Invalid configuration definition '%s': type is %s\" % (to_native(config), type(defs[config])))\n",
   "                value, origin = self.get_config_value_and_origin(config, configfile)\n",
   "                raise AnsibleError(\"Invalid settings supplied for %s: %s\\n\" % (config, to_native(e)), orig_exc=e)\n",
   "            self.data.update_setting(Setting(config, value, origin, defs[config].get('type', 'string')))\n"
  ]
 },
 "26": {
  "name": "value",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "131",
  "column": "16",
  "context": " isinstance(value, string_types):\n                value = value.split(os.pathsep)\n\n            if isinstance(value, Sequence):\n     ",
  "context_lines": "            else:\n                errmsg = 'temppath'\n\n        elif value_type == 'pathspec':\n            if isinstance(value, string_types):\n                value = value.split(os.pathsep)\n\n            if isinstance(value, Sequence):\n                value = [resolve_path(x, basedir=basedir) for x in value]\n            else:\n",
  "slicing": [
   "INTERNAL_DEFS = {'lookup': ('_terms',)}\n",
   "def _get_entry(plugin_type, plugin_name, config):\n",
   "    entry = ''\n",
   "        entry += 'plugin_type: %s ' % plugin_type\n",
   "            entry += 'plugin: %s ' % plugin_name\n",
   "    entry += 'setting: %s ' % config\n",
   "    return entry\n",
   "def ensure_type(value, value_type, origin=None):\n",
   "    errmsg = ''\n",
   "    basedir = None\n",
   "        basedir = origin\n",
   "        value_type = value_type.lower()\n",
   "        if value_type in ('boolean', 'bool'):\n",
   "            value = boolean(value, strict=False)\n",
   "        elif value_type in ('integer', 'int'):\n",
   "            value = int(value)\n",
   "        elif value_type == 'float':\n",
   "            value = float(value)\n",
   "        elif value_type == 'list':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            elif not isinstance(value, Sequence):\n",
   "                errmsg = 'list'\n",
   "        elif value_type == 'none':\n",
   "            if value == \"None\":\n",
   "                value = None\n",
   "            if value is not None:\n",
   "                errmsg = 'None'\n",
   "        elif value_type == 'path':\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                errmsg = 'path'\n",
   "        elif value_type in ('tmp', 'temppath', 'tmppath'):\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                if not os.path.exists(value):\n",
   "                    makedirs_safe(value, 0o700)\n",
   "                prefix = 'ansible-local-%s' % os.getpid()\n",
   "                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n",
   "                atexit.register(cleanup_tmp_file, value, warn=True)\n",
   "                errmsg = 'temppath'\n",
   "        elif value_type == 'pathspec':\n",
   "            if isinstance(value, string_types):\n",
   "                value = value.split(os.pathsep)\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathspec'\n",
   "        elif value_type == 'pathlist':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathlist'\n",
   "        elif value_type in ('str', 'string'):\n",
   "            if isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "                value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "                errmsg = 'string'\n",
   "        elif isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "            value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "        if errmsg:\n",
   "            raise ValueError('Invalid type provided for \"%s\": %s' % (errmsg, to_native(value)))\n",
   "    return to_text(value, errors='surrogate_or_strict', nonstring='passthru')\n",
   "        path = path.replace('{{CWD}}', os.getcwd())\n",
   "    return unfrackpath(path, follow=False, basedir=basedir)\n",
   "def get_config_type(cfile):\n",
   "    ftype = None\n",
   "        ext = os.path.splitext(cfile)[-1]\n",
   "        if ext in ('.ini', '.cfg'):\n",
   "            ftype = 'ini'\n",
   "        elif ext in ('.yaml', '.yml'):\n",
   "            ftype = 'yaml'\n",
   "            raise AnsibleOptionsError(\"Unsupported configuration file extension for %s: %s\" % (cfile, to_native(ext)))\n",
   "    return ftype\n",
   "def get_ini_config_value(p, entry):\n",
   "    value = None\n",
   "            value = p.get(entry.get('section', 'defaults'), entry.get('key', ''), raw=True)\n",
   "    return value\n",
   "def find_ini_config_file(warnings=None):\n",
   "        warnings = set()\n",
   "    SENTINEL = object\n",
   "    potential_paths = []\n",
   "    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n",
   "    if path_from_env is not SENTINEL:\n",
   "        path_from_env = unfrackpath(path_from_env, follow=False)\n",
   "        if os.path.isdir(to_bytes(path_from_env)):\n",
   "            path_from_env = os.path.join(path_from_env, \"ansible.cfg\")\n",
   "        potential_paths.append(path_from_env)\n",
   "    warn_cmd_public = False\n",
   "        cwd = os.getcwd()\n",
   "        perms = os.stat(cwd)\n",
   "        cwd_cfg = os.path.join(cwd, \"ansible.cfg\")\n",
   "        if perms.st_mode & stat.S_IWOTH:\n",
   "            if os.path.exists(cwd_cfg):\n",
   "                warn_cmd_public = True\n",
   "            potential_paths.append(to_text(cwd_cfg, errors='surrogate_or_strict'))\n",
   "    potential_paths.append(unfrackpath(\"~/.ansible.cfg\", follow=False))\n",
   "    potential_paths.append(\"/etc/ansible/ansible.cfg\")\n",
   "    for path in potential_paths:\n",
   "        b_path = to_bytes(path)\n",
   "        if os.path.exists(b_path) and os.access(b_path, os.R_OK):\n",
   "        path = None\n",
   "    if path_from_env != path and warn_cmd_public:\n",
   "        warnings.add(u\"Ansible is being run in a world writable directory (%s),\"\n",
   "                     % to_text(cwd))\n",
   "    return path\n",
   "        yml_file = to_bytes(yml_file)\n",
   "        if os.path.exists(yml_file):\n",
   "            with open(yml_file, 'rb') as config_def:\n",
   "                return yaml_load(config_def, Loader=SafeLoader) or {}\n",
   "            \"Missing base YAML definition file (bad install?): %s\" % to_native(yml_file))\n",
   "            cfile = self._config_file\n",
   "        ftype = get_config_type(cfile)\n",
   "        if cfile is not None:\n",
   "            if ftype == 'ini':\n",
   "                self._parsers[cfile] = configparser.ConfigParser()\n",
   "                with open(to_bytes(cfile), 'rb') as f:\n",
   "                        cfg_text = to_text(f.read(), errors='surrogate_or_strict')\n",
   "                        raise AnsibleOptionsError(\"Error reading config file(%s) because the config file was not utf8 encoded: %s\" % (cfile, to_native(e)))\n",
   "                        self._parsers[cfile].read_string(cfg_text)\n",
   "                        cfg_file = io.StringIO(cfg_text)\n",
   "                        self._parsers[cfile].readfp(cfg_file)\n",
   "                    raise AnsibleOptionsError(\"Error reading config file (%s): %s\" % (cfile, to_native(e)))\n",
   "                raise AnsibleOptionsError(\"Unsupported configuration file type: %s\" % to_native(ftype))\n",
   "        options = {}\n",
   "        defs = self.get_configuration_definitions(plugin_type, name)\n",
   "        for option in defs:\n",
   "            options[option] = self.get_config_value(option, plugin_type=plugin_type, plugin_name=name, keys=keys, variables=variables, direct=direct)\n",
   "        return options\n",
   "        pvars = []\n",
   "        for pdef in self.get_configuration_definitions(plugin_type, name).values():\n",
   "            if 'vars' in pdef and pdef['vars']:\n",
   "                for var_entry in pdef['vars']:\n",
   "                    pvars.append(var_entry['name'])\n",
   "        return pvars\n",
   "        ret = {}\n",
   "            ret = self._base_defs.get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(plugin_name, {}).get(name, None)\n",
   "        return ret\n",
   "        ret = {}\n",
   "            ret = self._base_defs\n",
   "            ret = self._plugins.get(plugin_type, {})\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, {})\n",
   "        return ret\n",
   "        value = None\n",
   "        origin = None\n",
   "        for entry in entry_list:\n",
   "            name = entry.get('name')\n",
   "                temp_value = container.get(name, None)\n",
   "                self.WARNINGS.add(u'value for config entry {0} contains invalid characters, ignoring...'.format(to_text(name)))\n",
   "            if temp_value is not None:  # only set if entry is defined in container\n",
   "                if isinstance(temp_value, AnsibleVaultEncryptedUnicode):\n",
   "                    temp_value = to_text(temp_value, errors='surrogate_or_strict')\n",
   "                value = temp_value\n",
   "                origin = name\n",
   "                if 'deprecated' in entry:\n",
   "                    self.DEPRECATED.append((entry['name'], entry['deprecated']))\n",
   "        return value, origin\n",
   "            value, _drop = self.get_config_value_and_origin(config, cfile=cfile, plugin_type=plugin_type, plugin_name=plugin_name,\n",
   "        return value\n",
   "        if cfile is None:\n",
   "            cfile = self._config_file\n",
   "        value = None\n",
   "        origin = None\n",
   "        defs = self.get_configuration_definitions(plugin_type, plugin_name)\n",
   "        if config in defs:\n",
   "            direct_aliases = []\n",
   "                direct_aliases = [direct[alias] for alias in defs[config].get('aliases', []) if alias in direct]\n",
   "                value = direct[config]\n",
   "                origin = 'Direct'\n",
   "            elif direct and direct_aliases:\n",
   "                value = direct_aliases[0]\n",
   "                origin = 'Direct'\n",
   "                if variables and defs[config].get('vars'):\n",
   "                    value, origin = self._loop_entries(variables, defs[config]['vars'])\n",
   "                    origin = 'var: %s' % origin\n",
   "                if value is None and keys and config in keys:\n",
   "                    value, origin = keys[config], 'keyword'\n",
   "                    origin = 'keyword: %s' % origin\n",
   "                if value is None and defs[config].get('env'):\n",
   "                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n",
   "                    origin = 'env: %s' % origin\n",
   "                if self._parsers.get(cfile, None) is None:\n",
   "                    self._parse_config_file(cfile)\n",
   "                if value is None and cfile is not None:\n",
   "                    ftype = get_config_type(cfile)\n",
   "                    if ftype and defs[config].get(ftype):\n",
   "                        if ftype == 'ini':\n",
   "                                for ini_entry in defs[config]['ini']:\n",
   "                                    temp_value = get_ini_config_value(self._parsers[cfile], ini_entry)\n",
   "                                    if temp_value is not None:\n",
   "                                        value = temp_value\n",
   "                                        origin = cfile\n",
   "                                        if 'deprecated' in ini_entry:\n",
   "                                            self.DEPRECATED.append(('[%s]%s' % (ini_entry['section'], ini_entry['key']), ini_entry['deprecated']))\n",
   "                                sys.stderr.write(\"Error while loading ini config %s: %s\" % (cfile, to_native(e)))\n",
   "                        elif ftype == 'yaml':\n",
   "                            origin = cfile\n",
   "                if value is None:\n",
   "                    if defs[config].get('required', False):\n",
   "                        if not plugin_type or config not in INTERNAL_DEFS.get(plugin_type, {}):\n",
   "                                               to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "                        value = defs[config].get('default')\n",
   "                        origin = 'default'\n",
   "                        if plugin_type is None and isinstance(value, string_types) and (value.startswith('{{') and value.endswith('}}')):\n",
   "                            return value, origin\n",
   "                value = ensure_type(value, defs[config].get('type'), origin=origin)\n",
   "                if origin.startswith('env:') and value == '':\n",
   "                    origin = 'default'\n",
   "                    value = ensure_type(defs[config].get('default'), defs[config].get('type'), origin=origin)\n",
   "                                              (to_native(_get_entry(plugin_type, plugin_name, config)), to_native(e)))\n",
   "            if 'deprecated' in defs[config] and origin != 'default':\n",
   "                self.DEPRECATED.append((config, defs[config].get('deprecated')))\n",
   "            raise AnsibleError('Requested entry (%s) was not defined in configuration.' % to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "        return value, origin\n",
   "        if plugin_type not in self._plugins:\n",
   "            self._plugins[plugin_type] = {}\n",
   "        self._plugins[plugin_type][name] = defs\n",
   "        if defs is None:\n",
   "        if not isinstance(defs, dict):\n",
   "            raise AnsibleOptionsError(\"Invalid configuration definition type: %s for %s\" % (type(defs), defs))\n",
   "        for config in defs:\n",
   "            if not isinstance(defs[config], dict):\n",
   "                raise AnsibleOptionsError(\"Invalid configuration definition '%s': type is %s\" % (to_native(config), type(defs[config])))\n",
   "                value, origin = self.get_config_value_and_origin(config, configfile)\n",
   "                raise AnsibleError(\"Invalid settings supplied for %s: %s\\n\" % (config, to_native(e)), orig_exc=e)\n",
   "            self.data.update_setting(Setting(config, value, origin, defs[config].get('type', 'string')))\n"
  ]
 },
 "27": {
  "name": "ftype",
  "type": "NoneType",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "175",
  "column": "4",
  "context": "neric file type?\ndef get_config_type(cfile):\n\n    ftype = None\n    if cfile is not None:\n        ext = os.path.sp",
  "context_lines": "        path = path.replace('{{CWD}}', os.getcwd())\n\n    return unfrackpath(path, follow=False, basedir=basedir)\n\n\n# FIXME: generic file type?\ndef get_config_type(cfile):\n\n    ftype = None\n    if cfile is not None:\n        ext = os.path.splitext(cfile)[-1]\n        if ext in ('.ini', '.cfg'):\n            ftype = 'ini'\n",
  "slicing": [
   "    ftype = None\n",
   "    return ftype\n",
   "            if ftype == 'ini':\n",
   "                raise AnsibleOptionsError(\"Unsupported configuration file type: %s\" % to_native(ftype))\n",
   "                    if ftype and defs[config].get(ftype):\n",
   "                        if ftype == 'ini':\n",
   "                        elif ftype == 'yaml':\n"
  ]
 },
 "28": {
  "name": "SENTINEL",
  "type": "type",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "210",
  "column": "4",
  "context": "t use None because we could set path to None.\n    SENTINEL = object\n\n    potential_paths = []\n\n    # Environment setti",
  "context_lines": "        # Note: In this case, warnings does nothing\n        warnings = set()\n\n    # A value that can never be a valid path so that we can tell if ANSIBLE_CONFIG was set later\n    # We can't use None because we could set path to None.\n    SENTINEL = object\n\n    potential_paths = []\n\n    # Environment setting\n    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n",
  "slicing": [
   "INTERNAL_DEFS = {'lookup': ('_terms',)}\n",
   "def _get_entry(plugin_type, plugin_name, config):\n",
   "    entry = ''\n",
   "        entry += 'plugin_type: %s ' % plugin_type\n",
   "            entry += 'plugin: %s ' % plugin_name\n",
   "    entry += 'setting: %s ' % config\n",
   "    return entry\n",
   "def ensure_type(value, value_type, origin=None):\n",
   "    errmsg = ''\n",
   "    basedir = None\n",
   "        basedir = origin\n",
   "        value_type = value_type.lower()\n",
   "        if value_type in ('boolean', 'bool'):\n",
   "            value = boolean(value, strict=False)\n",
   "        elif value_type in ('integer', 'int'):\n",
   "            value = int(value)\n",
   "        elif value_type == 'float':\n",
   "            value = float(value)\n",
   "        elif value_type == 'list':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            elif not isinstance(value, Sequence):\n",
   "                errmsg = 'list'\n",
   "        elif value_type == 'none':\n",
   "            if value == \"None\":\n",
   "                value = None\n",
   "            if value is not None:\n",
   "                errmsg = 'None'\n",
   "        elif value_type == 'path':\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                errmsg = 'path'\n",
   "        elif value_type in ('tmp', 'temppath', 'tmppath'):\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                if not os.path.exists(value):\n",
   "                    makedirs_safe(value, 0o700)\n",
   "                prefix = 'ansible-local-%s' % os.getpid()\n",
   "                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n",
   "                atexit.register(cleanup_tmp_file, value, warn=True)\n",
   "                errmsg = 'temppath'\n",
   "        elif value_type == 'pathspec':\n",
   "            if isinstance(value, string_types):\n",
   "                value = value.split(os.pathsep)\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathspec'\n",
   "        elif value_type == 'pathlist':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathlist'\n",
   "        elif value_type in ('str', 'string'):\n",
   "            if isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "                value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "                errmsg = 'string'\n",
   "        elif isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "            value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "        if errmsg:\n",
   "            raise ValueError('Invalid type provided for \"%s\": %s' % (errmsg, to_native(value)))\n",
   "    return to_text(value, errors='surrogate_or_strict', nonstring='passthru')\n",
   "        path = path.replace('{{CWD}}', os.getcwd())\n",
   "    return unfrackpath(path, follow=False, basedir=basedir)\n",
   "def get_config_type(cfile):\n",
   "    ftype = None\n",
   "        ext = os.path.splitext(cfile)[-1]\n",
   "        if ext in ('.ini', '.cfg'):\n",
   "            ftype = 'ini'\n",
   "        elif ext in ('.yaml', '.yml'):\n",
   "            ftype = 'yaml'\n",
   "            raise AnsibleOptionsError(\"Unsupported configuration file extension for %s: %s\" % (cfile, to_native(ext)))\n",
   "    return ftype\n",
   "def get_ini_config_value(p, entry):\n",
   "    value = None\n",
   "            value = p.get(entry.get('section', 'defaults'), entry.get('key', ''), raw=True)\n",
   "    return value\n",
   "def find_ini_config_file(warnings=None):\n",
   "        warnings = set()\n",
   "    SENTINEL = object\n",
   "    potential_paths = []\n",
   "    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n",
   "    if path_from_env is not SENTINEL:\n",
   "        path_from_env = unfrackpath(path_from_env, follow=False)\n",
   "        if os.path.isdir(to_bytes(path_from_env)):\n",
   "            path_from_env = os.path.join(path_from_env, \"ansible.cfg\")\n",
   "        potential_paths.append(path_from_env)\n",
   "    warn_cmd_public = False\n",
   "        cwd = os.getcwd()\n",
   "        perms = os.stat(cwd)\n",
   "        cwd_cfg = os.path.join(cwd, \"ansible.cfg\")\n",
   "        if perms.st_mode & stat.S_IWOTH:\n",
   "            if os.path.exists(cwd_cfg):\n",
   "                warn_cmd_public = True\n",
   "            potential_paths.append(to_text(cwd_cfg, errors='surrogate_or_strict'))\n",
   "    potential_paths.append(unfrackpath(\"~/.ansible.cfg\", follow=False))\n",
   "    potential_paths.append(\"/etc/ansible/ansible.cfg\")\n",
   "    for path in potential_paths:\n",
   "        b_path = to_bytes(path)\n",
   "        if os.path.exists(b_path) and os.access(b_path, os.R_OK):\n",
   "        path = None\n",
   "    if path_from_env != path and warn_cmd_public:\n",
   "        warnings.add(u\"Ansible is being run in a world writable directory (%s),\"\n",
   "                     % to_text(cwd))\n",
   "    return path\n",
   "        yml_file = to_bytes(yml_file)\n",
   "        if os.path.exists(yml_file):\n",
   "            with open(yml_file, 'rb') as config_def:\n",
   "                return yaml_load(config_def, Loader=SafeLoader) or {}\n",
   "            \"Missing base YAML definition file (bad install?): %s\" % to_native(yml_file))\n",
   "            cfile = self._config_file\n",
   "        ftype = get_config_type(cfile)\n",
   "        if cfile is not None:\n",
   "            if ftype == 'ini':\n",
   "                self._parsers[cfile] = configparser.ConfigParser()\n",
   "                with open(to_bytes(cfile), 'rb') as f:\n",
   "                        cfg_text = to_text(f.read(), errors='surrogate_or_strict')\n",
   "                        raise AnsibleOptionsError(\"Error reading config file(%s) because the config file was not utf8 encoded: %s\" % (cfile, to_native(e)))\n",
   "                        self._parsers[cfile].read_string(cfg_text)\n",
   "                        cfg_file = io.StringIO(cfg_text)\n",
   "                        self._parsers[cfile].readfp(cfg_file)\n",
   "                    raise AnsibleOptionsError(\"Error reading config file (%s): %s\" % (cfile, to_native(e)))\n",
   "                raise AnsibleOptionsError(\"Unsupported configuration file type: %s\" % to_native(ftype))\n",
   "        options = {}\n",
   "        defs = self.get_configuration_definitions(plugin_type, name)\n",
   "        for option in defs:\n",
   "            options[option] = self.get_config_value(option, plugin_type=plugin_type, plugin_name=name, keys=keys, variables=variables, direct=direct)\n",
   "        return options\n",
   "        pvars = []\n",
   "        for pdef in self.get_configuration_definitions(plugin_type, name).values():\n",
   "            if 'vars' in pdef and pdef['vars']:\n",
   "                for var_entry in pdef['vars']:\n",
   "                    pvars.append(var_entry['name'])\n",
   "        return pvars\n",
   "        ret = {}\n",
   "            ret = self._base_defs.get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(plugin_name, {}).get(name, None)\n",
   "        return ret\n",
   "        ret = {}\n",
   "            ret = self._base_defs\n",
   "            ret = self._plugins.get(plugin_type, {})\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, {})\n",
   "        return ret\n",
   "        value = None\n",
   "        origin = None\n",
   "        for entry in entry_list:\n",
   "            name = entry.get('name')\n",
   "                temp_value = container.get(name, None)\n",
   "                self.WARNINGS.add(u'value for config entry {0} contains invalid characters, ignoring...'.format(to_text(name)))\n",
   "            if temp_value is not None:  # only set if entry is defined in container\n",
   "                if isinstance(temp_value, AnsibleVaultEncryptedUnicode):\n",
   "                    temp_value = to_text(temp_value, errors='surrogate_or_strict')\n",
   "                value = temp_value\n",
   "                origin = name\n",
   "                if 'deprecated' in entry:\n",
   "                    self.DEPRECATED.append((entry['name'], entry['deprecated']))\n",
   "        return value, origin\n",
   "            value, _drop = self.get_config_value_and_origin(config, cfile=cfile, plugin_type=plugin_type, plugin_name=plugin_name,\n",
   "        return value\n",
   "        if cfile is None:\n",
   "            cfile = self._config_file\n",
   "        value = None\n",
   "        origin = None\n",
   "        defs = self.get_configuration_definitions(plugin_type, plugin_name)\n",
   "        if config in defs:\n",
   "            direct_aliases = []\n",
   "                direct_aliases = [direct[alias] for alias in defs[config].get('aliases', []) if alias in direct]\n",
   "                value = direct[config]\n",
   "                origin = 'Direct'\n",
   "            elif direct and direct_aliases:\n",
   "                value = direct_aliases[0]\n",
   "                origin = 'Direct'\n",
   "                if variables and defs[config].get('vars'):\n",
   "                    value, origin = self._loop_entries(variables, defs[config]['vars'])\n",
   "                    origin = 'var: %s' % origin\n",
   "                if value is None and keys and config in keys:\n",
   "                    value, origin = keys[config], 'keyword'\n",
   "                    origin = 'keyword: %s' % origin\n",
   "                if value is None and defs[config].get('env'):\n",
   "                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n",
   "                    origin = 'env: %s' % origin\n",
   "                if self._parsers.get(cfile, None) is None:\n",
   "                    self._parse_config_file(cfile)\n",
   "                if value is None and cfile is not None:\n",
   "                    ftype = get_config_type(cfile)\n",
   "                    if ftype and defs[config].get(ftype):\n",
   "                        if ftype == 'ini':\n",
   "                                for ini_entry in defs[config]['ini']:\n",
   "                                    temp_value = get_ini_config_value(self._parsers[cfile], ini_entry)\n",
   "                                    if temp_value is not None:\n",
   "                                        value = temp_value\n",
   "                                        origin = cfile\n",
   "                                        if 'deprecated' in ini_entry:\n",
   "                                            self.DEPRECATED.append(('[%s]%s' % (ini_entry['section'], ini_entry['key']), ini_entry['deprecated']))\n",
   "                                sys.stderr.write(\"Error while loading ini config %s: %s\" % (cfile, to_native(e)))\n",
   "                        elif ftype == 'yaml':\n",
   "                            origin = cfile\n",
   "                if value is None:\n",
   "                    if defs[config].get('required', False):\n",
   "                        if not plugin_type or config not in INTERNAL_DEFS.get(plugin_type, {}):\n",
   "                                               to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "                        value = defs[config].get('default')\n",
   "                        origin = 'default'\n",
   "                        if plugin_type is None and isinstance(value, string_types) and (value.startswith('{{') and value.endswith('}}')):\n",
   "                            return value, origin\n",
   "                value = ensure_type(value, defs[config].get('type'), origin=origin)\n",
   "                if origin.startswith('env:') and value == '':\n",
   "                    origin = 'default'\n",
   "                    value = ensure_type(defs[config].get('default'), defs[config].get('type'), origin=origin)\n",
   "                                              (to_native(_get_entry(plugin_type, plugin_name, config)), to_native(e)))\n",
   "            if 'deprecated' in defs[config] and origin != 'default':\n",
   "                self.DEPRECATED.append((config, defs[config].get('deprecated')))\n",
   "            raise AnsibleError('Requested entry (%s) was not defined in configuration.' % to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "        return value, origin\n",
   "        if plugin_type not in self._plugins:\n",
   "            self._plugins[plugin_type] = {}\n",
   "        self._plugins[plugin_type][name] = defs\n",
   "        if defs is None:\n",
   "        if not isinstance(defs, dict):\n",
   "            raise AnsibleOptionsError(\"Invalid configuration definition type: %s for %s\" % (type(defs), defs))\n",
   "        for config in defs:\n",
   "            if not isinstance(defs[config], dict):\n",
   "                raise AnsibleOptionsError(\"Invalid configuration definition '%s': type is %s\" % (to_native(config), type(defs[config])))\n",
   "                value, origin = self.get_config_value_and_origin(config, configfile)\n",
   "                raise AnsibleError(\"Invalid settings supplied for %s: %s\\n\" % (config, to_native(e)), orig_exc=e)\n",
   "            self.data.update_setting(Setting(config, value, origin, defs[config].get('type', 'string')))\n"
  ]
 },
 "29": {
  "name": "path_from_env",
  "type": "type",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "215",
  "column": "4",
  "context": "tential_paths = []\n\n    # Environment setting\n    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n    if path_from_env is not SENTINEL:\n        path",
  "context_lines": "    # We can't use None because we could set path to None.\n    SENTINEL = object\n\n    potential_paths = []\n\n    # Environment setting\n    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n    if path_from_env is not SENTINEL:\n        path_from_env = unfrackpath(path_from_env, follow=False)\n        if os.path.isdir(to_bytes(path_from_env)):\n            path_from_env = os.path.join(path_from_env, \"ansible.cfg\")\n",
  "slicing": [
   "INTERNAL_DEFS = {'lookup': ('_terms',)}\n",
   "def _get_entry(plugin_type, plugin_name, config):\n",
   "    entry = ''\n",
   "        entry += 'plugin_type: %s ' % plugin_type\n",
   "            entry += 'plugin: %s ' % plugin_name\n",
   "    entry += 'setting: %s ' % config\n",
   "    return entry\n",
   "def ensure_type(value, value_type, origin=None):\n",
   "    errmsg = ''\n",
   "    basedir = None\n",
   "        basedir = origin\n",
   "        value_type = value_type.lower()\n",
   "        if value_type in ('boolean', 'bool'):\n",
   "            value = boolean(value, strict=False)\n",
   "        elif value_type in ('integer', 'int'):\n",
   "            value = int(value)\n",
   "        elif value_type == 'float':\n",
   "            value = float(value)\n",
   "        elif value_type == 'list':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            elif not isinstance(value, Sequence):\n",
   "                errmsg = 'list'\n",
   "        elif value_type == 'none':\n",
   "            if value == \"None\":\n",
   "                value = None\n",
   "            if value is not None:\n",
   "                errmsg = 'None'\n",
   "        elif value_type == 'path':\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                errmsg = 'path'\n",
   "        elif value_type in ('tmp', 'temppath', 'tmppath'):\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                if not os.path.exists(value):\n",
   "                    makedirs_safe(value, 0o700)\n",
   "                prefix = 'ansible-local-%s' % os.getpid()\n",
   "                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n",
   "                atexit.register(cleanup_tmp_file, value, warn=True)\n",
   "                errmsg = 'temppath'\n",
   "        elif value_type == 'pathspec':\n",
   "            if isinstance(value, string_types):\n",
   "                value = value.split(os.pathsep)\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathspec'\n",
   "        elif value_type == 'pathlist':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathlist'\n",
   "        elif value_type in ('str', 'string'):\n",
   "            if isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "                value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "                errmsg = 'string'\n",
   "        elif isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "            value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "        if errmsg:\n",
   "            raise ValueError('Invalid type provided for \"%s\": %s' % (errmsg, to_native(value)))\n",
   "    return to_text(value, errors='surrogate_or_strict', nonstring='passthru')\n",
   "        path = path.replace('{{CWD}}', os.getcwd())\n",
   "    return unfrackpath(path, follow=False, basedir=basedir)\n",
   "def get_config_type(cfile):\n",
   "    ftype = None\n",
   "        ext = os.path.splitext(cfile)[-1]\n",
   "        if ext in ('.ini', '.cfg'):\n",
   "            ftype = 'ini'\n",
   "        elif ext in ('.yaml', '.yml'):\n",
   "            ftype = 'yaml'\n",
   "            raise AnsibleOptionsError(\"Unsupported configuration file extension for %s: %s\" % (cfile, to_native(ext)))\n",
   "    return ftype\n",
   "def get_ini_config_value(p, entry):\n",
   "    value = None\n",
   "            value = p.get(entry.get('section', 'defaults'), entry.get('key', ''), raw=True)\n",
   "    return value\n",
   "def find_ini_config_file(warnings=None):\n",
   "        warnings = set()\n",
   "    SENTINEL = object\n",
   "    potential_paths = []\n",
   "    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n",
   "    if path_from_env is not SENTINEL:\n",
   "        path_from_env = unfrackpath(path_from_env, follow=False)\n",
   "        if os.path.isdir(to_bytes(path_from_env)):\n",
   "            path_from_env = os.path.join(path_from_env, \"ansible.cfg\")\n",
   "        potential_paths.append(path_from_env)\n",
   "    warn_cmd_public = False\n",
   "        cwd = os.getcwd()\n",
   "        perms = os.stat(cwd)\n",
   "        cwd_cfg = os.path.join(cwd, \"ansible.cfg\")\n",
   "        if perms.st_mode & stat.S_IWOTH:\n",
   "            if os.path.exists(cwd_cfg):\n",
   "                warn_cmd_public = True\n",
   "            potential_paths.append(to_text(cwd_cfg, errors='surrogate_or_strict'))\n",
   "    potential_paths.append(unfrackpath(\"~/.ansible.cfg\", follow=False))\n",
   "    potential_paths.append(\"/etc/ansible/ansible.cfg\")\n",
   "    for path in potential_paths:\n",
   "        b_path = to_bytes(path)\n",
   "        if os.path.exists(b_path) and os.access(b_path, os.R_OK):\n",
   "        path = None\n",
   "    if path_from_env != path and warn_cmd_public:\n",
   "        warnings.add(u\"Ansible is being run in a world writable directory (%s),\"\n",
   "                     % to_text(cwd))\n",
   "    return path\n",
   "        yml_file = to_bytes(yml_file)\n",
   "        if os.path.exists(yml_file):\n",
   "            with open(yml_file, 'rb') as config_def:\n",
   "                return yaml_load(config_def, Loader=SafeLoader) or {}\n",
   "            \"Missing base YAML definition file (bad install?): %s\" % to_native(yml_file))\n",
   "            cfile = self._config_file\n",
   "        ftype = get_config_type(cfile)\n",
   "        if cfile is not None:\n",
   "            if ftype == 'ini':\n",
   "                self._parsers[cfile] = configparser.ConfigParser()\n",
   "                with open(to_bytes(cfile), 'rb') as f:\n",
   "                        cfg_text = to_text(f.read(), errors='surrogate_or_strict')\n",
   "                        raise AnsibleOptionsError(\"Error reading config file(%s) because the config file was not utf8 encoded: %s\" % (cfile, to_native(e)))\n",
   "                        self._parsers[cfile].read_string(cfg_text)\n",
   "                        cfg_file = io.StringIO(cfg_text)\n",
   "                        self._parsers[cfile].readfp(cfg_file)\n",
   "                    raise AnsibleOptionsError(\"Error reading config file (%s): %s\" % (cfile, to_native(e)))\n",
   "                raise AnsibleOptionsError(\"Unsupported configuration file type: %s\" % to_native(ftype))\n",
   "        options = {}\n",
   "        defs = self.get_configuration_definitions(plugin_type, name)\n",
   "        for option in defs:\n",
   "            options[option] = self.get_config_value(option, plugin_type=plugin_type, plugin_name=name, keys=keys, variables=variables, direct=direct)\n",
   "        return options\n",
   "        pvars = []\n",
   "        for pdef in self.get_configuration_definitions(plugin_type, name).values():\n",
   "            if 'vars' in pdef and pdef['vars']:\n",
   "                for var_entry in pdef['vars']:\n",
   "                    pvars.append(var_entry['name'])\n",
   "        return pvars\n",
   "        ret = {}\n",
   "            ret = self._base_defs.get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(plugin_name, {}).get(name, None)\n",
   "        return ret\n",
   "        ret = {}\n",
   "            ret = self._base_defs\n",
   "            ret = self._plugins.get(plugin_type, {})\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, {})\n",
   "        return ret\n",
   "        value = None\n",
   "        origin = None\n",
   "        for entry in entry_list:\n",
   "            name = entry.get('name')\n",
   "                temp_value = container.get(name, None)\n",
   "                self.WARNINGS.add(u'value for config entry {0} contains invalid characters, ignoring...'.format(to_text(name)))\n",
   "            if temp_value is not None:  # only set if entry is defined in container\n",
   "                if isinstance(temp_value, AnsibleVaultEncryptedUnicode):\n",
   "                    temp_value = to_text(temp_value, errors='surrogate_or_strict')\n",
   "                value = temp_value\n",
   "                origin = name\n",
   "                if 'deprecated' in entry:\n",
   "                    self.DEPRECATED.append((entry['name'], entry['deprecated']))\n",
   "        return value, origin\n",
   "            value, _drop = self.get_config_value_and_origin(config, cfile=cfile, plugin_type=plugin_type, plugin_name=plugin_name,\n",
   "        return value\n",
   "        if cfile is None:\n",
   "            cfile = self._config_file\n",
   "        value = None\n",
   "        origin = None\n",
   "        defs = self.get_configuration_definitions(plugin_type, plugin_name)\n",
   "        if config in defs:\n",
   "            direct_aliases = []\n",
   "                direct_aliases = [direct[alias] for alias in defs[config].get('aliases', []) if alias in direct]\n",
   "                value = direct[config]\n",
   "                origin = 'Direct'\n",
   "            elif direct and direct_aliases:\n",
   "                value = direct_aliases[0]\n",
   "                origin = 'Direct'\n",
   "                if variables and defs[config].get('vars'):\n",
   "                    value, origin = self._loop_entries(variables, defs[config]['vars'])\n",
   "                    origin = 'var: %s' % origin\n",
   "                if value is None and keys and config in keys:\n",
   "                    value, origin = keys[config], 'keyword'\n",
   "                    origin = 'keyword: %s' % origin\n",
   "                if value is None and defs[config].get('env'):\n",
   "                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n",
   "                    origin = 'env: %s' % origin\n",
   "                if self._parsers.get(cfile, None) is None:\n",
   "                    self._parse_config_file(cfile)\n",
   "                if value is None and cfile is not None:\n",
   "                    ftype = get_config_type(cfile)\n",
   "                    if ftype and defs[config].get(ftype):\n",
   "                        if ftype == 'ini':\n",
   "                                for ini_entry in defs[config]['ini']:\n",
   "                                    temp_value = get_ini_config_value(self._parsers[cfile], ini_entry)\n",
   "                                    if temp_value is not None:\n",
   "                                        value = temp_value\n",
   "                                        origin = cfile\n",
   "                                        if 'deprecated' in ini_entry:\n",
   "                                            self.DEPRECATED.append(('[%s]%s' % (ini_entry['section'], ini_entry['key']), ini_entry['deprecated']))\n",
   "                                sys.stderr.write(\"Error while loading ini config %s: %s\" % (cfile, to_native(e)))\n",
   "                        elif ftype == 'yaml':\n",
   "                            origin = cfile\n",
   "                if value is None:\n",
   "                    if defs[config].get('required', False):\n",
   "                        if not plugin_type or config not in INTERNAL_DEFS.get(plugin_type, {}):\n",
   "                                               to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "                        value = defs[config].get('default')\n",
   "                        origin = 'default'\n",
   "                        if plugin_type is None and isinstance(value, string_types) and (value.startswith('{{') and value.endswith('}}')):\n",
   "                            return value, origin\n",
   "                value = ensure_type(value, defs[config].get('type'), origin=origin)\n",
   "                if origin.startswith('env:') and value == '':\n",
   "                    origin = 'default'\n",
   "                    value = ensure_type(defs[config].get('default'), defs[config].get('type'), origin=origin)\n",
   "                                              (to_native(_get_entry(plugin_type, plugin_name, config)), to_native(e)))\n",
   "            if 'deprecated' in defs[config] and origin != 'default':\n",
   "                self.DEPRECATED.append((config, defs[config].get('deprecated')))\n",
   "            raise AnsibleError('Requested entry (%s) was not defined in configuration.' % to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "        return value, origin\n",
   "        if plugin_type not in self._plugins:\n",
   "            self._plugins[plugin_type] = {}\n",
   "        self._plugins[plugin_type][name] = defs\n",
   "        if defs is None:\n",
   "        if not isinstance(defs, dict):\n",
   "            raise AnsibleOptionsError(\"Invalid configuration definition type: %s for %s\" % (type(defs), defs))\n",
   "        for config in defs:\n",
   "            if not isinstance(defs[config], dict):\n",
   "                raise AnsibleOptionsError(\"Invalid configuration definition '%s': type is %s\" % (to_native(config), type(defs[config])))\n",
   "                value, origin = self.get_config_value_and_origin(config, configfile)\n",
   "                raise AnsibleError(\"Invalid settings supplied for %s: %s\\n\" % (config, to_native(e)), orig_exc=e)\n",
   "            self.data.update_setting(Setting(config, value, origin, defs[config].get('type', 'string')))\n"
  ]
 },
 "30": {
  "name": "path",
  "type": "NoneType",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "250",
  "column": "8",
  "context": "th, os.R_OK):\n            break\n    else:\n        path = None\n\n    # Emit a warning if all the following are tru",
  "context_lines": "        b_path = to_bytes(path)\n        if os.path.exists(b_path) and os.access(b_path, os.R_OK):\n            break\n    else:\n        path = None\n\n    # Emit a warning if all the following are true:\n    # * We did not use a config from ANSIBLE_CONFIG\n    # * There's an ansible.cfg in the current working directory that we skipped\n",
  "slicing": [
   "        path = None\n",
   "    if path_from_env != path and warn_cmd_public:\n",
   "    return path\n"
  ]
 },
 "31": {
  "name": "cfile",
  "type": "NoneType",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "308",
  "column": "12",
  "context": "ge/nomerge\n\n        if cfile is None:\n            cfile = self._config_file\n\n        ftype = get_config_type(cfile)\n        if",
  "context_lines": "    def _parse_config_file(self, cfile=None):\n        ''' return flat configuration settings from file(s) '''\n        # TODO: take list of files with merge/nomerge\n\n        if cfile is None:\n            cfile = self._config_file\n\n        ftype = get_config_type(cfile)\n        if cfile is not None:\n            if ftype == 'ini':\n",
  "slicing": [
   "INTERNAL_DEFS = {'lookup': ('_terms',)}\n",
   "def _get_entry(plugin_type, plugin_name, config):\n",
   "    entry = ''\n",
   "        entry += 'plugin_type: %s ' % plugin_type\n",
   "            entry += 'plugin: %s ' % plugin_name\n",
   "    entry += 'setting: %s ' % config\n",
   "    return entry\n",
   "def ensure_type(value, value_type, origin=None):\n",
   "    errmsg = ''\n",
   "    basedir = None\n",
   "        basedir = origin\n",
   "        value_type = value_type.lower()\n",
   "        if value_type in ('boolean', 'bool'):\n",
   "            value = boolean(value, strict=False)\n",
   "        elif value_type in ('integer', 'int'):\n",
   "            value = int(value)\n",
   "        elif value_type == 'float':\n",
   "            value = float(value)\n",
   "        elif value_type == 'list':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            elif not isinstance(value, Sequence):\n",
   "                errmsg = 'list'\n",
   "        elif value_type == 'none':\n",
   "            if value == \"None\":\n",
   "                value = None\n",
   "            if value is not None:\n",
   "                errmsg = 'None'\n",
   "        elif value_type == 'path':\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                errmsg = 'path'\n",
   "        elif value_type in ('tmp', 'temppath', 'tmppath'):\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                if not os.path.exists(value):\n",
   "                    makedirs_safe(value, 0o700)\n",
   "                prefix = 'ansible-local-%s' % os.getpid()\n",
   "                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n",
   "                atexit.register(cleanup_tmp_file, value, warn=True)\n",
   "                errmsg = 'temppath'\n",
   "        elif value_type == 'pathspec':\n",
   "            if isinstance(value, string_types):\n",
   "                value = value.split(os.pathsep)\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathspec'\n",
   "        elif value_type == 'pathlist':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathlist'\n",
   "        elif value_type in ('str', 'string'):\n",
   "            if isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "                value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "                errmsg = 'string'\n",
   "        elif isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "            value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "        if errmsg:\n",
   "            raise ValueError('Invalid type provided for \"%s\": %s' % (errmsg, to_native(value)))\n",
   "    return to_text(value, errors='surrogate_or_strict', nonstring='passthru')\n",
   "        path = path.replace('{{CWD}}', os.getcwd())\n",
   "    return unfrackpath(path, follow=False, basedir=basedir)\n",
   "def get_config_type(cfile):\n",
   "    ftype = None\n",
   "        ext = os.path.splitext(cfile)[-1]\n",
   "        if ext in ('.ini', '.cfg'):\n",
   "            ftype = 'ini'\n",
   "        elif ext in ('.yaml', '.yml'):\n",
   "            ftype = 'yaml'\n",
   "            raise AnsibleOptionsError(\"Unsupported configuration file extension for %s: %s\" % (cfile, to_native(ext)))\n",
   "    return ftype\n",
   "def get_ini_config_value(p, entry):\n",
   "    value = None\n",
   "            value = p.get(entry.get('section', 'defaults'), entry.get('key', ''), raw=True)\n",
   "    return value\n",
   "def find_ini_config_file(warnings=None):\n",
   "        warnings = set()\n",
   "    SENTINEL = object\n",
   "    potential_paths = []\n",
   "    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n",
   "    if path_from_env is not SENTINEL:\n",
   "        path_from_env = unfrackpath(path_from_env, follow=False)\n",
   "        if os.path.isdir(to_bytes(path_from_env)):\n",
   "            path_from_env = os.path.join(path_from_env, \"ansible.cfg\")\n",
   "        potential_paths.append(path_from_env)\n",
   "    warn_cmd_public = False\n",
   "        cwd = os.getcwd()\n",
   "        perms = os.stat(cwd)\n",
   "        cwd_cfg = os.path.join(cwd, \"ansible.cfg\")\n",
   "        if perms.st_mode & stat.S_IWOTH:\n",
   "            if os.path.exists(cwd_cfg):\n",
   "                warn_cmd_public = True\n",
   "            potential_paths.append(to_text(cwd_cfg, errors='surrogate_or_strict'))\n",
   "    potential_paths.append(unfrackpath(\"~/.ansible.cfg\", follow=False))\n",
   "    potential_paths.append(\"/etc/ansible/ansible.cfg\")\n",
   "    for path in potential_paths:\n",
   "        b_path = to_bytes(path)\n",
   "        if os.path.exists(b_path) and os.access(b_path, os.R_OK):\n",
   "        path = None\n",
   "    if path_from_env != path and warn_cmd_public:\n",
   "        warnings.add(u\"Ansible is being run in a world writable directory (%s),\"\n",
   "                     % to_text(cwd))\n",
   "    return path\n",
   "        yml_file = to_bytes(yml_file)\n",
   "        if os.path.exists(yml_file):\n",
   "            with open(yml_file, 'rb') as config_def:\n",
   "                return yaml_load(config_def, Loader=SafeLoader) or {}\n",
   "            \"Missing base YAML definition file (bad install?): %s\" % to_native(yml_file))\n",
   "            cfile = self._config_file\n",
   "        ftype = get_config_type(cfile)\n",
   "        if cfile is not None:\n",
   "            if ftype == 'ini':\n",
   "                self._parsers[cfile] = configparser.ConfigParser()\n",
   "                with open(to_bytes(cfile), 'rb') as f:\n",
   "                        cfg_text = to_text(f.read(), errors='surrogate_or_strict')\n",
   "                        raise AnsibleOptionsError(\"Error reading config file(%s) because the config file was not utf8 encoded: %s\" % (cfile, to_native(e)))\n",
   "                        self._parsers[cfile].read_string(cfg_text)\n",
   "                        cfg_file = io.StringIO(cfg_text)\n",
   "                        self._parsers[cfile].readfp(cfg_file)\n",
   "                    raise AnsibleOptionsError(\"Error reading config file (%s): %s\" % (cfile, to_native(e)))\n",
   "                raise AnsibleOptionsError(\"Unsupported configuration file type: %s\" % to_native(ftype))\n",
   "        options = {}\n",
   "        defs = self.get_configuration_definitions(plugin_type, name)\n",
   "        for option in defs:\n",
   "            options[option] = self.get_config_value(option, plugin_type=plugin_type, plugin_name=name, keys=keys, variables=variables, direct=direct)\n",
   "        return options\n",
   "        pvars = []\n",
   "        for pdef in self.get_configuration_definitions(plugin_type, name).values():\n",
   "            if 'vars' in pdef and pdef['vars']:\n",
   "                for var_entry in pdef['vars']:\n",
   "                    pvars.append(var_entry['name'])\n",
   "        return pvars\n",
   "        ret = {}\n",
   "            ret = self._base_defs.get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(plugin_name, {}).get(name, None)\n",
   "        return ret\n",
   "        ret = {}\n",
   "            ret = self._base_defs\n",
   "            ret = self._plugins.get(plugin_type, {})\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, {})\n",
   "        return ret\n",
   "        value = None\n",
   "        origin = None\n",
   "        for entry in entry_list:\n",
   "            name = entry.get('name')\n",
   "                temp_value = container.get(name, None)\n",
   "                self.WARNINGS.add(u'value for config entry {0} contains invalid characters, ignoring...'.format(to_text(name)))\n",
   "            if temp_value is not None:  # only set if entry is defined in container\n",
   "                if isinstance(temp_value, AnsibleVaultEncryptedUnicode):\n",
   "                    temp_value = to_text(temp_value, errors='surrogate_or_strict')\n",
   "                value = temp_value\n",
   "                origin = name\n",
   "                if 'deprecated' in entry:\n",
   "                    self.DEPRECATED.append((entry['name'], entry['deprecated']))\n",
   "        return value, origin\n",
   "            value, _drop = self.get_config_value_and_origin(config, cfile=cfile, plugin_type=plugin_type, plugin_name=plugin_name,\n",
   "        return value\n",
   "        if cfile is None:\n",
   "            cfile = self._config_file\n",
   "        value = None\n",
   "        origin = None\n",
   "        defs = self.get_configuration_definitions(plugin_type, plugin_name)\n",
   "        if config in defs:\n",
   "            direct_aliases = []\n",
   "                direct_aliases = [direct[alias] for alias in defs[config].get('aliases', []) if alias in direct]\n",
   "                value = direct[config]\n",
   "                origin = 'Direct'\n",
   "            elif direct and direct_aliases:\n",
   "                value = direct_aliases[0]\n",
   "                origin = 'Direct'\n",
   "                if variables and defs[config].get('vars'):\n",
   "                    value, origin = self._loop_entries(variables, defs[config]['vars'])\n",
   "                    origin = 'var: %s' % origin\n",
   "                if value is None and keys and config in keys:\n",
   "                    value, origin = keys[config], 'keyword'\n",
   "                    origin = 'keyword: %s' % origin\n",
   "                if value is None and defs[config].get('env'):\n",
   "                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n",
   "                    origin = 'env: %s' % origin\n",
   "                if self._parsers.get(cfile, None) is None:\n",
   "                    self._parse_config_file(cfile)\n",
   "                if value is None and cfile is not None:\n",
   "                    ftype = get_config_type(cfile)\n",
   "                    if ftype and defs[config].get(ftype):\n",
   "                        if ftype == 'ini':\n",
   "                                for ini_entry in defs[config]['ini']:\n",
   "                                    temp_value = get_ini_config_value(self._parsers[cfile], ini_entry)\n",
   "                                    if temp_value is not None:\n",
   "                                        value = temp_value\n",
   "                                        origin = cfile\n",
   "                                        if 'deprecated' in ini_entry:\n",
   "                                            self.DEPRECATED.append(('[%s]%s' % (ini_entry['section'], ini_entry['key']), ini_entry['deprecated']))\n",
   "                                sys.stderr.write(\"Error while loading ini config %s: %s\" % (cfile, to_native(e)))\n",
   "                        elif ftype == 'yaml':\n",
   "                            origin = cfile\n",
   "                if value is None:\n",
   "                    if defs[config].get('required', False):\n",
   "                        if not plugin_type or config not in INTERNAL_DEFS.get(plugin_type, {}):\n",
   "                                               to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "                        value = defs[config].get('default')\n",
   "                        origin = 'default'\n",
   "                        if plugin_type is None and isinstance(value, string_types) and (value.startswith('{{') and value.endswith('}}')):\n",
   "                            return value, origin\n",
   "                value = ensure_type(value, defs[config].get('type'), origin=origin)\n",
   "                if origin.startswith('env:') and value == '':\n",
   "                    origin = 'default'\n",
   "                    value = ensure_type(defs[config].get('default'), defs[config].get('type'), origin=origin)\n",
   "                                              (to_native(_get_entry(plugin_type, plugin_name, config)), to_native(e)))\n",
   "            if 'deprecated' in defs[config] and origin != 'default':\n",
   "                self.DEPRECATED.append((config, defs[config].get('deprecated')))\n",
   "            raise AnsibleError('Requested entry (%s) was not defined in configuration.' % to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "        return value, origin\n",
   "        if plugin_type not in self._plugins:\n",
   "            self._plugins[plugin_type] = {}\n",
   "        self._plugins[plugin_type][name] = defs\n",
   "        if defs is None:\n",
   "        if not isinstance(defs, dict):\n",
   "            raise AnsibleOptionsError(\"Invalid configuration definition type: %s for %s\" % (type(defs), defs))\n",
   "        for config in defs:\n",
   "            if not isinstance(defs[config], dict):\n",
   "                raise AnsibleOptionsError(\"Invalid configuration definition '%s': type is %s\" % (to_native(config), type(defs[config])))\n",
   "                value, origin = self.get_config_value_and_origin(config, configfile)\n",
   "                raise AnsibleError(\"Invalid settings supplied for %s: %s\\n\" % (config, to_native(e)), orig_exc=e)\n",
   "            self.data.update_setting(Setting(config, value, origin, defs[config].get('type', 'string')))\n"
  ]
 },
 "32": {
  "name": "value",
  "type": "NoneType",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "384",
  "column": "8",
  "context": "peat code for value entry assignment '''\n\n        value = None\n        origin = None\n        for entry in entry_l",
  "context_lines": "            ret = self._plugins.get(plugin_type, {}).get(name, {})\n\n        return ret\n\n    def _loop_entries(self, container, entry_list):\n        ''' repeat code for value entry assignment '''\n\n        value = None\n        origin = None\n        for entry in entry_list:\n            name = entry.get('name')\n            try:\n",
  "slicing": [
   "INTERNAL_DEFS = {'lookup': ('_terms',)}\n",
   "def _get_entry(plugin_type, plugin_name, config):\n",
   "    entry = ''\n",
   "        entry += 'plugin_type: %s ' % plugin_type\n",
   "            entry += 'plugin: %s ' % plugin_name\n",
   "    entry += 'setting: %s ' % config\n",
   "    return entry\n",
   "def ensure_type(value, value_type, origin=None):\n",
   "    errmsg = ''\n",
   "    basedir = None\n",
   "        basedir = origin\n",
   "        value_type = value_type.lower()\n",
   "        if value_type in ('boolean', 'bool'):\n",
   "            value = boolean(value, strict=False)\n",
   "        elif value_type in ('integer', 'int'):\n",
   "            value = int(value)\n",
   "        elif value_type == 'float':\n",
   "            value = float(value)\n",
   "        elif value_type == 'list':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            elif not isinstance(value, Sequence):\n",
   "                errmsg = 'list'\n",
   "        elif value_type == 'none':\n",
   "            if value == \"None\":\n",
   "                value = None\n",
   "            if value is not None:\n",
   "                errmsg = 'None'\n",
   "        elif value_type == 'path':\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                errmsg = 'path'\n",
   "        elif value_type in ('tmp', 'temppath', 'tmppath'):\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                if not os.path.exists(value):\n",
   "                    makedirs_safe(value, 0o700)\n",
   "                prefix = 'ansible-local-%s' % os.getpid()\n",
   "                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n",
   "                atexit.register(cleanup_tmp_file, value, warn=True)\n",
   "                errmsg = 'temppath'\n",
   "        elif value_type == 'pathspec':\n",
   "            if isinstance(value, string_types):\n",
   "                value = value.split(os.pathsep)\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathspec'\n",
   "        elif value_type == 'pathlist':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathlist'\n",
   "        elif value_type in ('str', 'string'):\n",
   "            if isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "                value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "                errmsg = 'string'\n",
   "        elif isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "            value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "        if errmsg:\n",
   "            raise ValueError('Invalid type provided for \"%s\": %s' % (errmsg, to_native(value)))\n",
   "    return to_text(value, errors='surrogate_or_strict', nonstring='passthru')\n",
   "        path = path.replace('{{CWD}}', os.getcwd())\n",
   "    return unfrackpath(path, follow=False, basedir=basedir)\n",
   "def get_config_type(cfile):\n",
   "    ftype = None\n",
   "        ext = os.path.splitext(cfile)[-1]\n",
   "        if ext in ('.ini', '.cfg'):\n",
   "            ftype = 'ini'\n",
   "        elif ext in ('.yaml', '.yml'):\n",
   "            ftype = 'yaml'\n",
   "            raise AnsibleOptionsError(\"Unsupported configuration file extension for %s: %s\" % (cfile, to_native(ext)))\n",
   "    return ftype\n",
   "def get_ini_config_value(p, entry):\n",
   "    value = None\n",
   "            value = p.get(entry.get('section', 'defaults'), entry.get('key', ''), raw=True)\n",
   "    return value\n",
   "def find_ini_config_file(warnings=None):\n",
   "        warnings = set()\n",
   "    SENTINEL = object\n",
   "    potential_paths = []\n",
   "    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n",
   "    if path_from_env is not SENTINEL:\n",
   "        path_from_env = unfrackpath(path_from_env, follow=False)\n",
   "        if os.path.isdir(to_bytes(path_from_env)):\n",
   "            path_from_env = os.path.join(path_from_env, \"ansible.cfg\")\n",
   "        potential_paths.append(path_from_env)\n",
   "    warn_cmd_public = False\n",
   "        cwd = os.getcwd()\n",
   "        perms = os.stat(cwd)\n",
   "        cwd_cfg = os.path.join(cwd, \"ansible.cfg\")\n",
   "        if perms.st_mode & stat.S_IWOTH:\n",
   "            if os.path.exists(cwd_cfg):\n",
   "                warn_cmd_public = True\n",
   "            potential_paths.append(to_text(cwd_cfg, errors='surrogate_or_strict'))\n",
   "    potential_paths.append(unfrackpath(\"~/.ansible.cfg\", follow=False))\n",
   "    potential_paths.append(\"/etc/ansible/ansible.cfg\")\n",
   "    for path in potential_paths:\n",
   "        b_path = to_bytes(path)\n",
   "        if os.path.exists(b_path) and os.access(b_path, os.R_OK):\n",
   "        path = None\n",
   "    if path_from_env != path and warn_cmd_public:\n",
   "        warnings.add(u\"Ansible is being run in a world writable directory (%s),\"\n",
   "                     % to_text(cwd))\n",
   "    return path\n",
   "        yml_file = to_bytes(yml_file)\n",
   "        if os.path.exists(yml_file):\n",
   "            with open(yml_file, 'rb') as config_def:\n",
   "                return yaml_load(config_def, Loader=SafeLoader) or {}\n",
   "            \"Missing base YAML definition file (bad install?): %s\" % to_native(yml_file))\n",
   "            cfile = self._config_file\n",
   "        ftype = get_config_type(cfile)\n",
   "        if cfile is not None:\n",
   "            if ftype == 'ini':\n",
   "                self._parsers[cfile] = configparser.ConfigParser()\n",
   "                with open(to_bytes(cfile), 'rb') as f:\n",
   "                        cfg_text = to_text(f.read(), errors='surrogate_or_strict')\n",
   "                        raise AnsibleOptionsError(\"Error reading config file(%s) because the config file was not utf8 encoded: %s\" % (cfile, to_native(e)))\n",
   "                        self._parsers[cfile].read_string(cfg_text)\n",
   "                        cfg_file = io.StringIO(cfg_text)\n",
   "                        self._parsers[cfile].readfp(cfg_file)\n",
   "                    raise AnsibleOptionsError(\"Error reading config file (%s): %s\" % (cfile, to_native(e)))\n",
   "                raise AnsibleOptionsError(\"Unsupported configuration file type: %s\" % to_native(ftype))\n",
   "        options = {}\n",
   "        defs = self.get_configuration_definitions(plugin_type, name)\n",
   "        for option in defs:\n",
   "            options[option] = self.get_config_value(option, plugin_type=plugin_type, plugin_name=name, keys=keys, variables=variables, direct=direct)\n",
   "        return options\n",
   "        pvars = []\n",
   "        for pdef in self.get_configuration_definitions(plugin_type, name).values():\n",
   "            if 'vars' in pdef and pdef['vars']:\n",
   "                for var_entry in pdef['vars']:\n",
   "                    pvars.append(var_entry['name'])\n",
   "        return pvars\n",
   "        ret = {}\n",
   "            ret = self._base_defs.get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(plugin_name, {}).get(name, None)\n",
   "        return ret\n",
   "        ret = {}\n",
   "            ret = self._base_defs\n",
   "            ret = self._plugins.get(plugin_type, {})\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, {})\n",
   "        return ret\n",
   "        value = None\n",
   "        origin = None\n",
   "        for entry in entry_list:\n",
   "            name = entry.get('name')\n",
   "                temp_value = container.get(name, None)\n",
   "                self.WARNINGS.add(u'value for config entry {0} contains invalid characters, ignoring...'.format(to_text(name)))\n",
   "            if temp_value is not None:  # only set if entry is defined in container\n",
   "                if isinstance(temp_value, AnsibleVaultEncryptedUnicode):\n",
   "                    temp_value = to_text(temp_value, errors='surrogate_or_strict')\n",
   "                value = temp_value\n",
   "                origin = name\n",
   "                if 'deprecated' in entry:\n",
   "                    self.DEPRECATED.append((entry['name'], entry['deprecated']))\n",
   "        return value, origin\n",
   "            value, _drop = self.get_config_value_and_origin(config, cfile=cfile, plugin_type=plugin_type, plugin_name=plugin_name,\n",
   "        return value\n",
   "        if cfile is None:\n",
   "            cfile = self._config_file\n",
   "        value = None\n",
   "        origin = None\n",
   "        defs = self.get_configuration_definitions(plugin_type, plugin_name)\n",
   "        if config in defs:\n",
   "            direct_aliases = []\n",
   "                direct_aliases = [direct[alias] for alias in defs[config].get('aliases', []) if alias in direct]\n",
   "                value = direct[config]\n",
   "                origin = 'Direct'\n",
   "            elif direct and direct_aliases:\n",
   "                value = direct_aliases[0]\n",
   "                origin = 'Direct'\n",
   "                if variables and defs[config].get('vars'):\n",
   "                    value, origin = self._loop_entries(variables, defs[config]['vars'])\n",
   "                    origin = 'var: %s' % origin\n",
   "                if value is None and keys and config in keys:\n",
   "                    value, origin = keys[config], 'keyword'\n",
   "                    origin = 'keyword: %s' % origin\n",
   "                if value is None and defs[config].get('env'):\n",
   "                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n",
   "                    origin = 'env: %s' % origin\n",
   "                if self._parsers.get(cfile, None) is None:\n",
   "                    self._parse_config_file(cfile)\n",
   "                if value is None and cfile is not None:\n",
   "                    ftype = get_config_type(cfile)\n",
   "                    if ftype and defs[config].get(ftype):\n",
   "                        if ftype == 'ini':\n",
   "                                for ini_entry in defs[config]['ini']:\n",
   "                                    temp_value = get_ini_config_value(self._parsers[cfile], ini_entry)\n",
   "                                    if temp_value is not None:\n",
   "                                        value = temp_value\n",
   "                                        origin = cfile\n",
   "                                        if 'deprecated' in ini_entry:\n",
   "                                            self.DEPRECATED.append(('[%s]%s' % (ini_entry['section'], ini_entry['key']), ini_entry['deprecated']))\n",
   "                                sys.stderr.write(\"Error while loading ini config %s: %s\" % (cfile, to_native(e)))\n",
   "                        elif ftype == 'yaml':\n",
   "                            origin = cfile\n",
   "                if value is None:\n",
   "                    if defs[config].get('required', False):\n",
   "                        if not plugin_type or config not in INTERNAL_DEFS.get(plugin_type, {}):\n",
   "                                               to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "                        value = defs[config].get('default')\n",
   "                        origin = 'default'\n",
   "                        if plugin_type is None and isinstance(value, string_types) and (value.startswith('{{') and value.endswith('}}')):\n",
   "                            return value, origin\n",
   "                value = ensure_type(value, defs[config].get('type'), origin=origin)\n",
   "                if origin.startswith('env:') and value == '':\n",
   "                    origin = 'default'\n",
   "                    value = ensure_type(defs[config].get('default'), defs[config].get('type'), origin=origin)\n",
   "                                              (to_native(_get_entry(plugin_type, plugin_name, config)), to_native(e)))\n",
   "            if 'deprecated' in defs[config] and origin != 'default':\n",
   "                self.DEPRECATED.append((config, defs[config].get('deprecated')))\n",
   "            raise AnsibleError('Requested entry (%s) was not defined in configuration.' % to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "        return value, origin\n",
   "        if plugin_type not in self._plugins:\n",
   "            self._plugins[plugin_type] = {}\n",
   "        self._plugins[plugin_type][name] = defs\n",
   "        if defs is None:\n",
   "        if not isinstance(defs, dict):\n",
   "            raise AnsibleOptionsError(\"Invalid configuration definition type: %s for %s\" % (type(defs), defs))\n",
   "        for config in defs:\n",
   "            if not isinstance(defs[config], dict):\n",
   "                raise AnsibleOptionsError(\"Invalid configuration definition '%s': type is %s\" % (to_native(config), type(defs[config])))\n",
   "                value, origin = self.get_config_value_and_origin(config, configfile)\n",
   "                raise AnsibleError(\"Invalid settings supplied for %s: %s\\n\" % (config, to_native(e)), orig_exc=e)\n",
   "            self.data.update_setting(Setting(config, value, origin, defs[config].get('type', 'string')))\n"
  ]
 },
 "33": {
  "name": "origin",
  "type": "NoneType",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "385",
  "column": "8",
  "context": "ntry assignment '''\n\n        value = None\n        origin = None\n        for entry in entry_list:\n            name ",
  "context_lines": "        return ret\n\n    def _loop_entries(self, container, entry_list):\n        ''' repeat code for value entry assignment '''\n\n        value = None\n        origin = None\n        for entry in entry_list:\n            name = entry.get('name')\n            try:\n                temp_value = container.get(name, None)\n",
  "slicing": [
   "INTERNAL_DEFS = {'lookup': ('_terms',)}\n",
   "def _get_entry(plugin_type, plugin_name, config):\n",
   "    entry = ''\n",
   "        entry += 'plugin_type: %s ' % plugin_type\n",
   "            entry += 'plugin: %s ' % plugin_name\n",
   "    entry += 'setting: %s ' % config\n",
   "    return entry\n",
   "def ensure_type(value, value_type, origin=None):\n",
   "    errmsg = ''\n",
   "    basedir = None\n",
   "        basedir = origin\n",
   "        value_type = value_type.lower()\n",
   "        if value_type in ('boolean', 'bool'):\n",
   "            value = boolean(value, strict=False)\n",
   "        elif value_type in ('integer', 'int'):\n",
   "            value = int(value)\n",
   "        elif value_type == 'float':\n",
   "            value = float(value)\n",
   "        elif value_type == 'list':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            elif not isinstance(value, Sequence):\n",
   "                errmsg = 'list'\n",
   "        elif value_type == 'none':\n",
   "            if value == \"None\":\n",
   "                value = None\n",
   "            if value is not None:\n",
   "                errmsg = 'None'\n",
   "        elif value_type == 'path':\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                errmsg = 'path'\n",
   "        elif value_type in ('tmp', 'temppath', 'tmppath'):\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                if not os.path.exists(value):\n",
   "                    makedirs_safe(value, 0o700)\n",
   "                prefix = 'ansible-local-%s' % os.getpid()\n",
   "                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n",
   "                atexit.register(cleanup_tmp_file, value, warn=True)\n",
   "                errmsg = 'temppath'\n",
   "        elif value_type == 'pathspec':\n",
   "            if isinstance(value, string_types):\n",
   "                value = value.split(os.pathsep)\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathspec'\n",
   "        elif value_type == 'pathlist':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathlist'\n",
   "        elif value_type in ('str', 'string'):\n",
   "            if isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "                value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "                errmsg = 'string'\n",
   "        elif isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "            value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "        if errmsg:\n",
   "            raise ValueError('Invalid type provided for \"%s\": %s' % (errmsg, to_native(value)))\n",
   "    return to_text(value, errors='surrogate_or_strict', nonstring='passthru')\n",
   "        path = path.replace('{{CWD}}', os.getcwd())\n",
   "    return unfrackpath(path, follow=False, basedir=basedir)\n",
   "def get_config_type(cfile):\n",
   "    ftype = None\n",
   "        ext = os.path.splitext(cfile)[-1]\n",
   "        if ext in ('.ini', '.cfg'):\n",
   "            ftype = 'ini'\n",
   "        elif ext in ('.yaml', '.yml'):\n",
   "            ftype = 'yaml'\n",
   "            raise AnsibleOptionsError(\"Unsupported configuration file extension for %s: %s\" % (cfile, to_native(ext)))\n",
   "    return ftype\n",
   "def get_ini_config_value(p, entry):\n",
   "    value = None\n",
   "            value = p.get(entry.get('section', 'defaults'), entry.get('key', ''), raw=True)\n",
   "    return value\n",
   "def find_ini_config_file(warnings=None):\n",
   "        warnings = set()\n",
   "    SENTINEL = object\n",
   "    potential_paths = []\n",
   "    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n",
   "    if path_from_env is not SENTINEL:\n",
   "        path_from_env = unfrackpath(path_from_env, follow=False)\n",
   "        if os.path.isdir(to_bytes(path_from_env)):\n",
   "            path_from_env = os.path.join(path_from_env, \"ansible.cfg\")\n",
   "        potential_paths.append(path_from_env)\n",
   "    warn_cmd_public = False\n",
   "        cwd = os.getcwd()\n",
   "        perms = os.stat(cwd)\n",
   "        cwd_cfg = os.path.join(cwd, \"ansible.cfg\")\n",
   "        if perms.st_mode & stat.S_IWOTH:\n",
   "            if os.path.exists(cwd_cfg):\n",
   "                warn_cmd_public = True\n",
   "            potential_paths.append(to_text(cwd_cfg, errors='surrogate_or_strict'))\n",
   "    potential_paths.append(unfrackpath(\"~/.ansible.cfg\", follow=False))\n",
   "    potential_paths.append(\"/etc/ansible/ansible.cfg\")\n",
   "    for path in potential_paths:\n",
   "        b_path = to_bytes(path)\n",
   "        if os.path.exists(b_path) and os.access(b_path, os.R_OK):\n",
   "        path = None\n",
   "    if path_from_env != path and warn_cmd_public:\n",
   "        warnings.add(u\"Ansible is being run in a world writable directory (%s),\"\n",
   "                     % to_text(cwd))\n",
   "    return path\n",
   "        yml_file = to_bytes(yml_file)\n",
   "        if os.path.exists(yml_file):\n",
   "            with open(yml_file, 'rb') as config_def:\n",
   "                return yaml_load(config_def, Loader=SafeLoader) or {}\n",
   "            \"Missing base YAML definition file (bad install?): %s\" % to_native(yml_file))\n",
   "            cfile = self._config_file\n",
   "        ftype = get_config_type(cfile)\n",
   "        if cfile is not None:\n",
   "            if ftype == 'ini':\n",
   "                self._parsers[cfile] = configparser.ConfigParser()\n",
   "                with open(to_bytes(cfile), 'rb') as f:\n",
   "                        cfg_text = to_text(f.read(), errors='surrogate_or_strict')\n",
   "                        raise AnsibleOptionsError(\"Error reading config file(%s) because the config file was not utf8 encoded: %s\" % (cfile, to_native(e)))\n",
   "                        self._parsers[cfile].read_string(cfg_text)\n",
   "                        cfg_file = io.StringIO(cfg_text)\n",
   "                        self._parsers[cfile].readfp(cfg_file)\n",
   "                    raise AnsibleOptionsError(\"Error reading config file (%s): %s\" % (cfile, to_native(e)))\n",
   "                raise AnsibleOptionsError(\"Unsupported configuration file type: %s\" % to_native(ftype))\n",
   "        options = {}\n",
   "        defs = self.get_configuration_definitions(plugin_type, name)\n",
   "        for option in defs:\n",
   "            options[option] = self.get_config_value(option, plugin_type=plugin_type, plugin_name=name, keys=keys, variables=variables, direct=direct)\n",
   "        return options\n",
   "        pvars = []\n",
   "        for pdef in self.get_configuration_definitions(plugin_type, name).values():\n",
   "            if 'vars' in pdef and pdef['vars']:\n",
   "                for var_entry in pdef['vars']:\n",
   "                    pvars.append(var_entry['name'])\n",
   "        return pvars\n",
   "        ret = {}\n",
   "            ret = self._base_defs.get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(plugin_name, {}).get(name, None)\n",
   "        return ret\n",
   "        ret = {}\n",
   "            ret = self._base_defs\n",
   "            ret = self._plugins.get(plugin_type, {})\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, {})\n",
   "        return ret\n",
   "        value = None\n",
   "        origin = None\n",
   "        for entry in entry_list:\n",
   "            name = entry.get('name')\n",
   "                temp_value = container.get(name, None)\n",
   "                self.WARNINGS.add(u'value for config entry {0} contains invalid characters, ignoring...'.format(to_text(name)))\n",
   "            if temp_value is not None:  # only set if entry is defined in container\n",
   "                if isinstance(temp_value, AnsibleVaultEncryptedUnicode):\n",
   "                    temp_value = to_text(temp_value, errors='surrogate_or_strict')\n",
   "                value = temp_value\n",
   "                origin = name\n",
   "                if 'deprecated' in entry:\n",
   "                    self.DEPRECATED.append((entry['name'], entry['deprecated']))\n",
   "        return value, origin\n",
   "            value, _drop = self.get_config_value_and_origin(config, cfile=cfile, plugin_type=plugin_type, plugin_name=plugin_name,\n",
   "        return value\n",
   "        if cfile is None:\n",
   "            cfile = self._config_file\n",
   "        value = None\n",
   "        origin = None\n",
   "        defs = self.get_configuration_definitions(plugin_type, plugin_name)\n",
   "        if config in defs:\n",
   "            direct_aliases = []\n",
   "                direct_aliases = [direct[alias] for alias in defs[config].get('aliases', []) if alias in direct]\n",
   "                value = direct[config]\n",
   "                origin = 'Direct'\n",
   "            elif direct and direct_aliases:\n",
   "                value = direct_aliases[0]\n",
   "                origin = 'Direct'\n",
   "                if variables and defs[config].get('vars'):\n",
   "                    value, origin = self._loop_entries(variables, defs[config]['vars'])\n",
   "                    origin = 'var: %s' % origin\n",
   "                if value is None and keys and config in keys:\n",
   "                    value, origin = keys[config], 'keyword'\n",
   "                    origin = 'keyword: %s' % origin\n",
   "                if value is None and defs[config].get('env'):\n",
   "                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n",
   "                    origin = 'env: %s' % origin\n",
   "                if self._parsers.get(cfile, None) is None:\n",
   "                    self._parse_config_file(cfile)\n",
   "                if value is None and cfile is not None:\n",
   "                    ftype = get_config_type(cfile)\n",
   "                    if ftype and defs[config].get(ftype):\n",
   "                        if ftype == 'ini':\n",
   "                                for ini_entry in defs[config]['ini']:\n",
   "                                    temp_value = get_ini_config_value(self._parsers[cfile], ini_entry)\n",
   "                                    if temp_value is not None:\n",
   "                                        value = temp_value\n",
   "                                        origin = cfile\n",
   "                                        if 'deprecated' in ini_entry:\n",
   "                                            self.DEPRECATED.append(('[%s]%s' % (ini_entry['section'], ini_entry['key']), ini_entry['deprecated']))\n",
   "                                sys.stderr.write(\"Error while loading ini config %s: %s\" % (cfile, to_native(e)))\n",
   "                        elif ftype == 'yaml':\n",
   "                            origin = cfile\n",
   "                if value is None:\n",
   "                    if defs[config].get('required', False):\n",
   "                        if not plugin_type or config not in INTERNAL_DEFS.get(plugin_type, {}):\n",
   "                                               to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "                        value = defs[config].get('default')\n",
   "                        origin = 'default'\n",
   "                        if plugin_type is None and isinstance(value, string_types) and (value.startswith('{{') and value.endswith('}}')):\n",
   "                            return value, origin\n",
   "                value = ensure_type(value, defs[config].get('type'), origin=origin)\n",
   "                if origin.startswith('env:') and value == '':\n",
   "                    origin = 'default'\n",
   "                    value = ensure_type(defs[config].get('default'), defs[config].get('type'), origin=origin)\n",
   "                                              (to_native(_get_entry(plugin_type, plugin_name, config)), to_native(e)))\n",
   "            if 'deprecated' in defs[config] and origin != 'default':\n",
   "                self.DEPRECATED.append((config, defs[config].get('deprecated')))\n",
   "            raise AnsibleError('Requested entry (%s) was not defined in configuration.' % to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "        return value, origin\n",
   "        if plugin_type not in self._plugins:\n",
   "            self._plugins[plugin_type] = {}\n",
   "        self._plugins[plugin_type][name] = defs\n",
   "        if defs is None:\n",
   "        if not isinstance(defs, dict):\n",
   "            raise AnsibleOptionsError(\"Invalid configuration definition type: %s for %s\" % (type(defs), defs))\n",
   "        for config in defs:\n",
   "            if not isinstance(defs[config], dict):\n",
   "                raise AnsibleOptionsError(\"Invalid configuration definition '%s': type is %s\" % (to_native(config), type(defs[config])))\n",
   "                value, origin = self.get_config_value_and_origin(config, configfile)\n",
   "                raise AnsibleError(\"Invalid settings supplied for %s: %s\\n\" % (config, to_native(e)), orig_exc=e)\n",
   "            self.data.update_setting(Setting(config, value, origin, defs[config].get('type', 'string')))\n"
  ]
 },
 "34": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "387",
  "column": "12",
  "context": "None\n        for entry in entry_list:\n            name = entry.get('name')\n            try:\n                temp_value = cont",
  "context_lines": "        ''' repeat code for value entry assignment '''\n\n        value = None\n        origin = None\n        for entry in entry_list:\n            name = entry.get('name')\n            try:\n                temp_value = container.get(name, None)\n            except UnicodeEncodeError:\n                self.WARNINGS.add(u'value for config entry {0} contains invalid characters, ignoring...'.format(to_text(name)))\n",
  "slicing": [
   "INTERNAL_DEFS = {'lookup': ('_terms',)}\n",
   "def _get_entry(plugin_type, plugin_name, config):\n",
   "    entry = ''\n",
   "        entry += 'plugin_type: %s ' % plugin_type\n",
   "            entry += 'plugin: %s ' % plugin_name\n",
   "    entry += 'setting: %s ' % config\n",
   "    return entry\n",
   "def ensure_type(value, value_type, origin=None):\n",
   "    errmsg = ''\n",
   "    basedir = None\n",
   "        basedir = origin\n",
   "        value_type = value_type.lower()\n",
   "        if value_type in ('boolean', 'bool'):\n",
   "            value = boolean(value, strict=False)\n",
   "        elif value_type in ('integer', 'int'):\n",
   "            value = int(value)\n",
   "        elif value_type == 'float':\n",
   "            value = float(value)\n",
   "        elif value_type == 'list':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            elif not isinstance(value, Sequence):\n",
   "                errmsg = 'list'\n",
   "        elif value_type == 'none':\n",
   "            if value == \"None\":\n",
   "                value = None\n",
   "            if value is not None:\n",
   "                errmsg = 'None'\n",
   "        elif value_type == 'path':\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                errmsg = 'path'\n",
   "        elif value_type in ('tmp', 'temppath', 'tmppath'):\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                if not os.path.exists(value):\n",
   "                    makedirs_safe(value, 0o700)\n",
   "                prefix = 'ansible-local-%s' % os.getpid()\n",
   "                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n",
   "                atexit.register(cleanup_tmp_file, value, warn=True)\n",
   "                errmsg = 'temppath'\n",
   "        elif value_type == 'pathspec':\n",
   "            if isinstance(value, string_types):\n",
   "                value = value.split(os.pathsep)\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathspec'\n",
   "        elif value_type == 'pathlist':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathlist'\n",
   "        elif value_type in ('str', 'string'):\n",
   "            if isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "                value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "                errmsg = 'string'\n",
   "        elif isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "            value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "        if errmsg:\n",
   "            raise ValueError('Invalid type provided for \"%s\": %s' % (errmsg, to_native(value)))\n",
   "    return to_text(value, errors='surrogate_or_strict', nonstring='passthru')\n",
   "        path = path.replace('{{CWD}}', os.getcwd())\n",
   "    return unfrackpath(path, follow=False, basedir=basedir)\n",
   "def get_config_type(cfile):\n",
   "    ftype = None\n",
   "        ext = os.path.splitext(cfile)[-1]\n",
   "        if ext in ('.ini', '.cfg'):\n",
   "            ftype = 'ini'\n",
   "        elif ext in ('.yaml', '.yml'):\n",
   "            ftype = 'yaml'\n",
   "            raise AnsibleOptionsError(\"Unsupported configuration file extension for %s: %s\" % (cfile, to_native(ext)))\n",
   "    return ftype\n",
   "def get_ini_config_value(p, entry):\n",
   "    value = None\n",
   "            value = p.get(entry.get('section', 'defaults'), entry.get('key', ''), raw=True)\n",
   "    return value\n",
   "def find_ini_config_file(warnings=None):\n",
   "        warnings = set()\n",
   "    SENTINEL = object\n",
   "    potential_paths = []\n",
   "    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n",
   "    if path_from_env is not SENTINEL:\n",
   "        path_from_env = unfrackpath(path_from_env, follow=False)\n",
   "        if os.path.isdir(to_bytes(path_from_env)):\n",
   "            path_from_env = os.path.join(path_from_env, \"ansible.cfg\")\n",
   "        potential_paths.append(path_from_env)\n",
   "    warn_cmd_public = False\n",
   "        cwd = os.getcwd()\n",
   "        perms = os.stat(cwd)\n",
   "        cwd_cfg = os.path.join(cwd, \"ansible.cfg\")\n",
   "        if perms.st_mode & stat.S_IWOTH:\n",
   "            if os.path.exists(cwd_cfg):\n",
   "                warn_cmd_public = True\n",
   "            potential_paths.append(to_text(cwd_cfg, errors='surrogate_or_strict'))\n",
   "    potential_paths.append(unfrackpath(\"~/.ansible.cfg\", follow=False))\n",
   "    potential_paths.append(\"/etc/ansible/ansible.cfg\")\n",
   "    for path in potential_paths:\n",
   "        b_path = to_bytes(path)\n",
   "        if os.path.exists(b_path) and os.access(b_path, os.R_OK):\n",
   "        path = None\n",
   "    if path_from_env != path and warn_cmd_public:\n",
   "        warnings.add(u\"Ansible is being run in a world writable directory (%s),\"\n",
   "                     % to_text(cwd))\n",
   "    return path\n",
   "        yml_file = to_bytes(yml_file)\n",
   "        if os.path.exists(yml_file):\n",
   "            with open(yml_file, 'rb') as config_def:\n",
   "                return yaml_load(config_def, Loader=SafeLoader) or {}\n",
   "            \"Missing base YAML definition file (bad install?): %s\" % to_native(yml_file))\n",
   "            cfile = self._config_file\n",
   "        ftype = get_config_type(cfile)\n",
   "        if cfile is not None:\n",
   "            if ftype == 'ini':\n",
   "                self._parsers[cfile] = configparser.ConfigParser()\n",
   "                with open(to_bytes(cfile), 'rb') as f:\n",
   "                        cfg_text = to_text(f.read(), errors='surrogate_or_strict')\n",
   "                        raise AnsibleOptionsError(\"Error reading config file(%s) because the config file was not utf8 encoded: %s\" % (cfile, to_native(e)))\n",
   "                        self._parsers[cfile].read_string(cfg_text)\n",
   "                        cfg_file = io.StringIO(cfg_text)\n",
   "                        self._parsers[cfile].readfp(cfg_file)\n",
   "                    raise AnsibleOptionsError(\"Error reading config file (%s): %s\" % (cfile, to_native(e)))\n",
   "                raise AnsibleOptionsError(\"Unsupported configuration file type: %s\" % to_native(ftype))\n",
   "        options = {}\n",
   "        defs = self.get_configuration_definitions(plugin_type, name)\n",
   "        for option in defs:\n",
   "            options[option] = self.get_config_value(option, plugin_type=plugin_type, plugin_name=name, keys=keys, variables=variables, direct=direct)\n",
   "        return options\n",
   "        pvars = []\n",
   "        for pdef in self.get_configuration_definitions(plugin_type, name).values():\n",
   "            if 'vars' in pdef and pdef['vars']:\n",
   "                for var_entry in pdef['vars']:\n",
   "                    pvars.append(var_entry['name'])\n",
   "        return pvars\n",
   "        ret = {}\n",
   "            ret = self._base_defs.get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(plugin_name, {}).get(name, None)\n",
   "        return ret\n",
   "        ret = {}\n",
   "            ret = self._base_defs\n",
   "            ret = self._plugins.get(plugin_type, {})\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, {})\n",
   "        return ret\n",
   "        value = None\n",
   "        origin = None\n",
   "        for entry in entry_list:\n",
   "            name = entry.get('name')\n",
   "                temp_value = container.get(name, None)\n",
   "                self.WARNINGS.add(u'value for config entry {0} contains invalid characters, ignoring...'.format(to_text(name)))\n",
   "            if temp_value is not None:  # only set if entry is defined in container\n",
   "                if isinstance(temp_value, AnsibleVaultEncryptedUnicode):\n",
   "                    temp_value = to_text(temp_value, errors='surrogate_or_strict')\n",
   "                value = temp_value\n",
   "                origin = name\n",
   "                if 'deprecated' in entry:\n",
   "                    self.DEPRECATED.append((entry['name'], entry['deprecated']))\n",
   "        return value, origin\n",
   "            value, _drop = self.get_config_value_and_origin(config, cfile=cfile, plugin_type=plugin_type, plugin_name=plugin_name,\n",
   "        return value\n",
   "        if cfile is None:\n",
   "            cfile = self._config_file\n",
   "        value = None\n",
   "        origin = None\n",
   "        defs = self.get_configuration_definitions(plugin_type, plugin_name)\n",
   "        if config in defs:\n",
   "            direct_aliases = []\n",
   "                direct_aliases = [direct[alias] for alias in defs[config].get('aliases', []) if alias in direct]\n",
   "                value = direct[config]\n",
   "                origin = 'Direct'\n",
   "            elif direct and direct_aliases:\n",
   "                value = direct_aliases[0]\n",
   "                origin = 'Direct'\n",
   "                if variables and defs[config].get('vars'):\n",
   "                    value, origin = self._loop_entries(variables, defs[config]['vars'])\n",
   "                    origin = 'var: %s' % origin\n",
   "                if value is None and keys and config in keys:\n",
   "                    value, origin = keys[config], 'keyword'\n",
   "                    origin = 'keyword: %s' % origin\n",
   "                if value is None and defs[config].get('env'):\n",
   "                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n",
   "                    origin = 'env: %s' % origin\n",
   "                if self._parsers.get(cfile, None) is None:\n",
   "                    self._parse_config_file(cfile)\n",
   "                if value is None and cfile is not None:\n",
   "                    ftype = get_config_type(cfile)\n",
   "                    if ftype and defs[config].get(ftype):\n",
   "                        if ftype == 'ini':\n",
   "                                for ini_entry in defs[config]['ini']:\n",
   "                                    temp_value = get_ini_config_value(self._parsers[cfile], ini_entry)\n",
   "                                    if temp_value is not None:\n",
   "                                        value = temp_value\n",
   "                                        origin = cfile\n",
   "                                        if 'deprecated' in ini_entry:\n",
   "                                            self.DEPRECATED.append(('[%s]%s' % (ini_entry['section'], ini_entry['key']), ini_entry['deprecated']))\n",
   "                                sys.stderr.write(\"Error while loading ini config %s: %s\" % (cfile, to_native(e)))\n",
   "                        elif ftype == 'yaml':\n",
   "                            origin = cfile\n",
   "                if value is None:\n",
   "                    if defs[config].get('required', False):\n",
   "                        if not plugin_type or config not in INTERNAL_DEFS.get(plugin_type, {}):\n",
   "                                               to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "                        value = defs[config].get('default')\n",
   "                        origin = 'default'\n",
   "                        if plugin_type is None and isinstance(value, string_types) and (value.startswith('{{') and value.endswith('}}')):\n",
   "                            return value, origin\n",
   "                value = ensure_type(value, defs[config].get('type'), origin=origin)\n",
   "                if origin.startswith('env:') and value == '':\n",
   "                    origin = 'default'\n",
   "                    value = ensure_type(defs[config].get('default'), defs[config].get('type'), origin=origin)\n",
   "                                              (to_native(_get_entry(plugin_type, plugin_name, config)), to_native(e)))\n",
   "            if 'deprecated' in defs[config] and origin != 'default':\n",
   "                self.DEPRECATED.append((config, defs[config].get('deprecated')))\n",
   "            raise AnsibleError('Requested entry (%s) was not defined in configuration.' % to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "        return value, origin\n",
   "        if plugin_type not in self._plugins:\n",
   "            self._plugins[plugin_type] = {}\n",
   "        self._plugins[plugin_type][name] = defs\n",
   "        if defs is None:\n",
   "        if not isinstance(defs, dict):\n",
   "            raise AnsibleOptionsError(\"Invalid configuration definition type: %s for %s\" % (type(defs), defs))\n",
   "        for config in defs:\n",
   "            if not isinstance(defs[config], dict):\n",
   "                raise AnsibleOptionsError(\"Invalid configuration definition '%s': type is %s\" % (to_native(config), type(defs[config])))\n",
   "                value, origin = self.get_config_value_and_origin(config, configfile)\n",
   "                raise AnsibleError(\"Invalid settings supplied for %s: %s\\n\" % (config, to_native(e)), orig_exc=e)\n",
   "            self.data.update_setting(Setting(config, value, origin, defs[config].get('type', 'string')))\n"
  ]
 },
 "35": {
  "name": "cfile",
  "type": "NoneType",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "423",
  "column": "12",
  "context": "one:\n            # use default config\n            cfile = self._config_file\n\n        # Note: sources that are lists listed in ",
  "context_lines": "    def get_config_value_and_origin(self, config, cfile=None, plugin_type=None, plugin_name=None, keys=None, variables=None, direct=None):\n        ''' Given a config key figure out the actual value and report on the origin of the settings '''\n        if cfile is None:\n            # use default config\n            cfile = self._config_file\n\n        # Note: sources that are lists listed in low to high precedence (last one wins)\n        value = None\n        origin = None\n\n",
  "slicing": [
   "INTERNAL_DEFS = {'lookup': ('_terms',)}\n",
   "def _get_entry(plugin_type, plugin_name, config):\n",
   "    entry = ''\n",
   "        entry += 'plugin_type: %s ' % plugin_type\n",
   "            entry += 'plugin: %s ' % plugin_name\n",
   "    entry += 'setting: %s ' % config\n",
   "    return entry\n",
   "def ensure_type(value, value_type, origin=None):\n",
   "    errmsg = ''\n",
   "    basedir = None\n",
   "        basedir = origin\n",
   "        value_type = value_type.lower()\n",
   "        if value_type in ('boolean', 'bool'):\n",
   "            value = boolean(value, strict=False)\n",
   "        elif value_type in ('integer', 'int'):\n",
   "            value = int(value)\n",
   "        elif value_type == 'float':\n",
   "            value = float(value)\n",
   "        elif value_type == 'list':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            elif not isinstance(value, Sequence):\n",
   "                errmsg = 'list'\n",
   "        elif value_type == 'none':\n",
   "            if value == \"None\":\n",
   "                value = None\n",
   "            if value is not None:\n",
   "                errmsg = 'None'\n",
   "        elif value_type == 'path':\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                errmsg = 'path'\n",
   "        elif value_type in ('tmp', 'temppath', 'tmppath'):\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                if not os.path.exists(value):\n",
   "                    makedirs_safe(value, 0o700)\n",
   "                prefix = 'ansible-local-%s' % os.getpid()\n",
   "                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n",
   "                atexit.register(cleanup_tmp_file, value, warn=True)\n",
   "                errmsg = 'temppath'\n",
   "        elif value_type == 'pathspec':\n",
   "            if isinstance(value, string_types):\n",
   "                value = value.split(os.pathsep)\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathspec'\n",
   "        elif value_type == 'pathlist':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathlist'\n",
   "        elif value_type in ('str', 'string'):\n",
   "            if isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "                value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "                errmsg = 'string'\n",
   "        elif isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "            value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "        if errmsg:\n",
   "            raise ValueError('Invalid type provided for \"%s\": %s' % (errmsg, to_native(value)))\n",
   "    return to_text(value, errors='surrogate_or_strict', nonstring='passthru')\n",
   "        path = path.replace('{{CWD}}', os.getcwd())\n",
   "    return unfrackpath(path, follow=False, basedir=basedir)\n",
   "def get_config_type(cfile):\n",
   "    ftype = None\n",
   "        ext = os.path.splitext(cfile)[-1]\n",
   "        if ext in ('.ini', '.cfg'):\n",
   "            ftype = 'ini'\n",
   "        elif ext in ('.yaml', '.yml'):\n",
   "            ftype = 'yaml'\n",
   "            raise AnsibleOptionsError(\"Unsupported configuration file extension for %s: %s\" % (cfile, to_native(ext)))\n",
   "    return ftype\n",
   "def get_ini_config_value(p, entry):\n",
   "    value = None\n",
   "            value = p.get(entry.get('section', 'defaults'), entry.get('key', ''), raw=True)\n",
   "    return value\n",
   "def find_ini_config_file(warnings=None):\n",
   "        warnings = set()\n",
   "    SENTINEL = object\n",
   "    potential_paths = []\n",
   "    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n",
   "    if path_from_env is not SENTINEL:\n",
   "        path_from_env = unfrackpath(path_from_env, follow=False)\n",
   "        if os.path.isdir(to_bytes(path_from_env)):\n",
   "            path_from_env = os.path.join(path_from_env, \"ansible.cfg\")\n",
   "        potential_paths.append(path_from_env)\n",
   "    warn_cmd_public = False\n",
   "        cwd = os.getcwd()\n",
   "        perms = os.stat(cwd)\n",
   "        cwd_cfg = os.path.join(cwd, \"ansible.cfg\")\n",
   "        if perms.st_mode & stat.S_IWOTH:\n",
   "            if os.path.exists(cwd_cfg):\n",
   "                warn_cmd_public = True\n",
   "            potential_paths.append(to_text(cwd_cfg, errors='surrogate_or_strict'))\n",
   "    potential_paths.append(unfrackpath(\"~/.ansible.cfg\", follow=False))\n",
   "    potential_paths.append(\"/etc/ansible/ansible.cfg\")\n",
   "    for path in potential_paths:\n",
   "        b_path = to_bytes(path)\n",
   "        if os.path.exists(b_path) and os.access(b_path, os.R_OK):\n",
   "        path = None\n",
   "    if path_from_env != path and warn_cmd_public:\n",
   "        warnings.add(u\"Ansible is being run in a world writable directory (%s),\"\n",
   "                     % to_text(cwd))\n",
   "    return path\n",
   "        yml_file = to_bytes(yml_file)\n",
   "        if os.path.exists(yml_file):\n",
   "            with open(yml_file, 'rb') as config_def:\n",
   "                return yaml_load(config_def, Loader=SafeLoader) or {}\n",
   "            \"Missing base YAML definition file (bad install?): %s\" % to_native(yml_file))\n",
   "            cfile = self._config_file\n",
   "        ftype = get_config_type(cfile)\n",
   "        if cfile is not None:\n",
   "            if ftype == 'ini':\n",
   "                self._parsers[cfile] = configparser.ConfigParser()\n",
   "                with open(to_bytes(cfile), 'rb') as f:\n",
   "                        cfg_text = to_text(f.read(), errors='surrogate_or_strict')\n",
   "                        raise AnsibleOptionsError(\"Error reading config file(%s) because the config file was not utf8 encoded: %s\" % (cfile, to_native(e)))\n",
   "                        self._parsers[cfile].read_string(cfg_text)\n",
   "                        cfg_file = io.StringIO(cfg_text)\n",
   "                        self._parsers[cfile].readfp(cfg_file)\n",
   "                    raise AnsibleOptionsError(\"Error reading config file (%s): %s\" % (cfile, to_native(e)))\n",
   "                raise AnsibleOptionsError(\"Unsupported configuration file type: %s\" % to_native(ftype))\n",
   "        options = {}\n",
   "        defs = self.get_configuration_definitions(plugin_type, name)\n",
   "        for option in defs:\n",
   "            options[option] = self.get_config_value(option, plugin_type=plugin_type, plugin_name=name, keys=keys, variables=variables, direct=direct)\n",
   "        return options\n",
   "        pvars = []\n",
   "        for pdef in self.get_configuration_definitions(plugin_type, name).values():\n",
   "            if 'vars' in pdef and pdef['vars']:\n",
   "                for var_entry in pdef['vars']:\n",
   "                    pvars.append(var_entry['name'])\n",
   "        return pvars\n",
   "        ret = {}\n",
   "            ret = self._base_defs.get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(plugin_name, {}).get(name, None)\n",
   "        return ret\n",
   "        ret = {}\n",
   "            ret = self._base_defs\n",
   "            ret = self._plugins.get(plugin_type, {})\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, {})\n",
   "        return ret\n",
   "        value = None\n",
   "        origin = None\n",
   "        for entry in entry_list:\n",
   "            name = entry.get('name')\n",
   "                temp_value = container.get(name, None)\n",
   "                self.WARNINGS.add(u'value for config entry {0} contains invalid characters, ignoring...'.format(to_text(name)))\n",
   "            if temp_value is not None:  # only set if entry is defined in container\n",
   "                if isinstance(temp_value, AnsibleVaultEncryptedUnicode):\n",
   "                    temp_value = to_text(temp_value, errors='surrogate_or_strict')\n",
   "                value = temp_value\n",
   "                origin = name\n",
   "                if 'deprecated' in entry:\n",
   "                    self.DEPRECATED.append((entry['name'], entry['deprecated']))\n",
   "        return value, origin\n",
   "            value, _drop = self.get_config_value_and_origin(config, cfile=cfile, plugin_type=plugin_type, plugin_name=plugin_name,\n",
   "        return value\n",
   "        if cfile is None:\n",
   "            cfile = self._config_file\n",
   "        value = None\n",
   "        origin = None\n",
   "        defs = self.get_configuration_definitions(plugin_type, plugin_name)\n",
   "        if config in defs:\n",
   "            direct_aliases = []\n",
   "                direct_aliases = [direct[alias] for alias in defs[config].get('aliases', []) if alias in direct]\n",
   "                value = direct[config]\n",
   "                origin = 'Direct'\n",
   "            elif direct and direct_aliases:\n",
   "                value = direct_aliases[0]\n",
   "                origin = 'Direct'\n",
   "                if variables and defs[config].get('vars'):\n",
   "                    value, origin = self._loop_entries(variables, defs[config]['vars'])\n",
   "                    origin = 'var: %s' % origin\n",
   "                if value is None and keys and config in keys:\n",
   "                    value, origin = keys[config], 'keyword'\n",
   "                    origin = 'keyword: %s' % origin\n",
   "                if value is None and defs[config].get('env'):\n",
   "                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n",
   "                    origin = 'env: %s' % origin\n",
   "                if self._parsers.get(cfile, None) is None:\n",
   "                    self._parse_config_file(cfile)\n",
   "                if value is None and cfile is not None:\n",
   "                    ftype = get_config_type(cfile)\n",
   "                    if ftype and defs[config].get(ftype):\n",
   "                        if ftype == 'ini':\n",
   "                                for ini_entry in defs[config]['ini']:\n",
   "                                    temp_value = get_ini_config_value(self._parsers[cfile], ini_entry)\n",
   "                                    if temp_value is not None:\n",
   "                                        value = temp_value\n",
   "                                        origin = cfile\n",
   "                                        if 'deprecated' in ini_entry:\n",
   "                                            self.DEPRECATED.append(('[%s]%s' % (ini_entry['section'], ini_entry['key']), ini_entry['deprecated']))\n",
   "                                sys.stderr.write(\"Error while loading ini config %s: %s\" % (cfile, to_native(e)))\n",
   "                        elif ftype == 'yaml':\n",
   "                            origin = cfile\n",
   "                if value is None:\n",
   "                    if defs[config].get('required', False):\n",
   "                        if not plugin_type or config not in INTERNAL_DEFS.get(plugin_type, {}):\n",
   "                                               to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "                        value = defs[config].get('default')\n",
   "                        origin = 'default'\n",
   "                        if plugin_type is None and isinstance(value, string_types) and (value.startswith('{{') and value.endswith('}}')):\n",
   "                            return value, origin\n",
   "                value = ensure_type(value, defs[config].get('type'), origin=origin)\n",
   "                if origin.startswith('env:') and value == '':\n",
   "                    origin = 'default'\n",
   "                    value = ensure_type(defs[config].get('default'), defs[config].get('type'), origin=origin)\n",
   "                                              (to_native(_get_entry(plugin_type, plugin_name, config)), to_native(e)))\n",
   "            if 'deprecated' in defs[config] and origin != 'default':\n",
   "                self.DEPRECATED.append((config, defs[config].get('deprecated')))\n",
   "            raise AnsibleError('Requested entry (%s) was not defined in configuration.' % to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "        return value, origin\n",
   "        if plugin_type not in self._plugins:\n",
   "            self._plugins[plugin_type] = {}\n",
   "        self._plugins[plugin_type][name] = defs\n",
   "        if defs is None:\n",
   "        if not isinstance(defs, dict):\n",
   "            raise AnsibleOptionsError(\"Invalid configuration definition type: %s for %s\" % (type(defs), defs))\n",
   "        for config in defs:\n",
   "            if not isinstance(defs[config], dict):\n",
   "                raise AnsibleOptionsError(\"Invalid configuration definition '%s': type is %s\" % (to_native(config), type(defs[config])))\n",
   "                value, origin = self.get_config_value_and_origin(config, configfile)\n",
   "                raise AnsibleError(\"Invalid settings supplied for %s: %s\\n\" % (config, to_native(e)), orig_exc=e)\n",
   "            self.data.update_setting(Setting(config, value, origin, defs[config].get('type', 'string')))\n"
  ]
 },
 "36": {
  "name": "value",
  "type": "NoneType",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "426",
  "column": "8",
  "context": "in low to high precedence (last one wins)\n        value = None\n        origin = None\n\n        defs = self.get_con",
  "context_lines": "        if cfile is None:\n            # use default config\n            cfile = self._config_file\n\n        # Note: sources that are lists listed in low to high precedence (last one wins)\n        value = None\n        origin = None\n\n        defs = self.get_configuration_definitions(plugin_type, plugin_name)\n        if config in defs:\n\n            # direct setting via plugin arguments, can set to None so we bypass rest of processing/defaults\n",
  "slicing": [
   "INTERNAL_DEFS = {'lookup': ('_terms',)}\n",
   "def _get_entry(plugin_type, plugin_name, config):\n",
   "    entry = ''\n",
   "        entry += 'plugin_type: %s ' % plugin_type\n",
   "            entry += 'plugin: %s ' % plugin_name\n",
   "    entry += 'setting: %s ' % config\n",
   "    return entry\n",
   "def ensure_type(value, value_type, origin=None):\n",
   "    errmsg = ''\n",
   "    basedir = None\n",
   "        basedir = origin\n",
   "        value_type = value_type.lower()\n",
   "        if value_type in ('boolean', 'bool'):\n",
   "            value = boolean(value, strict=False)\n",
   "        elif value_type in ('integer', 'int'):\n",
   "            value = int(value)\n",
   "        elif value_type == 'float':\n",
   "            value = float(value)\n",
   "        elif value_type == 'list':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            elif not isinstance(value, Sequence):\n",
   "                errmsg = 'list'\n",
   "        elif value_type == 'none':\n",
   "            if value == \"None\":\n",
   "                value = None\n",
   "            if value is not None:\n",
   "                errmsg = 'None'\n",
   "        elif value_type == 'path':\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                errmsg = 'path'\n",
   "        elif value_type in ('tmp', 'temppath', 'tmppath'):\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                if not os.path.exists(value):\n",
   "                    makedirs_safe(value, 0o700)\n",
   "                prefix = 'ansible-local-%s' % os.getpid()\n",
   "                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n",
   "                atexit.register(cleanup_tmp_file, value, warn=True)\n",
   "                errmsg = 'temppath'\n",
   "        elif value_type == 'pathspec':\n",
   "            if isinstance(value, string_types):\n",
   "                value = value.split(os.pathsep)\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathspec'\n",
   "        elif value_type == 'pathlist':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathlist'\n",
   "        elif value_type in ('str', 'string'):\n",
   "            if isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "                value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "                errmsg = 'string'\n",
   "        elif isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "            value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "        if errmsg:\n",
   "            raise ValueError('Invalid type provided for \"%s\": %s' % (errmsg, to_native(value)))\n",
   "    return to_text(value, errors='surrogate_or_strict', nonstring='passthru')\n",
   "        path = path.replace('{{CWD}}', os.getcwd())\n",
   "    return unfrackpath(path, follow=False, basedir=basedir)\n",
   "def get_config_type(cfile):\n",
   "    ftype = None\n",
   "        ext = os.path.splitext(cfile)[-1]\n",
   "        if ext in ('.ini', '.cfg'):\n",
   "            ftype = 'ini'\n",
   "        elif ext in ('.yaml', '.yml'):\n",
   "            ftype = 'yaml'\n",
   "            raise AnsibleOptionsError(\"Unsupported configuration file extension for %s: %s\" % (cfile, to_native(ext)))\n",
   "    return ftype\n",
   "def get_ini_config_value(p, entry):\n",
   "    value = None\n",
   "            value = p.get(entry.get('section', 'defaults'), entry.get('key', ''), raw=True)\n",
   "    return value\n",
   "def find_ini_config_file(warnings=None):\n",
   "        warnings = set()\n",
   "    SENTINEL = object\n",
   "    potential_paths = []\n",
   "    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n",
   "    if path_from_env is not SENTINEL:\n",
   "        path_from_env = unfrackpath(path_from_env, follow=False)\n",
   "        if os.path.isdir(to_bytes(path_from_env)):\n",
   "            path_from_env = os.path.join(path_from_env, \"ansible.cfg\")\n",
   "        potential_paths.append(path_from_env)\n",
   "    warn_cmd_public = False\n",
   "        cwd = os.getcwd()\n",
   "        perms = os.stat(cwd)\n",
   "        cwd_cfg = os.path.join(cwd, \"ansible.cfg\")\n",
   "        if perms.st_mode & stat.S_IWOTH:\n",
   "            if os.path.exists(cwd_cfg):\n",
   "                warn_cmd_public = True\n",
   "            potential_paths.append(to_text(cwd_cfg, errors='surrogate_or_strict'))\n",
   "    potential_paths.append(unfrackpath(\"~/.ansible.cfg\", follow=False))\n",
   "    potential_paths.append(\"/etc/ansible/ansible.cfg\")\n",
   "    for path in potential_paths:\n",
   "        b_path = to_bytes(path)\n",
   "        if os.path.exists(b_path) and os.access(b_path, os.R_OK):\n",
   "        path = None\n",
   "    if path_from_env != path and warn_cmd_public:\n",
   "        warnings.add(u\"Ansible is being run in a world writable directory (%s),\"\n",
   "                     % to_text(cwd))\n",
   "    return path\n",
   "        yml_file = to_bytes(yml_file)\n",
   "        if os.path.exists(yml_file):\n",
   "            with open(yml_file, 'rb') as config_def:\n",
   "                return yaml_load(config_def, Loader=SafeLoader) or {}\n",
   "            \"Missing base YAML definition file (bad install?): %s\" % to_native(yml_file))\n",
   "            cfile = self._config_file\n",
   "        ftype = get_config_type(cfile)\n",
   "        if cfile is not None:\n",
   "            if ftype == 'ini':\n",
   "                self._parsers[cfile] = configparser.ConfigParser()\n",
   "                with open(to_bytes(cfile), 'rb') as f:\n",
   "                        cfg_text = to_text(f.read(), errors='surrogate_or_strict')\n",
   "                        raise AnsibleOptionsError(\"Error reading config file(%s) because the config file was not utf8 encoded: %s\" % (cfile, to_native(e)))\n",
   "                        self._parsers[cfile].read_string(cfg_text)\n",
   "                        cfg_file = io.StringIO(cfg_text)\n",
   "                        self._parsers[cfile].readfp(cfg_file)\n",
   "                    raise AnsibleOptionsError(\"Error reading config file (%s): %s\" % (cfile, to_native(e)))\n",
   "                raise AnsibleOptionsError(\"Unsupported configuration file type: %s\" % to_native(ftype))\n",
   "        options = {}\n",
   "        defs = self.get_configuration_definitions(plugin_type, name)\n",
   "        for option in defs:\n",
   "            options[option] = self.get_config_value(option, plugin_type=plugin_type, plugin_name=name, keys=keys, variables=variables, direct=direct)\n",
   "        return options\n",
   "        pvars = []\n",
   "        for pdef in self.get_configuration_definitions(plugin_type, name).values():\n",
   "            if 'vars' in pdef and pdef['vars']:\n",
   "                for var_entry in pdef['vars']:\n",
   "                    pvars.append(var_entry['name'])\n",
   "        return pvars\n",
   "        ret = {}\n",
   "            ret = self._base_defs.get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(plugin_name, {}).get(name, None)\n",
   "        return ret\n",
   "        ret = {}\n",
   "            ret = self._base_defs\n",
   "            ret = self._plugins.get(plugin_type, {})\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, {})\n",
   "        return ret\n",
   "        value = None\n",
   "        origin = None\n",
   "        for entry in entry_list:\n",
   "            name = entry.get('name')\n",
   "                temp_value = container.get(name, None)\n",
   "                self.WARNINGS.add(u'value for config entry {0} contains invalid characters, ignoring...'.format(to_text(name)))\n",
   "            if temp_value is not None:  # only set if entry is defined in container\n",
   "                if isinstance(temp_value, AnsibleVaultEncryptedUnicode):\n",
   "                    temp_value = to_text(temp_value, errors='surrogate_or_strict')\n",
   "                value = temp_value\n",
   "                origin = name\n",
   "                if 'deprecated' in entry:\n",
   "                    self.DEPRECATED.append((entry['name'], entry['deprecated']))\n",
   "        return value, origin\n",
   "            value, _drop = self.get_config_value_and_origin(config, cfile=cfile, plugin_type=plugin_type, plugin_name=plugin_name,\n",
   "        return value\n",
   "        if cfile is None:\n",
   "            cfile = self._config_file\n",
   "        value = None\n",
   "        origin = None\n",
   "        defs = self.get_configuration_definitions(plugin_type, plugin_name)\n",
   "        if config in defs:\n",
   "            direct_aliases = []\n",
   "                direct_aliases = [direct[alias] for alias in defs[config].get('aliases', []) if alias in direct]\n",
   "                value = direct[config]\n",
   "                origin = 'Direct'\n",
   "            elif direct and direct_aliases:\n",
   "                value = direct_aliases[0]\n",
   "                origin = 'Direct'\n",
   "                if variables and defs[config].get('vars'):\n",
   "                    value, origin = self._loop_entries(variables, defs[config]['vars'])\n",
   "                    origin = 'var: %s' % origin\n",
   "                if value is None and keys and config in keys:\n",
   "                    value, origin = keys[config], 'keyword'\n",
   "                    origin = 'keyword: %s' % origin\n",
   "                if value is None and defs[config].get('env'):\n",
   "                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n",
   "                    origin = 'env: %s' % origin\n",
   "                if self._parsers.get(cfile, None) is None:\n",
   "                    self._parse_config_file(cfile)\n",
   "                if value is None and cfile is not None:\n",
   "                    ftype = get_config_type(cfile)\n",
   "                    if ftype and defs[config].get(ftype):\n",
   "                        if ftype == 'ini':\n",
   "                                for ini_entry in defs[config]['ini']:\n",
   "                                    temp_value = get_ini_config_value(self._parsers[cfile], ini_entry)\n",
   "                                    if temp_value is not None:\n",
   "                                        value = temp_value\n",
   "                                        origin = cfile\n",
   "                                        if 'deprecated' in ini_entry:\n",
   "                                            self.DEPRECATED.append(('[%s]%s' % (ini_entry['section'], ini_entry['key']), ini_entry['deprecated']))\n",
   "                                sys.stderr.write(\"Error while loading ini config %s: %s\" % (cfile, to_native(e)))\n",
   "                        elif ftype == 'yaml':\n",
   "                            origin = cfile\n",
   "                if value is None:\n",
   "                    if defs[config].get('required', False):\n",
   "                        if not plugin_type or config not in INTERNAL_DEFS.get(plugin_type, {}):\n",
   "                                               to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "                        value = defs[config].get('default')\n",
   "                        origin = 'default'\n",
   "                        if plugin_type is None and isinstance(value, string_types) and (value.startswith('{{') and value.endswith('}}')):\n",
   "                            return value, origin\n",
   "                value = ensure_type(value, defs[config].get('type'), origin=origin)\n",
   "                if origin.startswith('env:') and value == '':\n",
   "                    origin = 'default'\n",
   "                    value = ensure_type(defs[config].get('default'), defs[config].get('type'), origin=origin)\n",
   "                                              (to_native(_get_entry(plugin_type, plugin_name, config)), to_native(e)))\n",
   "            if 'deprecated' in defs[config] and origin != 'default':\n",
   "                self.DEPRECATED.append((config, defs[config].get('deprecated')))\n",
   "            raise AnsibleError('Requested entry (%s) was not defined in configuration.' % to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "        return value, origin\n",
   "        if plugin_type not in self._plugins:\n",
   "            self._plugins[plugin_type] = {}\n",
   "        self._plugins[plugin_type][name] = defs\n",
   "        if defs is None:\n",
   "        if not isinstance(defs, dict):\n",
   "            raise AnsibleOptionsError(\"Invalid configuration definition type: %s for %s\" % (type(defs), defs))\n",
   "        for config in defs:\n",
   "            if not isinstance(defs[config], dict):\n",
   "                raise AnsibleOptionsError(\"Invalid configuration definition '%s': type is %s\" % (to_native(config), type(defs[config])))\n",
   "                value, origin = self.get_config_value_and_origin(config, configfile)\n",
   "                raise AnsibleError(\"Invalid settings supplied for %s: %s\\n\" % (config, to_native(e)), orig_exc=e)\n",
   "            self.data.update_setting(Setting(config, value, origin, defs[config].get('type', 'string')))\n"
  ]
 },
 "37": {
  "name": "origin",
  "type": "NoneType",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "427",
  "column": "8",
  "context": "ence (last one wins)\n        value = None\n        origin = None\n\n        defs = self.get_configuration_definitions",
  "context_lines": "            # use default config\n            cfile = self._config_file\n\n        # Note: sources that are lists listed in low to high precedence (last one wins)\n        value = None\n        origin = None\n\n        defs = self.get_configuration_definitions(plugin_type, plugin_name)\n        if config in defs:\n\n            # direct setting via plugin arguments, can set to None so we bypass rest of processing/defaults\n",
  "slicing": [
   "INTERNAL_DEFS = {'lookup': ('_terms',)}\n",
   "def _get_entry(plugin_type, plugin_name, config):\n",
   "    entry = ''\n",
   "        entry += 'plugin_type: %s ' % plugin_type\n",
   "            entry += 'plugin: %s ' % plugin_name\n",
   "    entry += 'setting: %s ' % config\n",
   "    return entry\n",
   "def ensure_type(value, value_type, origin=None):\n",
   "    errmsg = ''\n",
   "    basedir = None\n",
   "        basedir = origin\n",
   "        value_type = value_type.lower()\n",
   "        if value_type in ('boolean', 'bool'):\n",
   "            value = boolean(value, strict=False)\n",
   "        elif value_type in ('integer', 'int'):\n",
   "            value = int(value)\n",
   "        elif value_type == 'float':\n",
   "            value = float(value)\n",
   "        elif value_type == 'list':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            elif not isinstance(value, Sequence):\n",
   "                errmsg = 'list'\n",
   "        elif value_type == 'none':\n",
   "            if value == \"None\":\n",
   "                value = None\n",
   "            if value is not None:\n",
   "                errmsg = 'None'\n",
   "        elif value_type == 'path':\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                errmsg = 'path'\n",
   "        elif value_type in ('tmp', 'temppath', 'tmppath'):\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                if not os.path.exists(value):\n",
   "                    makedirs_safe(value, 0o700)\n",
   "                prefix = 'ansible-local-%s' % os.getpid()\n",
   "                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n",
   "                atexit.register(cleanup_tmp_file, value, warn=True)\n",
   "                errmsg = 'temppath'\n",
   "        elif value_type == 'pathspec':\n",
   "            if isinstance(value, string_types):\n",
   "                value = value.split(os.pathsep)\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathspec'\n",
   "        elif value_type == 'pathlist':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathlist'\n",
   "        elif value_type in ('str', 'string'):\n",
   "            if isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "                value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "                errmsg = 'string'\n",
   "        elif isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "            value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "        if errmsg:\n",
   "            raise ValueError('Invalid type provided for \"%s\": %s' % (errmsg, to_native(value)))\n",
   "    return to_text(value, errors='surrogate_or_strict', nonstring='passthru')\n",
   "        path = path.replace('{{CWD}}', os.getcwd())\n",
   "    return unfrackpath(path, follow=False, basedir=basedir)\n",
   "def get_config_type(cfile):\n",
   "    ftype = None\n",
   "        ext = os.path.splitext(cfile)[-1]\n",
   "        if ext in ('.ini', '.cfg'):\n",
   "            ftype = 'ini'\n",
   "        elif ext in ('.yaml', '.yml'):\n",
   "            ftype = 'yaml'\n",
   "            raise AnsibleOptionsError(\"Unsupported configuration file extension for %s: %s\" % (cfile, to_native(ext)))\n",
   "    return ftype\n",
   "def get_ini_config_value(p, entry):\n",
   "    value = None\n",
   "            value = p.get(entry.get('section', 'defaults'), entry.get('key', ''), raw=True)\n",
   "    return value\n",
   "def find_ini_config_file(warnings=None):\n",
   "        warnings = set()\n",
   "    SENTINEL = object\n",
   "    potential_paths = []\n",
   "    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n",
   "    if path_from_env is not SENTINEL:\n",
   "        path_from_env = unfrackpath(path_from_env, follow=False)\n",
   "        if os.path.isdir(to_bytes(path_from_env)):\n",
   "            path_from_env = os.path.join(path_from_env, \"ansible.cfg\")\n",
   "        potential_paths.append(path_from_env)\n",
   "    warn_cmd_public = False\n",
   "        cwd = os.getcwd()\n",
   "        perms = os.stat(cwd)\n",
   "        cwd_cfg = os.path.join(cwd, \"ansible.cfg\")\n",
   "        if perms.st_mode & stat.S_IWOTH:\n",
   "            if os.path.exists(cwd_cfg):\n",
   "                warn_cmd_public = True\n",
   "            potential_paths.append(to_text(cwd_cfg, errors='surrogate_or_strict'))\n",
   "    potential_paths.append(unfrackpath(\"~/.ansible.cfg\", follow=False))\n",
   "    potential_paths.append(\"/etc/ansible/ansible.cfg\")\n",
   "    for path in potential_paths:\n",
   "        b_path = to_bytes(path)\n",
   "        if os.path.exists(b_path) and os.access(b_path, os.R_OK):\n",
   "        path = None\n",
   "    if path_from_env != path and warn_cmd_public:\n",
   "        warnings.add(u\"Ansible is being run in a world writable directory (%s),\"\n",
   "                     % to_text(cwd))\n",
   "    return path\n",
   "        yml_file = to_bytes(yml_file)\n",
   "        if os.path.exists(yml_file):\n",
   "            with open(yml_file, 'rb') as config_def:\n",
   "                return yaml_load(config_def, Loader=SafeLoader) or {}\n",
   "            \"Missing base YAML definition file (bad install?): %s\" % to_native(yml_file))\n",
   "            cfile = self._config_file\n",
   "        ftype = get_config_type(cfile)\n",
   "        if cfile is not None:\n",
   "            if ftype == 'ini':\n",
   "                self._parsers[cfile] = configparser.ConfigParser()\n",
   "                with open(to_bytes(cfile), 'rb') as f:\n",
   "                        cfg_text = to_text(f.read(), errors='surrogate_or_strict')\n",
   "                        raise AnsibleOptionsError(\"Error reading config file(%s) because the config file was not utf8 encoded: %s\" % (cfile, to_native(e)))\n",
   "                        self._parsers[cfile].read_string(cfg_text)\n",
   "                        cfg_file = io.StringIO(cfg_text)\n",
   "                        self._parsers[cfile].readfp(cfg_file)\n",
   "                    raise AnsibleOptionsError(\"Error reading config file (%s): %s\" % (cfile, to_native(e)))\n",
   "                raise AnsibleOptionsError(\"Unsupported configuration file type: %s\" % to_native(ftype))\n",
   "        options = {}\n",
   "        defs = self.get_configuration_definitions(plugin_type, name)\n",
   "        for option in defs:\n",
   "            options[option] = self.get_config_value(option, plugin_type=plugin_type, plugin_name=name, keys=keys, variables=variables, direct=direct)\n",
   "        return options\n",
   "        pvars = []\n",
   "        for pdef in self.get_configuration_definitions(plugin_type, name).values():\n",
   "            if 'vars' in pdef and pdef['vars']:\n",
   "                for var_entry in pdef['vars']:\n",
   "                    pvars.append(var_entry['name'])\n",
   "        return pvars\n",
   "        ret = {}\n",
   "            ret = self._base_defs.get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(plugin_name, {}).get(name, None)\n",
   "        return ret\n",
   "        ret = {}\n",
   "            ret = self._base_defs\n",
   "            ret = self._plugins.get(plugin_type, {})\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, {})\n",
   "        return ret\n",
   "        value = None\n",
   "        origin = None\n",
   "        for entry in entry_list:\n",
   "            name = entry.get('name')\n",
   "                temp_value = container.get(name, None)\n",
   "                self.WARNINGS.add(u'value for config entry {0} contains invalid characters, ignoring...'.format(to_text(name)))\n",
   "            if temp_value is not None:  # only set if entry is defined in container\n",
   "                if isinstance(temp_value, AnsibleVaultEncryptedUnicode):\n",
   "                    temp_value = to_text(temp_value, errors='surrogate_or_strict')\n",
   "                value = temp_value\n",
   "                origin = name\n",
   "                if 'deprecated' in entry:\n",
   "                    self.DEPRECATED.append((entry['name'], entry['deprecated']))\n",
   "        return value, origin\n",
   "            value, _drop = self.get_config_value_and_origin(config, cfile=cfile, plugin_type=plugin_type, plugin_name=plugin_name,\n",
   "        return value\n",
   "        if cfile is None:\n",
   "            cfile = self._config_file\n",
   "        value = None\n",
   "        origin = None\n",
   "        defs = self.get_configuration_definitions(plugin_type, plugin_name)\n",
   "        if config in defs:\n",
   "            direct_aliases = []\n",
   "                direct_aliases = [direct[alias] for alias in defs[config].get('aliases', []) if alias in direct]\n",
   "                value = direct[config]\n",
   "                origin = 'Direct'\n",
   "            elif direct and direct_aliases:\n",
   "                value = direct_aliases[0]\n",
   "                origin = 'Direct'\n",
   "                if variables and defs[config].get('vars'):\n",
   "                    value, origin = self._loop_entries(variables, defs[config]['vars'])\n",
   "                    origin = 'var: %s' % origin\n",
   "                if value is None and keys and config in keys:\n",
   "                    value, origin = keys[config], 'keyword'\n",
   "                    origin = 'keyword: %s' % origin\n",
   "                if value is None and defs[config].get('env'):\n",
   "                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n",
   "                    origin = 'env: %s' % origin\n",
   "                if self._parsers.get(cfile, None) is None:\n",
   "                    self._parse_config_file(cfile)\n",
   "                if value is None and cfile is not None:\n",
   "                    ftype = get_config_type(cfile)\n",
   "                    if ftype and defs[config].get(ftype):\n",
   "                        if ftype == 'ini':\n",
   "                                for ini_entry in defs[config]['ini']:\n",
   "                                    temp_value = get_ini_config_value(self._parsers[cfile], ini_entry)\n",
   "                                    if temp_value is not None:\n",
   "                                        value = temp_value\n",
   "                                        origin = cfile\n",
   "                                        if 'deprecated' in ini_entry:\n",
   "                                            self.DEPRECATED.append(('[%s]%s' % (ini_entry['section'], ini_entry['key']), ini_entry['deprecated']))\n",
   "                                sys.stderr.write(\"Error while loading ini config %s: %s\" % (cfile, to_native(e)))\n",
   "                        elif ftype == 'yaml':\n",
   "                            origin = cfile\n",
   "                if value is None:\n",
   "                    if defs[config].get('required', False):\n",
   "                        if not plugin_type or config not in INTERNAL_DEFS.get(plugin_type, {}):\n",
   "                                               to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "                        value = defs[config].get('default')\n",
   "                        origin = 'default'\n",
   "                        if plugin_type is None and isinstance(value, string_types) and (value.startswith('{{') and value.endswith('}}')):\n",
   "                            return value, origin\n",
   "                value = ensure_type(value, defs[config].get('type'), origin=origin)\n",
   "                if origin.startswith('env:') and value == '':\n",
   "                    origin = 'default'\n",
   "                    value = ensure_type(defs[config].get('default'), defs[config].get('type'), origin=origin)\n",
   "                                              (to_native(_get_entry(plugin_type, plugin_name, config)), to_native(e)))\n",
   "            if 'deprecated' in defs[config] and origin != 'default':\n",
   "                self.DEPRECATED.append((config, defs[config].get('deprecated')))\n",
   "            raise AnsibleError('Requested entry (%s) was not defined in configuration.' % to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "        return value, origin\n",
   "        if plugin_type not in self._plugins:\n",
   "            self._plugins[plugin_type] = {}\n",
   "        self._plugins[plugin_type][name] = defs\n",
   "        if defs is None:\n",
   "        if not isinstance(defs, dict):\n",
   "            raise AnsibleOptionsError(\"Invalid configuration definition type: %s for %s\" % (type(defs), defs))\n",
   "        for config in defs:\n",
   "            if not isinstance(defs[config], dict):\n",
   "                raise AnsibleOptionsError(\"Invalid configuration definition '%s': type is %s\" % (to_native(config), type(defs[config])))\n",
   "                value, origin = self.get_config_value_and_origin(config, configfile)\n",
   "                raise AnsibleError(\"Invalid settings supplied for %s: %s\\n\" % (config, to_native(e)), orig_exc=e)\n",
   "            self.data.update_setting(Setting(config, value, origin, defs[config].get('type', 'string')))\n"
  ]
 },
 "38": {
  "name": "value",
  "type": "NoneType",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "456",
  "column": "20",
  "context": " and defs[config].get('env'):\n                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n                    origin = 'env: %s' % origin\n\n ",
  "context_lines": "                    value, origin = keys[config], 'keyword'\n                    origin = 'keyword: %s' % origin\n\n                # env vars are next precedence\n                if value is None and defs[config].get('env'):\n                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n                    origin = 'env: %s' % origin\n\n                # try config file entries next, if we have one\n                if self._parsers.get(cfile, None) is None:\n                    self._parse_config_file(cfile)\n\n",
  "slicing": [
   "INTERNAL_DEFS = {'lookup': ('_terms',)}\n",
   "def _get_entry(plugin_type, plugin_name, config):\n",
   "    entry = ''\n",
   "        entry += 'plugin_type: %s ' % plugin_type\n",
   "            entry += 'plugin: %s ' % plugin_name\n",
   "    entry += 'setting: %s ' % config\n",
   "    return entry\n",
   "def ensure_type(value, value_type, origin=None):\n",
   "    errmsg = ''\n",
   "    basedir = None\n",
   "        basedir = origin\n",
   "        value_type = value_type.lower()\n",
   "        if value_type in ('boolean', 'bool'):\n",
   "            value = boolean(value, strict=False)\n",
   "        elif value_type in ('integer', 'int'):\n",
   "            value = int(value)\n",
   "        elif value_type == 'float':\n",
   "            value = float(value)\n",
   "        elif value_type == 'list':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            elif not isinstance(value, Sequence):\n",
   "                errmsg = 'list'\n",
   "        elif value_type == 'none':\n",
   "            if value == \"None\":\n",
   "                value = None\n",
   "            if value is not None:\n",
   "                errmsg = 'None'\n",
   "        elif value_type == 'path':\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                errmsg = 'path'\n",
   "        elif value_type in ('tmp', 'temppath', 'tmppath'):\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                if not os.path.exists(value):\n",
   "                    makedirs_safe(value, 0o700)\n",
   "                prefix = 'ansible-local-%s' % os.getpid()\n",
   "                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n",
   "                atexit.register(cleanup_tmp_file, value, warn=True)\n",
   "                errmsg = 'temppath'\n",
   "        elif value_type == 'pathspec':\n",
   "            if isinstance(value, string_types):\n",
   "                value = value.split(os.pathsep)\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathspec'\n",
   "        elif value_type == 'pathlist':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathlist'\n",
   "        elif value_type in ('str', 'string'):\n",
   "            if isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "                value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "                errmsg = 'string'\n",
   "        elif isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "            value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "        if errmsg:\n",
   "            raise ValueError('Invalid type provided for \"%s\": %s' % (errmsg, to_native(value)))\n",
   "    return to_text(value, errors='surrogate_or_strict', nonstring='passthru')\n",
   "        path = path.replace('{{CWD}}', os.getcwd())\n",
   "    return unfrackpath(path, follow=False, basedir=basedir)\n",
   "def get_config_type(cfile):\n",
   "    ftype = None\n",
   "        ext = os.path.splitext(cfile)[-1]\n",
   "        if ext in ('.ini', '.cfg'):\n",
   "            ftype = 'ini'\n",
   "        elif ext in ('.yaml', '.yml'):\n",
   "            ftype = 'yaml'\n",
   "            raise AnsibleOptionsError(\"Unsupported configuration file extension for %s: %s\" % (cfile, to_native(ext)))\n",
   "    return ftype\n",
   "def get_ini_config_value(p, entry):\n",
   "    value = None\n",
   "            value = p.get(entry.get('section', 'defaults'), entry.get('key', ''), raw=True)\n",
   "    return value\n",
   "def find_ini_config_file(warnings=None):\n",
   "        warnings = set()\n",
   "    SENTINEL = object\n",
   "    potential_paths = []\n",
   "    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n",
   "    if path_from_env is not SENTINEL:\n",
   "        path_from_env = unfrackpath(path_from_env, follow=False)\n",
   "        if os.path.isdir(to_bytes(path_from_env)):\n",
   "            path_from_env = os.path.join(path_from_env, \"ansible.cfg\")\n",
   "        potential_paths.append(path_from_env)\n",
   "    warn_cmd_public = False\n",
   "        cwd = os.getcwd()\n",
   "        perms = os.stat(cwd)\n",
   "        cwd_cfg = os.path.join(cwd, \"ansible.cfg\")\n",
   "        if perms.st_mode & stat.S_IWOTH:\n",
   "            if os.path.exists(cwd_cfg):\n",
   "                warn_cmd_public = True\n",
   "            potential_paths.append(to_text(cwd_cfg, errors='surrogate_or_strict'))\n",
   "    potential_paths.append(unfrackpath(\"~/.ansible.cfg\", follow=False))\n",
   "    potential_paths.append(\"/etc/ansible/ansible.cfg\")\n",
   "    for path in potential_paths:\n",
   "        b_path = to_bytes(path)\n",
   "        if os.path.exists(b_path) and os.access(b_path, os.R_OK):\n",
   "        path = None\n",
   "    if path_from_env != path and warn_cmd_public:\n",
   "        warnings.add(u\"Ansible is being run in a world writable directory (%s),\"\n",
   "                     % to_text(cwd))\n",
   "    return path\n",
   "        yml_file = to_bytes(yml_file)\n",
   "        if os.path.exists(yml_file):\n",
   "            with open(yml_file, 'rb') as config_def:\n",
   "                return yaml_load(config_def, Loader=SafeLoader) or {}\n",
   "            \"Missing base YAML definition file (bad install?): %s\" % to_native(yml_file))\n",
   "            cfile = self._config_file\n",
   "        ftype = get_config_type(cfile)\n",
   "        if cfile is not None:\n",
   "            if ftype == 'ini':\n",
   "                self._parsers[cfile] = configparser.ConfigParser()\n",
   "                with open(to_bytes(cfile), 'rb') as f:\n",
   "                        cfg_text = to_text(f.read(), errors='surrogate_or_strict')\n",
   "                        raise AnsibleOptionsError(\"Error reading config file(%s) because the config file was not utf8 encoded: %s\" % (cfile, to_native(e)))\n",
   "                        self._parsers[cfile].read_string(cfg_text)\n",
   "                        cfg_file = io.StringIO(cfg_text)\n",
   "                        self._parsers[cfile].readfp(cfg_file)\n",
   "                    raise AnsibleOptionsError(\"Error reading config file (%s): %s\" % (cfile, to_native(e)))\n",
   "                raise AnsibleOptionsError(\"Unsupported configuration file type: %s\" % to_native(ftype))\n",
   "        options = {}\n",
   "        defs = self.get_configuration_definitions(plugin_type, name)\n",
   "        for option in defs:\n",
   "            options[option] = self.get_config_value(option, plugin_type=plugin_type, plugin_name=name, keys=keys, variables=variables, direct=direct)\n",
   "        return options\n",
   "        pvars = []\n",
   "        for pdef in self.get_configuration_definitions(plugin_type, name).values():\n",
   "            if 'vars' in pdef and pdef['vars']:\n",
   "                for var_entry in pdef['vars']:\n",
   "                    pvars.append(var_entry['name'])\n",
   "        return pvars\n",
   "        ret = {}\n",
   "            ret = self._base_defs.get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(plugin_name, {}).get(name, None)\n",
   "        return ret\n",
   "        ret = {}\n",
   "            ret = self._base_defs\n",
   "            ret = self._plugins.get(plugin_type, {})\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, {})\n",
   "        return ret\n",
   "        value = None\n",
   "        origin = None\n",
   "        for entry in entry_list:\n",
   "            name = entry.get('name')\n",
   "                temp_value = container.get(name, None)\n",
   "                self.WARNINGS.add(u'value for config entry {0} contains invalid characters, ignoring...'.format(to_text(name)))\n",
   "            if temp_value is not None:  # only set if entry is defined in container\n",
   "                if isinstance(temp_value, AnsibleVaultEncryptedUnicode):\n",
   "                    temp_value = to_text(temp_value, errors='surrogate_or_strict')\n",
   "                value = temp_value\n",
   "                origin = name\n",
   "                if 'deprecated' in entry:\n",
   "                    self.DEPRECATED.append((entry['name'], entry['deprecated']))\n",
   "        return value, origin\n",
   "            value, _drop = self.get_config_value_and_origin(config, cfile=cfile, plugin_type=plugin_type, plugin_name=plugin_name,\n",
   "        return value\n",
   "        if cfile is None:\n",
   "            cfile = self._config_file\n",
   "        value = None\n",
   "        origin = None\n",
   "        defs = self.get_configuration_definitions(plugin_type, plugin_name)\n",
   "        if config in defs:\n",
   "            direct_aliases = []\n",
   "                direct_aliases = [direct[alias] for alias in defs[config].get('aliases', []) if alias in direct]\n",
   "                value = direct[config]\n",
   "                origin = 'Direct'\n",
   "            elif direct and direct_aliases:\n",
   "                value = direct_aliases[0]\n",
   "                origin = 'Direct'\n",
   "                if variables and defs[config].get('vars'):\n",
   "                    value, origin = self._loop_entries(variables, defs[config]['vars'])\n",
   "                    origin = 'var: %s' % origin\n",
   "                if value is None and keys and config in keys:\n",
   "                    value, origin = keys[config], 'keyword'\n",
   "                    origin = 'keyword: %s' % origin\n",
   "                if value is None and defs[config].get('env'):\n",
   "                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n",
   "                    origin = 'env: %s' % origin\n",
   "                if self._parsers.get(cfile, None) is None:\n",
   "                    self._parse_config_file(cfile)\n",
   "                if value is None and cfile is not None:\n",
   "                    ftype = get_config_type(cfile)\n",
   "                    if ftype and defs[config].get(ftype):\n",
   "                        if ftype == 'ini':\n",
   "                                for ini_entry in defs[config]['ini']:\n",
   "                                    temp_value = get_ini_config_value(self._parsers[cfile], ini_entry)\n",
   "                                    if temp_value is not None:\n",
   "                                        value = temp_value\n",
   "                                        origin = cfile\n",
   "                                        if 'deprecated' in ini_entry:\n",
   "                                            self.DEPRECATED.append(('[%s]%s' % (ini_entry['section'], ini_entry['key']), ini_entry['deprecated']))\n",
   "                                sys.stderr.write(\"Error while loading ini config %s: %s\" % (cfile, to_native(e)))\n",
   "                        elif ftype == 'yaml':\n",
   "                            origin = cfile\n",
   "                if value is None:\n",
   "                    if defs[config].get('required', False):\n",
   "                        if not plugin_type or config not in INTERNAL_DEFS.get(plugin_type, {}):\n",
   "                                               to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "                        value = defs[config].get('default')\n",
   "                        origin = 'default'\n",
   "                        if plugin_type is None and isinstance(value, string_types) and (value.startswith('{{') and value.endswith('}}')):\n",
   "                            return value, origin\n",
   "                value = ensure_type(value, defs[config].get('type'), origin=origin)\n",
   "                if origin.startswith('env:') and value == '':\n",
   "                    origin = 'default'\n",
   "                    value = ensure_type(defs[config].get('default'), defs[config].get('type'), origin=origin)\n",
   "                                              (to_native(_get_entry(plugin_type, plugin_name, config)), to_native(e)))\n",
   "            if 'deprecated' in defs[config] and origin != 'default':\n",
   "                self.DEPRECATED.append((config, defs[config].get('deprecated')))\n",
   "            raise AnsibleError('Requested entry (%s) was not defined in configuration.' % to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "        return value, origin\n",
   "        if plugin_type not in self._plugins:\n",
   "            self._plugins[plugin_type] = {}\n",
   "        self._plugins[plugin_type][name] = defs\n",
   "        if defs is None:\n",
   "        if not isinstance(defs, dict):\n",
   "            raise AnsibleOptionsError(\"Invalid configuration definition type: %s for %s\" % (type(defs), defs))\n",
   "        for config in defs:\n",
   "            if not isinstance(defs[config], dict):\n",
   "                raise AnsibleOptionsError(\"Invalid configuration definition '%s': type is %s\" % (to_native(config), type(defs[config])))\n",
   "                value, origin = self.get_config_value_and_origin(config, configfile)\n",
   "                raise AnsibleError(\"Invalid settings supplied for %s: %s\\n\" % (config, to_native(e)), orig_exc=e)\n",
   "            self.data.update_setting(Setting(config, value, origin, defs[config].get('type', 'string')))\n"
  ]
 },
 "39": {
  "name": "origin",
  "type": "NoneType",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "456",
  "column": "27",
  "context": "fs[config].get('env'):\n                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n                    origin = 'env: %s' % origin\n\n ",
  "context_lines": "                    value, origin = keys[config], 'keyword'\n                    origin = 'keyword: %s' % origin\n\n                # env vars are next precedence\n                if value is None and defs[config].get('env'):\n                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n                    origin = 'env: %s' % origin\n\n                # try config file entries next, if we have one\n                if self._parsers.get(cfile, None) is None:\n                    self._parse_config_file(cfile)\n\n",
  "slicing": [
   "INTERNAL_DEFS = {'lookup': ('_terms',)}\n",
   "def _get_entry(plugin_type, plugin_name, config):\n",
   "    entry = ''\n",
   "        entry += 'plugin_type: %s ' % plugin_type\n",
   "            entry += 'plugin: %s ' % plugin_name\n",
   "    entry += 'setting: %s ' % config\n",
   "    return entry\n",
   "def ensure_type(value, value_type, origin=None):\n",
   "    errmsg = ''\n",
   "    basedir = None\n",
   "        basedir = origin\n",
   "        value_type = value_type.lower()\n",
   "        if value_type in ('boolean', 'bool'):\n",
   "            value = boolean(value, strict=False)\n",
   "        elif value_type in ('integer', 'int'):\n",
   "            value = int(value)\n",
   "        elif value_type == 'float':\n",
   "            value = float(value)\n",
   "        elif value_type == 'list':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            elif not isinstance(value, Sequence):\n",
   "                errmsg = 'list'\n",
   "        elif value_type == 'none':\n",
   "            if value == \"None\":\n",
   "                value = None\n",
   "            if value is not None:\n",
   "                errmsg = 'None'\n",
   "        elif value_type == 'path':\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                errmsg = 'path'\n",
   "        elif value_type in ('tmp', 'temppath', 'tmppath'):\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                if not os.path.exists(value):\n",
   "                    makedirs_safe(value, 0o700)\n",
   "                prefix = 'ansible-local-%s' % os.getpid()\n",
   "                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n",
   "                atexit.register(cleanup_tmp_file, value, warn=True)\n",
   "                errmsg = 'temppath'\n",
   "        elif value_type == 'pathspec':\n",
   "            if isinstance(value, string_types):\n",
   "                value = value.split(os.pathsep)\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathspec'\n",
   "        elif value_type == 'pathlist':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathlist'\n",
   "        elif value_type in ('str', 'string'):\n",
   "            if isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "                value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "                errmsg = 'string'\n",
   "        elif isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "            value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "        if errmsg:\n",
   "            raise ValueError('Invalid type provided for \"%s\": %s' % (errmsg, to_native(value)))\n",
   "    return to_text(value, errors='surrogate_or_strict', nonstring='passthru')\n",
   "        path = path.replace('{{CWD}}', os.getcwd())\n",
   "    return unfrackpath(path, follow=False, basedir=basedir)\n",
   "def get_config_type(cfile):\n",
   "    ftype = None\n",
   "        ext = os.path.splitext(cfile)[-1]\n",
   "        if ext in ('.ini', '.cfg'):\n",
   "            ftype = 'ini'\n",
   "        elif ext in ('.yaml', '.yml'):\n",
   "            ftype = 'yaml'\n",
   "            raise AnsibleOptionsError(\"Unsupported configuration file extension for %s: %s\" % (cfile, to_native(ext)))\n",
   "    return ftype\n",
   "def get_ini_config_value(p, entry):\n",
   "    value = None\n",
   "            value = p.get(entry.get('section', 'defaults'), entry.get('key', ''), raw=True)\n",
   "    return value\n",
   "def find_ini_config_file(warnings=None):\n",
   "        warnings = set()\n",
   "    SENTINEL = object\n",
   "    potential_paths = []\n",
   "    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n",
   "    if path_from_env is not SENTINEL:\n",
   "        path_from_env = unfrackpath(path_from_env, follow=False)\n",
   "        if os.path.isdir(to_bytes(path_from_env)):\n",
   "            path_from_env = os.path.join(path_from_env, \"ansible.cfg\")\n",
   "        potential_paths.append(path_from_env)\n",
   "    warn_cmd_public = False\n",
   "        cwd = os.getcwd()\n",
   "        perms = os.stat(cwd)\n",
   "        cwd_cfg = os.path.join(cwd, \"ansible.cfg\")\n",
   "        if perms.st_mode & stat.S_IWOTH:\n",
   "            if os.path.exists(cwd_cfg):\n",
   "                warn_cmd_public = True\n",
   "            potential_paths.append(to_text(cwd_cfg, errors='surrogate_or_strict'))\n",
   "    potential_paths.append(unfrackpath(\"~/.ansible.cfg\", follow=False))\n",
   "    potential_paths.append(\"/etc/ansible/ansible.cfg\")\n",
   "    for path in potential_paths:\n",
   "        b_path = to_bytes(path)\n",
   "        if os.path.exists(b_path) and os.access(b_path, os.R_OK):\n",
   "        path = None\n",
   "    if path_from_env != path and warn_cmd_public:\n",
   "        warnings.add(u\"Ansible is being run in a world writable directory (%s),\"\n",
   "                     % to_text(cwd))\n",
   "    return path\n",
   "        yml_file = to_bytes(yml_file)\n",
   "        if os.path.exists(yml_file):\n",
   "            with open(yml_file, 'rb') as config_def:\n",
   "                return yaml_load(config_def, Loader=SafeLoader) or {}\n",
   "            \"Missing base YAML definition file (bad install?): %s\" % to_native(yml_file))\n",
   "            cfile = self._config_file\n",
   "        ftype = get_config_type(cfile)\n",
   "        if cfile is not None:\n",
   "            if ftype == 'ini':\n",
   "                self._parsers[cfile] = configparser.ConfigParser()\n",
   "                with open(to_bytes(cfile), 'rb') as f:\n",
   "                        cfg_text = to_text(f.read(), errors='surrogate_or_strict')\n",
   "                        raise AnsibleOptionsError(\"Error reading config file(%s) because the config file was not utf8 encoded: %s\" % (cfile, to_native(e)))\n",
   "                        self._parsers[cfile].read_string(cfg_text)\n",
   "                        cfg_file = io.StringIO(cfg_text)\n",
   "                        self._parsers[cfile].readfp(cfg_file)\n",
   "                    raise AnsibleOptionsError(\"Error reading config file (%s): %s\" % (cfile, to_native(e)))\n",
   "                raise AnsibleOptionsError(\"Unsupported configuration file type: %s\" % to_native(ftype))\n",
   "        options = {}\n",
   "        defs = self.get_configuration_definitions(plugin_type, name)\n",
   "        for option in defs:\n",
   "            options[option] = self.get_config_value(option, plugin_type=plugin_type, plugin_name=name, keys=keys, variables=variables, direct=direct)\n",
   "        return options\n",
   "        pvars = []\n",
   "        for pdef in self.get_configuration_definitions(plugin_type, name).values():\n",
   "            if 'vars' in pdef and pdef['vars']:\n",
   "                for var_entry in pdef['vars']:\n",
   "                    pvars.append(var_entry['name'])\n",
   "        return pvars\n",
   "        ret = {}\n",
   "            ret = self._base_defs.get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(plugin_name, {}).get(name, None)\n",
   "        return ret\n",
   "        ret = {}\n",
   "            ret = self._base_defs\n",
   "            ret = self._plugins.get(plugin_type, {})\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, {})\n",
   "        return ret\n",
   "        value = None\n",
   "        origin = None\n",
   "        for entry in entry_list:\n",
   "            name = entry.get('name')\n",
   "                temp_value = container.get(name, None)\n",
   "                self.WARNINGS.add(u'value for config entry {0} contains invalid characters, ignoring...'.format(to_text(name)))\n",
   "            if temp_value is not None:  # only set if entry is defined in container\n",
   "                if isinstance(temp_value, AnsibleVaultEncryptedUnicode):\n",
   "                    temp_value = to_text(temp_value, errors='surrogate_or_strict')\n",
   "                value = temp_value\n",
   "                origin = name\n",
   "                if 'deprecated' in entry:\n",
   "                    self.DEPRECATED.append((entry['name'], entry['deprecated']))\n",
   "        return value, origin\n",
   "            value, _drop = self.get_config_value_and_origin(config, cfile=cfile, plugin_type=plugin_type, plugin_name=plugin_name,\n",
   "        return value\n",
   "        if cfile is None:\n",
   "            cfile = self._config_file\n",
   "        value = None\n",
   "        origin = None\n",
   "        defs = self.get_configuration_definitions(plugin_type, plugin_name)\n",
   "        if config in defs:\n",
   "            direct_aliases = []\n",
   "                direct_aliases = [direct[alias] for alias in defs[config].get('aliases', []) if alias in direct]\n",
   "                value = direct[config]\n",
   "                origin = 'Direct'\n",
   "            elif direct and direct_aliases:\n",
   "                value = direct_aliases[0]\n",
   "                origin = 'Direct'\n",
   "                if variables and defs[config].get('vars'):\n",
   "                    value, origin = self._loop_entries(variables, defs[config]['vars'])\n",
   "                    origin = 'var: %s' % origin\n",
   "                if value is None and keys and config in keys:\n",
   "                    value, origin = keys[config], 'keyword'\n",
   "                    origin = 'keyword: %s' % origin\n",
   "                if value is None and defs[config].get('env'):\n",
   "                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n",
   "                    origin = 'env: %s' % origin\n",
   "                if self._parsers.get(cfile, None) is None:\n",
   "                    self._parse_config_file(cfile)\n",
   "                if value is None and cfile is not None:\n",
   "                    ftype = get_config_type(cfile)\n",
   "                    if ftype and defs[config].get(ftype):\n",
   "                        if ftype == 'ini':\n",
   "                                for ini_entry in defs[config]['ini']:\n",
   "                                    temp_value = get_ini_config_value(self._parsers[cfile], ini_entry)\n",
   "                                    if temp_value is not None:\n",
   "                                        value = temp_value\n",
   "                                        origin = cfile\n",
   "                                        if 'deprecated' in ini_entry:\n",
   "                                            self.DEPRECATED.append(('[%s]%s' % (ini_entry['section'], ini_entry['key']), ini_entry['deprecated']))\n",
   "                                sys.stderr.write(\"Error while loading ini config %s: %s\" % (cfile, to_native(e)))\n",
   "                        elif ftype == 'yaml':\n",
   "                            origin = cfile\n",
   "                if value is None:\n",
   "                    if defs[config].get('required', False):\n",
   "                        if not plugin_type or config not in INTERNAL_DEFS.get(plugin_type, {}):\n",
   "                                               to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "                        value = defs[config].get('default')\n",
   "                        origin = 'default'\n",
   "                        if plugin_type is None and isinstance(value, string_types) and (value.startswith('{{') and value.endswith('}}')):\n",
   "                            return value, origin\n",
   "                value = ensure_type(value, defs[config].get('type'), origin=origin)\n",
   "                if origin.startswith('env:') and value == '':\n",
   "                    origin = 'default'\n",
   "                    value = ensure_type(defs[config].get('default'), defs[config].get('type'), origin=origin)\n",
   "                                              (to_native(_get_entry(plugin_type, plugin_name, config)), to_native(e)))\n",
   "            if 'deprecated' in defs[config] and origin != 'default':\n",
   "                self.DEPRECATED.append((config, defs[config].get('deprecated')))\n",
   "            raise AnsibleError('Requested entry (%s) was not defined in configuration.' % to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "        return value, origin\n",
   "        if plugin_type not in self._plugins:\n",
   "            self._plugins[plugin_type] = {}\n",
   "        self._plugins[plugin_type][name] = defs\n",
   "        if defs is None:\n",
   "        if not isinstance(defs, dict):\n",
   "            raise AnsibleOptionsError(\"Invalid configuration definition type: %s for %s\" % (type(defs), defs))\n",
   "        for config in defs:\n",
   "            if not isinstance(defs[config], dict):\n",
   "                raise AnsibleOptionsError(\"Invalid configuration definition '%s': type is %s\" % (to_native(config), type(defs[config])))\n",
   "                value, origin = self.get_config_value_and_origin(config, configfile)\n",
   "                raise AnsibleError(\"Invalid settings supplied for %s: %s\\n\" % (config, to_native(e)), orig_exc=e)\n",
   "            self.data.update_setting(Setting(config, value, origin, defs[config].get('type', 'string')))\n"
  ]
 },
 "40": {
  "name": "origin",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "457",
  "column": "20",
  "context": "environ, defs[config]['env'])\n                    origin = 'env: %s' % origin\n\n                # try config file entries next, i",
  "context_lines": "                    origin = 'keyword: %s' % origin\n\n                # env vars are next precedence\n                if value is None and defs[config].get('env'):\n                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n                    origin = 'env: %s' % origin\n\n                # try config file entries next, if we have one\n                if self._parsers.get(cfile, None) is None:\n                    self._parse_config_file(cfile)\n\n",
  "slicing": [
   "INTERNAL_DEFS = {'lookup': ('_terms',)}\n",
   "def _get_entry(plugin_type, plugin_name, config):\n",
   "    entry = ''\n",
   "        entry += 'plugin_type: %s ' % plugin_type\n",
   "            entry += 'plugin: %s ' % plugin_name\n",
   "    entry += 'setting: %s ' % config\n",
   "    return entry\n",
   "def ensure_type(value, value_type, origin=None):\n",
   "    errmsg = ''\n",
   "    basedir = None\n",
   "        basedir = origin\n",
   "        value_type = value_type.lower()\n",
   "        if value_type in ('boolean', 'bool'):\n",
   "            value = boolean(value, strict=False)\n",
   "        elif value_type in ('integer', 'int'):\n",
   "            value = int(value)\n",
   "        elif value_type == 'float':\n",
   "            value = float(value)\n",
   "        elif value_type == 'list':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            elif not isinstance(value, Sequence):\n",
   "                errmsg = 'list'\n",
   "        elif value_type == 'none':\n",
   "            if value == \"None\":\n",
   "                value = None\n",
   "            if value is not None:\n",
   "                errmsg = 'None'\n",
   "        elif value_type == 'path':\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                errmsg = 'path'\n",
   "        elif value_type in ('tmp', 'temppath', 'tmppath'):\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                if not os.path.exists(value):\n",
   "                    makedirs_safe(value, 0o700)\n",
   "                prefix = 'ansible-local-%s' % os.getpid()\n",
   "                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n",
   "                atexit.register(cleanup_tmp_file, value, warn=True)\n",
   "                errmsg = 'temppath'\n",
   "        elif value_type == 'pathspec':\n",
   "            if isinstance(value, string_types):\n",
   "                value = value.split(os.pathsep)\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathspec'\n",
   "        elif value_type == 'pathlist':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathlist'\n",
   "        elif value_type in ('str', 'string'):\n",
   "            if isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "                value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "                errmsg = 'string'\n",
   "        elif isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "            value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "        if errmsg:\n",
   "            raise ValueError('Invalid type provided for \"%s\": %s' % (errmsg, to_native(value)))\n",
   "    return to_text(value, errors='surrogate_or_strict', nonstring='passthru')\n",
   "        path = path.replace('{{CWD}}', os.getcwd())\n",
   "    return unfrackpath(path, follow=False, basedir=basedir)\n",
   "def get_config_type(cfile):\n",
   "    ftype = None\n",
   "        ext = os.path.splitext(cfile)[-1]\n",
   "        if ext in ('.ini', '.cfg'):\n",
   "            ftype = 'ini'\n",
   "        elif ext in ('.yaml', '.yml'):\n",
   "            ftype = 'yaml'\n",
   "            raise AnsibleOptionsError(\"Unsupported configuration file extension for %s: %s\" % (cfile, to_native(ext)))\n",
   "    return ftype\n",
   "def get_ini_config_value(p, entry):\n",
   "    value = None\n",
   "            value = p.get(entry.get('section', 'defaults'), entry.get('key', ''), raw=True)\n",
   "    return value\n",
   "def find_ini_config_file(warnings=None):\n",
   "        warnings = set()\n",
   "    SENTINEL = object\n",
   "    potential_paths = []\n",
   "    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n",
   "    if path_from_env is not SENTINEL:\n",
   "        path_from_env = unfrackpath(path_from_env, follow=False)\n",
   "        if os.path.isdir(to_bytes(path_from_env)):\n",
   "            path_from_env = os.path.join(path_from_env, \"ansible.cfg\")\n",
   "        potential_paths.append(path_from_env)\n",
   "    warn_cmd_public = False\n",
   "        cwd = os.getcwd()\n",
   "        perms = os.stat(cwd)\n",
   "        cwd_cfg = os.path.join(cwd, \"ansible.cfg\")\n",
   "        if perms.st_mode & stat.S_IWOTH:\n",
   "            if os.path.exists(cwd_cfg):\n",
   "                warn_cmd_public = True\n",
   "            potential_paths.append(to_text(cwd_cfg, errors='surrogate_or_strict'))\n",
   "    potential_paths.append(unfrackpath(\"~/.ansible.cfg\", follow=False))\n",
   "    potential_paths.append(\"/etc/ansible/ansible.cfg\")\n",
   "    for path in potential_paths:\n",
   "        b_path = to_bytes(path)\n",
   "        if os.path.exists(b_path) and os.access(b_path, os.R_OK):\n",
   "        path = None\n",
   "    if path_from_env != path and warn_cmd_public:\n",
   "        warnings.add(u\"Ansible is being run in a world writable directory (%s),\"\n",
   "                     % to_text(cwd))\n",
   "    return path\n",
   "        yml_file = to_bytes(yml_file)\n",
   "        if os.path.exists(yml_file):\n",
   "            with open(yml_file, 'rb') as config_def:\n",
   "                return yaml_load(config_def, Loader=SafeLoader) or {}\n",
   "            \"Missing base YAML definition file (bad install?): %s\" % to_native(yml_file))\n",
   "            cfile = self._config_file\n",
   "        ftype = get_config_type(cfile)\n",
   "        if cfile is not None:\n",
   "            if ftype == 'ini':\n",
   "                self._parsers[cfile] = configparser.ConfigParser()\n",
   "                with open(to_bytes(cfile), 'rb') as f:\n",
   "                        cfg_text = to_text(f.read(), errors='surrogate_or_strict')\n",
   "                        raise AnsibleOptionsError(\"Error reading config file(%s) because the config file was not utf8 encoded: %s\" % (cfile, to_native(e)))\n",
   "                        self._parsers[cfile].read_string(cfg_text)\n",
   "                        cfg_file = io.StringIO(cfg_text)\n",
   "                        self._parsers[cfile].readfp(cfg_file)\n",
   "                    raise AnsibleOptionsError(\"Error reading config file (%s): %s\" % (cfile, to_native(e)))\n",
   "                raise AnsibleOptionsError(\"Unsupported configuration file type: %s\" % to_native(ftype))\n",
   "        options = {}\n",
   "        defs = self.get_configuration_definitions(plugin_type, name)\n",
   "        for option in defs:\n",
   "            options[option] = self.get_config_value(option, plugin_type=plugin_type, plugin_name=name, keys=keys, variables=variables, direct=direct)\n",
   "        return options\n",
   "        pvars = []\n",
   "        for pdef in self.get_configuration_definitions(plugin_type, name).values():\n",
   "            if 'vars' in pdef and pdef['vars']:\n",
   "                for var_entry in pdef['vars']:\n",
   "                    pvars.append(var_entry['name'])\n",
   "        return pvars\n",
   "        ret = {}\n",
   "            ret = self._base_defs.get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(plugin_name, {}).get(name, None)\n",
   "        return ret\n",
   "        ret = {}\n",
   "            ret = self._base_defs\n",
   "            ret = self._plugins.get(plugin_type, {})\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, {})\n",
   "        return ret\n",
   "        value = None\n",
   "        origin = None\n",
   "        for entry in entry_list:\n",
   "            name = entry.get('name')\n",
   "                temp_value = container.get(name, None)\n",
   "                self.WARNINGS.add(u'value for config entry {0} contains invalid characters, ignoring...'.format(to_text(name)))\n",
   "            if temp_value is not None:  # only set if entry is defined in container\n",
   "                if isinstance(temp_value, AnsibleVaultEncryptedUnicode):\n",
   "                    temp_value = to_text(temp_value, errors='surrogate_or_strict')\n",
   "                value = temp_value\n",
   "                origin = name\n",
   "                if 'deprecated' in entry:\n",
   "                    self.DEPRECATED.append((entry['name'], entry['deprecated']))\n",
   "        return value, origin\n",
   "            value, _drop = self.get_config_value_and_origin(config, cfile=cfile, plugin_type=plugin_type, plugin_name=plugin_name,\n",
   "        return value\n",
   "        if cfile is None:\n",
   "            cfile = self._config_file\n",
   "        value = None\n",
   "        origin = None\n",
   "        defs = self.get_configuration_definitions(plugin_type, plugin_name)\n",
   "        if config in defs:\n",
   "            direct_aliases = []\n",
   "                direct_aliases = [direct[alias] for alias in defs[config].get('aliases', []) if alias in direct]\n",
   "                value = direct[config]\n",
   "                origin = 'Direct'\n",
   "            elif direct and direct_aliases:\n",
   "                value = direct_aliases[0]\n",
   "                origin = 'Direct'\n",
   "                if variables and defs[config].get('vars'):\n",
   "                    value, origin = self._loop_entries(variables, defs[config]['vars'])\n",
   "                    origin = 'var: %s' % origin\n",
   "                if value is None and keys and config in keys:\n",
   "                    value, origin = keys[config], 'keyword'\n",
   "                    origin = 'keyword: %s' % origin\n",
   "                if value is None and defs[config].get('env'):\n",
   "                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n",
   "                    origin = 'env: %s' % origin\n",
   "                if self._parsers.get(cfile, None) is None:\n",
   "                    self._parse_config_file(cfile)\n",
   "                if value is None and cfile is not None:\n",
   "                    ftype = get_config_type(cfile)\n",
   "                    if ftype and defs[config].get(ftype):\n",
   "                        if ftype == 'ini':\n",
   "                                for ini_entry in defs[config]['ini']:\n",
   "                                    temp_value = get_ini_config_value(self._parsers[cfile], ini_entry)\n",
   "                                    if temp_value is not None:\n",
   "                                        value = temp_value\n",
   "                                        origin = cfile\n",
   "                                        if 'deprecated' in ini_entry:\n",
   "                                            self.DEPRECATED.append(('[%s]%s' % (ini_entry['section'], ini_entry['key']), ini_entry['deprecated']))\n",
   "                                sys.stderr.write(\"Error while loading ini config %s: %s\" % (cfile, to_native(e)))\n",
   "                        elif ftype == 'yaml':\n",
   "                            origin = cfile\n",
   "                if value is None:\n",
   "                    if defs[config].get('required', False):\n",
   "                        if not plugin_type or config not in INTERNAL_DEFS.get(plugin_type, {}):\n",
   "                                               to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "                        value = defs[config].get('default')\n",
   "                        origin = 'default'\n",
   "                        if plugin_type is None and isinstance(value, string_types) and (value.startswith('{{') and value.endswith('}}')):\n",
   "                            return value, origin\n",
   "                value = ensure_type(value, defs[config].get('type'), origin=origin)\n",
   "                if origin.startswith('env:') and value == '':\n",
   "                    origin = 'default'\n",
   "                    value = ensure_type(defs[config].get('default'), defs[config].get('type'), origin=origin)\n",
   "                                              (to_native(_get_entry(plugin_type, plugin_name, config)), to_native(e)))\n",
   "            if 'deprecated' in defs[config] and origin != 'default':\n",
   "                self.DEPRECATED.append((config, defs[config].get('deprecated')))\n",
   "            raise AnsibleError('Requested entry (%s) was not defined in configuration.' % to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "        return value, origin\n",
   "        if plugin_type not in self._plugins:\n",
   "            self._plugins[plugin_type] = {}\n",
   "        self._plugins[plugin_type][name] = defs\n",
   "        if defs is None:\n",
   "        if not isinstance(defs, dict):\n",
   "            raise AnsibleOptionsError(\"Invalid configuration definition type: %s for %s\" % (type(defs), defs))\n",
   "        for config in defs:\n",
   "            if not isinstance(defs[config], dict):\n",
   "                raise AnsibleOptionsError(\"Invalid configuration definition '%s': type is %s\" % (to_native(config), type(defs[config])))\n",
   "                value, origin = self.get_config_value_and_origin(config, configfile)\n",
   "                raise AnsibleError(\"Invalid settings supplied for %s: %s\\n\" % (config, to_native(e)), orig_exc=e)\n",
   "            self.data.update_setting(Setting(config, value, origin, defs[config].get('type', 'string')))\n"
  ]
 },
 "41": {
  "name": "value",
  "type": "dict|NoneType|bool|int|list|float|str",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "489",
  "column": "24",
  "context": "                    else:\n                        value = defs[config].get('default')\n                        origin = 'default'\n       ",
  "context_lines": "                        if not plugin_type or config not in INTERNAL_DEFS.get(plugin_type, {}):\n                            raise AnsibleError(\"No setting was provided for required configuration %s\" %\n                                               to_native(_get_entry(plugin_type, plugin_name, config)))\n                    else:\n                        value = defs[config].get('default')\n                        origin = 'default'\n                        # skip typing as this is a templated default that will be resolved later in constants, which has needed vars\n                        if plugin_type is None and isinstance(value, string_types) and (value.startswith('{{') and value.endswith('}}')):\n                            return value, origin\n\n",
  "slicing": [
   "INTERNAL_DEFS = {'lookup': ('_terms',)}\n",
   "def _get_entry(plugin_type, plugin_name, config):\n",
   "    entry = ''\n",
   "        entry += 'plugin_type: %s ' % plugin_type\n",
   "            entry += 'plugin: %s ' % plugin_name\n",
   "    entry += 'setting: %s ' % config\n",
   "    return entry\n",
   "def ensure_type(value, value_type, origin=None):\n",
   "    errmsg = ''\n",
   "    basedir = None\n",
   "        basedir = origin\n",
   "        value_type = value_type.lower()\n",
   "        if value_type in ('boolean', 'bool'):\n",
   "            value = boolean(value, strict=False)\n",
   "        elif value_type in ('integer', 'int'):\n",
   "            value = int(value)\n",
   "        elif value_type == 'float':\n",
   "            value = float(value)\n",
   "        elif value_type == 'list':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            elif not isinstance(value, Sequence):\n",
   "                errmsg = 'list'\n",
   "        elif value_type == 'none':\n",
   "            if value == \"None\":\n",
   "                value = None\n",
   "            if value is not None:\n",
   "                errmsg = 'None'\n",
   "        elif value_type == 'path':\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                errmsg = 'path'\n",
   "        elif value_type in ('tmp', 'temppath', 'tmppath'):\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                if not os.path.exists(value):\n",
   "                    makedirs_safe(value, 0o700)\n",
   "                prefix = 'ansible-local-%s' % os.getpid()\n",
   "                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n",
   "                atexit.register(cleanup_tmp_file, value, warn=True)\n",
   "                errmsg = 'temppath'\n",
   "        elif value_type == 'pathspec':\n",
   "            if isinstance(value, string_types):\n",
   "                value = value.split(os.pathsep)\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathspec'\n",
   "        elif value_type == 'pathlist':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathlist'\n",
   "        elif value_type in ('str', 'string'):\n",
   "            if isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "                value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "                errmsg = 'string'\n",
   "        elif isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "            value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "        if errmsg:\n",
   "            raise ValueError('Invalid type provided for \"%s\": %s' % (errmsg, to_native(value)))\n",
   "    return to_text(value, errors='surrogate_or_strict', nonstring='passthru')\n",
   "        path = path.replace('{{CWD}}', os.getcwd())\n",
   "    return unfrackpath(path, follow=False, basedir=basedir)\n",
   "def get_config_type(cfile):\n",
   "    ftype = None\n",
   "        ext = os.path.splitext(cfile)[-1]\n",
   "        if ext in ('.ini', '.cfg'):\n",
   "            ftype = 'ini'\n",
   "        elif ext in ('.yaml', '.yml'):\n",
   "            ftype = 'yaml'\n",
   "            raise AnsibleOptionsError(\"Unsupported configuration file extension for %s: %s\" % (cfile, to_native(ext)))\n",
   "    return ftype\n",
   "def get_ini_config_value(p, entry):\n",
   "    value = None\n",
   "            value = p.get(entry.get('section', 'defaults'), entry.get('key', ''), raw=True)\n",
   "    return value\n",
   "def find_ini_config_file(warnings=None):\n",
   "        warnings = set()\n",
   "    SENTINEL = object\n",
   "    potential_paths = []\n",
   "    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n",
   "    if path_from_env is not SENTINEL:\n",
   "        path_from_env = unfrackpath(path_from_env, follow=False)\n",
   "        if os.path.isdir(to_bytes(path_from_env)):\n",
   "            path_from_env = os.path.join(path_from_env, \"ansible.cfg\")\n",
   "        potential_paths.append(path_from_env)\n",
   "    warn_cmd_public = False\n",
   "        cwd = os.getcwd()\n",
   "        perms = os.stat(cwd)\n",
   "        cwd_cfg = os.path.join(cwd, \"ansible.cfg\")\n",
   "        if perms.st_mode & stat.S_IWOTH:\n",
   "            if os.path.exists(cwd_cfg):\n",
   "                warn_cmd_public = True\n",
   "            potential_paths.append(to_text(cwd_cfg, errors='surrogate_or_strict'))\n",
   "    potential_paths.append(unfrackpath(\"~/.ansible.cfg\", follow=False))\n",
   "    potential_paths.append(\"/etc/ansible/ansible.cfg\")\n",
   "    for path in potential_paths:\n",
   "        b_path = to_bytes(path)\n",
   "        if os.path.exists(b_path) and os.access(b_path, os.R_OK):\n",
   "        path = None\n",
   "    if path_from_env != path and warn_cmd_public:\n",
   "        warnings.add(u\"Ansible is being run in a world writable directory (%s),\"\n",
   "                     % to_text(cwd))\n",
   "    return path\n",
   "        yml_file = to_bytes(yml_file)\n",
   "        if os.path.exists(yml_file):\n",
   "            with open(yml_file, 'rb') as config_def:\n",
   "                return yaml_load(config_def, Loader=SafeLoader) or {}\n",
   "            \"Missing base YAML definition file (bad install?): %s\" % to_native(yml_file))\n",
   "            cfile = self._config_file\n",
   "        ftype = get_config_type(cfile)\n",
   "        if cfile is not None:\n",
   "            if ftype == 'ini':\n",
   "                self._parsers[cfile] = configparser.ConfigParser()\n",
   "                with open(to_bytes(cfile), 'rb') as f:\n",
   "                        cfg_text = to_text(f.read(), errors='surrogate_or_strict')\n",
   "                        raise AnsibleOptionsError(\"Error reading config file(%s) because the config file was not utf8 encoded: %s\" % (cfile, to_native(e)))\n",
   "                        self._parsers[cfile].read_string(cfg_text)\n",
   "                        cfg_file = io.StringIO(cfg_text)\n",
   "                        self._parsers[cfile].readfp(cfg_file)\n",
   "                    raise AnsibleOptionsError(\"Error reading config file (%s): %s\" % (cfile, to_native(e)))\n",
   "                raise AnsibleOptionsError(\"Unsupported configuration file type: %s\" % to_native(ftype))\n",
   "        options = {}\n",
   "        defs = self.get_configuration_definitions(plugin_type, name)\n",
   "        for option in defs:\n",
   "            options[option] = self.get_config_value(option, plugin_type=plugin_type, plugin_name=name, keys=keys, variables=variables, direct=direct)\n",
   "        return options\n",
   "        pvars = []\n",
   "        for pdef in self.get_configuration_definitions(plugin_type, name).values():\n",
   "            if 'vars' in pdef and pdef['vars']:\n",
   "                for var_entry in pdef['vars']:\n",
   "                    pvars.append(var_entry['name'])\n",
   "        return pvars\n",
   "        ret = {}\n",
   "            ret = self._base_defs.get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(plugin_name, {}).get(name, None)\n",
   "        return ret\n",
   "        ret = {}\n",
   "            ret = self._base_defs\n",
   "            ret = self._plugins.get(plugin_type, {})\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, {})\n",
   "        return ret\n",
   "        value = None\n",
   "        origin = None\n",
   "        for entry in entry_list:\n",
   "            name = entry.get('name')\n",
   "                temp_value = container.get(name, None)\n",
   "                self.WARNINGS.add(u'value for config entry {0} contains invalid characters, ignoring...'.format(to_text(name)))\n",
   "            if temp_value is not None:  # only set if entry is defined in container\n",
   "                if isinstance(temp_value, AnsibleVaultEncryptedUnicode):\n",
   "                    temp_value = to_text(temp_value, errors='surrogate_or_strict')\n",
   "                value = temp_value\n",
   "                origin = name\n",
   "                if 'deprecated' in entry:\n",
   "                    self.DEPRECATED.append((entry['name'], entry['deprecated']))\n",
   "        return value, origin\n",
   "            value, _drop = self.get_config_value_and_origin(config, cfile=cfile, plugin_type=plugin_type, plugin_name=plugin_name,\n",
   "        return value\n",
   "        if cfile is None:\n",
   "            cfile = self._config_file\n",
   "        value = None\n",
   "        origin = None\n",
   "        defs = self.get_configuration_definitions(plugin_type, plugin_name)\n",
   "        if config in defs:\n",
   "            direct_aliases = []\n",
   "                direct_aliases = [direct[alias] for alias in defs[config].get('aliases', []) if alias in direct]\n",
   "                value = direct[config]\n",
   "                origin = 'Direct'\n",
   "            elif direct and direct_aliases:\n",
   "                value = direct_aliases[0]\n",
   "                origin = 'Direct'\n",
   "                if variables and defs[config].get('vars'):\n",
   "                    value, origin = self._loop_entries(variables, defs[config]['vars'])\n",
   "                    origin = 'var: %s' % origin\n",
   "                if value is None and keys and config in keys:\n",
   "                    value, origin = keys[config], 'keyword'\n",
   "                    origin = 'keyword: %s' % origin\n",
   "                if value is None and defs[config].get('env'):\n",
   "                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n",
   "                    origin = 'env: %s' % origin\n",
   "                if self._parsers.get(cfile, None) is None:\n",
   "                    self._parse_config_file(cfile)\n",
   "                if value is None and cfile is not None:\n",
   "                    ftype = get_config_type(cfile)\n",
   "                    if ftype and defs[config].get(ftype):\n",
   "                        if ftype == 'ini':\n",
   "                                for ini_entry in defs[config]['ini']:\n",
   "                                    temp_value = get_ini_config_value(self._parsers[cfile], ini_entry)\n",
   "                                    if temp_value is not None:\n",
   "                                        value = temp_value\n",
   "                                        origin = cfile\n",
   "                                        if 'deprecated' in ini_entry:\n",
   "                                            self.DEPRECATED.append(('[%s]%s' % (ini_entry['section'], ini_entry['key']), ini_entry['deprecated']))\n",
   "                                sys.stderr.write(\"Error while loading ini config %s: %s\" % (cfile, to_native(e)))\n",
   "                        elif ftype == 'yaml':\n",
   "                            origin = cfile\n",
   "                if value is None:\n",
   "                    if defs[config].get('required', False):\n",
   "                        if not plugin_type or config not in INTERNAL_DEFS.get(plugin_type, {}):\n",
   "                                               to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "                        value = defs[config].get('default')\n",
   "                        origin = 'default'\n",
   "                        if plugin_type is None and isinstance(value, string_types) and (value.startswith('{{') and value.endswith('}}')):\n",
   "                            return value, origin\n",
   "                value = ensure_type(value, defs[config].get('type'), origin=origin)\n",
   "                if origin.startswith('env:') and value == '':\n",
   "                    origin = 'default'\n",
   "                    value = ensure_type(defs[config].get('default'), defs[config].get('type'), origin=origin)\n",
   "                                              (to_native(_get_entry(plugin_type, plugin_name, config)), to_native(e)))\n",
   "            if 'deprecated' in defs[config] and origin != 'default':\n",
   "                self.DEPRECATED.append((config, defs[config].get('deprecated')))\n",
   "            raise AnsibleError('Requested entry (%s) was not defined in configuration.' % to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "        return value, origin\n",
   "        if plugin_type not in self._plugins:\n",
   "            self._plugins[plugin_type] = {}\n",
   "        self._plugins[plugin_type][name] = defs\n",
   "        if defs is None:\n",
   "        if not isinstance(defs, dict):\n",
   "            raise AnsibleOptionsError(\"Invalid configuration definition type: %s for %s\" % (type(defs), defs))\n",
   "        for config in defs:\n",
   "            if not isinstance(defs[config], dict):\n",
   "                raise AnsibleOptionsError(\"Invalid configuration definition '%s': type is %s\" % (to_native(config), type(defs[config])))\n",
   "                value, origin = self.get_config_value_and_origin(config, configfile)\n",
   "                raise AnsibleError(\"Invalid settings supplied for %s: %s\\n\" % (config, to_native(e)), orig_exc=e)\n",
   "            self.data.update_setting(Setting(config, value, origin, defs[config].get('type', 'string')))\n"
  ]
 },
 "42": {
  "name": "origin",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "490",
  "column": "24",
  "context": "fs[config].get('default')\n                        origin = 'default'\n                        # skip typing as this is a",
  "context_lines": "                            raise AnsibleError(\"No setting was provided for required configuration %s\" %\n                                               to_native(_get_entry(plugin_type, plugin_name, config)))\n                    else:\n                        value = defs[config].get('default')\n                        origin = 'default'\n                        # skip typing as this is a templated default that will be resolved later in constants, which has needed vars\n                        if plugin_type is None and isinstance(value, string_types) and (value.startswith('{{') and value.endswith('}}')):\n                            return value, origin\n\n            # ensure correct type, can raise exceptions on mismatched types\n",
  "slicing": [
   "INTERNAL_DEFS = {'lookup': ('_terms',)}\n",
   "def _get_entry(plugin_type, plugin_name, config):\n",
   "    entry = ''\n",
   "        entry += 'plugin_type: %s ' % plugin_type\n",
   "            entry += 'plugin: %s ' % plugin_name\n",
   "    entry += 'setting: %s ' % config\n",
   "    return entry\n",
   "def ensure_type(value, value_type, origin=None):\n",
   "    errmsg = ''\n",
   "    basedir = None\n",
   "        basedir = origin\n",
   "        value_type = value_type.lower()\n",
   "        if value_type in ('boolean', 'bool'):\n",
   "            value = boolean(value, strict=False)\n",
   "        elif value_type in ('integer', 'int'):\n",
   "            value = int(value)\n",
   "        elif value_type == 'float':\n",
   "            value = float(value)\n",
   "        elif value_type == 'list':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            elif not isinstance(value, Sequence):\n",
   "                errmsg = 'list'\n",
   "        elif value_type == 'none':\n",
   "            if value == \"None\":\n",
   "                value = None\n",
   "            if value is not None:\n",
   "                errmsg = 'None'\n",
   "        elif value_type == 'path':\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                errmsg = 'path'\n",
   "        elif value_type in ('tmp', 'temppath', 'tmppath'):\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                if not os.path.exists(value):\n",
   "                    makedirs_safe(value, 0o700)\n",
   "                prefix = 'ansible-local-%s' % os.getpid()\n",
   "                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n",
   "                atexit.register(cleanup_tmp_file, value, warn=True)\n",
   "                errmsg = 'temppath'\n",
   "        elif value_type == 'pathspec':\n",
   "            if isinstance(value, string_types):\n",
   "                value = value.split(os.pathsep)\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathspec'\n",
   "        elif value_type == 'pathlist':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathlist'\n",
   "        elif value_type in ('str', 'string'):\n",
   "            if isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "                value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "                errmsg = 'string'\n",
   "        elif isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "            value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "        if errmsg:\n",
   "            raise ValueError('Invalid type provided for \"%s\": %s' % (errmsg, to_native(value)))\n",
   "    return to_text(value, errors='surrogate_or_strict', nonstring='passthru')\n",
   "        path = path.replace('{{CWD}}', os.getcwd())\n",
   "    return unfrackpath(path, follow=False, basedir=basedir)\n",
   "def get_config_type(cfile):\n",
   "    ftype = None\n",
   "        ext = os.path.splitext(cfile)[-1]\n",
   "        if ext in ('.ini', '.cfg'):\n",
   "            ftype = 'ini'\n",
   "        elif ext in ('.yaml', '.yml'):\n",
   "            ftype = 'yaml'\n",
   "            raise AnsibleOptionsError(\"Unsupported configuration file extension for %s: %s\" % (cfile, to_native(ext)))\n",
   "    return ftype\n",
   "def get_ini_config_value(p, entry):\n",
   "    value = None\n",
   "            value = p.get(entry.get('section', 'defaults'), entry.get('key', ''), raw=True)\n",
   "    return value\n",
   "def find_ini_config_file(warnings=None):\n",
   "        warnings = set()\n",
   "    SENTINEL = object\n",
   "    potential_paths = []\n",
   "    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n",
   "    if path_from_env is not SENTINEL:\n",
   "        path_from_env = unfrackpath(path_from_env, follow=False)\n",
   "        if os.path.isdir(to_bytes(path_from_env)):\n",
   "            path_from_env = os.path.join(path_from_env, \"ansible.cfg\")\n",
   "        potential_paths.append(path_from_env)\n",
   "    warn_cmd_public = False\n",
   "        cwd = os.getcwd()\n",
   "        perms = os.stat(cwd)\n",
   "        cwd_cfg = os.path.join(cwd, \"ansible.cfg\")\n",
   "        if perms.st_mode & stat.S_IWOTH:\n",
   "            if os.path.exists(cwd_cfg):\n",
   "                warn_cmd_public = True\n",
   "            potential_paths.append(to_text(cwd_cfg, errors='surrogate_or_strict'))\n",
   "    potential_paths.append(unfrackpath(\"~/.ansible.cfg\", follow=False))\n",
   "    potential_paths.append(\"/etc/ansible/ansible.cfg\")\n",
   "    for path in potential_paths:\n",
   "        b_path = to_bytes(path)\n",
   "        if os.path.exists(b_path) and os.access(b_path, os.R_OK):\n",
   "        path = None\n",
   "    if path_from_env != path and warn_cmd_public:\n",
   "        warnings.add(u\"Ansible is being run in a world writable directory (%s),\"\n",
   "                     % to_text(cwd))\n",
   "    return path\n",
   "        yml_file = to_bytes(yml_file)\n",
   "        if os.path.exists(yml_file):\n",
   "            with open(yml_file, 'rb') as config_def:\n",
   "                return yaml_load(config_def, Loader=SafeLoader) or {}\n",
   "            \"Missing base YAML definition file (bad install?): %s\" % to_native(yml_file))\n",
   "            cfile = self._config_file\n",
   "        ftype = get_config_type(cfile)\n",
   "        if cfile is not None:\n",
   "            if ftype == 'ini':\n",
   "                self._parsers[cfile] = configparser.ConfigParser()\n",
   "                with open(to_bytes(cfile), 'rb') as f:\n",
   "                        cfg_text = to_text(f.read(), errors='surrogate_or_strict')\n",
   "                        raise AnsibleOptionsError(\"Error reading config file(%s) because the config file was not utf8 encoded: %s\" % (cfile, to_native(e)))\n",
   "                        self._parsers[cfile].read_string(cfg_text)\n",
   "                        cfg_file = io.StringIO(cfg_text)\n",
   "                        self._parsers[cfile].readfp(cfg_file)\n",
   "                    raise AnsibleOptionsError(\"Error reading config file (%s): %s\" % (cfile, to_native(e)))\n",
   "                raise AnsibleOptionsError(\"Unsupported configuration file type: %s\" % to_native(ftype))\n",
   "        options = {}\n",
   "        defs = self.get_configuration_definitions(plugin_type, name)\n",
   "        for option in defs:\n",
   "            options[option] = self.get_config_value(option, plugin_type=plugin_type, plugin_name=name, keys=keys, variables=variables, direct=direct)\n",
   "        return options\n",
   "        pvars = []\n",
   "        for pdef in self.get_configuration_definitions(plugin_type, name).values():\n",
   "            if 'vars' in pdef and pdef['vars']:\n",
   "                for var_entry in pdef['vars']:\n",
   "                    pvars.append(var_entry['name'])\n",
   "        return pvars\n",
   "        ret = {}\n",
   "            ret = self._base_defs.get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(plugin_name, {}).get(name, None)\n",
   "        return ret\n",
   "        ret = {}\n",
   "            ret = self._base_defs\n",
   "            ret = self._plugins.get(plugin_type, {})\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, {})\n",
   "        return ret\n",
   "        value = None\n",
   "        origin = None\n",
   "        for entry in entry_list:\n",
   "            name = entry.get('name')\n",
   "                temp_value = container.get(name, None)\n",
   "                self.WARNINGS.add(u'value for config entry {0} contains invalid characters, ignoring...'.format(to_text(name)))\n",
   "            if temp_value is not None:  # only set if entry is defined in container\n",
   "                if isinstance(temp_value, AnsibleVaultEncryptedUnicode):\n",
   "                    temp_value = to_text(temp_value, errors='surrogate_or_strict')\n",
   "                value = temp_value\n",
   "                origin = name\n",
   "                if 'deprecated' in entry:\n",
   "                    self.DEPRECATED.append((entry['name'], entry['deprecated']))\n",
   "        return value, origin\n",
   "            value, _drop = self.get_config_value_and_origin(config, cfile=cfile, plugin_type=plugin_type, plugin_name=plugin_name,\n",
   "        return value\n",
   "        if cfile is None:\n",
   "            cfile = self._config_file\n",
   "        value = None\n",
   "        origin = None\n",
   "        defs = self.get_configuration_definitions(plugin_type, plugin_name)\n",
   "        if config in defs:\n",
   "            direct_aliases = []\n",
   "                direct_aliases = [direct[alias] for alias in defs[config].get('aliases', []) if alias in direct]\n",
   "                value = direct[config]\n",
   "                origin = 'Direct'\n",
   "            elif direct and direct_aliases:\n",
   "                value = direct_aliases[0]\n",
   "                origin = 'Direct'\n",
   "                if variables and defs[config].get('vars'):\n",
   "                    value, origin = self._loop_entries(variables, defs[config]['vars'])\n",
   "                    origin = 'var: %s' % origin\n",
   "                if value is None and keys and config in keys:\n",
   "                    value, origin = keys[config], 'keyword'\n",
   "                    origin = 'keyword: %s' % origin\n",
   "                if value is None and defs[config].get('env'):\n",
   "                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n",
   "                    origin = 'env: %s' % origin\n",
   "                if self._parsers.get(cfile, None) is None:\n",
   "                    self._parse_config_file(cfile)\n",
   "                if value is None and cfile is not None:\n",
   "                    ftype = get_config_type(cfile)\n",
   "                    if ftype and defs[config].get(ftype):\n",
   "                        if ftype == 'ini':\n",
   "                                for ini_entry in defs[config]['ini']:\n",
   "                                    temp_value = get_ini_config_value(self._parsers[cfile], ini_entry)\n",
   "                                    if temp_value is not None:\n",
   "                                        value = temp_value\n",
   "                                        origin = cfile\n",
   "                                        if 'deprecated' in ini_entry:\n",
   "                                            self.DEPRECATED.append(('[%s]%s' % (ini_entry['section'], ini_entry['key']), ini_entry['deprecated']))\n",
   "                                sys.stderr.write(\"Error while loading ini config %s: %s\" % (cfile, to_native(e)))\n",
   "                        elif ftype == 'yaml':\n",
   "                            origin = cfile\n",
   "                if value is None:\n",
   "                    if defs[config].get('required', False):\n",
   "                        if not plugin_type or config not in INTERNAL_DEFS.get(plugin_type, {}):\n",
   "                                               to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "                        value = defs[config].get('default')\n",
   "                        origin = 'default'\n",
   "                        if plugin_type is None and isinstance(value, string_types) and (value.startswith('{{') and value.endswith('}}')):\n",
   "                            return value, origin\n",
   "                value = ensure_type(value, defs[config].get('type'), origin=origin)\n",
   "                if origin.startswith('env:') and value == '':\n",
   "                    origin = 'default'\n",
   "                    value = ensure_type(defs[config].get('default'), defs[config].get('type'), origin=origin)\n",
   "                                              (to_native(_get_entry(plugin_type, plugin_name, config)), to_native(e)))\n",
   "            if 'deprecated' in defs[config] and origin != 'default':\n",
   "                self.DEPRECATED.append((config, defs[config].get('deprecated')))\n",
   "            raise AnsibleError('Requested entry (%s) was not defined in configuration.' % to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "        return value, origin\n",
   "        if plugin_type not in self._plugins:\n",
   "            self._plugins[plugin_type] = {}\n",
   "        self._plugins[plugin_type][name] = defs\n",
   "        if defs is None:\n",
   "        if not isinstance(defs, dict):\n",
   "            raise AnsibleOptionsError(\"Invalid configuration definition type: %s for %s\" % (type(defs), defs))\n",
   "        for config in defs:\n",
   "            if not isinstance(defs[config], dict):\n",
   "                raise AnsibleOptionsError(\"Invalid configuration definition '%s': type is %s\" % (to_native(config), type(defs[config])))\n",
   "                value, origin = self.get_config_value_and_origin(config, configfile)\n",
   "                raise AnsibleError(\"Invalid settings supplied for %s: %s\\n\" % (config, to_native(e)), orig_exc=e)\n",
   "            self.data.update_setting(Setting(config, value, origin, defs[config].get('type', 'string')))\n"
  ]
 },
 "43": {
  "name": "configfile",
  "type": "NoneType",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "529",
  "column": "12",
  "context": "_defs\n\n        if configfile is None:\n            configfile = self._config_file\n\n        if not isinstance(defs, dict):\n          ",
  "context_lines": "        ''' really: update constants '''\n\n        if defs is None:\n            defs = self._base_defs\n\n        if configfile is None:\n            configfile = self._config_file\n\n        if not isinstance(defs, dict):\n            raise AnsibleOptionsError(\"Invalid configuration definition type: %s for %s\" % (type(defs), defs))\n\n        # update the constant for config file\n",
  "slicing": [
   "INTERNAL_DEFS = {'lookup': ('_terms',)}\n",
   "def _get_entry(plugin_type, plugin_name, config):\n",
   "    entry = ''\n",
   "        entry += 'plugin_type: %s ' % plugin_type\n",
   "            entry += 'plugin: %s ' % plugin_name\n",
   "    entry += 'setting: %s ' % config\n",
   "    return entry\n",
   "def ensure_type(value, value_type, origin=None):\n",
   "    errmsg = ''\n",
   "    basedir = None\n",
   "        basedir = origin\n",
   "        value_type = value_type.lower()\n",
   "        if value_type in ('boolean', 'bool'):\n",
   "            value = boolean(value, strict=False)\n",
   "        elif value_type in ('integer', 'int'):\n",
   "            value = int(value)\n",
   "        elif value_type == 'float':\n",
   "            value = float(value)\n",
   "        elif value_type == 'list':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            elif not isinstance(value, Sequence):\n",
   "                errmsg = 'list'\n",
   "        elif value_type == 'none':\n",
   "            if value == \"None\":\n",
   "                value = None\n",
   "            if value is not None:\n",
   "                errmsg = 'None'\n",
   "        elif value_type == 'path':\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                errmsg = 'path'\n",
   "        elif value_type in ('tmp', 'temppath', 'tmppath'):\n",
   "            if isinstance(value, string_types):\n",
   "                value = resolve_path(value, basedir=basedir)\n",
   "                if not os.path.exists(value):\n",
   "                    makedirs_safe(value, 0o700)\n",
   "                prefix = 'ansible-local-%s' % os.getpid()\n",
   "                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n",
   "                atexit.register(cleanup_tmp_file, value, warn=True)\n",
   "                errmsg = 'temppath'\n",
   "        elif value_type == 'pathspec':\n",
   "            if isinstance(value, string_types):\n",
   "                value = value.split(os.pathsep)\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathspec'\n",
   "        elif value_type == 'pathlist':\n",
   "            if isinstance(value, string_types):\n",
   "                value = [x.strip() for x in value.split(',')]\n",
   "            if isinstance(value, Sequence):\n",
   "                value = [resolve_path(x, basedir=basedir) for x in value]\n",
   "                errmsg = 'pathlist'\n",
   "        elif value_type in ('str', 'string'):\n",
   "            if isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "                value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "                errmsg = 'string'\n",
   "        elif isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n",
   "            value = unquote(to_text(value, errors='surrogate_or_strict'))\n",
   "        if errmsg:\n",
   "            raise ValueError('Invalid type provided for \"%s\": %s' % (errmsg, to_native(value)))\n",
   "    return to_text(value, errors='surrogate_or_strict', nonstring='passthru')\n",
   "        path = path.replace('{{CWD}}', os.getcwd())\n",
   "    return unfrackpath(path, follow=False, basedir=basedir)\n",
   "def get_config_type(cfile):\n",
   "    ftype = None\n",
   "        ext = os.path.splitext(cfile)[-1]\n",
   "        if ext in ('.ini', '.cfg'):\n",
   "            ftype = 'ini'\n",
   "        elif ext in ('.yaml', '.yml'):\n",
   "            ftype = 'yaml'\n",
   "            raise AnsibleOptionsError(\"Unsupported configuration file extension for %s: %s\" % (cfile, to_native(ext)))\n",
   "    return ftype\n",
   "def get_ini_config_value(p, entry):\n",
   "    value = None\n",
   "            value = p.get(entry.get('section', 'defaults'), entry.get('key', ''), raw=True)\n",
   "    return value\n",
   "def find_ini_config_file(warnings=None):\n",
   "        warnings = set()\n",
   "    SENTINEL = object\n",
   "    potential_paths = []\n",
   "    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n",
   "    if path_from_env is not SENTINEL:\n",
   "        path_from_env = unfrackpath(path_from_env, follow=False)\n",
   "        if os.path.isdir(to_bytes(path_from_env)):\n",
   "            path_from_env = os.path.join(path_from_env, \"ansible.cfg\")\n",
   "        potential_paths.append(path_from_env)\n",
   "    warn_cmd_public = False\n",
   "        cwd = os.getcwd()\n",
   "        perms = os.stat(cwd)\n",
   "        cwd_cfg = os.path.join(cwd, \"ansible.cfg\")\n",
   "        if perms.st_mode & stat.S_IWOTH:\n",
   "            if os.path.exists(cwd_cfg):\n",
   "                warn_cmd_public = True\n",
   "            potential_paths.append(to_text(cwd_cfg, errors='surrogate_or_strict'))\n",
   "    potential_paths.append(unfrackpath(\"~/.ansible.cfg\", follow=False))\n",
   "    potential_paths.append(\"/etc/ansible/ansible.cfg\")\n",
   "    for path in potential_paths:\n",
   "        b_path = to_bytes(path)\n",
   "        if os.path.exists(b_path) and os.access(b_path, os.R_OK):\n",
   "        path = None\n",
   "    if path_from_env != path and warn_cmd_public:\n",
   "        warnings.add(u\"Ansible is being run in a world writable directory (%s),\"\n",
   "                     % to_text(cwd))\n",
   "    return path\n",
   "        yml_file = to_bytes(yml_file)\n",
   "        if os.path.exists(yml_file):\n",
   "            with open(yml_file, 'rb') as config_def:\n",
   "                return yaml_load(config_def, Loader=SafeLoader) or {}\n",
   "            \"Missing base YAML definition file (bad install?): %s\" % to_native(yml_file))\n",
   "            cfile = self._config_file\n",
   "        ftype = get_config_type(cfile)\n",
   "        if cfile is not None:\n",
   "            if ftype == 'ini':\n",
   "                self._parsers[cfile] = configparser.ConfigParser()\n",
   "                with open(to_bytes(cfile), 'rb') as f:\n",
   "                        cfg_text = to_text(f.read(), errors='surrogate_or_strict')\n",
   "                        raise AnsibleOptionsError(\"Error reading config file(%s) because the config file was not utf8 encoded: %s\" % (cfile, to_native(e)))\n",
   "                        self._parsers[cfile].read_string(cfg_text)\n",
   "                        cfg_file = io.StringIO(cfg_text)\n",
   "                        self._parsers[cfile].readfp(cfg_file)\n",
   "                    raise AnsibleOptionsError(\"Error reading config file (%s): %s\" % (cfile, to_native(e)))\n",
   "                raise AnsibleOptionsError(\"Unsupported configuration file type: %s\" % to_native(ftype))\n",
   "        options = {}\n",
   "        defs = self.get_configuration_definitions(plugin_type, name)\n",
   "        for option in defs:\n",
   "            options[option] = self.get_config_value(option, plugin_type=plugin_type, plugin_name=name, keys=keys, variables=variables, direct=direct)\n",
   "        return options\n",
   "        pvars = []\n",
   "        for pdef in self.get_configuration_definitions(plugin_type, name).values():\n",
   "            if 'vars' in pdef and pdef['vars']:\n",
   "                for var_entry in pdef['vars']:\n",
   "                    pvars.append(var_entry['name'])\n",
   "        return pvars\n",
   "        ret = {}\n",
   "            ret = self._base_defs.get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, None)\n",
   "            ret = self._plugins.get(plugin_type, {}).get(plugin_name, {}).get(name, None)\n",
   "        return ret\n",
   "        ret = {}\n",
   "            ret = self._base_defs\n",
   "            ret = self._plugins.get(plugin_type, {})\n",
   "            ret = self._plugins.get(plugin_type, {}).get(name, {})\n",
   "        return ret\n",
   "        value = None\n",
   "        origin = None\n",
   "        for entry in entry_list:\n",
   "            name = entry.get('name')\n",
   "                temp_value = container.get(name, None)\n",
   "                self.WARNINGS.add(u'value for config entry {0} contains invalid characters, ignoring...'.format(to_text(name)))\n",
   "            if temp_value is not None:  # only set if entry is defined in container\n",
   "                if isinstance(temp_value, AnsibleVaultEncryptedUnicode):\n",
   "                    temp_value = to_text(temp_value, errors='surrogate_or_strict')\n",
   "                value = temp_value\n",
   "                origin = name\n",
   "                if 'deprecated' in entry:\n",
   "                    self.DEPRECATED.append((entry['name'], entry['deprecated']))\n",
   "        return value, origin\n",
   "            value, _drop = self.get_config_value_and_origin(config, cfile=cfile, plugin_type=plugin_type, plugin_name=plugin_name,\n",
   "        return value\n",
   "        if cfile is None:\n",
   "            cfile = self._config_file\n",
   "        value = None\n",
   "        origin = None\n",
   "        defs = self.get_configuration_definitions(plugin_type, plugin_name)\n",
   "        if config in defs:\n",
   "            direct_aliases = []\n",
   "                direct_aliases = [direct[alias] for alias in defs[config].get('aliases', []) if alias in direct]\n",
   "                value = direct[config]\n",
   "                origin = 'Direct'\n",
   "            elif direct and direct_aliases:\n",
   "                value = direct_aliases[0]\n",
   "                origin = 'Direct'\n",
   "                if variables and defs[config].get('vars'):\n",
   "                    value, origin = self._loop_entries(variables, defs[config]['vars'])\n",
   "                    origin = 'var: %s' % origin\n",
   "                if value is None and keys and config in keys:\n",
   "                    value, origin = keys[config], 'keyword'\n",
   "                    origin = 'keyword: %s' % origin\n",
   "                if value is None and defs[config].get('env'):\n",
   "                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n",
   "                    origin = 'env: %s' % origin\n",
   "                if self._parsers.get(cfile, None) is None:\n",
   "                    self._parse_config_file(cfile)\n",
   "                if value is None and cfile is not None:\n",
   "                    ftype = get_config_type(cfile)\n",
   "                    if ftype and defs[config].get(ftype):\n",
   "                        if ftype == 'ini':\n",
   "                                for ini_entry in defs[config]['ini']:\n",
   "                                    temp_value = get_ini_config_value(self._parsers[cfile], ini_entry)\n",
   "                                    if temp_value is not None:\n",
   "                                        value = temp_value\n",
   "                                        origin = cfile\n",
   "                                        if 'deprecated' in ini_entry:\n",
   "                                            self.DEPRECATED.append(('[%s]%s' % (ini_entry['section'], ini_entry['key']), ini_entry['deprecated']))\n",
   "                                sys.stderr.write(\"Error while loading ini config %s: %s\" % (cfile, to_native(e)))\n",
   "                        elif ftype == 'yaml':\n",
   "                            origin = cfile\n",
   "                if value is None:\n",
   "                    if defs[config].get('required', False):\n",
   "                        if not plugin_type or config not in INTERNAL_DEFS.get(plugin_type, {}):\n",
   "                                               to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "                        value = defs[config].get('default')\n",
   "                        origin = 'default'\n",
   "                        if plugin_type is None and isinstance(value, string_types) and (value.startswith('{{') and value.endswith('}}')):\n",
   "                            return value, origin\n",
   "                value = ensure_type(value, defs[config].get('type'), origin=origin)\n",
   "                if origin.startswith('env:') and value == '':\n",
   "                    origin = 'default'\n",
   "                    value = ensure_type(defs[config].get('default'), defs[config].get('type'), origin=origin)\n",
   "                                              (to_native(_get_entry(plugin_type, plugin_name, config)), to_native(e)))\n",
   "            if 'deprecated' in defs[config] and origin != 'default':\n",
   "                self.DEPRECATED.append((config, defs[config].get('deprecated')))\n",
   "            raise AnsibleError('Requested entry (%s) was not defined in configuration.' % to_native(_get_entry(plugin_type, plugin_name, config)))\n",
   "        return value, origin\n",
   "        if plugin_type not in self._plugins:\n",
   "            self._plugins[plugin_type] = {}\n",
   "        self._plugins[plugin_type][name] = defs\n",
   "        if defs is None:\n",
   "            defs = self._base_defs\n",
   "            configfile = self._config_file\n",
   "        if not isinstance(defs, dict):\n",
   "            raise AnsibleOptionsError(\"Invalid configuration definition type: %s for %s\" % (type(defs), defs))\n",
   "        self.data.update_setting(Setting('CONFIG_FILE', configfile, '', 'string'))\n",
   "        for config in defs:\n",
   "            if not isinstance(defs[config], dict):\n",
   "                raise AnsibleOptionsError(\"Invalid configuration definition '%s': type is %s\" % (to_native(config), type(defs[config])))\n",
   "                value, origin = self.get_config_value_and_origin(config, configfile)\n",
   "                raise AnsibleError(\"Invalid settings supplied for %s: %s\\n\" % (config, to_native(e)), orig_exc=e)\n",
   "            self.data.update_setting(Setting(config, value, origin, defs[config].get('type', 'string')))\n"
  ]
 },
 "44": {
  "name": "origin",
  "type": "NoneType",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/config/manager.py",
  "lineno": "537",
  "column": "8",
  "context": "CONFIG_FILE', configfile, '', 'string'))\n\n        origin = None\n        # env and config defs can have several ent",
  "context_lines": "        if not isinstance(defs, dict):\n            raise AnsibleOptionsError(\"Invalid configuration definition type: %s for %s\" % (type(defs), defs))\n\n        # update the constant for config file\n        self.data.update_setting(Setting('CONFIG_FILE', configfile, '', 'string'))\n\n        origin = None\n        # env and config defs can have several entries, ordered in list from lowest to highest precedence\n        for config in defs:\n            if not isinstance(defs[config], dict):\n                raise AnsibleOptionsError(\"Invalid configuration definition '%s': type is %s\" % (to_native(config), type(defs[config])))\n\n",
  "slicing": [
   "        origin = None\n",
   "            self.data.update_setting(Setting(config, value, origin, defs[config].get('type', 'string')))\n"
  ]
 },
 "45": {
  "name": "_validate_rescue",
  "type": "function",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/playbook/block.py",
  "lineno": "169",
  "column": "4",
  "context": " used without 'block'\" % name, obj=self._ds)\n\n    _validate_rescue = _validate_always\n\n    def get_dep_chain(self):\n        if self._dep",
  "context_lines": "            raise AnsibleParserError(\"A malformed block was encountered while loading always\", obj=self._ds, orig_exc=e)\n\n    def _validate_always(self, attr, name, value):\n        if value and not self.block:\n            raise AnsibleParserError(\"'%s' keyword cannot be used without 'block'\" % name, obj=self._ds)\n\n    _validate_rescue = _validate_always\n\n    def get_dep_chain(self):\n        if self._dep_chain is None:\n            if self._parent:\n",
  "slicing": [
   "        all_vars = self.vars.copy()\n",
   "            all_vars.update(self._parent.get_vars())\n",
   "        return all_vars\n",
   "        implicit = not Block.is_block(data)\n",
   "        b = Block(play=play, parent_block=parent_block, role=role, task_include=task_include, use_handlers=use_handlers, implicit=implicit)\n",
   "        return b.load_data(data, variable_manager=variable_manager, loader=loader)\n",
   "        is_block = False\n",
   "            for attr in ('block', 'rescue', 'always'):\n",
   "                if attr in ds:\n",
   "                    is_block = True\n",
   "        return is_block\n",
   "    _validate_rescue = _validate_always\n",
   "        def _dupe_task_list(task_list, new_block):\n",
   "            new_task_list = []\n",
   "            for task in task_list:\n",
   "                new_task = task.copy(exclude_parent=True)\n",
   "                if task._parent:\n",
   "                    new_task._parent = task._parent.copy(exclude_tasks=True)\n",
   "                    if task._parent == new_block:\n",
   "                        new_task._parent = new_block\n",
   "                        cur_obj = new_task._parent\n",
   "                        while cur_obj._parent and cur_obj._parent != new_block:\n",
   "                            cur_obj = cur_obj._parent\n",
   "                        cur_obj._parent = new_block\n",
   "                    new_task._parent = new_block\n",
   "                new_task_list.append(new_task)\n",
   "            return new_task_list\n",
   "        new_me = super(Block, self).copy()\n",
   "        new_me._play = self._play\n",
   "        new_me._use_handlers = self._use_handlers\n",
   "        new_me._eor = self._eor\n",
   "            new_me._dep_chain = self._dep_chain[:]\n",
   "        new_me._parent = None\n",
   "            new_me._parent = self._parent.copy(exclude_tasks=True)\n",
   "            new_me.block = _dupe_task_list(self.block or [], new_me)\n",
   "            new_me.rescue = _dupe_task_list(self.rescue or [], new_me)\n",
   "            new_me.always = _dupe_task_list(self.always or [], new_me)\n",
   "        new_me._role = None\n",
   "            new_me._role = self._role\n",
   "        new_me.validate()\n",
   "        return new_me\n",
   "        data = dict()\n",
   "        for attr in self._valid_attrs:\n",
   "            if attr not in ('block', 'rescue', 'always'):\n",
   "                data[attr] = getattr(self, attr)\n",
   "        data['dep_chain'] = self.get_dep_chain()\n",
   "        data['eor'] = self._eor\n",
   "            data['role'] = self._role.serialize()\n",
   "            data['parent'] = self._parent.copy(exclude_tasks=True).serialize()\n",
   "            data['parent_type'] = self._parent.__class__.__name__\n",
   "        return data\n",
   "        for attr in self._valid_attrs:\n",
   "            if attr in data and attr not in ('block', 'rescue', 'always'):\n",
   "                setattr(self, attr, data.get(attr))\n",
   "        self._dep_chain = data.get('dep_chain', None)\n",
   "        self._eor = data.get('eor', False)\n",
   "        role_data = data.get('role')\n",
   "        if role_data:\n",
   "            r = Role()\n",
   "            r.deserialize(role_data)\n",
   "            self._role = r\n",
   "        parent_data = data.get('parent')\n",
   "        if parent_data:\n",
   "            parent_type = data.get('parent_type')\n",
   "            if parent_type == 'Block':\n",
   "                p = Block()\n",
   "            elif parent_type == 'TaskInclude':\n",
   "                p = TaskInclude()\n",
   "            elif parent_type == 'HandlerTaskInclude':\n",
   "                p = HandlerTaskInclude()\n",
   "            p.deserialize(parent_data)\n",
   "            self._parent = p\n",
   "        dep_chain = self.get_dep_chain()\n",
   "        if dep_chain:\n",
   "            for dep in dep_chain:\n",
   "                dep.set_loader(loader)\n",
   "        extend = self._valid_attrs[attr].extend\n",
   "        prepend = self._valid_attrs[attr].prepend\n",
   "            value = self._attributes[attr]\n",
   "                _parent = self._parent\n",
   "                _parent = self._parent._parent\n",
   "            if _parent and (value is Sentinel or extend):\n",
   "                    if getattr(_parent, 'statically_loaded', True):\n",
   "                        if hasattr(_parent, '_get_parent_attribute'):\n",
   "                            parent_value = _parent._get_parent_attribute(attr)\n",
   "                            parent_value = _parent._attributes.get(attr, Sentinel)\n",
   "                        if extend:\n",
   "                            value = self._extend_value(value, parent_value, prepend)\n",
   "                            value = parent_value\n",
   "            if self._role and (value is Sentinel or extend):\n",
   "                    parent_value = self._role._attributes.get(attr, Sentinel)\n",
   "                    if extend:\n",
   "                        value = self._extend_value(value, parent_value, prepend)\n",
   "                        value = parent_value\n",
   "                    dep_chain = self.get_dep_chain()\n",
   "                    if dep_chain and (value is Sentinel or extend):\n",
   "                        dep_chain.reverse()\n",
   "                        for dep in dep_chain:\n",
   "                            dep_value = dep._attributes.get(attr, Sentinel)\n",
   "                            if extend:\n",
   "                                value = self._extend_value(value, dep_value, prepend)\n",
   "                                value = dep_value\n",
   "                            if value is not Sentinel and not extend:\n",
   "            if self._play and (value is Sentinel or extend):\n",
   "                    play_value = self._play._attributes.get(attr, Sentinel)\n",
   "                    if play_value is not Sentinel:\n",
   "                        if extend:\n",
   "                            value = self._extend_value(value, play_value, prepend)\n",
   "                            value = play_value\n",
   "        return value\n",
   "            for task in target:\n",
   "                if isinstance(task, Block):\n",
   "                    filtered_block = evaluate_block(task)\n",
   "                    if filtered_block.has_tasks():\n",
   "                        tmp_list.append(filtered_block)\n",
   "                elif (task.action == 'meta' or\n",
   "                        (task.action == 'include' and task.evaluate_tags([], self._play.skip_tags, all_vars=all_vars)) or\n",
   "                        task.evaluate_tags(self._play.only_tags, self._play.skip_tags, all_vars=all_vars)):\n",
   "                    tmp_list.append(task)\n",
   "            new_block._parent = block._parent\n",
   "            new_block.block = evaluate_and_append_task(block.block)\n",
   "            new_block.rescue = evaluate_and_append_task(block.rescue)\n",
   "            new_block.always = evaluate_and_append_task(block.always)\n",
   "            return new_block\n"
  ]
 },
 "46": {
  "name": "untagged",
  "type": "frozenset",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/playbook/taggable.py",
  "lineno": "30",
  "column": "4",
  "context": "le.template import Templar\n\n\nclass Taggable:\n\n    untagged = frozenset(['untagged'])\n    _tags = FieldAttribute(isa='list', default=lis",
  "context_lines": "from ansible.module_utils.six import string_types\nfrom ansible.playbook.attribute import FieldAttribute\nfrom ansible.template import Templar\n\n\nclass Taggable:\n\n    untagged = frozenset(['untagged'])\n    _tags = FieldAttribute(isa='list', default=list, listof=(string_types, int), extend=True)\n\n    def _load_tags(self, attr, ds):\n        if isinstance(ds, list):\n            return ds\n",
  "slicing": [
   "    untagged = frozenset(['untagged'])\n"
  ]
 },
 "47": {
  "name": "method",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/playbook/base.py",
  "lineno": "105",
  "column": "20",
  "context": "ts value from a parent object\n                    method = \"_get_attr_%s\" % attr_name\n                    if method in src_dict or metho",
  "context_lines": "                    # here we selectively assign the getter based on a few\n                    # things, such as whether we have a _get_attr_<name>\n                    # method, or if the attribute is marked as not inheriting\n                    # its value from a parent object\n                    method = \"_get_attr_%s\" % attr_name\n                    if method in src_dict or method in dst_dict:\n                        getter = partial(_generic_g_method, attr_name)\n                    elif ('_get_parent_attribute' in dst_dict or '_get_parent_attribute' in src_dict) and value.inherit:\n                        getter = partial(_generic_g_parent, attr_name)\n",
  "slicing": [
   "display = Display()\n",
   "        value = self._attributes[prop_name]\n",
   "    if value is Sentinel:\n",
   "        value = self._attr_defaults[prop_name]\n",
   "    return value\n",
   "        method = \"_get_attr_%s\" % prop_name\n",
   "        return getattr(self, method)()\n",
   "            value = self._attributes[prop_name]\n",
   "                value = self._get_parent_attribute(prop_name)\n",
   "                value = self._attributes[prop_name]\n",
   "    if value is Sentinel:\n",
   "        value = self._attr_defaults[prop_name]\n",
   "    return value\n",
   "    self._attributes[prop_name] = value\n",
   "        def _create_attrs(src_dict, dst_dict):\n",
   "            keys = list(src_dict.keys())\n",
   "            for attr_name in keys:\n",
   "                value = src_dict[attr_name]\n",
   "                if isinstance(value, Attribute):\n",
   "                    if attr_name.startswith('_'):\n",
   "                        attr_name = attr_name[1:]\n",
   "                    method = \"_get_attr_%s\" % attr_name\n",
   "                    if method in src_dict or method in dst_dict:\n",
   "                        getter = partial(_generic_g_method, attr_name)\n",
   "                    elif ('_get_parent_attribute' in dst_dict or '_get_parent_attribute' in src_dict) and value.inherit:\n",
   "                        getter = partial(_generic_g_parent, attr_name)\n",
   "                        getter = partial(_generic_g, attr_name)\n",
   "                    setter = partial(_generic_s, attr_name)\n",
   "                    deleter = partial(_generic_d, attr_name)\n",
   "                    dst_dict[attr_name] = property(getter, setter, deleter)\n",
   "                    dst_dict['_valid_attrs'][attr_name] = value\n",
   "                    dst_dict['_attributes'][attr_name] = Sentinel\n",
   "                    dst_dict['_attr_defaults'][attr_name] = value.default\n",
   "                    if value.alias is not None:\n",
   "                        dst_dict[value.alias] = property(getter, setter, deleter)\n",
   "                        dst_dict['_valid_attrs'][value.alias] = value\n",
   "                        dst_dict['_alias_attrs'][value.alias] = attr_name\n",
   "        def _process_parents(parents, dst_dict):\n",
   "            for parent in parents:\n",
   "                if hasattr(parent, '__dict__'):\n",
   "                    _create_attrs(parent.__dict__, dst_dict)\n",
   "                    new_dst_dict = parent.__dict__.copy()\n",
   "                    new_dst_dict.update(dst_dict)\n",
   "                    _process_parents(parent.__bases__, new_dst_dict)\n",
   "        _create_attrs(dct, dct)\n",
   "        _process_parents(parents, dct)\n",
   "        return super(BaseMeta, cls).__new__(cls, name, parents, dct)\n",
   "        for key, value in self._attr_defaults.items():\n",
   "            if callable(value):\n",
   "                self._attr_defaults[key] = value()\n",
   "            display.debug(\"DUMPING OBJECT ------------------------------------------------------\")\n",
   "        display.debug(\"%s- %s (%s, id=%s)\" % (\" \" * depth, self.__class__.__name__, self, id(self)))\n",
   "            dep_chain = self._parent.get_dep_chain()\n",
   "            if dep_chain:\n",
   "                for dep in dep_chain:\n",
   "                    dep.dump_me(depth + 2)\n",
   "        ds = self.preprocess_data(ds)\n",
   "        self._validate_attributes(ds)\n",
   "        for name, attr in sorted(iteritems(self._valid_attrs), key=operator.itemgetter(1)):\n",
   "            target_name = name\n",
   "            if name in self._alias_attrs:\n",
   "                target_name = self._alias_attrs[name]\n",
   "            if name in ds:\n",
   "                method = getattr(self, '_load_%s' % name, None)\n",
   "                if method:\n",
   "                    self._attributes[target_name] = method(name, ds[name])\n",
   "                    self._attributes[target_name] = ds[name]\n",
   "        value = templar.template(value)\n",
   "        valid_values = frozenset(('always', 'on_failed', 'on_unreachable', 'on_skipped', 'never'))\n",
   "        if value and isinstance(value, string_types) and value not in valid_values:\n",
   "            raise AnsibleParserError(\"'%s' is not a valid value for debugger. Must be one of %s\" % (value, ', '.join(valid_values)), obj=self.get_ds())\n",
   "        return value\n",
   "        valid_attrs = frozenset(self._valid_attrs.keys())\n",
   "        for key in ds:\n",
   "            if key not in valid_attrs:\n",
   "                raise AnsibleParserError(\"'%s' is not a valid attribute for a %s\" % (key, self.__class__.__name__), obj=ds)\n",
   "        all_vars = {} if all_vars is None else all_vars\n",
   "            for (name, attribute) in iteritems(self._valid_attrs):\n",
   "                if name in self._alias_attrs:\n",
   "                    name = self._alias_attrs[name]\n",
   "                method = getattr(self, '_validate_%s' % name, None)\n",
   "                if method:\n",
   "                    method(attribute, name, getattr(self, name))\n",
   "                    value = self._attributes[name]\n",
   "                    if value is not None:\n",
   "                        if attribute.isa == 'string' and isinstance(value, (list, dict)):\n",
   "                                \" however the incoming data structure is a %s\" % (name, type(value)), obj=self.get_ds()\n",
   "            for name in self._valid_attrs.keys():\n",
   "                self._attributes[name] = getattr(self, name)\n",
   "        new_me = self.__class__()\n",
   "        for name in self._valid_attrs.keys():\n",
   "            if name in self._alias_attrs:\n",
   "            new_me._attributes[name] = shallowcopy(self._attributes[name])\n",
   "            new_me._attr_defaults[name] = shallowcopy(self._attr_defaults[name])\n",
   "        new_me._loader = self._loader\n",
   "        new_me._variable_manager = self._variable_manager\n",
   "        new_me._validated = self._validated\n",
   "        new_me._finalized = self._finalized\n",
   "        new_me._uuid = self._uuid\n",
   "            new_me._ds = self._ds\n",
   "        return new_me\n",
   "        if attribute.isa == 'string':\n",
   "            value = to_text(value)\n",
   "        elif attribute.isa == 'int':\n",
   "            value = int(value)\n",
   "        elif attribute.isa == 'float':\n",
   "            value = float(value)\n",
   "        elif attribute.isa == 'bool':\n",
   "            value = boolean(value, strict=True)\n",
   "        elif attribute.isa == 'percent':\n",
   "            if isinstance(value, string_types) and '%' in value:\n",
   "                value = value.replace('%', '')\n",
   "            value = float(value)\n",
   "        elif attribute.isa == 'list':\n",
   "            if value is None:\n",
   "                value = []\n",
   "            elif not isinstance(value, list):\n",
   "                value = [value]\n",
   "            if attribute.listof is not None:\n",
   "                for item in value:\n",
   "                    if not isinstance(item, attribute.listof):\n",
   "                                                 \"but the item '%s' is a %s\" % (name, attribute.listof, item, type(item)), obj=self.get_ds())\n",
   "                    elif attribute.required and attribute.listof == string_types:\n",
   "                        if item is None or item.strip() == \"\":\n",
   "                            raise AnsibleParserError(\"the field '%s' is required, and cannot have empty values\" % (name,), obj=self.get_ds())\n",
   "        elif attribute.isa == 'set':\n",
   "            if value is None:\n",
   "                value = set()\n",
   "            elif not isinstance(value, (list, set)):\n",
   "                if isinstance(value, string_types):\n",
   "                    value = value.split(',')\n",
   "                    value = [value]\n",
   "            if not isinstance(value, set):\n",
   "                value = set(value)\n",
   "        elif attribute.isa == 'dict':\n",
   "            if value is None:\n",
   "                value = dict()\n",
   "            elif not isinstance(value, dict):\n",
   "                raise TypeError(\"%s is not a dictionary\" % value)\n",
   "        elif attribute.isa == 'class':\n",
   "            if not isinstance(value, attribute.class_type):\n",
   "                raise TypeError(\"%s is not a valid %s (got a %s instead)\" % (name, attribute.class_type, type(value)))\n",
   "            value.post_validate(templar=templar)\n",
   "        return value\n",
   "        omit_value = templar.available_variables.get('omit')\n",
   "        for (name, attribute) in iteritems(self._valid_attrs):\n",
   "            if attribute.static:\n",
   "                value = getattr(self, name)\n",
   "                if name not in ('vars',) and templar.is_template(value):\n",
   "                    display.warning('\"%s\" is not templatable, but we found: %s, '\n",
   "                                    'it will not be templated and will be used \"as is\".' % (name, value))\n",
   "            if getattr(self, name) is None:\n",
   "                if not attribute.required:\n",
   "                    raise AnsibleParserError(\"the field '%s' is required but was not set\" % name)\n",
   "            elif not attribute.always_post_validate and self.__class__.__name__ not in ('Task', 'Handler', 'PlayContext'):\n",
   "                method = getattr(self, '_post_validate_%s' % name, None)\n",
   "                if method:\n",
   "                    value = method(attribute, getattr(self, name), templar)\n",
   "                elif attribute.isa == 'class':\n",
   "                    value = getattr(self, name)\n",
   "                    value = templar.template(getattr(self, name))\n",
   "                if omit_value is not None and value == omit_value:\n",
   "                    if callable(attribute.default):\n",
   "                        setattr(self, name, attribute.default())\n",
   "                        setattr(self, name, attribute.default)\n",
   "                if value is not None:\n",
   "                    value = self.get_validated_value(name, attribute, value, templar)\n",
   "                setattr(self, name, value)\n",
   "                value = getattr(self, name)\n",
   "                                         \"The error was: %s\" % (name, value, attribute.isa, e), obj=self.get_ds(), orig_exc=e)\n",
   "                if templar._fail_on_undefined_errors and name != 'name':\n",
   "                    if name == 'args':\n",
   "                        msg = \"The task includes an option with an undefined variable. The error was: %s\" % (to_native(e))\n",
   "                        msg = \"The field '%s' has an invalid value, which includes an undefined variable. The error was: %s\" % (name, to_native(e))\n",
   "                    raise AnsibleParserError(msg, obj=self.get_ds(), orig_exc=e)\n",
   "        def _validate_variable_keys(ds):\n",
   "            for key in ds:\n",
   "                if not isidentifier(key):\n",
   "                    raise TypeError(\"'%s' is not a valid variable name\" % key)\n",
   "            if isinstance(ds, dict):\n",
   "                _validate_variable_keys(ds)\n",
   "                return combine_vars(self.vars, ds)\n",
   "            elif isinstance(ds, list):\n",
   "                all_vars = self.vars\n",
   "                for item in ds:\n",
   "                    if not isinstance(item, dict):\n",
   "                    _validate_variable_keys(item)\n",
   "                    all_vars = combine_vars(all_vars, item)\n",
   "                return all_vars\n",
   "            elif ds is None:\n",
   "                                     obj=ds, orig_exc=e)\n",
   "            raise AnsibleParserError(\"Invalid variable name in vars specified for %s: %s\" % (self.__class__.__name__, e), obj=ds, orig_exc=e)\n",
   "        if not isinstance(value, list):\n",
   "            value = [value]\n",
   "            new_value = [new_value]\n",
   "        value = [v for v in value if v is not Sentinel]\n",
   "        new_value = [v for v in new_value if v is not Sentinel]\n",
   "            combined = new_value + value\n",
   "            combined = value + new_value\n",
   "        return [i for i, _ in itertools.groupby(combined) if i is not None]\n",
   "        attrs = {}\n",
   "        for (name, attribute) in iteritems(self._valid_attrs):\n",
   "            attr = getattr(self, name)\n",
   "            if attribute.isa == 'class' and hasattr(attr, 'serialize'):\n",
   "                attrs[name] = attr.serialize()\n",
   "                attrs[name] = attr\n",
   "        return attrs\n",
   "        for (attr, value) in iteritems(attrs):\n",
   "            if attr in self._valid_attrs:\n",
   "                attribute = self._valid_attrs[attr]\n",
   "                if attribute.isa == 'class' and isinstance(value, dict):\n",
   "                    obj = attribute.class_type()\n",
   "                    obj.deserialize(value)\n",
   "                    setattr(self, attr, obj)\n",
   "                    setattr(self, attr, value)\n",
   "            if name in data:\n",
   "                setattr(self, name, data[name])\n",
   "                if callable(attribute.default):\n",
   "                    setattr(self, name, attribute.default())\n",
   "                    setattr(self, name, attribute.default)\n"
  ]
 },
 "48": {
  "name": "new_dst_dict",
  "type": "dict",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/playbook/base.py",
  "lineno": "134",
  "column": "20",
  "context": "rs(parent.__dict__, dst_dict)\n                    new_dst_dict = parent.__dict__.copy()\n                    new_dst_dict.update(dst_dict)\n",
  "context_lines": "            '''\n            for parent in parents:\n                if hasattr(parent, '__dict__'):\n                    _create_attrs(parent.__dict__, dst_dict)\n                    new_dst_dict = parent.__dict__.copy()\n                    new_dst_dict.update(dst_dict)\n                    _process_parents(parent.__bases__, new_dst_dict)\n\n        # create some additional class attributes\n        dct['_attributes'] = {}\n",
  "slicing": [
   "display = Display()\n",
   "        value = self._attributes[prop_name]\n",
   "    if value is Sentinel:\n",
   "        value = self._attr_defaults[prop_name]\n",
   "    return value\n",
   "        method = \"_get_attr_%s\" % prop_name\n",
   "        return getattr(self, method)()\n",
   "            value = self._attributes[prop_name]\n",
   "                value = self._get_parent_attribute(prop_name)\n",
   "                value = self._attributes[prop_name]\n",
   "    if value is Sentinel:\n",
   "        value = self._attr_defaults[prop_name]\n",
   "    return value\n",
   "    self._attributes[prop_name] = value\n",
   "        def _create_attrs(src_dict, dst_dict):\n",
   "            keys = list(src_dict.keys())\n",
   "            for attr_name in keys:\n",
   "                value = src_dict[attr_name]\n",
   "                if isinstance(value, Attribute):\n",
   "                    if attr_name.startswith('_'):\n",
   "                        attr_name = attr_name[1:]\n",
   "                    method = \"_get_attr_%s\" % attr_name\n",
   "                    if method in src_dict or method in dst_dict:\n",
   "                        getter = partial(_generic_g_method, attr_name)\n",
   "                    elif ('_get_parent_attribute' in dst_dict or '_get_parent_attribute' in src_dict) and value.inherit:\n",
   "                        getter = partial(_generic_g_parent, attr_name)\n",
   "                        getter = partial(_generic_g, attr_name)\n",
   "                    setter = partial(_generic_s, attr_name)\n",
   "                    deleter = partial(_generic_d, attr_name)\n",
   "                    dst_dict[attr_name] = property(getter, setter, deleter)\n",
   "                    dst_dict['_valid_attrs'][attr_name] = value\n",
   "                    dst_dict['_attributes'][attr_name] = Sentinel\n",
   "                    dst_dict['_attr_defaults'][attr_name] = value.default\n",
   "                    if value.alias is not None:\n",
   "                        dst_dict[value.alias] = property(getter, setter, deleter)\n",
   "                        dst_dict['_valid_attrs'][value.alias] = value\n",
   "                        dst_dict['_alias_attrs'][value.alias] = attr_name\n",
   "        def _process_parents(parents, dst_dict):\n",
   "            for parent in parents:\n",
   "                if hasattr(parent, '__dict__'):\n",
   "                    _create_attrs(parent.__dict__, dst_dict)\n",
   "                    new_dst_dict = parent.__dict__.copy()\n",
   "                    new_dst_dict.update(dst_dict)\n",
   "                    _process_parents(parent.__bases__, new_dst_dict)\n",
   "        _create_attrs(dct, dct)\n",
   "        _process_parents(parents, dct)\n",
   "        return super(BaseMeta, cls).__new__(cls, name, parents, dct)\n",
   "        for key, value in self._attr_defaults.items():\n",
   "            if callable(value):\n",
   "                self._attr_defaults[key] = value()\n",
   "            display.debug(\"DUMPING OBJECT ------------------------------------------------------\")\n",
   "        display.debug(\"%s- %s (%s, id=%s)\" % (\" \" * depth, self.__class__.__name__, self, id(self)))\n",
   "            dep_chain = self._parent.get_dep_chain()\n",
   "            if dep_chain:\n",
   "                for dep in dep_chain:\n",
   "                    dep.dump_me(depth + 2)\n",
   "        ds = self.preprocess_data(ds)\n",
   "        self._validate_attributes(ds)\n",
   "        for name, attr in sorted(iteritems(self._valid_attrs), key=operator.itemgetter(1)):\n",
   "            target_name = name\n",
   "            if name in self._alias_attrs:\n",
   "                target_name = self._alias_attrs[name]\n",
   "            if name in ds:\n",
   "                method = getattr(self, '_load_%s' % name, None)\n",
   "                if method:\n",
   "                    self._attributes[target_name] = method(name, ds[name])\n",
   "                    self._attributes[target_name] = ds[name]\n",
   "        value = templar.template(value)\n",
   "        valid_values = frozenset(('always', 'on_failed', 'on_unreachable', 'on_skipped', 'never'))\n",
   "        if value and isinstance(value, string_types) and value not in valid_values:\n",
   "            raise AnsibleParserError(\"'%s' is not a valid value for debugger. Must be one of %s\" % (value, ', '.join(valid_values)), obj=self.get_ds())\n",
   "        return value\n",
   "        valid_attrs = frozenset(self._valid_attrs.keys())\n",
   "        for key in ds:\n",
   "            if key not in valid_attrs:\n",
   "                raise AnsibleParserError(\"'%s' is not a valid attribute for a %s\" % (key, self.__class__.__name__), obj=ds)\n",
   "        all_vars = {} if all_vars is None else all_vars\n",
   "            for (name, attribute) in iteritems(self._valid_attrs):\n",
   "                if name in self._alias_attrs:\n",
   "                    name = self._alias_attrs[name]\n",
   "                method = getattr(self, '_validate_%s' % name, None)\n",
   "                if method:\n",
   "                    method(attribute, name, getattr(self, name))\n",
   "                    value = self._attributes[name]\n",
   "                    if value is not None:\n",
   "                        if attribute.isa == 'string' and isinstance(value, (list, dict)):\n",
   "                                \" however the incoming data structure is a %s\" % (name, type(value)), obj=self.get_ds()\n",
   "            for name in self._valid_attrs.keys():\n",
   "                self._attributes[name] = getattr(self, name)\n",
   "        new_me = self.__class__()\n",
   "        for name in self._valid_attrs.keys():\n",
   "            if name in self._alias_attrs:\n",
   "            new_me._attributes[name] = shallowcopy(self._attributes[name])\n",
   "            new_me._attr_defaults[name] = shallowcopy(self._attr_defaults[name])\n",
   "        new_me._loader = self._loader\n",
   "        new_me._variable_manager = self._variable_manager\n",
   "        new_me._validated = self._validated\n",
   "        new_me._finalized = self._finalized\n",
   "        new_me._uuid = self._uuid\n",
   "            new_me._ds = self._ds\n",
   "        return new_me\n",
   "        if attribute.isa == 'string':\n",
   "            value = to_text(value)\n",
   "        elif attribute.isa == 'int':\n",
   "            value = int(value)\n",
   "        elif attribute.isa == 'float':\n",
   "            value = float(value)\n",
   "        elif attribute.isa == 'bool':\n",
   "            value = boolean(value, strict=True)\n",
   "        elif attribute.isa == 'percent':\n",
   "            if isinstance(value, string_types) and '%' in value:\n",
   "                value = value.replace('%', '')\n",
   "            value = float(value)\n",
   "        elif attribute.isa == 'list':\n",
   "            if value is None:\n",
   "                value = []\n",
   "            elif not isinstance(value, list):\n",
   "                value = [value]\n",
   "            if attribute.listof is not None:\n",
   "                for item in value:\n",
   "                    if not isinstance(item, attribute.listof):\n",
   "                                                 \"but the item '%s' is a %s\" % (name, attribute.listof, item, type(item)), obj=self.get_ds())\n",
   "                    elif attribute.required and attribute.listof == string_types:\n",
   "                        if item is None or item.strip() == \"\":\n",
   "                            raise AnsibleParserError(\"the field '%s' is required, and cannot have empty values\" % (name,), obj=self.get_ds())\n",
   "        elif attribute.isa == 'set':\n",
   "            if value is None:\n",
   "                value = set()\n",
   "            elif not isinstance(value, (list, set)):\n",
   "                if isinstance(value, string_types):\n",
   "                    value = value.split(',')\n",
   "                    value = [value]\n",
   "            if not isinstance(value, set):\n",
   "                value = set(value)\n",
   "        elif attribute.isa == 'dict':\n",
   "            if value is None:\n",
   "                value = dict()\n",
   "            elif not isinstance(value, dict):\n",
   "                raise TypeError(\"%s is not a dictionary\" % value)\n",
   "        elif attribute.isa == 'class':\n",
   "            if not isinstance(value, attribute.class_type):\n",
   "                raise TypeError(\"%s is not a valid %s (got a %s instead)\" % (name, attribute.class_type, type(value)))\n",
   "            value.post_validate(templar=templar)\n",
   "        return value\n",
   "        omit_value = templar.available_variables.get('omit')\n",
   "        for (name, attribute) in iteritems(self._valid_attrs):\n",
   "            if attribute.static:\n",
   "                value = getattr(self, name)\n",
   "                if name not in ('vars',) and templar.is_template(value):\n",
   "                    display.warning('\"%s\" is not templatable, but we found: %s, '\n",
   "                                    'it will not be templated and will be used \"as is\".' % (name, value))\n",
   "            if getattr(self, name) is None:\n",
   "                if not attribute.required:\n",
   "                    raise AnsibleParserError(\"the field '%s' is required but was not set\" % name)\n",
   "            elif not attribute.always_post_validate and self.__class__.__name__ not in ('Task', 'Handler', 'PlayContext'):\n",
   "                method = getattr(self, '_post_validate_%s' % name, None)\n",
   "                if method:\n",
   "                    value = method(attribute, getattr(self, name), templar)\n",
   "                elif attribute.isa == 'class':\n",
   "                    value = getattr(self, name)\n",
   "                    value = templar.template(getattr(self, name))\n",
   "                if omit_value is not None and value == omit_value:\n",
   "                    if callable(attribute.default):\n",
   "                        setattr(self, name, attribute.default())\n",
   "                        setattr(self, name, attribute.default)\n",
   "                if value is not None:\n",
   "                    value = self.get_validated_value(name, attribute, value, templar)\n",
   "                setattr(self, name, value)\n",
   "                value = getattr(self, name)\n",
   "                                         \"The error was: %s\" % (name, value, attribute.isa, e), obj=self.get_ds(), orig_exc=e)\n",
   "                if templar._fail_on_undefined_errors and name != 'name':\n",
   "                    if name == 'args':\n",
   "                        msg = \"The task includes an option with an undefined variable. The error was: %s\" % (to_native(e))\n",
   "                        msg = \"The field '%s' has an invalid value, which includes an undefined variable. The error was: %s\" % (name, to_native(e))\n",
   "                    raise AnsibleParserError(msg, obj=self.get_ds(), orig_exc=e)\n",
   "        def _validate_variable_keys(ds):\n",
   "            for key in ds:\n",
   "                if not isidentifier(key):\n",
   "                    raise TypeError(\"'%s' is not a valid variable name\" % key)\n",
   "            if isinstance(ds, dict):\n",
   "                _validate_variable_keys(ds)\n",
   "                return combine_vars(self.vars, ds)\n",
   "            elif isinstance(ds, list):\n",
   "                all_vars = self.vars\n",
   "                for item in ds:\n",
   "                    if not isinstance(item, dict):\n",
   "                    _validate_variable_keys(item)\n",
   "                    all_vars = combine_vars(all_vars, item)\n",
   "                return all_vars\n",
   "            elif ds is None:\n",
   "                                     obj=ds, orig_exc=e)\n",
   "            raise AnsibleParserError(\"Invalid variable name in vars specified for %s: %s\" % (self.__class__.__name__, e), obj=ds, orig_exc=e)\n",
   "        if not isinstance(value, list):\n",
   "            value = [value]\n",
   "            new_value = [new_value]\n",
   "        value = [v for v in value if v is not Sentinel]\n",
   "        new_value = [v for v in new_value if v is not Sentinel]\n",
   "            combined = new_value + value\n",
   "            combined = value + new_value\n",
   "        return [i for i, _ in itertools.groupby(combined) if i is not None]\n",
   "        attrs = {}\n",
   "        for (name, attribute) in iteritems(self._valid_attrs):\n",
   "            attr = getattr(self, name)\n",
   "            if attribute.isa == 'class' and hasattr(attr, 'serialize'):\n",
   "                attrs[name] = attr.serialize()\n",
   "                attrs[name] = attr\n",
   "        return attrs\n",
   "        for (attr, value) in iteritems(attrs):\n",
   "            if attr in self._valid_attrs:\n",
   "                attribute = self._valid_attrs[attr]\n",
   "                if attribute.isa == 'class' and isinstance(value, dict):\n",
   "                    obj = attribute.class_type()\n",
   "                    obj.deserialize(value)\n",
   "                    setattr(self, attr, obj)\n",
   "                    setattr(self, attr, value)\n",
   "            if name in data:\n",
   "                setattr(self, name, data[name])\n",
   "                if callable(attribute.default):\n",
   "                    setattr(self, name, attribute.default())\n",
   "                    setattr(self, name, attribute.default)\n"
  ]
 },
 "49": {
  "name": "BASE",
  "type": "frozenset",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/playbook/task_include.py",
  "lineno": "42",
  "column": "4",
  "context": "elated to the `- include: ...` task.\n    \"\"\"\n\n    BASE = frozenset(('file', '_raw_params'))  # directly assigned\n    OTHER_ARGS = frozenset(('apply',))  # assigned",
  "context_lines": "    \"\"\"\n    A task include is derived from a regular task to handle the special\n    circumstances related to the `- include: ...` task.\n    \"\"\"\n\n    BASE = frozenset(('file', '_raw_params'))  # directly assigned\n    OTHER_ARGS = frozenset(('apply',))  # assigned to matching property\n    VALID_ARGS = BASE.union(OTHER_ARGS)  # all valid args\n    VALID_INCLUDE_KEYWORDS = frozenset(('action', 'args', 'collections', 'debugger', 'ignore_errors', 'loop', 'loop_control',\n                                        'loop_with', 'name', 'no_log', 'register', 'run_once', 'tags', 'vars',\n",
  "slicing": [
   "    BASE = frozenset(('file', '_raw_params'))  # directly assigned\n",
   "    OTHER_ARGS = frozenset(('apply',))  # assigned to matching property\n",
   "    VALID_ARGS = BASE.union(OTHER_ARGS)  # all valid args\n"
  ]
 },
 "50": {
  "name": "OTHER_ARGS",
  "type": "frozenset",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/playbook/task_include.py",
  "lineno": "43",
  "column": "4",
  "context": "('file', '_raw_params'))  # directly assigned\n    OTHER_ARGS = frozenset(('apply',))  # assigned to matching property\n    VALID_ARGS = BASE.union(OTHER_ARGS)  # all val",
  "context_lines": "    A task include is derived from a regular task to handle the special\n    circumstances related to the `- include: ...` task.\n    \"\"\"\n\n    BASE = frozenset(('file', '_raw_params'))  # directly assigned\n    OTHER_ARGS = frozenset(('apply',))  # assigned to matching property\n    VALID_ARGS = BASE.union(OTHER_ARGS)  # all valid args\n    VALID_INCLUDE_KEYWORDS = frozenset(('action', 'args', 'collections', 'debugger', 'ignore_errors', 'loop', 'loop_control',\n                                        'loop_with', 'name', 'no_log', 'register', 'run_once', 'tags', 'vars',\n                                        'when'))\n\n",
  "slicing": [
   "    BASE = frozenset(('file', '_raw_params'))  # directly assigned\n",
   "    OTHER_ARGS = frozenset(('apply',))  # assigned to matching property\n",
   "    VALID_ARGS = BASE.union(OTHER_ARGS)  # all valid args\n"
  ]
 },
 "51": {
  "name": "VALID_ARGS",
  "type": "frozenset",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/playbook/task_include.py",
  "lineno": "44",
  "column": "4",
  "context": "(('apply',))  # assigned to matching property\n    VALID_ARGS = BASE.union(OTHER_ARGS)  # all valid args\n    VALID_INCLUDE_KEYWORDS = frozenset(('action', ",
  "context_lines": "    circumstances related to the `- include: ...` task.\n    \"\"\"\n\n    BASE = frozenset(('file', '_raw_params'))  # directly assigned\n    OTHER_ARGS = frozenset(('apply',))  # assigned to matching property\n    VALID_ARGS = BASE.union(OTHER_ARGS)  # all valid args\n    VALID_INCLUDE_KEYWORDS = frozenset(('action', 'args', 'collections', 'debugger', 'ignore_errors', 'loop', 'loop_control',\n                                        'loop_with', 'name', 'no_log', 'register', 'run_once', 'tags', 'vars',\n                                        'when'))\n\n    # =================================================================================\n",
  "slicing": [
   "    BASE = frozenset(('file', '_raw_params'))  # directly assigned\n",
   "    OTHER_ARGS = frozenset(('apply',))  # assigned to matching property\n",
   "    VALID_ARGS = BASE.union(OTHER_ARGS)  # all valid args\n"
  ]
 },
 "52": {
  "name": "VALID_INCLUDE_KEYWORDS",
  "type": "frozenset",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/playbook/task_include.py",
  "lineno": "45",
  "column": "4",
  "context": "GS = BASE.union(OTHER_ARGS)  # all valid args\n    VALID_INCLUDE_KEYWORDS = frozenset(('action', 'args', 'collections', 'debugger', 'ignore_errors', 'loop', 'loop_control',\n                                        'loop_with",
  "context_lines": "    \"\"\"\n\n    BASE = frozenset(('file', '_raw_params'))  # directly assigned\n    OTHER_ARGS = frozenset(('apply',))  # assigned to matching property\n    VALID_ARGS = BASE.union(OTHER_ARGS)  # all valid args\n    VALID_INCLUDE_KEYWORDS = frozenset(('action', 'args', 'collections', 'debugger', 'ignore_errors', 'loop', 'loop_control',\n                                        'loop_with', 'name', 'no_log', 'register', 'run_once', 'tags', 'vars',\n                                        'when'))\n\n    # =================================================================================\n    # ATTRIBUTES\n\n",
  "slicing": [
   "    BASE = frozenset(('file', '_raw_params'))  # directly assigned\n",
   "    OTHER_ARGS = frozenset(('apply',))  # assigned to matching property\n",
   "    VALID_ARGS = BASE.union(OTHER_ARGS)  # all valid args\n",
   "    VALID_INCLUDE_KEYWORDS = frozenset(('action', 'args', 'collections', 'debugger', 'ignore_errors', 'loop', 'loop_control',\n"
  ]
 },
 "53": {
  "name": "default_collection",
  "type": "NoneType",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/playbook/collectionsearch.py",
  "lineno": "17",
  "column": "4",
  "context": "ure_default_collection(collection_list=None):\n    default_collection = AnsibleCollectionConfig.default_collection\n\n    # Will be None when used as the default\n    i",
  "context_lines": "from ansible.template import is_template, Environment\nfrom ansible.utils.display import Display\n\ndisplay = Display()\n\n\ndef _ensure_default_collection(collection_list=None):\n    default_collection = AnsibleCollectionConfig.default_collection\n\n    # Will be None when used as the default\n    if collection_list is None:\n        collection_list = []\n\n",
  "slicing": [
   "    default_collection = AnsibleCollectionConfig.default_collection\n",
   "    if default_collection and default_collection not in collection_list:\n",
   "        collection_list.insert(0, default_collection)\n"
  ]
 },
 "54": {
  "name": "to_stderr",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/utils/display.py",
  "lineno": "341",
  "column": "8",
  "context": "rbose(self, msg, host=None, caplevel=2):\n\n        to_stderr = C.VERBOSE_TO_STDERR\n        if self.verbosity > caplevel:\n            ",
  "context_lines": "                self.display(\"%6d %0.5f: %s\" % (os.getpid(), time.time(), msg), color=C.COLOR_DEBUG)\n            else:\n                self.display(\"%6d %0.5f [%s]: %s\" % (os.getpid(), time.time(), host, msg), color=C.COLOR_DEBUG)\n\n    def verbose(self, msg, host=None, caplevel=2):\n\n        to_stderr = C.VERBOSE_TO_STDERR\n        if self.verbosity > caplevel:\n            if host is None:\n                self.display(msg, color=C.COLOR_VERBOSE, stderr=to_stderr)\n            else:\n",
  "slicing": [
   "    input = raw_input\n",
   "_LIBC = ctypes.cdll.LoadLibrary(ctypes.util.find_library('c'))\n",
   "_LIBC.wcwidth.argtypes = (ctypes.c_wchar,)\n",
   "_LIBC.wcswidth.argtypes = (ctypes.c_wchar_p, ctypes.c_int)\n",
   "_MAX_INT = 2 ** (ctypes.sizeof(ctypes.c_int) * 8 - 1) - 1\n",
   "_LOCALE_INITIALIZED = False\n",
   "_LOCALE_INITIALIZATION_ERR = None\n",
   "    if _LOCALE_INITIALIZED is False:\n",
   "            _LOCALE_INITIALIZATION_ERR = e\n",
   "            _LOCALE_INITIALIZED = True\n",
   "def get_text_width(text):\n",
   "    if _LOCALE_INITIALIZATION_ERR:\n",
   "            'cause Display to print incorrect line lengths' % _LOCALE_INITIALIZATION_ERR\n",
   "    elif not _LOCALE_INITIALIZED:\n",
   "        width = _LIBC.wcswidth(text, _MAX_INT)\n",
   "        width = -1\n",
   "    if width != -1:\n",
   "        return width\n",
   "    width = 0\n",
   "    counter = 0\n",
   "    for c in text:\n",
   "        counter += 1\n",
   "        if c in (u'\\x08', u'\\x7f', u'\\x94', u'\\x1b'):\n",
   "            width -= 1\n",
   "            counter -= 1\n",
   "            w = _LIBC.wcwidth(c)\n",
   "            w = -1\n",
   "        if w == -1:\n",
   "            w = 0\n",
   "        width += w\n",
   "    if width == 0 and counter and not _LOCALE_INITIALIZED:\n",
   "            'and get_text_width could not calculate text width of %r' % text\n",
   "    return width if width >= 0 else 0\n",
   "        self.blacklist = [logging.Filter(name) for name in blacklist]\n",
   "        return not any(f.filter(record) for f in self.blacklist)\n",
   "logger = None\n",
   "    path = C.DEFAULT_LOG_PATH\n",
   "    if path and (os.path.exists(path) and os.access(path, os.W_OK)) or os.access(os.path.dirname(path), os.W_OK):\n",
   "        logging.basicConfig(filename=path, level=logging.INFO,  # DO NOT set to logging.DEBUG\n",
   "        logger = logging.getLogger('ansible')\n",
   "        for handler in logging.root.handlers:\n",
   "            handler.addFilter(FilterBlackList(getattr(C, 'DEFAULT_LOG_FILTER', [])))\n",
   "            handler.addFilter(FilterUserInjector())\n",
   "        print(\"[WARNING]: log file at %s is not writeable and we cannot create it, aborting\\n\" % path, file=sys.stderr)\n",
   "color_to_log_level = {C.COLOR_ERROR: logging.ERROR,\n",
   "b_COW_PATHS = (\n",
   "                cmd = subprocess.Popen([self.b_cowsay, \"-l\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
   "                (out, err) = cmd.communicate()\n",
   "                self.cows_available = set([to_text(c) for c in out.split()])\n",
   "            for b_cow_path in b_COW_PATHS:\n",
   "                if os.path.exists(b_cow_path):\n",
   "                    self.b_cowsay = b_cow_path\n",
   "        nocolor = msg\n",
   "            has_newline = msg.endswith(u'\\n')\n",
   "            if has_newline:\n",
   "                msg2 = msg[:-1]\n",
   "                msg2 = msg\n",
   "                msg2 = stringc(msg2, color)\n",
   "            if has_newline or newline:\n",
   "                msg2 = msg2 + u'\\n'\n",
   "            msg2 = to_bytes(msg2, encoding=self._output_encoding(stderr=stderr))\n",
   "                msg2 = to_text(msg2, self._output_encoding(stderr=stderr), errors='replace')\n",
   "                fileobj = sys.stdout\n",
   "                fileobj = sys.stderr\n",
   "            fileobj.write(msg2)\n",
   "                fileobj.flush()\n",
   "        if logger and not screen_only:\n",
   "            msg2 = to_bytes(nocolor.lstrip(u'\\n'))\n",
   "                msg2 = to_text(msg2, self._output_encoding(stderr=stderr))\n",
   "            lvl = logging.INFO\n",
   "                    lvl = color_to_log_level[color]\n",
   "            logger.log(lvl, msg2)\n",
   "        to_stderr = C.VERBOSE_TO_STDERR\n",
   "                self.display(msg, color=C.COLOR_VERBOSE, stderr=to_stderr)\n",
   "                self.display(\"<%s> %s\" % (host, msg), color=C.COLOR_VERBOSE, stderr=to_stderr)\n",
   "        msg = msg.strip()\n",
   "        if msg and msg[-1] not in ['!', '?', '.']:\n",
   "            msg += '.'\n",
   "            collection_name = 'ansible-base'\n",
   "            header = '[DEPRECATED]: {0}'.format(msg)\n",
   "            removal_fragment = 'This feature was removed'\n",
   "            help_text = 'Please update your playbooks.'\n",
   "            header = '[DEPRECATION WARNING]: {0}'.format(msg)\n",
   "            removal_fragment = 'This feature will be removed'\n",
   "            help_text = 'Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg.'\n",
   "        if collection_name:\n",
   "            from_fragment = 'from {0}'.format(collection_name)\n",
   "            from_fragment = ''\n",
   "            when = 'in a release after {0}.'.format(date)\n",
   "            when = 'in version {0}.'.format(version)\n",
   "            when = 'in a future release.'\n",
   "        message_text = ' '.join(f for f in [header, removal_fragment, from_fragment, when, help_text] if f)\n",
   "        return message_text\n",
   "        message_text = self.get_deprecation_message(msg, version=version, removed=removed, date=date, collection_name=collection_name)\n",
   "            raise AnsibleError(message_text)\n",
   "        wrapped = textwrap.wrap(message_text, self.columns, drop_whitespace=False)\n",
   "        message_text = \"\\n\".join(wrapped) + \"\\n\"\n",
   "        if message_text not in self._deprecations:\n",
   "            self.display(message_text.strip(), color=C.COLOR_DEPRECATE, stderr=True)\n",
   "            self._deprecations[message_text] = 1\n",
   "            new_msg = \"[WARNING]: %s\" % msg\n",
   "            wrapped = textwrap.wrap(new_msg, self.columns)\n",
   "            new_msg = \"\\n\".join(wrapped) + \"\\n\"\n",
   "            new_msg = \"\\n[WARNING]: \\n%s\" % msg\n",
   "        if new_msg not in self._warns:\n",
   "            self.display(new_msg, color=C.COLOR_WARN, stderr=True)\n",
   "            self._warns[new_msg] = 1\n",
   "            self.warning(msg)\n",
   "        msg = to_text(msg)\n",
   "                self.banner_cowsay(msg)\n",
   "        msg = msg.strip()\n",
   "            star_len = self.columns - get_text_width(msg)\n",
   "            star_len = self.columns - len(msg)\n",
   "        if star_len <= 3:\n",
   "            star_len = 3\n",
   "        stars = u\"*\" * star_len\n",
   "        self.display(u\"\\n%s %s\" % (msg, stars), color=color)\n",
   "        if u\": [\" in msg:\n",
   "            msg = msg.replace(u\"[\", u\"\")\n",
   "            if msg.endswith(u\"]\"):\n",
   "                msg = msg[:-1]\n",
   "        runcmd = [self.b_cowsay, b\"-W\", b\"60\"]\n",
   "            thecow = self.noncow\n",
   "            if thecow == 'random':\n",
   "                thecow = random.choice(list(self.cows_available))\n",
   "            runcmd.append(b'-f')\n",
   "            runcmd.append(to_bytes(thecow))\n",
   "        runcmd.append(to_bytes(msg))\n",
   "        cmd = subprocess.Popen(runcmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
   "        (out, err) = cmd.communicate()\n",
   "        self.display(u\"%s\\n\" % to_text(out), color=color)\n",
   "            new_msg = u\"\\n[ERROR]: %s\" % msg\n",
   "            wrapped = textwrap.wrap(new_msg, self.columns)\n",
   "            new_msg = u\"\\n\".join(wrapped) + u\"\\n\"\n",
   "            new_msg = u\"ERROR! %s\" % msg\n",
   "        if new_msg not in self._errors:\n",
   "            self.display(new_msg, color=C.COLOR_ERROR, stderr=True)\n",
   "            self._errors[new_msg] = 1\n",
   "        prompt_string = to_bytes(msg, encoding=Display._output_encoding())\n",
   "            prompt_string = to_text(prompt_string)\n",
   "            return getpass.getpass(prompt_string)\n",
   "            return input(prompt_string)\n",
   "        result = None\n",
   "            do_prompt = self.prompt\n",
   "                msg = \"%s [%s]: \" % (prompt, default)\n",
   "                msg = \"%s: \" % prompt\n",
   "                msg = 'input for %s: ' % varname\n",
   "                    result = do_prompt(msg, private)\n",
   "                    second = do_prompt(\"confirm \" + msg, private)\n",
   "                    if result == second:\n",
   "                result = do_prompt(msg, private)\n",
   "            result = None\n",
   "        if not result and default is not None:\n",
   "            result = default\n",
   "            result = do_encrypt(result, encrypt, salt_size, salt)\n",
   "        result = to_text(result, errors='surrogate_or_strict')\n",
   "            result = wrap_var(result)\n",
   "        return result\n"
  ]
 },
 "55": {
  "name": "cur_id",
  "type": "int",
  "class": "build-in",
  "approach": "UNKNOWN",
  "file_path": "ansible/lib/ansible/utils/vars.py",
  "lineno": "48",
  "column": "4",
  "context": "[:8]\n\n\ndef get_unique_id():\n    global cur_id\n    cur_id += 1\n    return \"-\".join([\n        node_mac[0:8],\n     ",
  "context_lines": "node_mac = (\"%012x\" % uuid.getnode())[:12]\nrandom_int = (\"%08x\" % random.randint(0, _MAXSIZE))[:8]\n\n\ndef get_unique_id():\n    global cur_id\n    cur_id += 1\n    return \"-\".join([\n        node_mac[0:8],\n        node_mac[8:12],\n        random_int[0:4],\n",
  "slicing": [
   "    cur_id += 1\n",
   "        (\"%012x\" % cur_id)[:12],\n"
  ]
 },
 "56": {
  "name": "fragments",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/utils/plugin_docs.py",
  "lineno": "121",
  "column": "4",
  "context": "filename, fragment_loader, is_module=False):\n\n    fragments = doc.pop('extends_documentation_fragment', [])\n\n    if isinstance(fragments, string_types):\n     ",
  "context_lines": "        if options.get(collection_name_field) == collection_name:\n            del options[collection_name_field]\n\n    _process_versions_and_dates(fragment, is_module, return_docs, remove)\n\n\ndef add_fragments(doc, filename, fragment_loader, is_module=False):\n\n    fragments = doc.pop('extends_documentation_fragment', [])\n\n    if isinstance(fragments, string_types):\n        fragments = [fragments]\n\n    unknown_fragments = []\n\n",
  "slicing": [
   "    for key, value in source.items():\n",
   "        if key in target:\n",
   "            if isinstance(target[key], MutableMapping):\n",
   "                value.update(target[key])\n",
   "            elif isinstance(target[key], MutableSet):\n",
   "                value.add(target[key])\n",
   "            elif isinstance(target[key], MutableSequence):\n",
   "                value = sorted(frozenset(value + target[key]))\n",
   "                raise Exception(\"Attempt to extend a documentation fragement, invalid type for %s\" % key)\n",
   "        target[key] = value\n",
   "def _process_versions_and_dates(fragment, is_module, return_docs, callback):\n",
   "    def process_deprecation(deprecation, top_level=False):\n",
   "            callback(deprecation, 'version', 'removed_from_collection')\n",
   "    def process_option_specifiers(specifiers):\n",
   "        for specifier in specifiers:\n",
   "            if not isinstance(specifier, MutableMapping):\n",
   "            if 'version_added' in specifier:\n",
   "                callback(specifier, 'version_added', 'version_added_collection')\n",
   "            if isinstance(specifier.get('deprecated'), MutableMapping):\n",
   "                process_deprecation(specifier['deprecated'])\n",
   "    def process_options(options):\n",
   "        for option in options.values():\n",
   "            if not isinstance(option, MutableMapping):\n",
   "            if 'version_added' in option:\n",
   "                callback(option, 'version_added', 'version_added_collection')\n",
   "                if isinstance(option.get('env'), list):\n",
   "                    process_option_specifiers(option['env'])\n",
   "                if isinstance(option.get('ini'), list):\n",
   "                    process_option_specifiers(option['ini'])\n",
   "                if isinstance(option.get('vars'), list):\n",
   "                    process_option_specifiers(option['vars'])\n",
   "            if isinstance(option.get('suboptions'), MutableMapping):\n",
   "                process_options(option['suboptions'])\n",
   "    def process_return_values(return_values):\n",
   "        for return_value in return_values.values():\n",
   "            if not isinstance(return_value, MutableMapping):\n",
   "            if 'version_added' in return_value:\n",
   "                callback(return_value, 'version_added', 'version_added_collection')\n",
   "            if isinstance(return_value.get('contains'), MutableMapping):\n",
   "                process_return_values(return_value['contains'])\n",
   "        process_return_values(fragment)\n",
   "        callback(fragment, 'version_added', 'version_added_collection')\n",
   "def add_collection_to_versions_and_dates(fragment, collection_name, is_module, return_docs=False):\n",
   "        if collection_name_field not in options:\n",
   "            options[collection_name_field] = collection_name\n",
   "    _process_versions_and_dates(fragment, is_module, return_docs, add)\n",
   "        if options.get(collection_name_field) == collection_name:\n",
   "            del options[collection_name_field]\n",
   "    _process_versions_and_dates(fragment, is_module, return_docs, remove)\n",
   "    fragments = doc.pop('extends_documentation_fragment', [])\n",
   "    if isinstance(fragments, string_types):\n",
   "        fragments = [fragments]\n",
   "    unknown_fragments = []\n",
   "    for fragment_slug in fragments:\n",
   "        fragment_name = fragment_slug\n",
   "        fragment_var = 'DOCUMENTATION'\n",
   "        fragment_class = fragment_loader.get(fragment_name)\n",
   "        if fragment_class is None and '.' in fragment_slug:\n",
   "            splitname = fragment_slug.rsplit('.', 1)\n",
   "            fragment_name = splitname[0]\n",
   "            fragment_var = splitname[1].upper()\n",
   "            fragment_class = fragment_loader.get(fragment_name)\n",
   "        if fragment_class is None:\n",
   "            unknown_fragments.append(fragment_slug)\n",
   "        fragment_yaml = getattr(fragment_class, fragment_var, None)\n",
   "        if fragment_yaml is None:\n",
   "            if fragment_var != 'DOCUMENTATION':\n",
   "                unknown_fragments.append(fragment_slug)\n",
   "                fragment_yaml = '{}'  # TODO: this is still an error later since we require 'options' below...\n",
   "        fragment = AnsibleLoader(fragment_yaml, file_name=filename).get_single_data()\n",
   "        real_collection_name = 'ansible.builtin'\n",
   "        real_fragment_name = getattr(fragment_class, '_load_name')\n",
   "        if real_fragment_name.startswith('ansible_collections.'):\n",
   "            real_collection_name = '.'.join(real_fragment_name.split('.')[1:3])\n",
   "        add_collection_to_versions_and_dates(fragment, real_collection_name, is_module=is_module)\n",
   "        if 'notes' in fragment:\n",
   "            notes = fragment.pop('notes')\n",
   "            if notes:\n",
   "                doc['notes'].extend(notes)\n",
   "        if 'seealso' in fragment:\n",
   "            seealso = fragment.pop('seealso')\n",
   "            if seealso:\n",
   "                doc['seealso'].extend(seealso)\n",
   "        if 'options' not in fragment:\n",
   "            raise Exception(\"missing options in fragment (%s), possibly misformatted?: %s\" % (fragment_name, filename))\n",
   "                merge_fragment(doc['options'], fragment.pop('options'))\n",
   "                raise AnsibleError(\"%s options (%s) of unknown type: %s\" % (to_native(e), fragment_name, filename))\n",
   "            doc['options'] = fragment.pop('options')\n",
   "            merge_fragment(doc, fragment)\n",
   "            raise AnsibleError(\"%s (%s) of unknown type: %s\" % (to_native(e), fragment_name, filename))\n",
   "    if unknown_fragments:\n",
   "        raise AnsibleError('unknown doc_fragment(s) in file {0}: {1}'.format(filename, to_native(', '.join(unknown_fragments))))\n",
   "        if collection_name is not None:\n",
   "            add_collection_to_versions_and_dates(data['doc'], collection_name, is_module=is_module)\n",
   "        add_fragments(data['doc'], filename, fragment_loader=fragment_loader, is_module=is_module)\n",
   "        if collection_name is not None:\n",
   "            add_collection_to_versions_and_dates(data['returndocs'], collection_name, is_module=is_module, return_docs=True)\n"
  ]
 },
 "57": {
  "name": "version_re",
  "type": "_sre.SRE_Pattern",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/utils/version.py",
  "lineno": "134",
  "column": "4",
  "context": "off of ``distutils.version.Version``\n    \"\"\"\n\n    version_re = SEMVER_RE\n\n    def __init__(self, vstring=None):\n        sel",
  "context_lines": "class SemanticVersion(Version):\n    \"\"\"Version comparison class that implements Semantic Versioning 2.0.0\n\n    Based off of ``distutils.version.Version``\n    \"\"\"\n\n    version_re = SEMVER_RE\n\n    def __init__(self, vstring=None):\n        self.vstring = vstring\n        self.major = None\n",
  "slicing": [
   "SEMVER_RE = re.compile(\n",
   "    version_re = SEMVER_RE\n",
   "            version = loose_version.version[:]\n",
   "        extra_idx = 3\n",
   "        for marker in ('-', '+'):\n",
   "                idx = version.index(marker)\n",
   "                if idx < extra_idx:\n",
   "                    extra_idx = idx\n",
   "        version = version[:extra_idx]\n",
   "        if version and set(type(v) for v in version) != set((int,)):\n",
   "        extra = re.search('[+-].+$', loose_version.vstring)\n",
   "        version = version + [0] * (3 - len(version))\n",
   "                '.'.join(str(v) for v in version),\n",
   "                extra.group(0) if extra else ''\n",
   "        match = SEMVER_RE.match(vstring)\n",
   "        if not match:\n",
   "        (major, minor, patch, prerelease, buildmetadata) = match.group(1, 2, 3, 4, 5)\n",
   "        self.major = int(major)\n",
   "        self.minor = int(minor)\n",
   "        self.patch = int(patch)\n",
   "        if prerelease:\n",
   "            self.prerelease = tuple(_Numeric(x) if x.isdigit() else _Alpha(x) for x in prerelease.split('.'))\n",
   "        if buildmetadata:\n",
   "            self.buildmetadata = tuple(_Numeric(x) if x.isdigit() else _Alpha(x) for x in buildmetadata.split('.'))\n"
  ]
 },
 "58": {
  "name": "interesting_paths",
  "type": "NoneType|list",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "104",
  "column": "8",
  "context": "lf, path):\n        path = to_native(path)\n        interesting_paths = self._n_cached_collection_qualified_paths\n        if not interesting_paths:\n            inte",
  "context_lines": "        sys.path_hooks.insert(0, self._ansible_collection_path_hook)\n\n        AnsibleCollectionConfig.collection_finder = self\n\n    def _ansible_collection_path_hook(self, path):\n        path = to_native(path)\n        interesting_paths = self._n_cached_collection_qualified_paths\n        if not interesting_paths:\n            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n                                 self._n_collection_paths]\n            interesting_paths.insert(0, self._ansible_pkg_path)\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "59": {
  "name": "paths",
  "type": "NoneType|list",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "118",
  "column": "8",
  "context": "operty\n    def _n_collection_paths(self):\n        paths = self._n_cached_collection_paths\n        if not paths:\n            self._n_cached_c",
  "context_lines": "            return _AnsiblePathHookFinder(self, path)\n\n        raise ImportError('not interested')\n\n    @property\n    def _n_collection_paths(self):\n        paths = self._n_cached_collection_paths\n        if not paths:\n            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n        return paths\n\n    def set_playbook_paths(self, playbook_paths):\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "60": {
  "name": "paths",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "120",
  "column": "46",
  "context": "hs:\n            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n        return paths\n\n    def set_playbook_paths(s",
  "context_lines": "    @property\n    def _n_collection_paths(self):\n        paths = self._n_cached_collection_paths\n        if not paths:\n            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n        return paths\n\n    def set_playbook_paths(self, playbook_paths):\n        if isinstance(playbook_paths, string_types):\n            playbook_paths = [playbook_paths]\n\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "61": {
  "name": "split_name",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "148",
  "column": "8",
  "context": "and delegate to a special-purpose loader\n\n        split_name = fullname.split('.')\n        toplevel_pkg = split_name[0]\n        modul",
  "context_lines": "            return\n        reload_module(m)\n\n    def find_module(self, fullname, path=None):\n        # Figure out what's being asked for, and delegate to a special-purpose loader\n\n        split_name = fullname.split('.')\n        toplevel_pkg = split_name[0]\n        module_to_find = split_name[-1]\n        part_count = len(split_name)\n\n        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "62": {
  "name": "part_count",
  "type": "int",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "151",
  "column": "8",
  "context": "]\n        module_to_find = split_name[-1]\n        part_count = len(split_name)\n\n        if toplevel_pkg not in ['ansible', 'ansib",
  "context_lines": "        # Figure out what's being asked for, and delegate to a special-purpose loader\n\n        split_name = fullname.split('.')\n        toplevel_pkg = split_name[0]\n        module_to_find = split_name[-1]\n        part_count = len(split_name)\n\n        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n            # not interested in anything other than ansible_collections (and limited cases under ansible)\n            return None\n\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "63": {
  "name": "path",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "163",
  "column": "16",
  "context": "o the configured collection roots\n                path = self._n_collection_paths\n\n        if part_count > 1 and path is None:\n     ",
  "context_lines": "            if path:\n                raise ValueError('path should not be specified for top-level packages (trying to find {0})'.format(fullname))\n            else:\n                # seed the path to the configured collection roots\n                path = self._n_collection_paths\n\n        if part_count > 1 and path is None:\n            raise ValueError('path must be specified for subpackages (trying to find {0})'.format(fullname))\n\n        # NB: actual \"find\"ing is delegated to the constructors on the various loaders; they'll ImportError if not found\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "64": {
  "name": "_file_finder_hook",
  "type": "NoneType",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "200",
  "column": "8",
  "context": "def _get_filefinder_path_hook(self=None):\n        _file_finder_hook = None\n        if PY3:\n            # try to find the File",
  "context_lines": "            # cache the native FileFinder (take advantage of its filesystem cache for future find/load requests)\n            self._file_finder = None\n\n    # class init is fun- this method has a self arg that won't get used\n    def _get_filefinder_path_hook(self=None):\n        _file_finder_hook = None\n        if PY3:\n            # try to find the FileFinder hook to call for fallback path-based imports in Py3\n            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n            if len(_file_finder_hook) != 1:\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "65": {
  "name": "_filefinder_path_hook",
  "type": "function",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "210",
  "column": "4",
  "context": "er_hook[0]\n\n        return _file_finder_hook\n\n    _filefinder_path_hook = _get_filefinder_path_hook()\n\n    def find_module(self, fullname, path=None):\n ",
  "context_lines": "            if len(_file_finder_hook) != 1:\n                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n            _file_finder_hook = _file_finder_hook[0]\n\n        return _file_finder_hook\n\n    _filefinder_path_hook = _get_filefinder_path_hook()\n\n    def find_module(self, fullname, path=None):\n        # we ignore the passed in path here- use what we got from the path hook init\n        split_name = fullname.split('.')\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "    _filefinder_path_hook = _get_filefinder_path_hook()\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "66": {
  "name": "split_name",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "214",
  "column": "8",
  "context": "- use what we got from the path hook init\n        split_name = fullname.split('.')\n        toplevel_pkg = split_name[0]\n\n        if t",
  "context_lines": "        return _file_finder_hook\n\n    _filefinder_path_hook = _get_filefinder_path_hook()\n\n    def find_module(self, fullname, path=None):\n        # we ignore the passed in path here- use what we got from the path hook init\n        split_name = fullname.split('.')\n        toplevel_pkg = split_name[0]\n\n        if toplevel_pkg == 'ansible_collections':\n            # collections content? delegate to the collection finder\n            return self._collection_finder.find_module(fullname, path=[self._pathctx])\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "67": {
  "name": "spec",
  "type": "_frozen_importlib.ModuleSpec|NoneType",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "238",
  "column": "16",
  "context": "                     return None\n\n                spec = self._file_finder.find_spec(fullname)\n                if not spec:\n                    r",
  "context_lines": "                    except ImportError:\n                        # FUTURE: log at a high logging level? This is normal for things like python36.zip on the path, but\n                        # might not be in some other situation...\n                        return None\n\n                spec = self._file_finder.find_spec(fullname)\n                if not spec:\n                    return None\n                return spec.loader\n            else:\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "68": {
  "name": "filename",
  "type": "NoneType",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "411",
  "column": "8",
  "context": "y {1}'.format(fullname, self._fullname))\n\n        filename = self._source_code_path\n\n        if not filename and self.is_package(fulln",
  "context_lines": "        return '<ansible_synthetic_collection_package>'\n\n    def get_filename(self, fullname):\n        if fullname != self._fullname:\n            raise ValueError('this loader cannot find files for {0}, only {1}'.format(fullname, self._fullname))\n\n        filename = self._source_code_path\n\n        if not filename and self.is_package(fullname):\n            if len(self._subpackage_search_paths) == 1:\n                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "69": {
  "name": "filename",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "417",
  "column": "16",
  "context": "__synthetic__')\n            else:\n                filename = self._synthetic_filename(fullname)\n\n        return filename\n\n    def get_code(self, f",
  "context_lines": "        if not filename and self.is_package(fullname):\n            if len(self._subpackage_search_paths) == 1:\n                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n            else:\n                filename = self._synthetic_filename(fullname)\n\n        return filename\n\n    def get_code(self, fullname):\n        if self._compiled_code:\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "70": {
  "name": "filename",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "426",
  "column": "8",
  "context": "but it's the value we'll use for __file__\n        filename = self.get_filename(fullname)\n        if not filename:\n            filename = '<",
  "context_lines": "    def get_code(self, fullname):\n        if self._compiled_code:\n            return self._compiled_code\n\n        # this may or may not be an actual filename, but it's the value we'll use for __file__\n        filename = self.get_filename(fullname)\n        if not filename:\n            filename = '<string>'\n\n        source_code = self.get_source(fullname)\n        if not source_code:\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "71": {
  "name": "source_code",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "432",
  "column": "12",
  "context": "fullname)\n        if not source_code:\n            source_code = ''\n\n        self._compiled_code = compile(source=sour",
  "context_lines": "        if not filename:\n            filename = '<string>'\n\n        source_code = self.get_source(fullname)\n        if not source_code:\n            source_code = ''\n\n        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n\n        return self._compiled_code\n\n    def iter_modules(self, prefix):\n",
  "slicing": [
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n"
  ]
 },
 "72": {
  "name": "module",
  "type": "module",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "491",
  "column": "8",
  "context": "on_loader._meta_yml_to_dict is not set')\n\n        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n\n        module._collection_meta = {}\n        # TO",
  "context_lines": "            self._subpackage_search_paths = [self._subpackage_search_paths[0]]\n\n    def load_module(self, fullname):\n        if not _meta_yml_to_dict:\n            raise ValueError('ansible.utils.collection_loader._meta_yml_to_dict is not set')\n\n        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n\n        module._collection_meta = {}\n        # TODO: load collection metadata, cache in __loader__ state\n\n        collection_name = '.'.join(self._split_name[1:3])\n\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "73": {
  "name": "collection_name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "496",
  "column": "8",
  "context": "tion metadata, cache in __loader__ state\n\n        collection_name = '.'.join(self._split_name[1:3])\n\n        if collection_name == 'ansible.builtin':\n",
  "context_lines": "            raise ValueError('ansible.utils.collection_loader._meta_yml_to_dict is not set')\n\n        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n\n        module._collection_meta = {}\n        # TODO: load collection metadata, cache in __loader__ state\n\n        collection_name = '.'.join(self._split_name[1:3])\n\n        if collection_name == 'ansible.builtin':\n            # ansible.builtin is a synthetic collection, get its routing config from the Ansible distro\n            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "74": {
  "name": "ansible_pkg_path",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "500",
  "column": "12",
  "context": "outing config from the Ansible distro\n            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n            metadata_path = os.path.join(ansible_p",
  "context_lines": "        # TODO: load collection metadata, cache in __loader__ state\n\n        collection_name = '.'.join(self._split_name[1:3])\n\n        if collection_name == 'ansible.builtin':\n            # ansible.builtin is a synthetic collection, get its routing config from the Ansible distro\n            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n            with open(to_bytes(metadata_path), 'rb') as fd:\n                raw_routing = fd.read()\n        else:\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "75": {
  "name": "metadata_path",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "501",
  "column": "12",
  "context": "me(import_module('ansible').__file__)\n            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n            with open(to_bytes(metadata_path), 'rb",
  "context_lines": "        collection_name = '.'.join(self._split_name[1:3])\n\n        if collection_name == 'ansible.builtin':\n            # ansible.builtin is a synthetic collection, get its routing config from the Ansible distro\n            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n            with open(to_bytes(metadata_path), 'rb') as fd:\n                raw_routing = fd.read()\n        else:\n            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "76": {
  "name": "action_groups",
  "type": "dict",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "538",
  "column": "8",
  "context": "  #             redirect =  redirect[2:]\n\n        action_groups = meta_dict.pop('action_groups', {})\n        meta_dict['action_groups'] = {}\n        fo",
  "context_lines": "        #     for plugin_key, plugin_dict in iteritems(routing_type_dict):\n        #         redirect = plugin_dict.get('redirect', '')\n        #         if redirect.startswith('..'):\n        #             redirect =  redirect[2:]\n\n        action_groups = meta_dict.pop('action_groups', {})\n        meta_dict['action_groups'] = {}\n        for group_name in action_groups:\n            for action_name in action_groups[group_name]:\n                if action_name in meta_dict['action_groups']:\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "77": {
  "name": "split_name",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "625",
  "column": "8",
  "context": "ath_list):\n        self._redirect = None\n\n        split_name = fullname.split('.')\n        toplevel_pkg = split_name[0]\n        modul",
  "context_lines": "# by our path_hook importer (which proxies the built-in import mechanisms, allowing normal caching etc to occur)\nclass _AnsibleInternalRedirectLoader:\n    def __init__(self, fullname, path_list):\n        self._redirect = None\n\n        split_name = fullname.split('.')\n        toplevel_pkg = split_name[0]\n        module_to_load = split_name[-1]\n\n        if toplevel_pkg != 'ansible':\n            raise ImportError('not interested')\n\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "78": {
  "name": "routing_entry",
  "type": "NoneType|dict",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "634",
  "column": "8",
  "context": "t_collection_metadata('ansible.builtin')\n\n        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n        if routing_entry:\n            self._redire",
  "context_lines": "        module_to_load = split_name[-1]\n\n        if toplevel_pkg != 'ansible':\n            raise ImportError('not interested')\n\n        builtin_meta = _get_collection_metadata('ansible.builtin')\n\n        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n        if routing_entry:\n            self._redirect = routing_entry.get('redirect')\n\n        if not self._redirect:\n            raise ImportError('not redirected, go ask path_hook')\n\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "79": {
  "name": "VALID_REF_TYPES",
  "type": "frozenset",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "658",
  "column": "4",
  "context": "pect plugin loaders to get these dynamically?\n    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n                                                  ",
  "context_lines": "        sys.modules[fullname] = mod\n        return mod\n\n\nclass AnsibleCollectionRef:\n    # FUTURE: introspect plugin loaders to get these dynamically?\n    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n                                                     'doc_fragments', 'filter', 'httpapi', 'inventory', 'lookup',\n                                                     'module_utils', 'modules', 'netconf', 'role', 'shell', 'strategy',\n                                                     'terminal', 'test', 'vars'])\n\n    # FIXME: tighten this up to match Python identifier reqs, etc\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "80": {
  "name": "VALID_COLLECTION_NAME_RE",
  "type": "_sre.SRE_Pattern",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "664",
  "column": "4",
  "context": " this up to match Python identifier reqs, etc\n    VALID_COLLECTION_NAME_RE = re.compile(to_text(r'^(\\w+)\\.(\\w+)$'))\n    VALID_SUBDIRS_RE = re.compile(to_text(r'^\\w+(\\",
  "context_lines": "                                                     'doc_fragments', 'filter', 'httpapi', 'inventory', 'lookup',\n                                                     'module_utils', 'modules', 'netconf', 'role', 'shell', 'strategy',\n                                                     'terminal', 'test', 'vars'])\n\n    # FIXME: tighten this up to match Python identifier reqs, etc\n    VALID_COLLECTION_NAME_RE = re.compile(to_text(r'^(\\w+)\\.(\\w+)$'))\n    VALID_SUBDIRS_RE = re.compile(to_text(r'^\\w+(\\.\\w+)*$'))\n    VALID_FQCR_RE = re.compile(to_text(r'^\\w+\\.\\w+\\.\\w+(\\.\\w+)*$'))  # can have 0-N included subdirs as well\n\n    def __init__(self, collection_name, subdirs, resource, ref_type):\n        \"\"\"\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "    VALID_COLLECTION_NAME_RE = re.compile(to_text(r'^(\\w+)\\.(\\w+)$'))\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "81": {
  "name": "VALID_SUBDIRS_RE",
  "type": "_sre.SRE_Pattern",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "665",
  "column": "4",
  "context": "E_RE = re.compile(to_text(r'^(\\w+)\\.(\\w+)$'))\n    VALID_SUBDIRS_RE = re.compile(to_text(r'^\\w+(\\.\\w+)*$'))\n    VALID_FQCR_RE = re.compile(to_text(r'^\\w+\\.\\w+",
  "context_lines": "                                                     'module_utils', 'modules', 'netconf', 'role', 'shell', 'strategy',\n                                                     'terminal', 'test', 'vars'])\n\n    # FIXME: tighten this up to match Python identifier reqs, etc\n    VALID_COLLECTION_NAME_RE = re.compile(to_text(r'^(\\w+)\\.(\\w+)$'))\n    VALID_SUBDIRS_RE = re.compile(to_text(r'^\\w+(\\.\\w+)*$'))\n    VALID_FQCR_RE = re.compile(to_text(r'^\\w+\\.\\w+\\.\\w+(\\.\\w+)*$'))  # can have 0-N included subdirs as well\n\n    def __init__(self, collection_name, subdirs, resource, ref_type):\n        \"\"\"\n        Create an AnsibleCollectionRef from components\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "    VALID_SUBDIRS_RE = re.compile(to_text(r'^\\w+(\\.\\w+)*$'))\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "82": {
  "name": "VALID_FQCR_RE",
  "type": "_sre.SRE_Pattern",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "666",
  "column": "4",
  "context": "RS_RE = re.compile(to_text(r'^\\w+(\\.\\w+)*$'))\n    VALID_FQCR_RE = re.compile(to_text(r'^\\w+\\.\\w+\\.\\w+(\\.\\w+)*$'))  # can have 0-N included subdirs as well\n\n    def __init__(self, collection_name, subdirs, ",
  "context_lines": "                                                     'terminal', 'test', 'vars'])\n\n    # FIXME: tighten this up to match Python identifier reqs, etc\n    VALID_COLLECTION_NAME_RE = re.compile(to_text(r'^(\\w+)\\.(\\w+)$'))\n    VALID_SUBDIRS_RE = re.compile(to_text(r'^\\w+(\\.\\w+)*$'))\n    VALID_FQCR_RE = re.compile(to_text(r'^\\w+\\.\\w+\\.\\w+(\\.\\w+)*$'))  # can have 0-N included subdirs as well\n\n    def __init__(self, collection_name, subdirs, resource, ref_type):\n        \"\"\"\n        Create an AnsibleCollectionRef from components\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "    VALID_FQCR_RE = re.compile(to_text(r'^\\w+\\.\\w+\\.\\w+(\\.\\w+)*$'))  # can have 0-N included subdirs as well\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "83": {
  "name": "cur_value",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "923",
  "column": "4",
  "context": "\n\n\ndef _nested_dict_get(root_dict, key_list):\n    cur_value = root_dict\n    for key in key_list:\n        cur_value = cur_v",
  "context_lines": "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n            return redirect\n    return None\n\n\ndef _nested_dict_get(root_dict, key_list):\n    cur_value = root_dict\n    for key in key_list:\n        cur_value = cur_value.get(key)\n        if not cur_value:\n            return None\n\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "84": {
  "name": "cur_value",
  "type": "NoneType|dict",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "925",
  "column": "8",
  "context": "alue = root_dict\n    for key in key_list:\n        cur_value = cur_value.get(key)\n        if not cur_value:\n            return None\n",
  "context_lines": "    return None\n\n\ndef _nested_dict_get(root_dict, key_list):\n    cur_value = root_dict\n    for key in key_list:\n        cur_value = cur_value.get(key)\n        if not cur_value:\n            return None\n\n    return cur_value\n\n\ndef _iter_modules_impl(paths, prefix=''):\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "85": {
  "name": "_collection_meta",
  "type": "dict",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/utils/collection_loader/_collection_finder.py",
  "lineno": "969",
  "column": "4",
  "context": "ate collection {0}'.format(collection_name))\n\n    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n\n    if _collection_meta is None:\n        raise Va",
  "context_lines": "    try:\n        collection_pkg = import_module('ansible_collections.' + collection_name)\n    except ImportError:\n        raise ValueError('unable to locate collection {0}'.format(collection_name))\n\n    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n\n    if _collection_meta is None:\n        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n\n",
  "slicing": [
   "    def import_module(name):\n",
   "    reload_module = reload  # pylint:disable=undefined-variable\n",
   "    _meta_yml_to_dict = None\n",
   "            paths = [paths]\n",
   "        elif paths is None:\n",
   "            paths = []\n",
   "        paths = [os.path.expanduser(to_native(p, errors='surrogate_or_strict')) for p in paths]\n",
   "            for path in sys.path:\n",
   "                        path not in paths and\n",
   "                            os.path.join(path, 'ansible_collections'),\n",
   "                    paths.append(path)\n",
   "        self._n_configured_paths = paths\n",
   "        for mps in sys.meta_path:\n",
   "            if isinstance(mps, _AnsibleCollectionFinder):\n",
   "                sys.meta_path.remove(mps)\n",
   "        for ph in sys.path_hooks:\n",
   "            if hasattr(ph, '__self__') and isinstance(ph.__self__, _AnsibleCollectionFinder):\n",
   "                sys.path_hooks.remove(ph)\n",
   "        path = to_native(path)\n",
   "        interesting_paths = self._n_cached_collection_qualified_paths\n",
   "        if not interesting_paths:\n",
   "            interesting_paths = [os.path.join(p, 'ansible_collections') for p in\n",
   "            interesting_paths.insert(0, self._ansible_pkg_path)\n",
   "            self._n_cached_collection_qualified_paths = interesting_paths\n",
   "        if any(path.startswith(p) for p in interesting_paths):\n",
   "            return _AnsiblePathHookFinder(self, path)\n",
   "        paths = self._n_cached_collection_paths\n",
   "        if not paths:\n",
   "            self._n_cached_collection_paths = paths = self._n_playbook_paths + self._n_configured_paths\n",
   "        return paths\n",
   "            playbook_paths = [playbook_paths]\n",
   "        added_paths = set()\n",
   "        self._n_playbook_paths = [os.path.join(to_native(p), 'collections') for p in playbook_paths if not (p in added_paths or added_paths.add(p))]\n",
   "        for pkg in ['ansible_collections', 'ansible_collections.ansible']:\n",
   "            self._reload_hack(pkg)\n",
   "        m = sys.modules.get(fullname)\n",
   "        if not m:\n",
   "        reload_module(m)\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_find = split_name[-1]\n",
   "        part_count = len(split_name)\n",
   "        if toplevel_pkg not in ['ansible', 'ansible_collections']:\n",
   "        if part_count == 1:\n",
   "            if path:\n",
   "                path = self._n_collection_paths\n",
   "        if part_count > 1 and path is None:\n",
   "            if toplevel_pkg == 'ansible':\n",
   "                return _AnsibleInternalRedirectLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 1:\n",
   "                return _AnsibleCollectionRootPkgLoader(fullname=fullname, path_list=path)\n",
   "            if part_count == 2:  # ns pkg eg, ansible_collections, ansible_collections.somens\n",
   "                return _AnsibleCollectionNSPkgLoader(fullname=fullname, path_list=path)\n",
   "            elif part_count == 3:  # collection pkg eg, ansible_collections.somens.somecoll\n",
   "                return _AnsibleCollectionPkgLoader(fullname=fullname, path_list=path)\n",
   "            return _AnsibleCollectionLoader(fullname=fullname, path_list=path)\n",
   "        _file_finder_hook = None\n",
   "            _file_finder_hook = [ph for ph in sys.path_hooks if 'FileFinder' in repr(ph)]\n",
   "            if len(_file_finder_hook) != 1:\n",
   "                raise Exception('need exactly one FileFinder import hook (found {0})'.format(len(_file_finder_hook)))\n",
   "            _file_finder_hook = _file_finder_hook[0]\n",
   "        return _file_finder_hook\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        if toplevel_pkg == 'ansible_collections':\n",
   "                spec = self._file_finder.find_spec(fullname)\n",
   "                if not spec:\n",
   "                return spec.loader\n",
   "        self._candidate_paths = self._get_candidate_paths([to_native(p) for p in path_list])\n",
   "        return [os.path.join(p, self._package_to_load) for p in path_list]\n",
   "        return [p for p in candidate_paths if os.path.isdir(to_bytes(p))]\n",
   "        created_module = False\n",
   "        module = sys.modules.get(name)\n",
   "            if not module:\n",
   "                module = ModuleType(name)\n",
   "                created_module = True\n",
   "                sys.modules[name] = module\n",
   "            for attr, value in kwargs.items():\n",
   "                setattr(module, attr, value)\n",
   "            yield module\n",
   "            if created_module:\n",
   "                    sys.modules.pop(name)\n",
   "        has_code = True\n",
   "        package_path = os.path.join(to_native(path), to_native(leaf_name))\n",
   "        module_path = None\n",
   "        if os.path.isdir(to_bytes(package_path)):\n",
   "            module_path = os.path.join(package_path, '__init__.py')\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                module_path = os.path.join(package_path, '__synthetic__')\n",
   "                has_code = False\n",
   "            module_path = package_path + '.py'\n",
   "            package_path = None\n",
   "            if not os.path.isfile(to_bytes(module_path)):\n",
   "                raise ImportError('{0} not found at {1}'.format(leaf_name, path))\n",
   "        return module_path, has_code, package_path\n",
   "        module_attrs = dict(\n",
   "            module_attrs['__path__'] = self._subpackage_search_paths\n",
   "            module_attrs['__package__'] = fullname  # per PEP366\n",
   "        with self._new_or_existing_module(fullname, **module_attrs) as module:\n",
   "            exec(self.get_code(fullname), module.__dict__)\n",
   "            return module\n",
   "        if not path:\n",
   "        if not path[0] == '/':\n",
   "            candidate_paths = [path]\n",
   "        for p in candidate_paths:\n",
   "            b_path = to_bytes(p)\n",
   "            if os.path.isfile(b_path):\n",
   "                with open(b_path, 'rb') as fd:\n",
   "                    return fd.read()\n",
   "            elif b_path.endswith(b'__init__.py') and os.path.isdir(os.path.dirname(b_path)):\n",
   "        filename = self._source_code_path\n",
   "        if not filename and self.is_package(fullname):\n",
   "                filename = os.path.join(self._subpackage_search_paths[0], '__synthetic__')\n",
   "                filename = self._synthetic_filename(fullname)\n",
   "        return filename\n",
   "        filename = self.get_filename(fullname)\n",
   "        if not filename:\n",
   "            filename = '<string>'\n",
   "        source_code = self.get_source(fullname)\n",
   "        if not source_code:\n",
   "            source_code = ''\n",
   "        self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n",
   "        if not _meta_yml_to_dict:\n",
   "        module = super(_AnsibleCollectionPkgLoader, self).load_module(fullname)\n",
   "        module._collection_meta = {}\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        if collection_name == 'ansible.builtin':\n",
   "            ansible_pkg_path = os.path.dirname(import_module('ansible').__file__)\n",
   "            metadata_path = os.path.join(ansible_pkg_path, 'config/ansible_builtin_runtime.yml')\n",
   "            with open(to_bytes(metadata_path), 'rb') as fd:\n",
   "                raw_routing = fd.read()\n",
   "            b_routing_meta_path = to_bytes(os.path.join(module.__path__[0], 'meta/runtime.yml'))\n",
   "            if os.path.isfile(b_routing_meta_path):\n",
   "                with open(b_routing_meta_path, 'rb') as fd:\n",
   "                    raw_routing = fd.read()\n",
   "                raw_routing = ''\n",
   "            if raw_routing:\n",
   "                routing_dict = _meta_yml_to_dict(raw_routing, (collection_name, 'runtime.yml'))\n",
   "                module._collection_meta = self._canonicalize_meta(routing_dict)\n",
   "        AnsibleCollectionConfig.on_collection_load.fire(collection_name=collection_name, collection_path=os.path.dirname(module.__file__))\n",
   "        return module\n",
   "        action_groups = meta_dict.pop('action_groups', {})\n",
   "        for group_name in action_groups:\n",
   "            for action_name in action_groups[group_name]:\n",
   "                if action_name in meta_dict['action_groups']:\n",
   "                    meta_dict['action_groups'][action_name].append(group_name)\n",
   "                    meta_dict['action_groups'][action_name] = [group_name]\n",
   "        collection_name = '.'.join(self._split_name[1:3])\n",
   "        collection_meta = _get_collection_metadata(collection_name)\n",
   "        redirect = None\n",
   "        explicit_redirect = False\n",
   "        routing_entry = _nested_dict_get(collection_meta, ['import_redirection', self._fullname])\n",
   "        if routing_entry:\n",
   "            redirect = routing_entry.get('redirect')\n",
   "        if redirect:\n",
   "            explicit_redirect = True\n",
   "            redirect = _get_ancestor_redirect(self._redirected_package_map, self._fullname)\n",
   "        if redirect:\n",
   "            self._redirect_module = import_module(redirect)\n",
   "            if explicit_redirect and hasattr(self._redirect_module, '__path__') and self._redirect_module.__path__:\n",
   "                self._redirected_package_map[self._fullname] = redirect\n",
   "        if not candidate_paths:\n",
   "        found_path, has_code, package_path = self._module_file_from_path(self._package_to_load, candidate_paths[0])\n",
   "        if has_code:\n",
   "            self._source_code_path = found_path\n",
   "        if package_path:\n",
   "            return [package_path]  # always needs to be a list\n",
   "        split_name = fullname.split('.')\n",
   "        toplevel_pkg = split_name[0]\n",
   "        module_to_load = split_name[-1]\n",
   "        if toplevel_pkg != 'ansible':\n",
   "        builtin_meta = _get_collection_metadata('ansible.builtin')\n",
   "        routing_entry = _nested_dict_get(builtin_meta, ['import_redirection', fullname])\n",
   "        if routing_entry:\n",
   "            self._redirect = routing_entry.get('redirect')\n",
   "        mod = import_module(self._redirect)\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "    VALID_REF_TYPES = frozenset(to_text(r) for r in ['action', 'become', 'cache', 'callback', 'cliconf', 'connection',\n",
   "        collection_name = to_text(collection_name, errors='strict')\n",
   "            subdirs = to_text(subdirs, errors='strict')\n",
   "        resource = to_text(resource, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        if not self.is_valid_collection_name(collection_name):\n",
   "            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n",
   "        if ref_type not in self.VALID_REF_TYPES:\n",
   "            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n",
   "        self.collection = collection_name\n",
   "        if subdirs:\n",
   "            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n",
   "                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n",
   "            self.subdirs = subdirs\n",
   "        self.resource = resource\n",
   "        self.ref_type = ref_type\n",
   "        package_components = [u'ansible_collections', self.collection]\n",
   "        fqcr_components = [self.collection]\n",
   "        self.n_python_collection_package_name = to_native('.'.join(package_components))\n",
   "            package_components.append(u'roles')\n",
   "            package_components += [u'plugins', self.ref_type]\n",
   "            package_components.append(self.subdirs)\n",
   "            fqcr_components.append(self.subdirs)\n",
   "            package_components.append(self.resource)\n",
   "        fqcr_components.append(self.resource)\n",
   "        self.n_python_package_name = to_native('.'.join(package_components))\n",
   "        self._fqcr = u'.'.join(fqcr_components)\n",
   "        ref = to_text(ref, errors='strict')\n",
   "        ref_type = to_text(ref_type, errors='strict')\n",
   "        resource_splitname = ref.rsplit(u'.', 1)\n",
   "        package_remnant = resource_splitname[0]\n",
   "        resource = resource_splitname[1]\n",
   "        package_splitname = package_remnant.split(u'.', 2)\n",
   "        if len(package_splitname) == 3:\n",
   "            subdirs = package_splitname[2]\n",
   "            subdirs = u''\n",
   "        collection_name = u'.'.join(package_splitname[0:2])\n",
   "        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)\n",
   "            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n",
   "        legacy_plugin_dir_name = to_text(legacy_plugin_dir_name)\n",
   "        plugin_type = legacy_plugin_dir_name.replace(u'_plugins', u'')\n",
   "        if plugin_type == u'library':\n",
   "            plugin_type = u'modules'\n",
   "        if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n",
   "            raise ValueError('{0} cannot be mapped to a valid collection ref type'.format(to_native(legacy_plugin_dir_name)))\n",
   "        return plugin_type\n",
   "        ref = to_text(ref)\n",
   "        if not ref_type:\n",
   "            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n",
   "        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))\n",
   "        collection_name = to_text(collection_name)\n",
   "        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))\n",
   "    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n",
   "    if acr:\n",
   "        collection_list = [acr.collection]\n",
   "        subdirs = acr.subdirs\n",
   "        resource = acr.resource\n",
   "    elif not collection_list:\n",
   "        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n",
   "        subdirs = ''\n",
   "    for collection_name in collection_list:\n",
   "            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n",
   "            pkg = import_module(acr.n_python_package_name)\n",
   "            if pkg is not None:\n",
   "                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n",
   "                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n",
   "    path = to_native(path)\n",
   "    path_parts = path.split('/')\n",
   "    if path_parts.count('ansible_collections') != 1:\n",
   "    ac_pos = path_parts.index('ansible_collections')\n",
   "    if len(path_parts) < ac_pos + 3:\n",
   "    candidate_collection_name = '.'.join(path_parts[ac_pos + 1:ac_pos + 3])\n",
   "        imported_pkg_path = to_native(os.path.dirname(to_bytes(import_module('ansible_collections.' + candidate_collection_name).__file__)))\n",
   "    original_path_prefix = os.path.join('/', *path_parts[0:ac_pos + 3])\n",
   "    if original_path_prefix != imported_pkg_path:\n",
   "    return candidate_collection_name\n",
   "    cur_pkg = fullname\n",
   "    while cur_pkg:\n",
   "        cur_pkg = cur_pkg.rpartition('.')[0]\n",
   "        ancestor_redirect = redirected_package_map.get(cur_pkg)\n",
   "        if ancestor_redirect:\n",
   "            redirect = ancestor_redirect + fullname[len(cur_pkg):]\n",
   "            return redirect\n",
   "    cur_value = root_dict\n",
   "    for key in key_list:\n",
   "        cur_value = cur_value.get(key)\n",
   "        if not cur_value:\n",
   "    return cur_value\n",
   "        prefix = ''\n",
   "        prefix = to_native(prefix)\n",
   "    for b_path in map(to_bytes, paths):\n",
   "        if not os.path.isdir(b_path):\n",
   "        for b_basename in sorted(os.listdir(b_path)):\n",
   "            b_candidate_module_path = os.path.join(b_path, b_basename)\n",
   "            if os.path.isdir(b_candidate_module_path):\n",
   "                if b'.' in b_basename or b_basename == b'__pycache__':\n",
   "                yield prefix + to_native(b_basename), True\n",
   "                if b_basename.endswith(b'.py') and b_basename != b'__init__.py':\n",
   "                    yield prefix + to_native(os.path.splitext(b_basename)[0]), False\n",
   "    collection_name = to_native(collection_name)\n",
   "    if not collection_name or not isinstance(collection_name, string_types) or len(collection_name.split('.')) != 2:\n",
   "        collection_pkg = import_module('ansible_collections.' + collection_name)\n",
   "        raise ValueError('unable to locate collection {0}'.format(collection_name))\n",
   "    _collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "    if _collection_meta is None:\n",
   "        raise ValueError('collection metadata was not loaded for collection {0}'.format(collection_name))\n",
   "    return _collection_meta\n"
  ]
 },
 "86": {
  "name": "ITERATING_SETUP",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/executor/play_iterator.py",
  "lineno": "134",
  "column": "4",
  "context": "primary running states for the play iteration\n    ITERATING_SETUP = 0\n    ITERATING_TASKS = 1\n    ITERATING_RESCUE = 2\n ",
  "context_lines": "            new_state.always_child_state = self.always_child_state.copy()\n        return new_state\n\n\nclass PlayIterator:\n\n    # the primary running states for the play iteration\n    ITERATING_SETUP = 0\n    ITERATING_TASKS = 1\n    ITERATING_RESCUE = 2\n    ITERATING_ALWAYS = 3\n    ITERATING_COMPLETE = 4\n\n",
  "slicing": "    ITERATING_SETUP = 0\n"
 },
 "87": {
  "name": "ITERATING_TASKS",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/executor/play_iterator.py",
  "lineno": "135",
  "column": "4",
  "context": "or the play iteration\n    ITERATING_SETUP = 0\n    ITERATING_TASKS = 1\n    ITERATING_RESCUE = 2\n    ITERATING_ALWAYS = 3\n",
  "context_lines": "        return new_state\n\n\nclass PlayIterator:\n\n    # the primary running states for the play iteration\n    ITERATING_SETUP = 0\n    ITERATING_TASKS = 1\n    ITERATING_RESCUE = 2\n    ITERATING_ALWAYS = 3\n    ITERATING_COMPLETE = 4\n\n    # the failure states for the play iteration, which are powers\n",
  "slicing": "    ITERATING_TASKS = 1\n"
 },
 "88": {
  "name": "ITERATING_RESCUE",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/executor/play_iterator.py",
  "lineno": "136",
  "column": "4",
  "context": "  ITERATING_SETUP = 0\n    ITERATING_TASKS = 1\n    ITERATING_RESCUE = 2\n    ITERATING_ALWAYS = 3\n    ITERATING_COMPLETE = ",
  "context_lines": "class PlayIterator:\n\n    # the primary running states for the play iteration\n    ITERATING_SETUP = 0\n    ITERATING_TASKS = 1\n    ITERATING_RESCUE = 2\n    ITERATING_ALWAYS = 3\n    ITERATING_COMPLETE = 4\n\n    # the failure states for the play iteration, which are powers\n    # of 2 as they may be or'ed together in certain circumstances\n",
  "slicing": "    ITERATING_RESCUE = 2\n"
 },
 "89": {
  "name": "ITERATING_ALWAYS",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/executor/play_iterator.py",
  "lineno": "137",
  "column": "4",
  "context": " ITERATING_TASKS = 1\n    ITERATING_RESCUE = 2\n    ITERATING_ALWAYS = 3\n    ITERATING_COMPLETE = 4\n\n    # the failure stat",
  "context_lines": "    # the primary running states for the play iteration\n    ITERATING_SETUP = 0\n    ITERATING_TASKS = 1\n    ITERATING_RESCUE = 2\n    ITERATING_ALWAYS = 3\n    ITERATING_COMPLETE = 4\n\n    # the failure states for the play iteration, which are powers\n    # of 2 as they may be or'ed together in certain circumstances\n    FAILED_NONE = 0\n",
  "slicing": "    ITERATING_ALWAYS = 3\n"
 },
 "90": {
  "name": "ITERATING_COMPLETE",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/executor/play_iterator.py",
  "lineno": "138",
  "column": "4",
  "context": "ITERATING_RESCUE = 2\n    ITERATING_ALWAYS = 3\n    ITERATING_COMPLETE = 4\n\n    # the failure states for the play iteration, ",
  "context_lines": "    ITERATING_SETUP = 0\n    ITERATING_TASKS = 1\n    ITERATING_RESCUE = 2\n    ITERATING_ALWAYS = 3\n    ITERATING_COMPLETE = 4\n\n    # the failure states for the play iteration, which are powers\n    # of 2 as they may be or'ed together in certain circumstances\n    FAILED_NONE = 0\n",
  "slicing": "    ITERATING_COMPLETE = 4\n"
 },
 "91": {
  "name": "FAILED_NONE",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/executor/play_iterator.py",
  "lineno": "142",
  "column": "4",
  "context": "ay be or'ed together in certain circumstances\n    FAILED_NONE = 0\n    FAILED_SETUP = 1\n    FAILED_TASKS = 2\n    FAIL",
  "context_lines": "    ITERATING_ALWAYS = 3\n    ITERATING_COMPLETE = 4\n\n    # the failure states for the play iteration, which are powers\n    # of 2 as they may be or'ed together in certain circumstances\n    FAILED_NONE = 0\n    FAILED_SETUP = 1\n    FAILED_TASKS = 2\n    FAILED_RESCUE = 4\n    FAILED_ALWAYS = 8\n\n",
  "slicing": "    FAILED_NONE = 0\n"
 },
 "92": {
  "name": "FAILED_SETUP",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/executor/play_iterator.py",
  "lineno": "143",
  "column": "4",
  "context": " in certain circumstances\n    FAILED_NONE = 0\n    FAILED_SETUP = 1\n    FAILED_TASKS = 2\n    FAILED_RESCUE = 4\n    FAI",
  "context_lines": "    ITERATING_COMPLETE = 4\n\n    # the failure states for the play iteration, which are powers\n    # of 2 as they may be or'ed together in certain circumstances\n    FAILED_NONE = 0\n    FAILED_SETUP = 1\n    FAILED_TASKS = 2\n    FAILED_RESCUE = 4\n    FAILED_ALWAYS = 8\n\n    def __init__(self, inventory, play, play_context, variable_manager, all_vars, start_at_done=False):\n",
  "slicing": "    FAILED_SETUP = 1\n"
 },
 "93": {
  "name": "FAILED_TASKS",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/executor/play_iterator.py",
  "lineno": "144",
  "column": "4",
  "context": "nces\n    FAILED_NONE = 0\n    FAILED_SETUP = 1\n    FAILED_TASKS = 2\n    FAILED_RESCUE = 4\n    FAILED_ALWAYS = 8\n\n    d",
  "context_lines": "    # the failure states for the play iteration, which are powers\n    # of 2 as they may be or'ed together in certain circumstances\n    FAILED_NONE = 0\n    FAILED_SETUP = 1\n    FAILED_TASKS = 2\n    FAILED_RESCUE = 4\n    FAILED_ALWAYS = 8\n\n    def __init__(self, inventory, play, play_context, variable_manager, all_vars, start_at_done=False):\n        self._play = play\n",
  "slicing": "    FAILED_TASKS = 2\n"
 },
 "94": {
  "name": "FAILED_RESCUE",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/executor/play_iterator.py",
  "lineno": "145",
  "column": "4",
  "context": "= 0\n    FAILED_SETUP = 1\n    FAILED_TASKS = 2\n    FAILED_RESCUE = 4\n    FAILED_ALWAYS = 8\n\n    def __init__(self, inve",
  "context_lines": "    # of 2 as they may be or'ed together in certain circumstances\n    FAILED_NONE = 0\n    FAILED_SETUP = 1\n    FAILED_TASKS = 2\n    FAILED_RESCUE = 4\n    FAILED_ALWAYS = 8\n\n    def __init__(self, inventory, play, play_context, variable_manager, all_vars, start_at_done=False):\n        self._play = play\n        self._blocks = []\n",
  "slicing": "    FAILED_RESCUE = 4\n"
 },
 "95": {
  "name": "FAILED_ALWAYS",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/executor/play_iterator.py",
  "lineno": "146",
  "column": "4",
  "context": " 1\n    FAILED_TASKS = 2\n    FAILED_RESCUE = 4\n    FAILED_ALWAYS = 8\n\n    def __init__(self, inventory, play, play_cont",
  "context_lines": "    FAILED_NONE = 0\n    FAILED_SETUP = 1\n    FAILED_TASKS = 2\n    FAILED_RESCUE = 4\n    FAILED_ALWAYS = 8\n\n    def __init__(self, inventory, play, play_context, variable_manager, all_vars, start_at_done=False):\n        self._play = play\n        self._blocks = []\n",
  "slicing": "    FAILED_ALWAYS = 8\n"
 },
 "96": {
  "name": "RUN_OK",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/executor/task_queue_manager.py",
  "lineno": "64",
  "column": "4",
  "context": "ispatches the Play's tasks to hosts.\n    '''\n\n    RUN_OK = 0\n    RUN_ERROR = 1\n    RUN_FAILED_HOSTS = 2\n    RUN",
  "context_lines": "    work between all processes.\n\n    The queue manager is responsible for loading the play strategy plugin,\n    which dispatches the Play's tasks to hosts.\n    '''\n\n    RUN_OK = 0\n    RUN_ERROR = 1\n    RUN_FAILED_HOSTS = 2\n    RUN_UNREACHABLE_HOSTS = 4\n    RUN_FAILED_BREAK_PLAY = 8\n",
  "slicing": "    RUN_OK = 0\n"
 },
 "97": {
  "name": "RUN_ERROR",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/executor/task_queue_manager.py",
  "lineno": "65",
  "column": "4",
  "context": "lay's tasks to hosts.\n    '''\n\n    RUN_OK = 0\n    RUN_ERROR = 1\n    RUN_FAILED_HOSTS = 2\n    RUN_UNREACHABLE_HOSTS",
  "context_lines": "    The queue manager is responsible for loading the play strategy plugin,\n    which dispatches the Play's tasks to hosts.\n    '''\n\n    RUN_OK = 0\n    RUN_ERROR = 1\n    RUN_FAILED_HOSTS = 2\n    RUN_UNREACHABLE_HOSTS = 4\n    RUN_FAILED_BREAK_PLAY = 8\n    RUN_UNKNOWN_ERROR = 255\n\n",
  "slicing": "    RUN_ERROR = 1\n"
 },
 "98": {
  "name": "RUN_FAILED_HOSTS",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/executor/task_queue_manager.py",
  "lineno": "66",
  "column": "4",
  "context": "ts.\n    '''\n\n    RUN_OK = 0\n    RUN_ERROR = 1\n    RUN_FAILED_HOSTS = 2\n    RUN_UNREACHABLE_HOSTS = 4\n    RUN_FAILED_BREAK",
  "context_lines": "    which dispatches the Play's tasks to hosts.\n    '''\n\n    RUN_OK = 0\n    RUN_ERROR = 1\n    RUN_FAILED_HOSTS = 2\n    RUN_UNREACHABLE_HOSTS = 4\n    RUN_FAILED_BREAK_PLAY = 8\n    RUN_UNKNOWN_ERROR = 255\n\n    def __init__(self, inventory, variable_manager, loader, passwords, stdout_callback=None, run_additional_callbacks=True, run_tree=False, forks=None):\n\n",
  "slicing": "    RUN_FAILED_HOSTS = 2\n"
 },
 "99": {
  "name": "RUN_UNREACHABLE_HOSTS",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/executor/task_queue_manager.py",
  "lineno": "67",
  "column": "4",
  "context": " 0\n    RUN_ERROR = 1\n    RUN_FAILED_HOSTS = 2\n    RUN_UNREACHABLE_HOSTS = 4\n    RUN_FAILED_BREAK_PLAY = 8\n    RUN_UNKNOWN_ERRO",
  "context_lines": "    '''\n\n    RUN_OK = 0\n    RUN_ERROR = 1\n    RUN_FAILED_HOSTS = 2\n    RUN_UNREACHABLE_HOSTS = 4\n    RUN_FAILED_BREAK_PLAY = 8\n    RUN_UNKNOWN_ERROR = 255\n\n    def __init__(self, inventory, variable_manager, loader, passwords, stdout_callback=None, run_additional_callbacks=True, run_tree=False, forks=None):\n\n        self._inventory = inventory\n",
  "slicing": "    RUN_UNREACHABLE_HOSTS = 4\n"
 },
 "100": {
  "name": "RUN_FAILED_BREAK_PLAY",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/executor/task_queue_manager.py",
  "lineno": "68",
  "column": "4",
  "context": "AILED_HOSTS = 2\n    RUN_UNREACHABLE_HOSTS = 4\n    RUN_FAILED_BREAK_PLAY = 8\n    RUN_UNKNOWN_ERROR = 255\n\n    def __init__(self",
  "context_lines": "    RUN_OK = 0\n    RUN_ERROR = 1\n    RUN_FAILED_HOSTS = 2\n    RUN_UNREACHABLE_HOSTS = 4\n    RUN_FAILED_BREAK_PLAY = 8\n    RUN_UNKNOWN_ERROR = 255\n\n    def __init__(self, inventory, variable_manager, loader, passwords, stdout_callback=None, run_additional_callbacks=True, run_tree=False, forks=None):\n\n        self._inventory = inventory\n        self._variable_manager = variable_manager\n",
  "slicing": "    RUN_FAILED_BREAK_PLAY = 8\n"
 },
 "101": {
  "name": "RUN_UNKNOWN_ERROR",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/executor/task_queue_manager.py",
  "lineno": "69",
  "column": "4",
  "context": "HABLE_HOSTS = 4\n    RUN_FAILED_BREAK_PLAY = 8\n    RUN_UNKNOWN_ERROR = 255\n\n    def __init__(self, inventory, variable_manage",
  "context_lines": "    RUN_ERROR = 1\n    RUN_FAILED_HOSTS = 2\n    RUN_UNREACHABLE_HOSTS = 4\n    RUN_FAILED_BREAK_PLAY = 8\n    RUN_UNKNOWN_ERROR = 255\n\n    def __init__(self, inventory, variable_manager, loader, passwords, stdout_callback=None, run_additional_callbacks=True, run_tree=False, forks=None):\n\n        self._inventory = inventory\n        self._variable_manager = variable_manager\n",
  "slicing": "    RUN_UNKNOWN_ERROR = 255\n"
 },
 "102": {
  "name": "l",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/executor/module_common.py",
  "lineno": "399",
  "column": "8",
  "context": "= []\n    for line in source.splitlines():\n        l = line.strip()\n        if not l or l.startswith(u'#'):\n          ",
  "context_lines": "def _strip_comments(source):\n    # Strip comments and blank lines from the wrapper\n    buf = []\n    for line in source.splitlines():\n        l = line.strip()\n        if not l or l.startswith(u'#'):\n            continue\n        buf.append(line)\n    return u'\\n'.join(buf)\n\n\n",
  "slicing": "        l = line.strip()\n"
 },
 "103": {
  "name": "visit",
  "type": "function",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/executor/module_common.py",
  "lineno": "490",
  "column": "4",
  "context": "\n                        generic_visit(item)\n\n    visit = generic_visit\n\n    def visit_Import(self, node):\n        \"\"\"\n   ",
  "context_lines": "                    if isinstance(item, (Import, ImportFrom)):\n                        visit_map[item.__class__](item)\n                    elif isinstance(item, AST):\n                        generic_visit(item)\n\n    visit = generic_visit\n\n    def visit_Import(self, node):\n        \"\"\"\n        Handle import ansible.module_utils.MODLIB[.MODLIBn] [as asname]\n\n",
  "slicing": "    visit = generic_visit\n"
 },
 "104": {
  "name": "m",
  "type": "module",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/plugins/loader.py",
  "lineno": "311",
  "column": "12",
  "context": "if not hasattr(self, 'package_path'):\n            m = __import__(self.package)\n            parts = self.package.split('.')[1:]\n  ",
  "context_lines": "        ''' Gets the path of a Python package '''\n\n        if not self.package:\n            return []\n        if not hasattr(self, 'package_path'):\n            m = __import__(self.package)\n            parts = self.package.split('.')[1:]\n            for parent_mod in parts:\n                m = getattr(m, parent_mod)\n            self.package_path = os.path.dirname(m.__file__)\n",
  "slicing": [
   "    imp = None\n",
   "display = Display()\n",
   "get_with_context_result = namedtuple('get_with_context_result', ['object', 'plugin_load_context'])\n",
   "    return [(name, obj) for (name, obj) in globals().items() if isinstance(obj, PluginLoader)]\n",
   "    b_path = os.path.expanduser(to_bytes(path, errors='surrogate_or_strict'))\n",
   "    if os.path.isdir(b_path):\n",
   "        for name, obj in get_all_plugin_loaders():\n",
   "            if obj.subdir:\n",
   "                plugin_path = os.path.join(b_path, to_bytes(obj.subdir))\n",
   "                if os.path.isdir(plugin_path):\n",
   "                    obj.add_directory(to_text(plugin_path))\n",
   "        display.warning(\"Ignoring invalid path provided to plugin path: '%s' is not a directory\" % to_text(path))\n",
   "        shell_type = 'sh'\n",
   "                shell_filename = os.path.basename(executable)\n",
   "                    shell = shell_loader.get(shell_filename)\n",
   "                    shell = None\n",
   "                if shell is None:\n",
   "                    for shell in shell_loader.all():\n",
   "                        if shell_filename in shell.COMPATIBLE_SHELLS:\n",
   "                            shell_type = shell.SHELL_FAMILY\n",
   "    shell = shell_loader.get(shell_type)\n",
   "    if not shell:\n",
   "        raise AnsibleError(\"Could not find the shell plugin required (%s).\" % shell_type)\n",
   "        setattr(shell, 'executable', executable)\n",
   "    return shell\n",
   "    loader = getattr(sys.modules[__name__], '%s_loader' % which_loader)\n",
   "    for path in paths:\n",
   "        loader.add_directory(path, with_subdir=True)\n",
   "        self.path = path\n",
   "        warning_text = deprecation.get('warning_text', None)\n",
   "        removal_date = deprecation.get('removal_date', None)\n",
   "        removal_version = deprecation.get('removal_version', None)\n",
   "        if removal_date is not None:\n",
   "            removal_version = None\n",
   "        if not warning_text:\n",
   "            warning_text = '{0} has been deprecated'.format(name)\n",
   "        display.deprecated(warning_text, date=removal_date, version=removal_version, collection_name=collection_name)\n",
   "        if removal_date:\n",
   "            self.removal_date = removal_date\n",
   "        if removal_version:\n",
   "            self.removal_version = removal_version\n",
   "        self.deprecation_warnings.append(warning_text)\n",
   "        aliases = {} if aliases is None else aliases\n",
   "        self.aliases = aliases\n",
   "            config = [config]\n",
   "        elif not config:\n",
   "            config = []\n",
   "        self.config = config\n",
   "        class_name = data.get('class_name')\n",
   "        package = data.get('package')\n",
   "        config = data.get('config')\n",
   "        subdir = data.get('subdir')\n",
   "        aliases = data.get('aliases')\n",
   "        base_class = data.get('base_class')\n",
   "        PATH_CACHE[class_name] = data.get('PATH_CACHE')\n",
   "        PLUGIN_PATH_CACHE[class_name] = data.get('PLUGIN_PATH_CACHE')\n",
   "        self.__init__(class_name, package, config, subdir, aliases, base_class)\n",
   "        ret = []\n",
   "        for i in paths:\n",
   "            if i not in ret:\n",
   "                ret.append(i)\n",
   "        return os.pathsep.join(ret)\n",
   "        results = []\n",
   "        results.append(dir)\n",
   "        for root, subdirs, files in os.walk(dir, followlinks=True):\n",
   "            if '__init__.py' in files:\n",
   "                for x in subdirs:\n",
   "                    results.append(os.path.join(root, x))\n",
   "        return results\n",
   "            m = __import__(self.package)\n",
   "            parts = self.package.split('.')[1:]\n",
   "            for parent_mod in parts:\n",
   "                m = getattr(m, parent_mod)\n",
   "            self.package_path = os.path.dirname(m.__file__)\n",
   "        if subdirs:\n",
   "        ret = [PluginPathContext(p, False) for p in self._extra_dirs]\n",
   "            for path in self.config:\n",
   "                path = os.path.realpath(os.path.expanduser(path))\n",
   "                if subdirs:\n",
   "                    contents = glob.glob(\"%s/*\" % path) + glob.glob(\"%s/*/*\" % path)\n",
   "                    for c in contents:\n",
   "                        if os.path.isdir(c) and c not in ret:\n",
   "                            ret.append(PluginPathContext(c, False))\n",
   "                if path not in ret:\n",
   "                    ret.append(PluginPathContext(path, False))\n",
   "        ret.extend([PluginPathContext(p, True) for p in self._get_package_paths(subdirs=subdirs)])\n",
   "        ret.sort(key=lambda p: p.path.endswith('/windows'))\n",
   "        self._paths = ret\n",
   "        return ret\n",
   "        paths_with_context = self._get_paths_with_context(subdirs=subdirs)\n",
   "        return [path_with_context.path for path_with_context in paths_with_context]\n",
   "            type_name = get_plugin_class(self.class_name)\n",
   "            if type_name in C.CONFIGURABLE_PLUGINS:\n",
   "                dstring = AnsibleLoader(getattr(module, 'DOCUMENTATION', ''), file_name=path).get_single_data()\n",
   "                if dstring:\n",
   "                    add_fragments(dstring, path, fragment_loader=fragment_loader, is_module=(type_name == 'module'))\n",
   "                if dstring and 'options' in dstring and isinstance(dstring['options'], dict):\n",
   "                    C.config.initialize_plugin_configuration_definitions(type_name, name, dstring['options'])\n",
   "                    display.debug('Loaded config def from plugin (%s/%s)' % (type_name, name))\n",
   "        directory = os.path.realpath(directory)\n",
   "        if directory is not None:\n",
   "                directory = os.path.join(directory, self.subdir)\n",
   "            if directory not in self._extra_dirs:\n",
   "                self._extra_dirs.append(directory)\n",
   "                display.debug('Added %s to loader search path' % (directory))\n",
   "        collection_pkg = import_module(acr.n_python_collection_package_name)\n",
   "        if not collection_pkg:\n",
   "        collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "        if not collection_meta:\n",
   "            subdir_qualified_resource = '.'.join([acr.subdirs, acr.resource])\n",
   "            subdir_qualified_resource = acr.resource\n",
   "        entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource + extension, None)\n",
   "        if not entry:\n",
   "            entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource, None)\n",
   "        return entry\n",
   "        plugin_type = AnsibleCollectionRef.legacy_plugin_dir_to_plugin_type(self.subdir)\n",
   "        acr = AnsibleCollectionRef.from_fqcr(fq_name, plugin_type)\n",
   "        routing_metadata = self._query_collection_routing_meta(acr, plugin_type, extension=extension)\n",
   "        if routing_metadata:\n",
   "            deprecation = routing_metadata.get('deprecation', None)\n",
   "            plugin_load_context.record_deprecation(fq_name, deprecation, acr.collection)\n",
   "            tombstone = routing_metadata.get('tombstone', None)\n",
   "            if tombstone:\n",
   "                removal_date = tombstone.get('removal_date')\n",
   "                removal_version = tombstone.get('removal_version')\n",
   "                warning_text = tombstone.get('warning_text') or '{0} has been removed.'.format(fq_name)\n",
   "                removed_msg = display.get_deprecation_message(msg=warning_text, version=removal_version,\n",
   "                                                              date=removal_date, removed=True,\n",
   "                                                              collection_name=acr.collection)\n",
   "                plugin_load_context.removal_date = removal_date\n",
   "                plugin_load_context.removal_version = removal_version\n",
   "                plugin_load_context.exit_reason = removed_msg\n",
   "                raise AnsiblePluginRemovedError(removed_msg, plugin_load_context=plugin_load_context)\n",
   "            redirect = routing_metadata.get('redirect', None)\n",
   "            if redirect:\n",
   "                display.vv(\"redirecting (type: {0}) {1} to {2}\".format(plugin_type, fq_name, redirect))\n",
   "                return plugin_load_context.redirect(redirect)\n",
   "        n_resource = to_native(acr.resource, errors='strict')\n",
   "        full_name = '{0}.{1}'.format(acr.n_python_package_name, n_resource)\n",
   "            n_resource += extension\n",
   "        pkg = sys.modules.get(acr.n_python_package_name)\n",
   "        if not pkg:\n",
   "                pkg = import_module(acr.n_python_package_name)\n",
   "                return plugin_load_context.nope('Python package {0} not found'.format(acr.n_python_package_name))\n",
   "        pkg_path = os.path.dirname(pkg.__file__)\n",
   "        n_resource_path = os.path.join(pkg_path, n_resource)\n",
   "        if os.path.exists(n_resource_path):\n",
   "                full_name, to_text(n_resource_path), acr.collection, 'found exact match for {0} in {1}'.format(full_name, acr.collection))\n",
   "            return plugin_load_context.nope('no match for {0} in {1}'.format(to_text(n_resource), acr.collection))\n",
   "        found_files = [f\n",
   "                       for f in glob.iglob(os.path.join(pkg_path, n_resource) + '.*')\n",
   "                       if os.path.isfile(f) and not f.endswith(C.MODULE_IGNORE_EXTS)]\n",
   "        if not found_files:\n",
   "            return plugin_load_context.nope('failed fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        if len(found_files) > 1:\n",
   "            full_name, to_text(found_files[0]), acr.collection, 'found fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        result = self.find_plugin_with_context(name, mod_type, ignore_deprecated, check_aliases, collection_list)\n",
   "        if result.resolved and result.plugin_resolved_path:\n",
   "            return result.plugin_resolved_path\n",
   "        plugin_load_context = PluginLoadContext()\n",
   "        plugin_load_context.original_name = name\n",
   "            result = self._resolve_plugin_step(name, mod_type, ignore_deprecated, check_aliases, collection_list, plugin_load_context=plugin_load_context)\n",
   "            if result.pending_redirect:\n",
   "                if result.pending_redirect in result.redirect_list:\n",
   "                    raise AnsiblePluginCircularRedirect('plugin redirect loop resolving {0} (path: {1})'.format(result.original_name, result.redirect_list))\n",
   "                name = result.pending_redirect\n",
   "                result.pending_redirect = None\n",
   "                plugin_load_context = result\n",
   "        if plugin_load_context.error_list:\n",
   "            display.warning(\"errors were encountered during the plugin load for {0}:\\n{1}\".format(name, plugin_load_context.error_list))\n",
   "        return plugin_load_context\n",
   "        if not plugin_load_context:\n",
   "        plugin_load_context.redirect_list.append(name)\n",
   "        plugin_load_context.resolved = False\n",
   "        if name in _PLUGIN_FILTERS[self.package]:\n",
   "            plugin_load_context.exit_reason = '{0} matched a defined plugin filter'.format(name)\n",
   "            return plugin_load_context\n",
   "            suffix = mod_type\n",
   "            suffix = '.py'\n",
   "            suffix = ''\n",
   "        if (AnsibleCollectionRef.is_valid_fqcr(name) or collection_list) and not name.startswith('Ansible'):\n",
   "            if '.' in name or not collection_list:\n",
   "                candidates = [name]\n",
   "                candidates = ['{0}.{1}'.format(c, name) for c in collection_list]\n",
   "            for candidate_name in candidates:\n",
   "                    plugin_load_context.load_attempts.append(candidate_name)\n",
   "                    if candidate_name.startswith('ansible.legacy'):\n",
   "                        plugin_load_context = self._find_plugin_legacy(name.replace('ansible.legacy.', '', 1),\n",
   "                                                                       plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "                        plugin_load_context = self._find_fq_plugin(candidate_name, suffix, plugin_load_context=plugin_load_context)\n",
   "                        if candidate_name != plugin_load_context.original_name and candidate_name not in plugin_load_context.redirect_list:\n",
   "                            plugin_load_context.redirect_list.append(candidate_name)\n",
   "                    if plugin_load_context.resolved or plugin_load_context.pending_redirect:  # if we got an answer or need to chase down a redirect, return\n",
   "                        return plugin_load_context\n",
   "                    plugin_load_context.import_error_list.append(ie)\n",
   "                    plugin_load_context.error_list.append(to_native(ex))\n",
   "            if plugin_load_context.error_list:\n",
   "                display.debug(msg='plugin lookup for {0} failed; errors: {1}'.format(name, '; '.join(plugin_load_context.error_list)))\n",
   "            plugin_load_context.exit_reason = 'no matches found for {0}'.format(name)\n",
   "            return plugin_load_context\n",
   "        return self._find_plugin_legacy(name, plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "        plugin_load_context.resolved = False\n",
   "            name = self.aliases.get(name, name)\n",
   "        pull_cache = self._plugin_path_cache[suffix]\n",
   "            path_with_context = pull_cache[name]\n",
   "            plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "            plugin_load_context.plugin_resolved_name = name\n",
   "            plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "            plugin_load_context.resolved = True\n",
   "            return plugin_load_context\n",
   "        for path_context in (p for p in self._get_paths_with_context() if p.path not in self._searched_paths and os.path.isdir(p.path)):\n",
   "            path = path_context.path\n",
   "            display.debug('trying %s' % path)\n",
   "            plugin_load_context.load_attempts.append(path)\n",
   "                full_paths = (os.path.join(path, f) for f in os.listdir(path))\n",
   "                display.warning(\"Error accessing plugin paths: %s\" % to_text(e))\n",
   "            for full_path in (f for f in full_paths if os.path.isfile(f) and not f.endswith('__init__.py')):\n",
   "                full_name = os.path.basename(full_path)\n",
   "                if any(full_path.endswith(x) for x in C.MODULE_IGNORE_EXTS):\n",
   "                splitname = os.path.splitext(full_name)\n",
   "                base_name = splitname[0]\n",
   "                internal = path_context.internal\n",
   "                    extension = splitname[1]\n",
   "                    extension = ''\n",
   "                if base_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][full_name] = PluginPathContext(full_path, internal)\n",
   "                if base_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][full_name] = PluginPathContext(full_path, internal)\n",
   "            self._searched_paths.add(path)\n",
   "                path_with_context = pull_cache[name]\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        if not name.startswith('_'):\n",
   "            alias_name = '_' + name\n",
   "            if alias_name in pull_cache:\n",
   "                path_with_context = pull_cache[alias_name]\n",
   "                if not ignore_deprecated and not os.path.islink(path_with_context.path):\n",
   "                    display.deprecated('%s is kept for backwards compatibility but usage is discouraged. '  # pylint: disable=ansible-deprecated-no-version\n",
   "                                       'The module documentation details page may explain more about this rationale.' % name.lstrip('_'))\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = alias_name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        candidate_fqcr = 'ansible.builtin.{0}'.format(name)\n",
   "        if '.' not in name and AnsibleCollectionRef.is_valid_fqcr(candidate_fqcr):\n",
   "            return self._find_fq_plugin(fq_name=candidate_fqcr, extension=suffix, plugin_load_context=plugin_load_context)\n",
   "        return plugin_load_context.nope('{0} is not eligible for last-chance resolution'.format(name))\n",
   "            return self.find_plugin(name, collection_list=collection_list) is not None\n",
   "            display.debug('has_plugin error: {0}'.format(to_text(ex)))\n",
   "        if name.startswith('ansible_collections.'):\n",
   "            full_name = name\n",
   "            full_name = '.'.join([self.package, name])\n",
   "        if full_name in sys.modules:\n",
   "            return sys.modules[full_name]\n",
   "            if imp is None:\n",
   "                spec = importlib.util.spec_from_file_location(to_native(full_name), to_native(path))\n",
   "                module = importlib.util.module_from_spec(spec)\n",
   "                spec.loader.exec_module(module)\n",
   "                sys.modules[full_name] = module\n",
   "                with open(to_bytes(path), 'rb') as module_file:\n",
   "                    module = imp.load_source(to_native(full_name), to_native(path), module_file)\n",
   "        return module\n",
   "        setattr(obj, '_original_path', path)\n",
   "        setattr(obj, '_load_name', name)\n",
   "        setattr(obj, '_redirected_names', redirected_names or [])\n",
   "        return self.get_with_context(name, *args, **kwargs).object\n",
   "        found_in_cache = True\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        collection_list = kwargs.pop('collection_list', None)\n",
   "        if name in self.aliases:\n",
   "            name = self.aliases[name]\n",
   "        plugin_load_context = self.find_plugin_with_context(name, collection_list=collection_list)\n",
   "        if not plugin_load_context.resolved or not plugin_load_context.plugin_resolved_path:\n",
   "            return get_with_context_result(None, plugin_load_context)\n",
   "        name = plugin_load_context.plugin_resolved_name\n",
   "        path = plugin_load_context.plugin_resolved_path\n",
   "        redirected_names = plugin_load_context.redirect_list or []\n",
   "        if path not in self._module_cache:\n",
   "            self._module_cache[path] = self._load_module_source(name, path)\n",
   "            self._load_config_defs(name, self._module_cache[path], path)\n",
   "            found_in_cache = False\n",
   "        obj = getattr(self._module_cache[path], self.class_name)\n",
   "            module = __import__(self.package, fromlist=[self.base_class])\n",
   "                plugin_class = getattr(module, self.base_class)\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "            if not issubclass(obj, plugin_class):\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "        self._display_plugin_load(self.class_name, name, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "        if not class_only:\n",
   "                instance = object.__new__(obj)\n",
   "                self._update_object(instance, name, path, redirected_names)\n",
   "                obj.__init__(instance, *args, **kwargs)\n",
   "                obj = instance\n",
   "                    return get_with_context_result(None, plugin_load_context)\n",
   "        self._update_object(obj, name, path, redirected_names)\n",
   "        return get_with_context_result(obj, plugin_load_context)\n",
   "            msg = 'Loading %s \\'%s\\' from %s' % (class_name, os.path.basename(name), path)\n",
   "                msg = '%s (searched paths: %s)' % (msg, self.format_paths(searched_paths))\n",
   "            if found_in_cache or class_only:\n",
   "                msg = '%s (found_in_cache=%s, class_only=%s)' % (msg, found_in_cache, class_only)\n",
   "            display.debug(msg)\n",
   "        dedupe = kwargs.pop('_dedupe', True)\n",
   "        path_only = kwargs.pop('path_only', False)\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        if path_only and class_only:\n",
   "        all_matches = []\n",
   "        found_in_cache = True\n",
   "        for i in self._get_paths():\n",
   "            all_matches.extend(glob.glob(os.path.join(i, \"*.py\")))\n",
   "        loaded_modules = set()\n",
   "        for path in sorted(all_matches, key=os.path.basename):\n",
   "            name = os.path.splitext(path)[0]\n",
   "            basename = os.path.basename(name)\n",
   "            if basename == '__init__' or basename in _PLUGIN_FILTERS[self.package]:\n",
   "            if dedupe and basename in loaded_modules:\n",
   "            loaded_modules.add(basename)\n",
   "            if path_only:\n",
   "                yield path\n",
   "            if path not in self._module_cache:\n",
   "                        full_name = '{0}_{1}'.format(abs(hash(path)), basename)\n",
   "                        full_name = basename\n",
   "                    module = self._load_module_source(full_name, path)\n",
   "                    self._load_config_defs(basename, module, path)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                self._module_cache[path] = module\n",
   "                found_in_cache = False\n",
   "                obj = getattr(self._module_cache[path], self.class_name)\n",
   "                display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                module = __import__(self.package, fromlist=[self.base_class])\n",
   "                    plugin_class = getattr(module, self.base_class)\n",
   "                if not issubclass(obj, plugin_class):\n",
   "            self._display_plugin_load(self.class_name, basename, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "            if not class_only:\n",
   "                    obj = obj(*args, **kwargs)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be incomplete: %s\" % (path, to_text(e)))\n",
   "            self._update_object(obj, basename, path)\n",
   "            yield obj\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).find_plugin(name, collection_list=collection_list)\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).get(name, *args, **kwargs)\n",
   "        plugins = list(super(Jinja2Loader, self).all(*args, **kwargs))\n",
   "        plugins.reverse()\n",
   "        return plugins\n",
   "    filters = defaultdict(frozenset)\n",
   "        filter_cfg = '/etc/ansible/plugin_filters.yml'\n",
   "        filter_cfg = C.PLUGIN_FILTERS_CFG\n",
   "    if os.path.exists(filter_cfg):\n",
   "        with open(filter_cfg, 'rb') as f:\n",
   "                filter_data = from_yaml(f.read())\n",
   "                display.warning(u'The plugin filter file, {0} was not parsable.'\n",
   "                                u' Skipping: {1}'.format(filter_cfg, to_text(e)))\n",
   "                return filters\n",
   "            version = filter_data['filter_version']\n",
   "            display.warning(u'The plugin filter file, {0} was invalid.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "            return filters\n",
   "        version = to_text(version)\n",
   "        version = version.strip()\n",
   "        if version == u'1.0':\n",
   "                filters['ansible.modules'] = frozenset(filter_data['module_blacklist'])\n",
   "                display.warning(u'Unable to parse the plugin filter file {0} as'\n",
   "                                u' Skipping.'.format(filter_cfg))\n",
   "                return filters\n",
   "            filters['ansible.plugins.action'] = filters['ansible.modules']\n",
   "            display.warning(u'The plugin filter file, {0} was a version not recognized by this'\n",
   "                            u' version of Ansible. Skipping.'.format(filter_cfg))\n",
   "            display.warning(u'The plugin filter file, {0} does not exist.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "    if 'stat' in filters['ansible.modules']:\n",
   "                           ' from the blacklist.'.format(to_native(filter_cfg)))\n",
   "    return filters\n",
   "    display.vvvv(to_text('Loading collection {0} from {1}'.format(collection_name, collection_path)))\n",
   "        if not _does_collection_support_ansible_version(collection_meta.get('requires_ansible', ''), ansible_version):\n",
   "                display.warning(message)\n",
   "        display.warning('Error parsing collection metadata requires_ansible value from collection {0}: {1}'.format(collection_name, ex))\n",
   "        display.warning('packaging Python module unavailable; unable to validate collection Ansible version requirements')\n",
   "        display.warning('AnsibleCollectionFinder has already been configured')\n"
  ]
 },
 "105": {
  "name": "m",
  "type": "module",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/plugins/loader.py",
  "lineno": "314",
  "column": "16",
  "context": "         for parent_mod in parts:\n                m = getattr(m, parent_mod)\n            self.package_path = os.path.dirname(m.",
  "context_lines": "        if not hasattr(self, 'package_path'):\n            m = __import__(self.package)\n            parts = self.package.split('.')[1:]\n            for parent_mod in parts:\n                m = getattr(m, parent_mod)\n            self.package_path = os.path.dirname(m.__file__)\n        if subdirs:\n            return self._all_directories(self.package_path)\n        return [self.package_path]\n\n",
  "slicing": [
   "    imp = None\n",
   "display = Display()\n",
   "get_with_context_result = namedtuple('get_with_context_result', ['object', 'plugin_load_context'])\n",
   "    return [(name, obj) for (name, obj) in globals().items() if isinstance(obj, PluginLoader)]\n",
   "    b_path = os.path.expanduser(to_bytes(path, errors='surrogate_or_strict'))\n",
   "    if os.path.isdir(b_path):\n",
   "        for name, obj in get_all_plugin_loaders():\n",
   "            if obj.subdir:\n",
   "                plugin_path = os.path.join(b_path, to_bytes(obj.subdir))\n",
   "                if os.path.isdir(plugin_path):\n",
   "                    obj.add_directory(to_text(plugin_path))\n",
   "        display.warning(\"Ignoring invalid path provided to plugin path: '%s' is not a directory\" % to_text(path))\n",
   "        shell_type = 'sh'\n",
   "                shell_filename = os.path.basename(executable)\n",
   "                    shell = shell_loader.get(shell_filename)\n",
   "                    shell = None\n",
   "                if shell is None:\n",
   "                    for shell in shell_loader.all():\n",
   "                        if shell_filename in shell.COMPATIBLE_SHELLS:\n",
   "                            shell_type = shell.SHELL_FAMILY\n",
   "    shell = shell_loader.get(shell_type)\n",
   "    if not shell:\n",
   "        raise AnsibleError(\"Could not find the shell plugin required (%s).\" % shell_type)\n",
   "        setattr(shell, 'executable', executable)\n",
   "    return shell\n",
   "    loader = getattr(sys.modules[__name__], '%s_loader' % which_loader)\n",
   "    for path in paths:\n",
   "        loader.add_directory(path, with_subdir=True)\n",
   "        self.path = path\n",
   "        warning_text = deprecation.get('warning_text', None)\n",
   "        removal_date = deprecation.get('removal_date', None)\n",
   "        removal_version = deprecation.get('removal_version', None)\n",
   "        if removal_date is not None:\n",
   "            removal_version = None\n",
   "        if not warning_text:\n",
   "            warning_text = '{0} has been deprecated'.format(name)\n",
   "        display.deprecated(warning_text, date=removal_date, version=removal_version, collection_name=collection_name)\n",
   "        if removal_date:\n",
   "            self.removal_date = removal_date\n",
   "        if removal_version:\n",
   "            self.removal_version = removal_version\n",
   "        self.deprecation_warnings.append(warning_text)\n",
   "        aliases = {} if aliases is None else aliases\n",
   "        self.aliases = aliases\n",
   "            config = [config]\n",
   "        elif not config:\n",
   "            config = []\n",
   "        self.config = config\n",
   "        class_name = data.get('class_name')\n",
   "        package = data.get('package')\n",
   "        config = data.get('config')\n",
   "        subdir = data.get('subdir')\n",
   "        aliases = data.get('aliases')\n",
   "        base_class = data.get('base_class')\n",
   "        PATH_CACHE[class_name] = data.get('PATH_CACHE')\n",
   "        PLUGIN_PATH_CACHE[class_name] = data.get('PLUGIN_PATH_CACHE')\n",
   "        self.__init__(class_name, package, config, subdir, aliases, base_class)\n",
   "        ret = []\n",
   "        for i in paths:\n",
   "            if i not in ret:\n",
   "                ret.append(i)\n",
   "        return os.pathsep.join(ret)\n",
   "        results = []\n",
   "        results.append(dir)\n",
   "        for root, subdirs, files in os.walk(dir, followlinks=True):\n",
   "            if '__init__.py' in files:\n",
   "                for x in subdirs:\n",
   "                    results.append(os.path.join(root, x))\n",
   "        return results\n",
   "            m = __import__(self.package)\n",
   "            parts = self.package.split('.')[1:]\n",
   "            for parent_mod in parts:\n",
   "                m = getattr(m, parent_mod)\n",
   "            self.package_path = os.path.dirname(m.__file__)\n",
   "        if subdirs:\n",
   "        ret = [PluginPathContext(p, False) for p in self._extra_dirs]\n",
   "            for path in self.config:\n",
   "                path = os.path.realpath(os.path.expanduser(path))\n",
   "                if subdirs:\n",
   "                    contents = glob.glob(\"%s/*\" % path) + glob.glob(\"%s/*/*\" % path)\n",
   "                    for c in contents:\n",
   "                        if os.path.isdir(c) and c not in ret:\n",
   "                            ret.append(PluginPathContext(c, False))\n",
   "                if path not in ret:\n",
   "                    ret.append(PluginPathContext(path, False))\n",
   "        ret.extend([PluginPathContext(p, True) for p in self._get_package_paths(subdirs=subdirs)])\n",
   "        ret.sort(key=lambda p: p.path.endswith('/windows'))\n",
   "        self._paths = ret\n",
   "        return ret\n",
   "        paths_with_context = self._get_paths_with_context(subdirs=subdirs)\n",
   "        return [path_with_context.path for path_with_context in paths_with_context]\n",
   "            type_name = get_plugin_class(self.class_name)\n",
   "            if type_name in C.CONFIGURABLE_PLUGINS:\n",
   "                dstring = AnsibleLoader(getattr(module, 'DOCUMENTATION', ''), file_name=path).get_single_data()\n",
   "                if dstring:\n",
   "                    add_fragments(dstring, path, fragment_loader=fragment_loader, is_module=(type_name == 'module'))\n",
   "                if dstring and 'options' in dstring and isinstance(dstring['options'], dict):\n",
   "                    C.config.initialize_plugin_configuration_definitions(type_name, name, dstring['options'])\n",
   "                    display.debug('Loaded config def from plugin (%s/%s)' % (type_name, name))\n",
   "        directory = os.path.realpath(directory)\n",
   "        if directory is not None:\n",
   "                directory = os.path.join(directory, self.subdir)\n",
   "            if directory not in self._extra_dirs:\n",
   "                self._extra_dirs.append(directory)\n",
   "                display.debug('Added %s to loader search path' % (directory))\n",
   "        collection_pkg = import_module(acr.n_python_collection_package_name)\n",
   "        if not collection_pkg:\n",
   "        collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "        if not collection_meta:\n",
   "            subdir_qualified_resource = '.'.join([acr.subdirs, acr.resource])\n",
   "            subdir_qualified_resource = acr.resource\n",
   "        entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource + extension, None)\n",
   "        if not entry:\n",
   "            entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource, None)\n",
   "        return entry\n",
   "        plugin_type = AnsibleCollectionRef.legacy_plugin_dir_to_plugin_type(self.subdir)\n",
   "        acr = AnsibleCollectionRef.from_fqcr(fq_name, plugin_type)\n",
   "        routing_metadata = self._query_collection_routing_meta(acr, plugin_type, extension=extension)\n",
   "        if routing_metadata:\n",
   "            deprecation = routing_metadata.get('deprecation', None)\n",
   "            plugin_load_context.record_deprecation(fq_name, deprecation, acr.collection)\n",
   "            tombstone = routing_metadata.get('tombstone', None)\n",
   "            if tombstone:\n",
   "                removal_date = tombstone.get('removal_date')\n",
   "                removal_version = tombstone.get('removal_version')\n",
   "                warning_text = tombstone.get('warning_text') or '{0} has been removed.'.format(fq_name)\n",
   "                removed_msg = display.get_deprecation_message(msg=warning_text, version=removal_version,\n",
   "                                                              date=removal_date, removed=True,\n",
   "                                                              collection_name=acr.collection)\n",
   "                plugin_load_context.removal_date = removal_date\n",
   "                plugin_load_context.removal_version = removal_version\n",
   "                plugin_load_context.exit_reason = removed_msg\n",
   "                raise AnsiblePluginRemovedError(removed_msg, plugin_load_context=plugin_load_context)\n",
   "            redirect = routing_metadata.get('redirect', None)\n",
   "            if redirect:\n",
   "                display.vv(\"redirecting (type: {0}) {1} to {2}\".format(plugin_type, fq_name, redirect))\n",
   "                return plugin_load_context.redirect(redirect)\n",
   "        n_resource = to_native(acr.resource, errors='strict')\n",
   "        full_name = '{0}.{1}'.format(acr.n_python_package_name, n_resource)\n",
   "            n_resource += extension\n",
   "        pkg = sys.modules.get(acr.n_python_package_name)\n",
   "        if not pkg:\n",
   "                pkg = import_module(acr.n_python_package_name)\n",
   "                return plugin_load_context.nope('Python package {0} not found'.format(acr.n_python_package_name))\n",
   "        pkg_path = os.path.dirname(pkg.__file__)\n",
   "        n_resource_path = os.path.join(pkg_path, n_resource)\n",
   "        if os.path.exists(n_resource_path):\n",
   "                full_name, to_text(n_resource_path), acr.collection, 'found exact match for {0} in {1}'.format(full_name, acr.collection))\n",
   "            return plugin_load_context.nope('no match for {0} in {1}'.format(to_text(n_resource), acr.collection))\n",
   "        found_files = [f\n",
   "                       for f in glob.iglob(os.path.join(pkg_path, n_resource) + '.*')\n",
   "                       if os.path.isfile(f) and not f.endswith(C.MODULE_IGNORE_EXTS)]\n",
   "        if not found_files:\n",
   "            return plugin_load_context.nope('failed fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        if len(found_files) > 1:\n",
   "            full_name, to_text(found_files[0]), acr.collection, 'found fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        result = self.find_plugin_with_context(name, mod_type, ignore_deprecated, check_aliases, collection_list)\n",
   "        if result.resolved and result.plugin_resolved_path:\n",
   "            return result.plugin_resolved_path\n",
   "        plugin_load_context = PluginLoadContext()\n",
   "        plugin_load_context.original_name = name\n",
   "            result = self._resolve_plugin_step(name, mod_type, ignore_deprecated, check_aliases, collection_list, plugin_load_context=plugin_load_context)\n",
   "            if result.pending_redirect:\n",
   "                if result.pending_redirect in result.redirect_list:\n",
   "                    raise AnsiblePluginCircularRedirect('plugin redirect loop resolving {0} (path: {1})'.format(result.original_name, result.redirect_list))\n",
   "                name = result.pending_redirect\n",
   "                result.pending_redirect = None\n",
   "                plugin_load_context = result\n",
   "        if plugin_load_context.error_list:\n",
   "            display.warning(\"errors were encountered during the plugin load for {0}:\\n{1}\".format(name, plugin_load_context.error_list))\n",
   "        return plugin_load_context\n",
   "        if not plugin_load_context:\n",
   "        plugin_load_context.redirect_list.append(name)\n",
   "        plugin_load_context.resolved = False\n",
   "        if name in _PLUGIN_FILTERS[self.package]:\n",
   "            plugin_load_context.exit_reason = '{0} matched a defined plugin filter'.format(name)\n",
   "            return plugin_load_context\n",
   "            suffix = mod_type\n",
   "            suffix = '.py'\n",
   "            suffix = ''\n",
   "        if (AnsibleCollectionRef.is_valid_fqcr(name) or collection_list) and not name.startswith('Ansible'):\n",
   "            if '.' in name or not collection_list:\n",
   "                candidates = [name]\n",
   "                candidates = ['{0}.{1}'.format(c, name) for c in collection_list]\n",
   "            for candidate_name in candidates:\n",
   "                    plugin_load_context.load_attempts.append(candidate_name)\n",
   "                    if candidate_name.startswith('ansible.legacy'):\n",
   "                        plugin_load_context = self._find_plugin_legacy(name.replace('ansible.legacy.', '', 1),\n",
   "                                                                       plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "                        plugin_load_context = self._find_fq_plugin(candidate_name, suffix, plugin_load_context=plugin_load_context)\n",
   "                        if candidate_name != plugin_load_context.original_name and candidate_name not in plugin_load_context.redirect_list:\n",
   "                            plugin_load_context.redirect_list.append(candidate_name)\n",
   "                    if plugin_load_context.resolved or plugin_load_context.pending_redirect:  # if we got an answer or need to chase down a redirect, return\n",
   "                        return plugin_load_context\n",
   "                    plugin_load_context.import_error_list.append(ie)\n",
   "                    plugin_load_context.error_list.append(to_native(ex))\n",
   "            if plugin_load_context.error_list:\n",
   "                display.debug(msg='plugin lookup for {0} failed; errors: {1}'.format(name, '; '.join(plugin_load_context.error_list)))\n",
   "            plugin_load_context.exit_reason = 'no matches found for {0}'.format(name)\n",
   "            return plugin_load_context\n",
   "        return self._find_plugin_legacy(name, plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "        plugin_load_context.resolved = False\n",
   "            name = self.aliases.get(name, name)\n",
   "        pull_cache = self._plugin_path_cache[suffix]\n",
   "            path_with_context = pull_cache[name]\n",
   "            plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "            plugin_load_context.plugin_resolved_name = name\n",
   "            plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "            plugin_load_context.resolved = True\n",
   "            return plugin_load_context\n",
   "        for path_context in (p for p in self._get_paths_with_context() if p.path not in self._searched_paths and os.path.isdir(p.path)):\n",
   "            path = path_context.path\n",
   "            display.debug('trying %s' % path)\n",
   "            plugin_load_context.load_attempts.append(path)\n",
   "                full_paths = (os.path.join(path, f) for f in os.listdir(path))\n",
   "                display.warning(\"Error accessing plugin paths: %s\" % to_text(e))\n",
   "            for full_path in (f for f in full_paths if os.path.isfile(f) and not f.endswith('__init__.py')):\n",
   "                full_name = os.path.basename(full_path)\n",
   "                if any(full_path.endswith(x) for x in C.MODULE_IGNORE_EXTS):\n",
   "                splitname = os.path.splitext(full_name)\n",
   "                base_name = splitname[0]\n",
   "                internal = path_context.internal\n",
   "                    extension = splitname[1]\n",
   "                    extension = ''\n",
   "                if base_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][full_name] = PluginPathContext(full_path, internal)\n",
   "                if base_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][full_name] = PluginPathContext(full_path, internal)\n",
   "            self._searched_paths.add(path)\n",
   "                path_with_context = pull_cache[name]\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        if not name.startswith('_'):\n",
   "            alias_name = '_' + name\n",
   "            if alias_name in pull_cache:\n",
   "                path_with_context = pull_cache[alias_name]\n",
   "                if not ignore_deprecated and not os.path.islink(path_with_context.path):\n",
   "                    display.deprecated('%s is kept for backwards compatibility but usage is discouraged. '  # pylint: disable=ansible-deprecated-no-version\n",
   "                                       'The module documentation details page may explain more about this rationale.' % name.lstrip('_'))\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = alias_name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        candidate_fqcr = 'ansible.builtin.{0}'.format(name)\n",
   "        if '.' not in name and AnsibleCollectionRef.is_valid_fqcr(candidate_fqcr):\n",
   "            return self._find_fq_plugin(fq_name=candidate_fqcr, extension=suffix, plugin_load_context=plugin_load_context)\n",
   "        return plugin_load_context.nope('{0} is not eligible for last-chance resolution'.format(name))\n",
   "            return self.find_plugin(name, collection_list=collection_list) is not None\n",
   "            display.debug('has_plugin error: {0}'.format(to_text(ex)))\n",
   "        if name.startswith('ansible_collections.'):\n",
   "            full_name = name\n",
   "            full_name = '.'.join([self.package, name])\n",
   "        if full_name in sys.modules:\n",
   "            return sys.modules[full_name]\n",
   "            if imp is None:\n",
   "                spec = importlib.util.spec_from_file_location(to_native(full_name), to_native(path))\n",
   "                module = importlib.util.module_from_spec(spec)\n",
   "                spec.loader.exec_module(module)\n",
   "                sys.modules[full_name] = module\n",
   "                with open(to_bytes(path), 'rb') as module_file:\n",
   "                    module = imp.load_source(to_native(full_name), to_native(path), module_file)\n",
   "        return module\n",
   "        setattr(obj, '_original_path', path)\n",
   "        setattr(obj, '_load_name', name)\n",
   "        setattr(obj, '_redirected_names', redirected_names or [])\n",
   "        return self.get_with_context(name, *args, **kwargs).object\n",
   "        found_in_cache = True\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        collection_list = kwargs.pop('collection_list', None)\n",
   "        if name in self.aliases:\n",
   "            name = self.aliases[name]\n",
   "        plugin_load_context = self.find_plugin_with_context(name, collection_list=collection_list)\n",
   "        if not plugin_load_context.resolved or not plugin_load_context.plugin_resolved_path:\n",
   "            return get_with_context_result(None, plugin_load_context)\n",
   "        name = plugin_load_context.plugin_resolved_name\n",
   "        path = plugin_load_context.plugin_resolved_path\n",
   "        redirected_names = plugin_load_context.redirect_list or []\n",
   "        if path not in self._module_cache:\n",
   "            self._module_cache[path] = self._load_module_source(name, path)\n",
   "            self._load_config_defs(name, self._module_cache[path], path)\n",
   "            found_in_cache = False\n",
   "        obj = getattr(self._module_cache[path], self.class_name)\n",
   "            module = __import__(self.package, fromlist=[self.base_class])\n",
   "                plugin_class = getattr(module, self.base_class)\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "            if not issubclass(obj, plugin_class):\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "        self._display_plugin_load(self.class_name, name, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "        if not class_only:\n",
   "                instance = object.__new__(obj)\n",
   "                self._update_object(instance, name, path, redirected_names)\n",
   "                obj.__init__(instance, *args, **kwargs)\n",
   "                obj = instance\n",
   "                    return get_with_context_result(None, plugin_load_context)\n",
   "        self._update_object(obj, name, path, redirected_names)\n",
   "        return get_with_context_result(obj, plugin_load_context)\n",
   "            msg = 'Loading %s \\'%s\\' from %s' % (class_name, os.path.basename(name), path)\n",
   "                msg = '%s (searched paths: %s)' % (msg, self.format_paths(searched_paths))\n",
   "            if found_in_cache or class_only:\n",
   "                msg = '%s (found_in_cache=%s, class_only=%s)' % (msg, found_in_cache, class_only)\n",
   "            display.debug(msg)\n",
   "        dedupe = kwargs.pop('_dedupe', True)\n",
   "        path_only = kwargs.pop('path_only', False)\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        if path_only and class_only:\n",
   "        all_matches = []\n",
   "        found_in_cache = True\n",
   "        for i in self._get_paths():\n",
   "            all_matches.extend(glob.glob(os.path.join(i, \"*.py\")))\n",
   "        loaded_modules = set()\n",
   "        for path in sorted(all_matches, key=os.path.basename):\n",
   "            name = os.path.splitext(path)[0]\n",
   "            basename = os.path.basename(name)\n",
   "            if basename == '__init__' or basename in _PLUGIN_FILTERS[self.package]:\n",
   "            if dedupe and basename in loaded_modules:\n",
   "            loaded_modules.add(basename)\n",
   "            if path_only:\n",
   "                yield path\n",
   "            if path not in self._module_cache:\n",
   "                        full_name = '{0}_{1}'.format(abs(hash(path)), basename)\n",
   "                        full_name = basename\n",
   "                    module = self._load_module_source(full_name, path)\n",
   "                    self._load_config_defs(basename, module, path)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                self._module_cache[path] = module\n",
   "                found_in_cache = False\n",
   "                obj = getattr(self._module_cache[path], self.class_name)\n",
   "                display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                module = __import__(self.package, fromlist=[self.base_class])\n",
   "                    plugin_class = getattr(module, self.base_class)\n",
   "                if not issubclass(obj, plugin_class):\n",
   "            self._display_plugin_load(self.class_name, basename, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "            if not class_only:\n",
   "                    obj = obj(*args, **kwargs)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be incomplete: %s\" % (path, to_text(e)))\n",
   "            self._update_object(obj, basename, path)\n",
   "            yield obj\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).find_plugin(name, collection_list=collection_list)\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).get(name, *args, **kwargs)\n",
   "        plugins = list(super(Jinja2Loader, self).all(*args, **kwargs))\n",
   "        plugins.reverse()\n",
   "        return plugins\n",
   "    filters = defaultdict(frozenset)\n",
   "        filter_cfg = '/etc/ansible/plugin_filters.yml'\n",
   "        filter_cfg = C.PLUGIN_FILTERS_CFG\n",
   "    if os.path.exists(filter_cfg):\n",
   "        with open(filter_cfg, 'rb') as f:\n",
   "                filter_data = from_yaml(f.read())\n",
   "                display.warning(u'The plugin filter file, {0} was not parsable.'\n",
   "                                u' Skipping: {1}'.format(filter_cfg, to_text(e)))\n",
   "                return filters\n",
   "            version = filter_data['filter_version']\n",
   "            display.warning(u'The plugin filter file, {0} was invalid.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "            return filters\n",
   "        version = to_text(version)\n",
   "        version = version.strip()\n",
   "        if version == u'1.0':\n",
   "                filters['ansible.modules'] = frozenset(filter_data['module_blacklist'])\n",
   "                display.warning(u'Unable to parse the plugin filter file {0} as'\n",
   "                                u' Skipping.'.format(filter_cfg))\n",
   "                return filters\n",
   "            filters['ansible.plugins.action'] = filters['ansible.modules']\n",
   "            display.warning(u'The plugin filter file, {0} was a version not recognized by this'\n",
   "                            u' version of Ansible. Skipping.'.format(filter_cfg))\n",
   "            display.warning(u'The plugin filter file, {0} does not exist.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "    if 'stat' in filters['ansible.modules']:\n",
   "                           ' from the blacklist.'.format(to_native(filter_cfg)))\n",
   "    return filters\n",
   "    display.vvvv(to_text('Loading collection {0} from {1}'.format(collection_name, collection_path)))\n",
   "        if not _does_collection_support_ansible_version(collection_meta.get('requires_ansible', ''), ansible_version):\n",
   "                display.warning(message)\n",
   "        display.warning('Error parsing collection metadata requires_ansible value from collection {0}: {1}'.format(collection_name, ex))\n",
   "        display.warning('packaging Python module unavailable; unable to validate collection Ansible version requirements')\n",
   "        display.warning('AnsibleCollectionFinder has already been configured')\n"
  ]
 },
 "106": {
  "name": "path",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/loader.py",
  "lineno": "334",
  "column": "16",
  "context": "         for path in self.config:\n                path = os.path.realpath(os.path.expanduser(path))\n                if subdirs:\n                    co",
  "context_lines": "        ret = [PluginPathContext(p, False) for p in self._extra_dirs]\n\n        # look in any configured plugin paths, allow one level deep for subcategories\n        if self.config is not None:\n            for path in self.config:\n                path = os.path.realpath(os.path.expanduser(path))\n                if subdirs:\n                    contents = glob.glob(\"%s/*\" % path) + glob.glob(\"%s/*/*\" % path)\n                    for c in contents:\n                        if os.path.isdir(c) and c not in ret:\n",
  "slicing": [
   "    imp = None\n",
   "display = Display()\n",
   "get_with_context_result = namedtuple('get_with_context_result', ['object', 'plugin_load_context'])\n",
   "    return [(name, obj) for (name, obj) in globals().items() if isinstance(obj, PluginLoader)]\n",
   "    b_path = os.path.expanduser(to_bytes(path, errors='surrogate_or_strict'))\n",
   "    if os.path.isdir(b_path):\n",
   "        for name, obj in get_all_plugin_loaders():\n",
   "            if obj.subdir:\n",
   "                plugin_path = os.path.join(b_path, to_bytes(obj.subdir))\n",
   "                if os.path.isdir(plugin_path):\n",
   "                    obj.add_directory(to_text(plugin_path))\n",
   "        display.warning(\"Ignoring invalid path provided to plugin path: '%s' is not a directory\" % to_text(path))\n",
   "        shell_type = 'sh'\n",
   "                shell_filename = os.path.basename(executable)\n",
   "                    shell = shell_loader.get(shell_filename)\n",
   "                    shell = None\n",
   "                if shell is None:\n",
   "                    for shell in shell_loader.all():\n",
   "                        if shell_filename in shell.COMPATIBLE_SHELLS:\n",
   "                            shell_type = shell.SHELL_FAMILY\n",
   "    shell = shell_loader.get(shell_type)\n",
   "    if not shell:\n",
   "        raise AnsibleError(\"Could not find the shell plugin required (%s).\" % shell_type)\n",
   "        setattr(shell, 'executable', executable)\n",
   "    return shell\n",
   "    loader = getattr(sys.modules[__name__], '%s_loader' % which_loader)\n",
   "    for path in paths:\n",
   "        loader.add_directory(path, with_subdir=True)\n",
   "        self.path = path\n",
   "        warning_text = deprecation.get('warning_text', None)\n",
   "        removal_date = deprecation.get('removal_date', None)\n",
   "        removal_version = deprecation.get('removal_version', None)\n",
   "        if removal_date is not None:\n",
   "            removal_version = None\n",
   "        if not warning_text:\n",
   "            warning_text = '{0} has been deprecated'.format(name)\n",
   "        display.deprecated(warning_text, date=removal_date, version=removal_version, collection_name=collection_name)\n",
   "        if removal_date:\n",
   "            self.removal_date = removal_date\n",
   "        if removal_version:\n",
   "            self.removal_version = removal_version\n",
   "        self.deprecation_warnings.append(warning_text)\n",
   "        aliases = {} if aliases is None else aliases\n",
   "        self.aliases = aliases\n",
   "            config = [config]\n",
   "        elif not config:\n",
   "            config = []\n",
   "        self.config = config\n",
   "        class_name = data.get('class_name')\n",
   "        package = data.get('package')\n",
   "        config = data.get('config')\n",
   "        subdir = data.get('subdir')\n",
   "        aliases = data.get('aliases')\n",
   "        base_class = data.get('base_class')\n",
   "        PATH_CACHE[class_name] = data.get('PATH_CACHE')\n",
   "        PLUGIN_PATH_CACHE[class_name] = data.get('PLUGIN_PATH_CACHE')\n",
   "        self.__init__(class_name, package, config, subdir, aliases, base_class)\n",
   "        ret = []\n",
   "        for i in paths:\n",
   "            if i not in ret:\n",
   "                ret.append(i)\n",
   "        return os.pathsep.join(ret)\n",
   "        results = []\n",
   "        results.append(dir)\n",
   "        for root, subdirs, files in os.walk(dir, followlinks=True):\n",
   "            if '__init__.py' in files:\n",
   "                for x in subdirs:\n",
   "                    results.append(os.path.join(root, x))\n",
   "        return results\n",
   "            m = __import__(self.package)\n",
   "            parts = self.package.split('.')[1:]\n",
   "            for parent_mod in parts:\n",
   "                m = getattr(m, parent_mod)\n",
   "            self.package_path = os.path.dirname(m.__file__)\n",
   "        if subdirs:\n",
   "        ret = [PluginPathContext(p, False) for p in self._extra_dirs]\n",
   "            for path in self.config:\n",
   "                path = os.path.realpath(os.path.expanduser(path))\n",
   "                if subdirs:\n",
   "                    contents = glob.glob(\"%s/*\" % path) + glob.glob(\"%s/*/*\" % path)\n",
   "                    for c in contents:\n",
   "                        if os.path.isdir(c) and c not in ret:\n",
   "                            ret.append(PluginPathContext(c, False))\n",
   "                if path not in ret:\n",
   "                    ret.append(PluginPathContext(path, False))\n",
   "        ret.extend([PluginPathContext(p, True) for p in self._get_package_paths(subdirs=subdirs)])\n",
   "        ret.sort(key=lambda p: p.path.endswith('/windows'))\n",
   "        self._paths = ret\n",
   "        return ret\n",
   "        paths_with_context = self._get_paths_with_context(subdirs=subdirs)\n",
   "        return [path_with_context.path for path_with_context in paths_with_context]\n",
   "            type_name = get_plugin_class(self.class_name)\n",
   "            if type_name in C.CONFIGURABLE_PLUGINS:\n",
   "                dstring = AnsibleLoader(getattr(module, 'DOCUMENTATION', ''), file_name=path).get_single_data()\n",
   "                if dstring:\n",
   "                    add_fragments(dstring, path, fragment_loader=fragment_loader, is_module=(type_name == 'module'))\n",
   "                if dstring and 'options' in dstring and isinstance(dstring['options'], dict):\n",
   "                    C.config.initialize_plugin_configuration_definitions(type_name, name, dstring['options'])\n",
   "                    display.debug('Loaded config def from plugin (%s/%s)' % (type_name, name))\n",
   "        directory = os.path.realpath(directory)\n",
   "        if directory is not None:\n",
   "                directory = os.path.join(directory, self.subdir)\n",
   "            if directory not in self._extra_dirs:\n",
   "                self._extra_dirs.append(directory)\n",
   "                display.debug('Added %s to loader search path' % (directory))\n",
   "        collection_pkg = import_module(acr.n_python_collection_package_name)\n",
   "        if not collection_pkg:\n",
   "        collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "        if not collection_meta:\n",
   "            subdir_qualified_resource = '.'.join([acr.subdirs, acr.resource])\n",
   "            subdir_qualified_resource = acr.resource\n",
   "        entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource + extension, None)\n",
   "        if not entry:\n",
   "            entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource, None)\n",
   "        return entry\n",
   "        plugin_type = AnsibleCollectionRef.legacy_plugin_dir_to_plugin_type(self.subdir)\n",
   "        acr = AnsibleCollectionRef.from_fqcr(fq_name, plugin_type)\n",
   "        routing_metadata = self._query_collection_routing_meta(acr, plugin_type, extension=extension)\n",
   "        if routing_metadata:\n",
   "            deprecation = routing_metadata.get('deprecation', None)\n",
   "            plugin_load_context.record_deprecation(fq_name, deprecation, acr.collection)\n",
   "            tombstone = routing_metadata.get('tombstone', None)\n",
   "            if tombstone:\n",
   "                removal_date = tombstone.get('removal_date')\n",
   "                removal_version = tombstone.get('removal_version')\n",
   "                warning_text = tombstone.get('warning_text') or '{0} has been removed.'.format(fq_name)\n",
   "                removed_msg = display.get_deprecation_message(msg=warning_text, version=removal_version,\n",
   "                                                              date=removal_date, removed=True,\n",
   "                                                              collection_name=acr.collection)\n",
   "                plugin_load_context.removal_date = removal_date\n",
   "                plugin_load_context.removal_version = removal_version\n",
   "                plugin_load_context.exit_reason = removed_msg\n",
   "                raise AnsiblePluginRemovedError(removed_msg, plugin_load_context=plugin_load_context)\n",
   "            redirect = routing_metadata.get('redirect', None)\n",
   "            if redirect:\n",
   "                display.vv(\"redirecting (type: {0}) {1} to {2}\".format(plugin_type, fq_name, redirect))\n",
   "                return plugin_load_context.redirect(redirect)\n",
   "        n_resource = to_native(acr.resource, errors='strict')\n",
   "        full_name = '{0}.{1}'.format(acr.n_python_package_name, n_resource)\n",
   "            n_resource += extension\n",
   "        pkg = sys.modules.get(acr.n_python_package_name)\n",
   "        if not pkg:\n",
   "                pkg = import_module(acr.n_python_package_name)\n",
   "                return plugin_load_context.nope('Python package {0} not found'.format(acr.n_python_package_name))\n",
   "        pkg_path = os.path.dirname(pkg.__file__)\n",
   "        n_resource_path = os.path.join(pkg_path, n_resource)\n",
   "        if os.path.exists(n_resource_path):\n",
   "                full_name, to_text(n_resource_path), acr.collection, 'found exact match for {0} in {1}'.format(full_name, acr.collection))\n",
   "            return plugin_load_context.nope('no match for {0} in {1}'.format(to_text(n_resource), acr.collection))\n",
   "        found_files = [f\n",
   "                       for f in glob.iglob(os.path.join(pkg_path, n_resource) + '.*')\n",
   "                       if os.path.isfile(f) and not f.endswith(C.MODULE_IGNORE_EXTS)]\n",
   "        if not found_files:\n",
   "            return plugin_load_context.nope('failed fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        if len(found_files) > 1:\n",
   "            full_name, to_text(found_files[0]), acr.collection, 'found fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        result = self.find_plugin_with_context(name, mod_type, ignore_deprecated, check_aliases, collection_list)\n",
   "        if result.resolved and result.plugin_resolved_path:\n",
   "            return result.plugin_resolved_path\n",
   "        plugin_load_context = PluginLoadContext()\n",
   "        plugin_load_context.original_name = name\n",
   "            result = self._resolve_plugin_step(name, mod_type, ignore_deprecated, check_aliases, collection_list, plugin_load_context=plugin_load_context)\n",
   "            if result.pending_redirect:\n",
   "                if result.pending_redirect in result.redirect_list:\n",
   "                    raise AnsiblePluginCircularRedirect('plugin redirect loop resolving {0} (path: {1})'.format(result.original_name, result.redirect_list))\n",
   "                name = result.pending_redirect\n",
   "                result.pending_redirect = None\n",
   "                plugin_load_context = result\n",
   "        if plugin_load_context.error_list:\n",
   "            display.warning(\"errors were encountered during the plugin load for {0}:\\n{1}\".format(name, plugin_load_context.error_list))\n",
   "        return plugin_load_context\n",
   "        if not plugin_load_context:\n",
   "        plugin_load_context.redirect_list.append(name)\n",
   "        plugin_load_context.resolved = False\n",
   "        if name in _PLUGIN_FILTERS[self.package]:\n",
   "            plugin_load_context.exit_reason = '{0} matched a defined plugin filter'.format(name)\n",
   "            return plugin_load_context\n",
   "            suffix = mod_type\n",
   "            suffix = '.py'\n",
   "            suffix = ''\n",
   "        if (AnsibleCollectionRef.is_valid_fqcr(name) or collection_list) and not name.startswith('Ansible'):\n",
   "            if '.' in name or not collection_list:\n",
   "                candidates = [name]\n",
   "                candidates = ['{0}.{1}'.format(c, name) for c in collection_list]\n",
   "            for candidate_name in candidates:\n",
   "                    plugin_load_context.load_attempts.append(candidate_name)\n",
   "                    if candidate_name.startswith('ansible.legacy'):\n",
   "                        plugin_load_context = self._find_plugin_legacy(name.replace('ansible.legacy.', '', 1),\n",
   "                                                                       plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "                        plugin_load_context = self._find_fq_plugin(candidate_name, suffix, plugin_load_context=plugin_load_context)\n",
   "                        if candidate_name != plugin_load_context.original_name and candidate_name not in plugin_load_context.redirect_list:\n",
   "                            plugin_load_context.redirect_list.append(candidate_name)\n",
   "                    if plugin_load_context.resolved or plugin_load_context.pending_redirect:  # if we got an answer or need to chase down a redirect, return\n",
   "                        return plugin_load_context\n",
   "                    plugin_load_context.import_error_list.append(ie)\n",
   "                    plugin_load_context.error_list.append(to_native(ex))\n",
   "            if plugin_load_context.error_list:\n",
   "                display.debug(msg='plugin lookup for {0} failed; errors: {1}'.format(name, '; '.join(plugin_load_context.error_list)))\n",
   "            plugin_load_context.exit_reason = 'no matches found for {0}'.format(name)\n",
   "            return plugin_load_context\n",
   "        return self._find_plugin_legacy(name, plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "        plugin_load_context.resolved = False\n",
   "            name = self.aliases.get(name, name)\n",
   "        pull_cache = self._plugin_path_cache[suffix]\n",
   "            path_with_context = pull_cache[name]\n",
   "            plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "            plugin_load_context.plugin_resolved_name = name\n",
   "            plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "            plugin_load_context.resolved = True\n",
   "            return plugin_load_context\n",
   "        for path_context in (p for p in self._get_paths_with_context() if p.path not in self._searched_paths and os.path.isdir(p.path)):\n",
   "            path = path_context.path\n",
   "            display.debug('trying %s' % path)\n",
   "            plugin_load_context.load_attempts.append(path)\n",
   "                full_paths = (os.path.join(path, f) for f in os.listdir(path))\n",
   "                display.warning(\"Error accessing plugin paths: %s\" % to_text(e))\n",
   "            for full_path in (f for f in full_paths if os.path.isfile(f) and not f.endswith('__init__.py')):\n",
   "                full_name = os.path.basename(full_path)\n",
   "                if any(full_path.endswith(x) for x in C.MODULE_IGNORE_EXTS):\n",
   "                splitname = os.path.splitext(full_name)\n",
   "                base_name = splitname[0]\n",
   "                internal = path_context.internal\n",
   "                    extension = splitname[1]\n",
   "                    extension = ''\n",
   "                if base_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][full_name] = PluginPathContext(full_path, internal)\n",
   "                if base_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][full_name] = PluginPathContext(full_path, internal)\n",
   "            self._searched_paths.add(path)\n",
   "                path_with_context = pull_cache[name]\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        if not name.startswith('_'):\n",
   "            alias_name = '_' + name\n",
   "            if alias_name in pull_cache:\n",
   "                path_with_context = pull_cache[alias_name]\n",
   "                if not ignore_deprecated and not os.path.islink(path_with_context.path):\n",
   "                    display.deprecated('%s is kept for backwards compatibility but usage is discouraged. '  # pylint: disable=ansible-deprecated-no-version\n",
   "                                       'The module documentation details page may explain more about this rationale.' % name.lstrip('_'))\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = alias_name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        candidate_fqcr = 'ansible.builtin.{0}'.format(name)\n",
   "        if '.' not in name and AnsibleCollectionRef.is_valid_fqcr(candidate_fqcr):\n",
   "            return self._find_fq_plugin(fq_name=candidate_fqcr, extension=suffix, plugin_load_context=plugin_load_context)\n",
   "        return plugin_load_context.nope('{0} is not eligible for last-chance resolution'.format(name))\n",
   "            return self.find_plugin(name, collection_list=collection_list) is not None\n",
   "            display.debug('has_plugin error: {0}'.format(to_text(ex)))\n",
   "        if name.startswith('ansible_collections.'):\n",
   "            full_name = name\n",
   "            full_name = '.'.join([self.package, name])\n",
   "        if full_name in sys.modules:\n",
   "            return sys.modules[full_name]\n",
   "            if imp is None:\n",
   "                spec = importlib.util.spec_from_file_location(to_native(full_name), to_native(path))\n",
   "                module = importlib.util.module_from_spec(spec)\n",
   "                spec.loader.exec_module(module)\n",
   "                sys.modules[full_name] = module\n",
   "                with open(to_bytes(path), 'rb') as module_file:\n",
   "                    module = imp.load_source(to_native(full_name), to_native(path), module_file)\n",
   "        return module\n",
   "        setattr(obj, '_original_path', path)\n",
   "        setattr(obj, '_load_name', name)\n",
   "        setattr(obj, '_redirected_names', redirected_names or [])\n",
   "        return self.get_with_context(name, *args, **kwargs).object\n",
   "        found_in_cache = True\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        collection_list = kwargs.pop('collection_list', None)\n",
   "        if name in self.aliases:\n",
   "            name = self.aliases[name]\n",
   "        plugin_load_context = self.find_plugin_with_context(name, collection_list=collection_list)\n",
   "        if not plugin_load_context.resolved or not plugin_load_context.plugin_resolved_path:\n",
   "            return get_with_context_result(None, plugin_load_context)\n",
   "        name = plugin_load_context.plugin_resolved_name\n",
   "        path = plugin_load_context.plugin_resolved_path\n",
   "        redirected_names = plugin_load_context.redirect_list or []\n",
   "        if path not in self._module_cache:\n",
   "            self._module_cache[path] = self._load_module_source(name, path)\n",
   "            self._load_config_defs(name, self._module_cache[path], path)\n",
   "            found_in_cache = False\n",
   "        obj = getattr(self._module_cache[path], self.class_name)\n",
   "            module = __import__(self.package, fromlist=[self.base_class])\n",
   "                plugin_class = getattr(module, self.base_class)\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "            if not issubclass(obj, plugin_class):\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "        self._display_plugin_load(self.class_name, name, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "        if not class_only:\n",
   "                instance = object.__new__(obj)\n",
   "                self._update_object(instance, name, path, redirected_names)\n",
   "                obj.__init__(instance, *args, **kwargs)\n",
   "                obj = instance\n",
   "                    return get_with_context_result(None, plugin_load_context)\n",
   "        self._update_object(obj, name, path, redirected_names)\n",
   "        return get_with_context_result(obj, plugin_load_context)\n",
   "            msg = 'Loading %s \\'%s\\' from %s' % (class_name, os.path.basename(name), path)\n",
   "                msg = '%s (searched paths: %s)' % (msg, self.format_paths(searched_paths))\n",
   "            if found_in_cache or class_only:\n",
   "                msg = '%s (found_in_cache=%s, class_only=%s)' % (msg, found_in_cache, class_only)\n",
   "            display.debug(msg)\n",
   "        dedupe = kwargs.pop('_dedupe', True)\n",
   "        path_only = kwargs.pop('path_only', False)\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        if path_only and class_only:\n",
   "        all_matches = []\n",
   "        found_in_cache = True\n",
   "        for i in self._get_paths():\n",
   "            all_matches.extend(glob.glob(os.path.join(i, \"*.py\")))\n",
   "        loaded_modules = set()\n",
   "        for path in sorted(all_matches, key=os.path.basename):\n",
   "            name = os.path.splitext(path)[0]\n",
   "            basename = os.path.basename(name)\n",
   "            if basename == '__init__' or basename in _PLUGIN_FILTERS[self.package]:\n",
   "            if dedupe and basename in loaded_modules:\n",
   "            loaded_modules.add(basename)\n",
   "            if path_only:\n",
   "                yield path\n",
   "            if path not in self._module_cache:\n",
   "                        full_name = '{0}_{1}'.format(abs(hash(path)), basename)\n",
   "                        full_name = basename\n",
   "                    module = self._load_module_source(full_name, path)\n",
   "                    self._load_config_defs(basename, module, path)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                self._module_cache[path] = module\n",
   "                found_in_cache = False\n",
   "                obj = getattr(self._module_cache[path], self.class_name)\n",
   "                display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                module = __import__(self.package, fromlist=[self.base_class])\n",
   "                    plugin_class = getattr(module, self.base_class)\n",
   "                if not issubclass(obj, plugin_class):\n",
   "            self._display_plugin_load(self.class_name, basename, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "            if not class_only:\n",
   "                    obj = obj(*args, **kwargs)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be incomplete: %s\" % (path, to_text(e)))\n",
   "            self._update_object(obj, basename, path)\n",
   "            yield obj\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).find_plugin(name, collection_list=collection_list)\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).get(name, *args, **kwargs)\n",
   "        plugins = list(super(Jinja2Loader, self).all(*args, **kwargs))\n",
   "        plugins.reverse()\n",
   "        return plugins\n",
   "    filters = defaultdict(frozenset)\n",
   "        filter_cfg = '/etc/ansible/plugin_filters.yml'\n",
   "        filter_cfg = C.PLUGIN_FILTERS_CFG\n",
   "    if os.path.exists(filter_cfg):\n",
   "        with open(filter_cfg, 'rb') as f:\n",
   "                filter_data = from_yaml(f.read())\n",
   "                display.warning(u'The plugin filter file, {0} was not parsable.'\n",
   "                                u' Skipping: {1}'.format(filter_cfg, to_text(e)))\n",
   "                return filters\n",
   "            version = filter_data['filter_version']\n",
   "            display.warning(u'The plugin filter file, {0} was invalid.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "            return filters\n",
   "        version = to_text(version)\n",
   "        version = version.strip()\n",
   "        if version == u'1.0':\n",
   "                filters['ansible.modules'] = frozenset(filter_data['module_blacklist'])\n",
   "                display.warning(u'Unable to parse the plugin filter file {0} as'\n",
   "                                u' Skipping.'.format(filter_cfg))\n",
   "                return filters\n",
   "            filters['ansible.plugins.action'] = filters['ansible.modules']\n",
   "            display.warning(u'The plugin filter file, {0} was a version not recognized by this'\n",
   "                            u' version of Ansible. Skipping.'.format(filter_cfg))\n",
   "            display.warning(u'The plugin filter file, {0} does not exist.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "    if 'stat' in filters['ansible.modules']:\n",
   "                           ' from the blacklist.'.format(to_native(filter_cfg)))\n",
   "    return filters\n",
   "    display.vvvv(to_text('Loading collection {0} from {1}'.format(collection_name, collection_path)))\n",
   "        if not _does_collection_support_ansible_version(collection_meta.get('requires_ansible', ''), ansible_version):\n",
   "                display.warning(message)\n",
   "        display.warning('Error parsing collection metadata requires_ansible value from collection {0}: {1}'.format(collection_name, ex))\n",
   "        display.warning('packaging Python module unavailable; unable to validate collection Ansible version requirements')\n",
   "        display.warning('AnsibleCollectionFinder has already been configured')\n"
  ]
 },
 "107": {
  "name": "contents",
  "type": "list",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/loader.py",
  "lineno": "336",
  "column": "20",
  "context": ")\n                if subdirs:\n                    contents = glob.glob(\"%s/*\" % path) + glob.glob(\"%s/*/*\" % path)\n                    for c in contents:\n           ",
  "context_lines": "        if self.config is not None:\n            for path in self.config:\n                path = os.path.realpath(os.path.expanduser(path))\n                if subdirs:\n                    contents = glob.glob(\"%s/*\" % path) + glob.glob(\"%s/*/*\" % path)\n                    for c in contents:\n                        if os.path.isdir(c) and c not in ret:\n                            ret.append(PluginPathContext(c, False))\n                if path not in ret:\n",
  "slicing": [
   "    imp = None\n",
   "display = Display()\n",
   "get_with_context_result = namedtuple('get_with_context_result', ['object', 'plugin_load_context'])\n",
   "    return [(name, obj) for (name, obj) in globals().items() if isinstance(obj, PluginLoader)]\n",
   "    b_path = os.path.expanduser(to_bytes(path, errors='surrogate_or_strict'))\n",
   "    if os.path.isdir(b_path):\n",
   "        for name, obj in get_all_plugin_loaders():\n",
   "            if obj.subdir:\n",
   "                plugin_path = os.path.join(b_path, to_bytes(obj.subdir))\n",
   "                if os.path.isdir(plugin_path):\n",
   "                    obj.add_directory(to_text(plugin_path))\n",
   "        display.warning(\"Ignoring invalid path provided to plugin path: '%s' is not a directory\" % to_text(path))\n",
   "        shell_type = 'sh'\n",
   "                shell_filename = os.path.basename(executable)\n",
   "                    shell = shell_loader.get(shell_filename)\n",
   "                    shell = None\n",
   "                if shell is None:\n",
   "                    for shell in shell_loader.all():\n",
   "                        if shell_filename in shell.COMPATIBLE_SHELLS:\n",
   "                            shell_type = shell.SHELL_FAMILY\n",
   "    shell = shell_loader.get(shell_type)\n",
   "    if not shell:\n",
   "        raise AnsibleError(\"Could not find the shell plugin required (%s).\" % shell_type)\n",
   "        setattr(shell, 'executable', executable)\n",
   "    return shell\n",
   "    loader = getattr(sys.modules[__name__], '%s_loader' % which_loader)\n",
   "    for path in paths:\n",
   "        loader.add_directory(path, with_subdir=True)\n",
   "        self.path = path\n",
   "        warning_text = deprecation.get('warning_text', None)\n",
   "        removal_date = deprecation.get('removal_date', None)\n",
   "        removal_version = deprecation.get('removal_version', None)\n",
   "        if removal_date is not None:\n",
   "            removal_version = None\n",
   "        if not warning_text:\n",
   "            warning_text = '{0} has been deprecated'.format(name)\n",
   "        display.deprecated(warning_text, date=removal_date, version=removal_version, collection_name=collection_name)\n",
   "        if removal_date:\n",
   "            self.removal_date = removal_date\n",
   "        if removal_version:\n",
   "            self.removal_version = removal_version\n",
   "        self.deprecation_warnings.append(warning_text)\n",
   "        aliases = {} if aliases is None else aliases\n",
   "        self.aliases = aliases\n",
   "            config = [config]\n",
   "        elif not config:\n",
   "            config = []\n",
   "        self.config = config\n",
   "        class_name = data.get('class_name')\n",
   "        package = data.get('package')\n",
   "        config = data.get('config')\n",
   "        subdir = data.get('subdir')\n",
   "        aliases = data.get('aliases')\n",
   "        base_class = data.get('base_class')\n",
   "        PATH_CACHE[class_name] = data.get('PATH_CACHE')\n",
   "        PLUGIN_PATH_CACHE[class_name] = data.get('PLUGIN_PATH_CACHE')\n",
   "        self.__init__(class_name, package, config, subdir, aliases, base_class)\n",
   "        ret = []\n",
   "        for i in paths:\n",
   "            if i not in ret:\n",
   "                ret.append(i)\n",
   "        return os.pathsep.join(ret)\n",
   "        results = []\n",
   "        results.append(dir)\n",
   "        for root, subdirs, files in os.walk(dir, followlinks=True):\n",
   "            if '__init__.py' in files:\n",
   "                for x in subdirs:\n",
   "                    results.append(os.path.join(root, x))\n",
   "        return results\n",
   "            m = __import__(self.package)\n",
   "            parts = self.package.split('.')[1:]\n",
   "            for parent_mod in parts:\n",
   "                m = getattr(m, parent_mod)\n",
   "            self.package_path = os.path.dirname(m.__file__)\n",
   "        if subdirs:\n",
   "        ret = [PluginPathContext(p, False) for p in self._extra_dirs]\n",
   "            for path in self.config:\n",
   "                path = os.path.realpath(os.path.expanduser(path))\n",
   "                if subdirs:\n",
   "                    contents = glob.glob(\"%s/*\" % path) + glob.glob(\"%s/*/*\" % path)\n",
   "                    for c in contents:\n",
   "                        if os.path.isdir(c) and c not in ret:\n",
   "                            ret.append(PluginPathContext(c, False))\n",
   "                if path not in ret:\n",
   "                    ret.append(PluginPathContext(path, False))\n",
   "        ret.extend([PluginPathContext(p, True) for p in self._get_package_paths(subdirs=subdirs)])\n",
   "        ret.sort(key=lambda p: p.path.endswith('/windows'))\n",
   "        self._paths = ret\n",
   "        return ret\n",
   "        paths_with_context = self._get_paths_with_context(subdirs=subdirs)\n",
   "        return [path_with_context.path for path_with_context in paths_with_context]\n",
   "            type_name = get_plugin_class(self.class_name)\n",
   "            if type_name in C.CONFIGURABLE_PLUGINS:\n",
   "                dstring = AnsibleLoader(getattr(module, 'DOCUMENTATION', ''), file_name=path).get_single_data()\n",
   "                if dstring:\n",
   "                    add_fragments(dstring, path, fragment_loader=fragment_loader, is_module=(type_name == 'module'))\n",
   "                if dstring and 'options' in dstring and isinstance(dstring['options'], dict):\n",
   "                    C.config.initialize_plugin_configuration_definitions(type_name, name, dstring['options'])\n",
   "                    display.debug('Loaded config def from plugin (%s/%s)' % (type_name, name))\n",
   "        directory = os.path.realpath(directory)\n",
   "        if directory is not None:\n",
   "                directory = os.path.join(directory, self.subdir)\n",
   "            if directory not in self._extra_dirs:\n",
   "                self._extra_dirs.append(directory)\n",
   "                display.debug('Added %s to loader search path' % (directory))\n",
   "        collection_pkg = import_module(acr.n_python_collection_package_name)\n",
   "        if not collection_pkg:\n",
   "        collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "        if not collection_meta:\n",
   "            subdir_qualified_resource = '.'.join([acr.subdirs, acr.resource])\n",
   "            subdir_qualified_resource = acr.resource\n",
   "        entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource + extension, None)\n",
   "        if not entry:\n",
   "            entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource, None)\n",
   "        return entry\n",
   "        plugin_type = AnsibleCollectionRef.legacy_plugin_dir_to_plugin_type(self.subdir)\n",
   "        acr = AnsibleCollectionRef.from_fqcr(fq_name, plugin_type)\n",
   "        routing_metadata = self._query_collection_routing_meta(acr, plugin_type, extension=extension)\n",
   "        if routing_metadata:\n",
   "            deprecation = routing_metadata.get('deprecation', None)\n",
   "            plugin_load_context.record_deprecation(fq_name, deprecation, acr.collection)\n",
   "            tombstone = routing_metadata.get('tombstone', None)\n",
   "            if tombstone:\n",
   "                removal_date = tombstone.get('removal_date')\n",
   "                removal_version = tombstone.get('removal_version')\n",
   "                warning_text = tombstone.get('warning_text') or '{0} has been removed.'.format(fq_name)\n",
   "                removed_msg = display.get_deprecation_message(msg=warning_text, version=removal_version,\n",
   "                                                              date=removal_date, removed=True,\n",
   "                                                              collection_name=acr.collection)\n",
   "                plugin_load_context.removal_date = removal_date\n",
   "                plugin_load_context.removal_version = removal_version\n",
   "                plugin_load_context.exit_reason = removed_msg\n",
   "                raise AnsiblePluginRemovedError(removed_msg, plugin_load_context=plugin_load_context)\n",
   "            redirect = routing_metadata.get('redirect', None)\n",
   "            if redirect:\n",
   "                display.vv(\"redirecting (type: {0}) {1} to {2}\".format(plugin_type, fq_name, redirect))\n",
   "                return plugin_load_context.redirect(redirect)\n",
   "        n_resource = to_native(acr.resource, errors='strict')\n",
   "        full_name = '{0}.{1}'.format(acr.n_python_package_name, n_resource)\n",
   "            n_resource += extension\n",
   "        pkg = sys.modules.get(acr.n_python_package_name)\n",
   "        if not pkg:\n",
   "                pkg = import_module(acr.n_python_package_name)\n",
   "                return plugin_load_context.nope('Python package {0} not found'.format(acr.n_python_package_name))\n",
   "        pkg_path = os.path.dirname(pkg.__file__)\n",
   "        n_resource_path = os.path.join(pkg_path, n_resource)\n",
   "        if os.path.exists(n_resource_path):\n",
   "                full_name, to_text(n_resource_path), acr.collection, 'found exact match for {0} in {1}'.format(full_name, acr.collection))\n",
   "            return plugin_load_context.nope('no match for {0} in {1}'.format(to_text(n_resource), acr.collection))\n",
   "        found_files = [f\n",
   "                       for f in glob.iglob(os.path.join(pkg_path, n_resource) + '.*')\n",
   "                       if os.path.isfile(f) and not f.endswith(C.MODULE_IGNORE_EXTS)]\n",
   "        if not found_files:\n",
   "            return plugin_load_context.nope('failed fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        if len(found_files) > 1:\n",
   "            full_name, to_text(found_files[0]), acr.collection, 'found fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        result = self.find_plugin_with_context(name, mod_type, ignore_deprecated, check_aliases, collection_list)\n",
   "        if result.resolved and result.plugin_resolved_path:\n",
   "            return result.plugin_resolved_path\n",
   "        plugin_load_context = PluginLoadContext()\n",
   "        plugin_load_context.original_name = name\n",
   "            result = self._resolve_plugin_step(name, mod_type, ignore_deprecated, check_aliases, collection_list, plugin_load_context=plugin_load_context)\n",
   "            if result.pending_redirect:\n",
   "                if result.pending_redirect in result.redirect_list:\n",
   "                    raise AnsiblePluginCircularRedirect('plugin redirect loop resolving {0} (path: {1})'.format(result.original_name, result.redirect_list))\n",
   "                name = result.pending_redirect\n",
   "                result.pending_redirect = None\n",
   "                plugin_load_context = result\n",
   "        if plugin_load_context.error_list:\n",
   "            display.warning(\"errors were encountered during the plugin load for {0}:\\n{1}\".format(name, plugin_load_context.error_list))\n",
   "        return plugin_load_context\n",
   "        if not plugin_load_context:\n",
   "        plugin_load_context.redirect_list.append(name)\n",
   "        plugin_load_context.resolved = False\n",
   "        if name in _PLUGIN_FILTERS[self.package]:\n",
   "            plugin_load_context.exit_reason = '{0} matched a defined plugin filter'.format(name)\n",
   "            return plugin_load_context\n",
   "            suffix = mod_type\n",
   "            suffix = '.py'\n",
   "            suffix = ''\n",
   "        if (AnsibleCollectionRef.is_valid_fqcr(name) or collection_list) and not name.startswith('Ansible'):\n",
   "            if '.' in name or not collection_list:\n",
   "                candidates = [name]\n",
   "                candidates = ['{0}.{1}'.format(c, name) for c in collection_list]\n",
   "            for candidate_name in candidates:\n",
   "                    plugin_load_context.load_attempts.append(candidate_name)\n",
   "                    if candidate_name.startswith('ansible.legacy'):\n",
   "                        plugin_load_context = self._find_plugin_legacy(name.replace('ansible.legacy.', '', 1),\n",
   "                                                                       plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "                        plugin_load_context = self._find_fq_plugin(candidate_name, suffix, plugin_load_context=plugin_load_context)\n",
   "                        if candidate_name != plugin_load_context.original_name and candidate_name not in plugin_load_context.redirect_list:\n",
   "                            plugin_load_context.redirect_list.append(candidate_name)\n",
   "                    if plugin_load_context.resolved or plugin_load_context.pending_redirect:  # if we got an answer or need to chase down a redirect, return\n",
   "                        return plugin_load_context\n",
   "                    plugin_load_context.import_error_list.append(ie)\n",
   "                    plugin_load_context.error_list.append(to_native(ex))\n",
   "            if plugin_load_context.error_list:\n",
   "                display.debug(msg='plugin lookup for {0} failed; errors: {1}'.format(name, '; '.join(plugin_load_context.error_list)))\n",
   "            plugin_load_context.exit_reason = 'no matches found for {0}'.format(name)\n",
   "            return plugin_load_context\n",
   "        return self._find_plugin_legacy(name, plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "        plugin_load_context.resolved = False\n",
   "            name = self.aliases.get(name, name)\n",
   "        pull_cache = self._plugin_path_cache[suffix]\n",
   "            path_with_context = pull_cache[name]\n",
   "            plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "            plugin_load_context.plugin_resolved_name = name\n",
   "            plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "            plugin_load_context.resolved = True\n",
   "            return plugin_load_context\n",
   "        for path_context in (p for p in self._get_paths_with_context() if p.path not in self._searched_paths and os.path.isdir(p.path)):\n",
   "            path = path_context.path\n",
   "            display.debug('trying %s' % path)\n",
   "            plugin_load_context.load_attempts.append(path)\n",
   "                full_paths = (os.path.join(path, f) for f in os.listdir(path))\n",
   "                display.warning(\"Error accessing plugin paths: %s\" % to_text(e))\n",
   "            for full_path in (f for f in full_paths if os.path.isfile(f) and not f.endswith('__init__.py')):\n",
   "                full_name = os.path.basename(full_path)\n",
   "                if any(full_path.endswith(x) for x in C.MODULE_IGNORE_EXTS):\n",
   "                splitname = os.path.splitext(full_name)\n",
   "                base_name = splitname[0]\n",
   "                internal = path_context.internal\n",
   "                    extension = splitname[1]\n",
   "                    extension = ''\n",
   "                if base_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][full_name] = PluginPathContext(full_path, internal)\n",
   "                if base_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][full_name] = PluginPathContext(full_path, internal)\n",
   "            self._searched_paths.add(path)\n",
   "                path_with_context = pull_cache[name]\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        if not name.startswith('_'):\n",
   "            alias_name = '_' + name\n",
   "            if alias_name in pull_cache:\n",
   "                path_with_context = pull_cache[alias_name]\n",
   "                if not ignore_deprecated and not os.path.islink(path_with_context.path):\n",
   "                    display.deprecated('%s is kept for backwards compatibility but usage is discouraged. '  # pylint: disable=ansible-deprecated-no-version\n",
   "                                       'The module documentation details page may explain more about this rationale.' % name.lstrip('_'))\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = alias_name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        candidate_fqcr = 'ansible.builtin.{0}'.format(name)\n",
   "        if '.' not in name and AnsibleCollectionRef.is_valid_fqcr(candidate_fqcr):\n",
   "            return self._find_fq_plugin(fq_name=candidate_fqcr, extension=suffix, plugin_load_context=plugin_load_context)\n",
   "        return plugin_load_context.nope('{0} is not eligible for last-chance resolution'.format(name))\n",
   "            return self.find_plugin(name, collection_list=collection_list) is not None\n",
   "            display.debug('has_plugin error: {0}'.format(to_text(ex)))\n",
   "        if name.startswith('ansible_collections.'):\n",
   "            full_name = name\n",
   "            full_name = '.'.join([self.package, name])\n",
   "        if full_name in sys.modules:\n",
   "            return sys.modules[full_name]\n",
   "            if imp is None:\n",
   "                spec = importlib.util.spec_from_file_location(to_native(full_name), to_native(path))\n",
   "                module = importlib.util.module_from_spec(spec)\n",
   "                spec.loader.exec_module(module)\n",
   "                sys.modules[full_name] = module\n",
   "                with open(to_bytes(path), 'rb') as module_file:\n",
   "                    module = imp.load_source(to_native(full_name), to_native(path), module_file)\n",
   "        return module\n",
   "        setattr(obj, '_original_path', path)\n",
   "        setattr(obj, '_load_name', name)\n",
   "        setattr(obj, '_redirected_names', redirected_names or [])\n",
   "        return self.get_with_context(name, *args, **kwargs).object\n",
   "        found_in_cache = True\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        collection_list = kwargs.pop('collection_list', None)\n",
   "        if name in self.aliases:\n",
   "            name = self.aliases[name]\n",
   "        plugin_load_context = self.find_plugin_with_context(name, collection_list=collection_list)\n",
   "        if not plugin_load_context.resolved or not plugin_load_context.plugin_resolved_path:\n",
   "            return get_with_context_result(None, plugin_load_context)\n",
   "        name = plugin_load_context.plugin_resolved_name\n",
   "        path = plugin_load_context.plugin_resolved_path\n",
   "        redirected_names = plugin_load_context.redirect_list or []\n",
   "        if path not in self._module_cache:\n",
   "            self._module_cache[path] = self._load_module_source(name, path)\n",
   "            self._load_config_defs(name, self._module_cache[path], path)\n",
   "            found_in_cache = False\n",
   "        obj = getattr(self._module_cache[path], self.class_name)\n",
   "            module = __import__(self.package, fromlist=[self.base_class])\n",
   "                plugin_class = getattr(module, self.base_class)\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "            if not issubclass(obj, plugin_class):\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "        self._display_plugin_load(self.class_name, name, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "        if not class_only:\n",
   "                instance = object.__new__(obj)\n",
   "                self._update_object(instance, name, path, redirected_names)\n",
   "                obj.__init__(instance, *args, **kwargs)\n",
   "                obj = instance\n",
   "                    return get_with_context_result(None, plugin_load_context)\n",
   "        self._update_object(obj, name, path, redirected_names)\n",
   "        return get_with_context_result(obj, plugin_load_context)\n",
   "            msg = 'Loading %s \\'%s\\' from %s' % (class_name, os.path.basename(name), path)\n",
   "                msg = '%s (searched paths: %s)' % (msg, self.format_paths(searched_paths))\n",
   "            if found_in_cache or class_only:\n",
   "                msg = '%s (found_in_cache=%s, class_only=%s)' % (msg, found_in_cache, class_only)\n",
   "            display.debug(msg)\n",
   "        dedupe = kwargs.pop('_dedupe', True)\n",
   "        path_only = kwargs.pop('path_only', False)\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        if path_only and class_only:\n",
   "        all_matches = []\n",
   "        found_in_cache = True\n",
   "        for i in self._get_paths():\n",
   "            all_matches.extend(glob.glob(os.path.join(i, \"*.py\")))\n",
   "        loaded_modules = set()\n",
   "        for path in sorted(all_matches, key=os.path.basename):\n",
   "            name = os.path.splitext(path)[0]\n",
   "            basename = os.path.basename(name)\n",
   "            if basename == '__init__' or basename in _PLUGIN_FILTERS[self.package]:\n",
   "            if dedupe and basename in loaded_modules:\n",
   "            loaded_modules.add(basename)\n",
   "            if path_only:\n",
   "                yield path\n",
   "            if path not in self._module_cache:\n",
   "                        full_name = '{0}_{1}'.format(abs(hash(path)), basename)\n",
   "                        full_name = basename\n",
   "                    module = self._load_module_source(full_name, path)\n",
   "                    self._load_config_defs(basename, module, path)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                self._module_cache[path] = module\n",
   "                found_in_cache = False\n",
   "                obj = getattr(self._module_cache[path], self.class_name)\n",
   "                display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                module = __import__(self.package, fromlist=[self.base_class])\n",
   "                    plugin_class = getattr(module, self.base_class)\n",
   "                if not issubclass(obj, plugin_class):\n",
   "            self._display_plugin_load(self.class_name, basename, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "            if not class_only:\n",
   "                    obj = obj(*args, **kwargs)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be incomplete: %s\" % (path, to_text(e)))\n",
   "            self._update_object(obj, basename, path)\n",
   "            yield obj\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).find_plugin(name, collection_list=collection_list)\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).get(name, *args, **kwargs)\n",
   "        plugins = list(super(Jinja2Loader, self).all(*args, **kwargs))\n",
   "        plugins.reverse()\n",
   "        return plugins\n",
   "    filters = defaultdict(frozenset)\n",
   "        filter_cfg = '/etc/ansible/plugin_filters.yml'\n",
   "        filter_cfg = C.PLUGIN_FILTERS_CFG\n",
   "    if os.path.exists(filter_cfg):\n",
   "        with open(filter_cfg, 'rb') as f:\n",
   "                filter_data = from_yaml(f.read())\n",
   "                display.warning(u'The plugin filter file, {0} was not parsable.'\n",
   "                                u' Skipping: {1}'.format(filter_cfg, to_text(e)))\n",
   "                return filters\n",
   "            version = filter_data['filter_version']\n",
   "            display.warning(u'The plugin filter file, {0} was invalid.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "            return filters\n",
   "        version = to_text(version)\n",
   "        version = version.strip()\n",
   "        if version == u'1.0':\n",
   "                filters['ansible.modules'] = frozenset(filter_data['module_blacklist'])\n",
   "                display.warning(u'Unable to parse the plugin filter file {0} as'\n",
   "                                u' Skipping.'.format(filter_cfg))\n",
   "                return filters\n",
   "            filters['ansible.plugins.action'] = filters['ansible.modules']\n",
   "            display.warning(u'The plugin filter file, {0} was a version not recognized by this'\n",
   "                            u' version of Ansible. Skipping.'.format(filter_cfg))\n",
   "            display.warning(u'The plugin filter file, {0} does not exist.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "    if 'stat' in filters['ansible.modules']:\n",
   "                           ' from the blacklist.'.format(to_native(filter_cfg)))\n",
   "    return filters\n",
   "    display.vvvv(to_text('Loading collection {0} from {1}'.format(collection_name, collection_path)))\n",
   "        if not _does_collection_support_ansible_version(collection_meta.get('requires_ansible', ''), ansible_version):\n",
   "                display.warning(message)\n",
   "        display.warning('Error parsing collection metadata requires_ansible value from collection {0}: {1}'.format(collection_name, ex))\n",
   "        display.warning('packaging Python module unavailable; unable to validate collection Ansible version requirements')\n",
   "        display.warning('AnsibleCollectionFinder has already been configured')\n"
  ]
 },
 "108": {
  "name": "dstring",
  "type": "ansible.parsing.yaml.objects.AnsibleMapping",
  "class": "imported",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/loader.py",
  "lineno": "385",
  "column": "16",
  "context": "e_name in C.CONFIGURABLE_PLUGINS:\n                dstring = AnsibleLoader(getattr(module, 'DOCUMENTATION', ''), file_name=path).get_single_data()\n                if dstring:\n                    ad",
  "context_lines": "        if self.class_name:\n            type_name = get_plugin_class(self.class_name)\n\n            # if type name != 'module_doc_fragment':\n            if type_name in C.CONFIGURABLE_PLUGINS:\n                dstring = AnsibleLoader(getattr(module, 'DOCUMENTATION', ''), file_name=path).get_single_data()\n                if dstring:\n                    add_fragments(dstring, path, fragment_loader=fragment_loader, is_module=(type_name == 'module'))\n\n                if dstring and 'options' in dstring and isinstance(dstring['options'], dict):\n                    C.config.initialize_plugin_configuration_definitions(type_name, name, dstring['options'])\n",
  "slicing": [
   "    imp = None\n",
   "display = Display()\n",
   "get_with_context_result = namedtuple('get_with_context_result', ['object', 'plugin_load_context'])\n",
   "    return [(name, obj) for (name, obj) in globals().items() if isinstance(obj, PluginLoader)]\n",
   "    b_path = os.path.expanduser(to_bytes(path, errors='surrogate_or_strict'))\n",
   "    if os.path.isdir(b_path):\n",
   "        for name, obj in get_all_plugin_loaders():\n",
   "            if obj.subdir:\n",
   "                plugin_path = os.path.join(b_path, to_bytes(obj.subdir))\n",
   "                if os.path.isdir(plugin_path):\n",
   "                    obj.add_directory(to_text(plugin_path))\n",
   "        display.warning(\"Ignoring invalid path provided to plugin path: '%s' is not a directory\" % to_text(path))\n",
   "        shell_type = 'sh'\n",
   "                shell_filename = os.path.basename(executable)\n",
   "                    shell = shell_loader.get(shell_filename)\n",
   "                    shell = None\n",
   "                if shell is None:\n",
   "                    for shell in shell_loader.all():\n",
   "                        if shell_filename in shell.COMPATIBLE_SHELLS:\n",
   "                            shell_type = shell.SHELL_FAMILY\n",
   "    shell = shell_loader.get(shell_type)\n",
   "    if not shell:\n",
   "        raise AnsibleError(\"Could not find the shell plugin required (%s).\" % shell_type)\n",
   "        setattr(shell, 'executable', executable)\n",
   "    return shell\n",
   "    loader = getattr(sys.modules[__name__], '%s_loader' % which_loader)\n",
   "    for path in paths:\n",
   "        loader.add_directory(path, with_subdir=True)\n",
   "        self.path = path\n",
   "        warning_text = deprecation.get('warning_text', None)\n",
   "        removal_date = deprecation.get('removal_date', None)\n",
   "        removal_version = deprecation.get('removal_version', None)\n",
   "        if removal_date is not None:\n",
   "            removal_version = None\n",
   "        if not warning_text:\n",
   "            warning_text = '{0} has been deprecated'.format(name)\n",
   "        display.deprecated(warning_text, date=removal_date, version=removal_version, collection_name=collection_name)\n",
   "        if removal_date:\n",
   "            self.removal_date = removal_date\n",
   "        if removal_version:\n",
   "            self.removal_version = removal_version\n",
   "        self.deprecation_warnings.append(warning_text)\n",
   "        aliases = {} if aliases is None else aliases\n",
   "        self.aliases = aliases\n",
   "            config = [config]\n",
   "        elif not config:\n",
   "            config = []\n",
   "        self.config = config\n",
   "        class_name = data.get('class_name')\n",
   "        package = data.get('package')\n",
   "        config = data.get('config')\n",
   "        subdir = data.get('subdir')\n",
   "        aliases = data.get('aliases')\n",
   "        base_class = data.get('base_class')\n",
   "        PATH_CACHE[class_name] = data.get('PATH_CACHE')\n",
   "        PLUGIN_PATH_CACHE[class_name] = data.get('PLUGIN_PATH_CACHE')\n",
   "        self.__init__(class_name, package, config, subdir, aliases, base_class)\n",
   "        ret = []\n",
   "        for i in paths:\n",
   "            if i not in ret:\n",
   "                ret.append(i)\n",
   "        return os.pathsep.join(ret)\n",
   "        results = []\n",
   "        results.append(dir)\n",
   "        for root, subdirs, files in os.walk(dir, followlinks=True):\n",
   "            if '__init__.py' in files:\n",
   "                for x in subdirs:\n",
   "                    results.append(os.path.join(root, x))\n",
   "        return results\n",
   "            m = __import__(self.package)\n",
   "            parts = self.package.split('.')[1:]\n",
   "            for parent_mod in parts:\n",
   "                m = getattr(m, parent_mod)\n",
   "            self.package_path = os.path.dirname(m.__file__)\n",
   "        if subdirs:\n",
   "        ret = [PluginPathContext(p, False) for p in self._extra_dirs]\n",
   "            for path in self.config:\n",
   "                path = os.path.realpath(os.path.expanduser(path))\n",
   "                if subdirs:\n",
   "                    contents = glob.glob(\"%s/*\" % path) + glob.glob(\"%s/*/*\" % path)\n",
   "                    for c in contents:\n",
   "                        if os.path.isdir(c) and c not in ret:\n",
   "                            ret.append(PluginPathContext(c, False))\n",
   "                if path not in ret:\n",
   "                    ret.append(PluginPathContext(path, False))\n",
   "        ret.extend([PluginPathContext(p, True) for p in self._get_package_paths(subdirs=subdirs)])\n",
   "        ret.sort(key=lambda p: p.path.endswith('/windows'))\n",
   "        self._paths = ret\n",
   "        return ret\n",
   "        paths_with_context = self._get_paths_with_context(subdirs=subdirs)\n",
   "        return [path_with_context.path for path_with_context in paths_with_context]\n",
   "            type_name = get_plugin_class(self.class_name)\n",
   "            if type_name in C.CONFIGURABLE_PLUGINS:\n",
   "                dstring = AnsibleLoader(getattr(module, 'DOCUMENTATION', ''), file_name=path).get_single_data()\n",
   "                if dstring:\n",
   "                    add_fragments(dstring, path, fragment_loader=fragment_loader, is_module=(type_name == 'module'))\n",
   "                if dstring and 'options' in dstring and isinstance(dstring['options'], dict):\n",
   "                    C.config.initialize_plugin_configuration_definitions(type_name, name, dstring['options'])\n",
   "                    display.debug('Loaded config def from plugin (%s/%s)' % (type_name, name))\n",
   "        directory = os.path.realpath(directory)\n",
   "        if directory is not None:\n",
   "                directory = os.path.join(directory, self.subdir)\n",
   "            if directory not in self._extra_dirs:\n",
   "                self._extra_dirs.append(directory)\n",
   "                display.debug('Added %s to loader search path' % (directory))\n",
   "        collection_pkg = import_module(acr.n_python_collection_package_name)\n",
   "        if not collection_pkg:\n",
   "        collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "        if not collection_meta:\n",
   "            subdir_qualified_resource = '.'.join([acr.subdirs, acr.resource])\n",
   "            subdir_qualified_resource = acr.resource\n",
   "        entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource + extension, None)\n",
   "        if not entry:\n",
   "            entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource, None)\n",
   "        return entry\n",
   "        plugin_type = AnsibleCollectionRef.legacy_plugin_dir_to_plugin_type(self.subdir)\n",
   "        acr = AnsibleCollectionRef.from_fqcr(fq_name, plugin_type)\n",
   "        routing_metadata = self._query_collection_routing_meta(acr, plugin_type, extension=extension)\n",
   "        if routing_metadata:\n",
   "            deprecation = routing_metadata.get('deprecation', None)\n",
   "            plugin_load_context.record_deprecation(fq_name, deprecation, acr.collection)\n",
   "            tombstone = routing_metadata.get('tombstone', None)\n",
   "            if tombstone:\n",
   "                removal_date = tombstone.get('removal_date')\n",
   "                removal_version = tombstone.get('removal_version')\n",
   "                warning_text = tombstone.get('warning_text') or '{0} has been removed.'.format(fq_name)\n",
   "                removed_msg = display.get_deprecation_message(msg=warning_text, version=removal_version,\n",
   "                                                              date=removal_date, removed=True,\n",
   "                                                              collection_name=acr.collection)\n",
   "                plugin_load_context.removal_date = removal_date\n",
   "                plugin_load_context.removal_version = removal_version\n",
   "                plugin_load_context.exit_reason = removed_msg\n",
   "                raise AnsiblePluginRemovedError(removed_msg, plugin_load_context=plugin_load_context)\n",
   "            redirect = routing_metadata.get('redirect', None)\n",
   "            if redirect:\n",
   "                display.vv(\"redirecting (type: {0}) {1} to {2}\".format(plugin_type, fq_name, redirect))\n",
   "                return plugin_load_context.redirect(redirect)\n",
   "        n_resource = to_native(acr.resource, errors='strict')\n",
   "        full_name = '{0}.{1}'.format(acr.n_python_package_name, n_resource)\n",
   "            n_resource += extension\n",
   "        pkg = sys.modules.get(acr.n_python_package_name)\n",
   "        if not pkg:\n",
   "                pkg = import_module(acr.n_python_package_name)\n",
   "                return plugin_load_context.nope('Python package {0} not found'.format(acr.n_python_package_name))\n",
   "        pkg_path = os.path.dirname(pkg.__file__)\n",
   "        n_resource_path = os.path.join(pkg_path, n_resource)\n",
   "        if os.path.exists(n_resource_path):\n",
   "                full_name, to_text(n_resource_path), acr.collection, 'found exact match for {0} in {1}'.format(full_name, acr.collection))\n",
   "            return plugin_load_context.nope('no match for {0} in {1}'.format(to_text(n_resource), acr.collection))\n",
   "        found_files = [f\n",
   "                       for f in glob.iglob(os.path.join(pkg_path, n_resource) + '.*')\n",
   "                       if os.path.isfile(f) and not f.endswith(C.MODULE_IGNORE_EXTS)]\n",
   "        if not found_files:\n",
   "            return plugin_load_context.nope('failed fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        if len(found_files) > 1:\n",
   "            full_name, to_text(found_files[0]), acr.collection, 'found fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        result = self.find_plugin_with_context(name, mod_type, ignore_deprecated, check_aliases, collection_list)\n",
   "        if result.resolved and result.plugin_resolved_path:\n",
   "            return result.plugin_resolved_path\n",
   "        plugin_load_context = PluginLoadContext()\n",
   "        plugin_load_context.original_name = name\n",
   "            result = self._resolve_plugin_step(name, mod_type, ignore_deprecated, check_aliases, collection_list, plugin_load_context=plugin_load_context)\n",
   "            if result.pending_redirect:\n",
   "                if result.pending_redirect in result.redirect_list:\n",
   "                    raise AnsiblePluginCircularRedirect('plugin redirect loop resolving {0} (path: {1})'.format(result.original_name, result.redirect_list))\n",
   "                name = result.pending_redirect\n",
   "                result.pending_redirect = None\n",
   "                plugin_load_context = result\n",
   "        if plugin_load_context.error_list:\n",
   "            display.warning(\"errors were encountered during the plugin load for {0}:\\n{1}\".format(name, plugin_load_context.error_list))\n",
   "        return plugin_load_context\n",
   "        if not plugin_load_context:\n",
   "        plugin_load_context.redirect_list.append(name)\n",
   "        plugin_load_context.resolved = False\n",
   "        if name in _PLUGIN_FILTERS[self.package]:\n",
   "            plugin_load_context.exit_reason = '{0} matched a defined plugin filter'.format(name)\n",
   "            return plugin_load_context\n",
   "            suffix = mod_type\n",
   "            suffix = '.py'\n",
   "            suffix = ''\n",
   "        if (AnsibleCollectionRef.is_valid_fqcr(name) or collection_list) and not name.startswith('Ansible'):\n",
   "            if '.' in name or not collection_list:\n",
   "                candidates = [name]\n",
   "                candidates = ['{0}.{1}'.format(c, name) for c in collection_list]\n",
   "            for candidate_name in candidates:\n",
   "                    plugin_load_context.load_attempts.append(candidate_name)\n",
   "                    if candidate_name.startswith('ansible.legacy'):\n",
   "                        plugin_load_context = self._find_plugin_legacy(name.replace('ansible.legacy.', '', 1),\n",
   "                                                                       plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "                        plugin_load_context = self._find_fq_plugin(candidate_name, suffix, plugin_load_context=plugin_load_context)\n",
   "                        if candidate_name != plugin_load_context.original_name and candidate_name not in plugin_load_context.redirect_list:\n",
   "                            plugin_load_context.redirect_list.append(candidate_name)\n",
   "                    if plugin_load_context.resolved or plugin_load_context.pending_redirect:  # if we got an answer or need to chase down a redirect, return\n",
   "                        return plugin_load_context\n",
   "                    plugin_load_context.import_error_list.append(ie)\n",
   "                    plugin_load_context.error_list.append(to_native(ex))\n",
   "            if plugin_load_context.error_list:\n",
   "                display.debug(msg='plugin lookup for {0} failed; errors: {1}'.format(name, '; '.join(plugin_load_context.error_list)))\n",
   "            plugin_load_context.exit_reason = 'no matches found for {0}'.format(name)\n",
   "            return plugin_load_context\n",
   "        return self._find_plugin_legacy(name, plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "        plugin_load_context.resolved = False\n",
   "            name = self.aliases.get(name, name)\n",
   "        pull_cache = self._plugin_path_cache[suffix]\n",
   "            path_with_context = pull_cache[name]\n",
   "            plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "            plugin_load_context.plugin_resolved_name = name\n",
   "            plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "            plugin_load_context.resolved = True\n",
   "            return plugin_load_context\n",
   "        for path_context in (p for p in self._get_paths_with_context() if p.path not in self._searched_paths and os.path.isdir(p.path)):\n",
   "            path = path_context.path\n",
   "            display.debug('trying %s' % path)\n",
   "            plugin_load_context.load_attempts.append(path)\n",
   "                full_paths = (os.path.join(path, f) for f in os.listdir(path))\n",
   "                display.warning(\"Error accessing plugin paths: %s\" % to_text(e))\n",
   "            for full_path in (f for f in full_paths if os.path.isfile(f) and not f.endswith('__init__.py')):\n",
   "                full_name = os.path.basename(full_path)\n",
   "                if any(full_path.endswith(x) for x in C.MODULE_IGNORE_EXTS):\n",
   "                splitname = os.path.splitext(full_name)\n",
   "                base_name = splitname[0]\n",
   "                internal = path_context.internal\n",
   "                    extension = splitname[1]\n",
   "                    extension = ''\n",
   "                if base_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][full_name] = PluginPathContext(full_path, internal)\n",
   "                if base_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][full_name] = PluginPathContext(full_path, internal)\n",
   "            self._searched_paths.add(path)\n",
   "                path_with_context = pull_cache[name]\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        if not name.startswith('_'):\n",
   "            alias_name = '_' + name\n",
   "            if alias_name in pull_cache:\n",
   "                path_with_context = pull_cache[alias_name]\n",
   "                if not ignore_deprecated and not os.path.islink(path_with_context.path):\n",
   "                    display.deprecated('%s is kept for backwards compatibility but usage is discouraged. '  # pylint: disable=ansible-deprecated-no-version\n",
   "                                       'The module documentation details page may explain more about this rationale.' % name.lstrip('_'))\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = alias_name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        candidate_fqcr = 'ansible.builtin.{0}'.format(name)\n",
   "        if '.' not in name and AnsibleCollectionRef.is_valid_fqcr(candidate_fqcr):\n",
   "            return self._find_fq_plugin(fq_name=candidate_fqcr, extension=suffix, plugin_load_context=plugin_load_context)\n",
   "        return plugin_load_context.nope('{0} is not eligible for last-chance resolution'.format(name))\n",
   "            return self.find_plugin(name, collection_list=collection_list) is not None\n",
   "            display.debug('has_plugin error: {0}'.format(to_text(ex)))\n",
   "        if name.startswith('ansible_collections.'):\n",
   "            full_name = name\n",
   "            full_name = '.'.join([self.package, name])\n",
   "        if full_name in sys.modules:\n",
   "            return sys.modules[full_name]\n",
   "            if imp is None:\n",
   "                spec = importlib.util.spec_from_file_location(to_native(full_name), to_native(path))\n",
   "                module = importlib.util.module_from_spec(spec)\n",
   "                spec.loader.exec_module(module)\n",
   "                sys.modules[full_name] = module\n",
   "                with open(to_bytes(path), 'rb') as module_file:\n",
   "                    module = imp.load_source(to_native(full_name), to_native(path), module_file)\n",
   "        return module\n",
   "        setattr(obj, '_original_path', path)\n",
   "        setattr(obj, '_load_name', name)\n",
   "        setattr(obj, '_redirected_names', redirected_names or [])\n",
   "        return self.get_with_context(name, *args, **kwargs).object\n",
   "        found_in_cache = True\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        collection_list = kwargs.pop('collection_list', None)\n",
   "        if name in self.aliases:\n",
   "            name = self.aliases[name]\n",
   "        plugin_load_context = self.find_plugin_with_context(name, collection_list=collection_list)\n",
   "        if not plugin_load_context.resolved or not plugin_load_context.plugin_resolved_path:\n",
   "            return get_with_context_result(None, plugin_load_context)\n",
   "        name = plugin_load_context.plugin_resolved_name\n",
   "        path = plugin_load_context.plugin_resolved_path\n",
   "        redirected_names = plugin_load_context.redirect_list or []\n",
   "        if path not in self._module_cache:\n",
   "            self._module_cache[path] = self._load_module_source(name, path)\n",
   "            self._load_config_defs(name, self._module_cache[path], path)\n",
   "            found_in_cache = False\n",
   "        obj = getattr(self._module_cache[path], self.class_name)\n",
   "            module = __import__(self.package, fromlist=[self.base_class])\n",
   "                plugin_class = getattr(module, self.base_class)\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "            if not issubclass(obj, plugin_class):\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "        self._display_plugin_load(self.class_name, name, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "        if not class_only:\n",
   "                instance = object.__new__(obj)\n",
   "                self._update_object(instance, name, path, redirected_names)\n",
   "                obj.__init__(instance, *args, **kwargs)\n",
   "                obj = instance\n",
   "                    return get_with_context_result(None, plugin_load_context)\n",
   "        self._update_object(obj, name, path, redirected_names)\n",
   "        return get_with_context_result(obj, plugin_load_context)\n",
   "            msg = 'Loading %s \\'%s\\' from %s' % (class_name, os.path.basename(name), path)\n",
   "                msg = '%s (searched paths: %s)' % (msg, self.format_paths(searched_paths))\n",
   "            if found_in_cache or class_only:\n",
   "                msg = '%s (found_in_cache=%s, class_only=%s)' % (msg, found_in_cache, class_only)\n",
   "            display.debug(msg)\n",
   "        dedupe = kwargs.pop('_dedupe', True)\n",
   "        path_only = kwargs.pop('path_only', False)\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        if path_only and class_only:\n",
   "        all_matches = []\n",
   "        found_in_cache = True\n",
   "        for i in self._get_paths():\n",
   "            all_matches.extend(glob.glob(os.path.join(i, \"*.py\")))\n",
   "        loaded_modules = set()\n",
   "        for path in sorted(all_matches, key=os.path.basename):\n",
   "            name = os.path.splitext(path)[0]\n",
   "            basename = os.path.basename(name)\n",
   "            if basename == '__init__' or basename in _PLUGIN_FILTERS[self.package]:\n",
   "            if dedupe and basename in loaded_modules:\n",
   "            loaded_modules.add(basename)\n",
   "            if path_only:\n",
   "                yield path\n",
   "            if path not in self._module_cache:\n",
   "                        full_name = '{0}_{1}'.format(abs(hash(path)), basename)\n",
   "                        full_name = basename\n",
   "                    module = self._load_module_source(full_name, path)\n",
   "                    self._load_config_defs(basename, module, path)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                self._module_cache[path] = module\n",
   "                found_in_cache = False\n",
   "                obj = getattr(self._module_cache[path], self.class_name)\n",
   "                display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                module = __import__(self.package, fromlist=[self.base_class])\n",
   "                    plugin_class = getattr(module, self.base_class)\n",
   "                if not issubclass(obj, plugin_class):\n",
   "            self._display_plugin_load(self.class_name, basename, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "            if not class_only:\n",
   "                    obj = obj(*args, **kwargs)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be incomplete: %s\" % (path, to_text(e)))\n",
   "            self._update_object(obj, basename, path)\n",
   "            yield obj\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).find_plugin(name, collection_list=collection_list)\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).get(name, *args, **kwargs)\n",
   "        plugins = list(super(Jinja2Loader, self).all(*args, **kwargs))\n",
   "        plugins.reverse()\n",
   "        return plugins\n",
   "    filters = defaultdict(frozenset)\n",
   "        filter_cfg = '/etc/ansible/plugin_filters.yml'\n",
   "        filter_cfg = C.PLUGIN_FILTERS_CFG\n",
   "    if os.path.exists(filter_cfg):\n",
   "        with open(filter_cfg, 'rb') as f:\n",
   "                filter_data = from_yaml(f.read())\n",
   "                display.warning(u'The plugin filter file, {0} was not parsable.'\n",
   "                                u' Skipping: {1}'.format(filter_cfg, to_text(e)))\n",
   "                return filters\n",
   "            version = filter_data['filter_version']\n",
   "            display.warning(u'The plugin filter file, {0} was invalid.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "            return filters\n",
   "        version = to_text(version)\n",
   "        version = version.strip()\n",
   "        if version == u'1.0':\n",
   "                filters['ansible.modules'] = frozenset(filter_data['module_blacklist'])\n",
   "                display.warning(u'Unable to parse the plugin filter file {0} as'\n",
   "                                u' Skipping.'.format(filter_cfg))\n",
   "                return filters\n",
   "            filters['ansible.plugins.action'] = filters['ansible.modules']\n",
   "            display.warning(u'The plugin filter file, {0} was a version not recognized by this'\n",
   "                            u' version of Ansible. Skipping.'.format(filter_cfg))\n",
   "            display.warning(u'The plugin filter file, {0} does not exist.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "    if 'stat' in filters['ansible.modules']:\n",
   "                           ' from the blacklist.'.format(to_native(filter_cfg)))\n",
   "    return filters\n",
   "    display.vvvv(to_text('Loading collection {0} from {1}'.format(collection_name, collection_path)))\n",
   "        if not _does_collection_support_ansible_version(collection_meta.get('requires_ansible', ''), ansible_version):\n",
   "                display.warning(message)\n",
   "        display.warning('Error parsing collection metadata requires_ansible value from collection {0}: {1}'.format(collection_name, ex))\n",
   "        display.warning('packaging Python module unavailable; unable to validate collection Ansible version requirements')\n",
   "        display.warning('AnsibleCollectionFinder has already been configured')\n"
  ]
 },
 "109": {
  "name": "suffix",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/loader.py",
  "lineno": "581",
  "column": "12",
  "context": "the controller process (most plugins)\n            suffix = '.py'\n        else:\n            # Only Ansible Modules. ",
  "context_lines": "        if mod_type:\n            suffix = mod_type\n        elif self.class_name:\n            # Ansible plugins that run in the controller process (most plugins)\n            suffix = '.py'\n        else:\n            # Only Ansible Modules.  Ansible modules can be any executable so\n            # they can have any suffix\n            suffix = ''\n\n",
  "slicing": [
   "    imp = None\n",
   "display = Display()\n",
   "get_with_context_result = namedtuple('get_with_context_result', ['object', 'plugin_load_context'])\n",
   "    return [(name, obj) for (name, obj) in globals().items() if isinstance(obj, PluginLoader)]\n",
   "    b_path = os.path.expanduser(to_bytes(path, errors='surrogate_or_strict'))\n",
   "    if os.path.isdir(b_path):\n",
   "        for name, obj in get_all_plugin_loaders():\n",
   "            if obj.subdir:\n",
   "                plugin_path = os.path.join(b_path, to_bytes(obj.subdir))\n",
   "                if os.path.isdir(plugin_path):\n",
   "                    obj.add_directory(to_text(plugin_path))\n",
   "        display.warning(\"Ignoring invalid path provided to plugin path: '%s' is not a directory\" % to_text(path))\n",
   "        shell_type = 'sh'\n",
   "                shell_filename = os.path.basename(executable)\n",
   "                    shell = shell_loader.get(shell_filename)\n",
   "                    shell = None\n",
   "                if shell is None:\n",
   "                    for shell in shell_loader.all():\n",
   "                        if shell_filename in shell.COMPATIBLE_SHELLS:\n",
   "                            shell_type = shell.SHELL_FAMILY\n",
   "    shell = shell_loader.get(shell_type)\n",
   "    if not shell:\n",
   "        raise AnsibleError(\"Could not find the shell plugin required (%s).\" % shell_type)\n",
   "        setattr(shell, 'executable', executable)\n",
   "    return shell\n",
   "    loader = getattr(sys.modules[__name__], '%s_loader' % which_loader)\n",
   "    for path in paths:\n",
   "        loader.add_directory(path, with_subdir=True)\n",
   "        self.path = path\n",
   "        warning_text = deprecation.get('warning_text', None)\n",
   "        removal_date = deprecation.get('removal_date', None)\n",
   "        removal_version = deprecation.get('removal_version', None)\n",
   "        if removal_date is not None:\n",
   "            removal_version = None\n",
   "        if not warning_text:\n",
   "            warning_text = '{0} has been deprecated'.format(name)\n",
   "        display.deprecated(warning_text, date=removal_date, version=removal_version, collection_name=collection_name)\n",
   "        if removal_date:\n",
   "            self.removal_date = removal_date\n",
   "        if removal_version:\n",
   "            self.removal_version = removal_version\n",
   "        self.deprecation_warnings.append(warning_text)\n",
   "        aliases = {} if aliases is None else aliases\n",
   "        self.aliases = aliases\n",
   "            config = [config]\n",
   "        elif not config:\n",
   "            config = []\n",
   "        self.config = config\n",
   "        class_name = data.get('class_name')\n",
   "        package = data.get('package')\n",
   "        config = data.get('config')\n",
   "        subdir = data.get('subdir')\n",
   "        aliases = data.get('aliases')\n",
   "        base_class = data.get('base_class')\n",
   "        PATH_CACHE[class_name] = data.get('PATH_CACHE')\n",
   "        PLUGIN_PATH_CACHE[class_name] = data.get('PLUGIN_PATH_CACHE')\n",
   "        self.__init__(class_name, package, config, subdir, aliases, base_class)\n",
   "        ret = []\n",
   "        for i in paths:\n",
   "            if i not in ret:\n",
   "                ret.append(i)\n",
   "        return os.pathsep.join(ret)\n",
   "        results = []\n",
   "        results.append(dir)\n",
   "        for root, subdirs, files in os.walk(dir, followlinks=True):\n",
   "            if '__init__.py' in files:\n",
   "                for x in subdirs:\n",
   "                    results.append(os.path.join(root, x))\n",
   "        return results\n",
   "            m = __import__(self.package)\n",
   "            parts = self.package.split('.')[1:]\n",
   "            for parent_mod in parts:\n",
   "                m = getattr(m, parent_mod)\n",
   "            self.package_path = os.path.dirname(m.__file__)\n",
   "        if subdirs:\n",
   "        ret = [PluginPathContext(p, False) for p in self._extra_dirs]\n",
   "            for path in self.config:\n",
   "                path = os.path.realpath(os.path.expanduser(path))\n",
   "                if subdirs:\n",
   "                    contents = glob.glob(\"%s/*\" % path) + glob.glob(\"%s/*/*\" % path)\n",
   "                    for c in contents:\n",
   "                        if os.path.isdir(c) and c not in ret:\n",
   "                            ret.append(PluginPathContext(c, False))\n",
   "                if path not in ret:\n",
   "                    ret.append(PluginPathContext(path, False))\n",
   "        ret.extend([PluginPathContext(p, True) for p in self._get_package_paths(subdirs=subdirs)])\n",
   "        ret.sort(key=lambda p: p.path.endswith('/windows'))\n",
   "        self._paths = ret\n",
   "        return ret\n",
   "        paths_with_context = self._get_paths_with_context(subdirs=subdirs)\n",
   "        return [path_with_context.path for path_with_context in paths_with_context]\n",
   "            type_name = get_plugin_class(self.class_name)\n",
   "            if type_name in C.CONFIGURABLE_PLUGINS:\n",
   "                dstring = AnsibleLoader(getattr(module, 'DOCUMENTATION', ''), file_name=path).get_single_data()\n",
   "                if dstring:\n",
   "                    add_fragments(dstring, path, fragment_loader=fragment_loader, is_module=(type_name == 'module'))\n",
   "                if dstring and 'options' in dstring and isinstance(dstring['options'], dict):\n",
   "                    C.config.initialize_plugin_configuration_definitions(type_name, name, dstring['options'])\n",
   "                    display.debug('Loaded config def from plugin (%s/%s)' % (type_name, name))\n",
   "        directory = os.path.realpath(directory)\n",
   "        if directory is not None:\n",
   "                directory = os.path.join(directory, self.subdir)\n",
   "            if directory not in self._extra_dirs:\n",
   "                self._extra_dirs.append(directory)\n",
   "                display.debug('Added %s to loader search path' % (directory))\n",
   "        collection_pkg = import_module(acr.n_python_collection_package_name)\n",
   "        if not collection_pkg:\n",
   "        collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "        if not collection_meta:\n",
   "            subdir_qualified_resource = '.'.join([acr.subdirs, acr.resource])\n",
   "            subdir_qualified_resource = acr.resource\n",
   "        entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource + extension, None)\n",
   "        if not entry:\n",
   "            entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource, None)\n",
   "        return entry\n",
   "        plugin_type = AnsibleCollectionRef.legacy_plugin_dir_to_plugin_type(self.subdir)\n",
   "        acr = AnsibleCollectionRef.from_fqcr(fq_name, plugin_type)\n",
   "        routing_metadata = self._query_collection_routing_meta(acr, plugin_type, extension=extension)\n",
   "        if routing_metadata:\n",
   "            deprecation = routing_metadata.get('deprecation', None)\n",
   "            plugin_load_context.record_deprecation(fq_name, deprecation, acr.collection)\n",
   "            tombstone = routing_metadata.get('tombstone', None)\n",
   "            if tombstone:\n",
   "                removal_date = tombstone.get('removal_date')\n",
   "                removal_version = tombstone.get('removal_version')\n",
   "                warning_text = tombstone.get('warning_text') or '{0} has been removed.'.format(fq_name)\n",
   "                removed_msg = display.get_deprecation_message(msg=warning_text, version=removal_version,\n",
   "                                                              date=removal_date, removed=True,\n",
   "                                                              collection_name=acr.collection)\n",
   "                plugin_load_context.removal_date = removal_date\n",
   "                plugin_load_context.removal_version = removal_version\n",
   "                plugin_load_context.exit_reason = removed_msg\n",
   "                raise AnsiblePluginRemovedError(removed_msg, plugin_load_context=plugin_load_context)\n",
   "            redirect = routing_metadata.get('redirect', None)\n",
   "            if redirect:\n",
   "                display.vv(\"redirecting (type: {0}) {1} to {2}\".format(plugin_type, fq_name, redirect))\n",
   "                return plugin_load_context.redirect(redirect)\n",
   "        n_resource = to_native(acr.resource, errors='strict')\n",
   "        full_name = '{0}.{1}'.format(acr.n_python_package_name, n_resource)\n",
   "            n_resource += extension\n",
   "        pkg = sys.modules.get(acr.n_python_package_name)\n",
   "        if not pkg:\n",
   "                pkg = import_module(acr.n_python_package_name)\n",
   "                return plugin_load_context.nope('Python package {0} not found'.format(acr.n_python_package_name))\n",
   "        pkg_path = os.path.dirname(pkg.__file__)\n",
   "        n_resource_path = os.path.join(pkg_path, n_resource)\n",
   "        if os.path.exists(n_resource_path):\n",
   "                full_name, to_text(n_resource_path), acr.collection, 'found exact match for {0} in {1}'.format(full_name, acr.collection))\n",
   "            return plugin_load_context.nope('no match for {0} in {1}'.format(to_text(n_resource), acr.collection))\n",
   "        found_files = [f\n",
   "                       for f in glob.iglob(os.path.join(pkg_path, n_resource) + '.*')\n",
   "                       if os.path.isfile(f) and not f.endswith(C.MODULE_IGNORE_EXTS)]\n",
   "        if not found_files:\n",
   "            return plugin_load_context.nope('failed fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        if len(found_files) > 1:\n",
   "            full_name, to_text(found_files[0]), acr.collection, 'found fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        result = self.find_plugin_with_context(name, mod_type, ignore_deprecated, check_aliases, collection_list)\n",
   "        if result.resolved and result.plugin_resolved_path:\n",
   "            return result.plugin_resolved_path\n",
   "        plugin_load_context = PluginLoadContext()\n",
   "        plugin_load_context.original_name = name\n",
   "            result = self._resolve_plugin_step(name, mod_type, ignore_deprecated, check_aliases, collection_list, plugin_load_context=plugin_load_context)\n",
   "            if result.pending_redirect:\n",
   "                if result.pending_redirect in result.redirect_list:\n",
   "                    raise AnsiblePluginCircularRedirect('plugin redirect loop resolving {0} (path: {1})'.format(result.original_name, result.redirect_list))\n",
   "                name = result.pending_redirect\n",
   "                result.pending_redirect = None\n",
   "                plugin_load_context = result\n",
   "        if plugin_load_context.error_list:\n",
   "            display.warning(\"errors were encountered during the plugin load for {0}:\\n{1}\".format(name, plugin_load_context.error_list))\n",
   "        return plugin_load_context\n",
   "        if not plugin_load_context:\n",
   "        plugin_load_context.redirect_list.append(name)\n",
   "        plugin_load_context.resolved = False\n",
   "        if name in _PLUGIN_FILTERS[self.package]:\n",
   "            plugin_load_context.exit_reason = '{0} matched a defined plugin filter'.format(name)\n",
   "            return plugin_load_context\n",
   "            suffix = mod_type\n",
   "            suffix = '.py'\n",
   "            suffix = ''\n",
   "        if (AnsibleCollectionRef.is_valid_fqcr(name) or collection_list) and not name.startswith('Ansible'):\n",
   "            if '.' in name or not collection_list:\n",
   "                candidates = [name]\n",
   "                candidates = ['{0}.{1}'.format(c, name) for c in collection_list]\n",
   "            for candidate_name in candidates:\n",
   "                    plugin_load_context.load_attempts.append(candidate_name)\n",
   "                    if candidate_name.startswith('ansible.legacy'):\n",
   "                        plugin_load_context = self._find_plugin_legacy(name.replace('ansible.legacy.', '', 1),\n",
   "                                                                       plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "                        plugin_load_context = self._find_fq_plugin(candidate_name, suffix, plugin_load_context=plugin_load_context)\n",
   "                        if candidate_name != plugin_load_context.original_name and candidate_name not in plugin_load_context.redirect_list:\n",
   "                            plugin_load_context.redirect_list.append(candidate_name)\n",
   "                    if plugin_load_context.resolved or plugin_load_context.pending_redirect:  # if we got an answer or need to chase down a redirect, return\n",
   "                        return plugin_load_context\n",
   "                    plugin_load_context.import_error_list.append(ie)\n",
   "                    plugin_load_context.error_list.append(to_native(ex))\n",
   "            if plugin_load_context.error_list:\n",
   "                display.debug(msg='plugin lookup for {0} failed; errors: {1}'.format(name, '; '.join(plugin_load_context.error_list)))\n",
   "            plugin_load_context.exit_reason = 'no matches found for {0}'.format(name)\n",
   "            return plugin_load_context\n",
   "        return self._find_plugin_legacy(name, plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "        plugin_load_context.resolved = False\n",
   "            name = self.aliases.get(name, name)\n",
   "        pull_cache = self._plugin_path_cache[suffix]\n",
   "            path_with_context = pull_cache[name]\n",
   "            plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "            plugin_load_context.plugin_resolved_name = name\n",
   "            plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "            plugin_load_context.resolved = True\n",
   "            return plugin_load_context\n",
   "        for path_context in (p for p in self._get_paths_with_context() if p.path not in self._searched_paths and os.path.isdir(p.path)):\n",
   "            path = path_context.path\n",
   "            display.debug('trying %s' % path)\n",
   "            plugin_load_context.load_attempts.append(path)\n",
   "                full_paths = (os.path.join(path, f) for f in os.listdir(path))\n",
   "                display.warning(\"Error accessing plugin paths: %s\" % to_text(e))\n",
   "            for full_path in (f for f in full_paths if os.path.isfile(f) and not f.endswith('__init__.py')):\n",
   "                full_name = os.path.basename(full_path)\n",
   "                if any(full_path.endswith(x) for x in C.MODULE_IGNORE_EXTS):\n",
   "                splitname = os.path.splitext(full_name)\n",
   "                base_name = splitname[0]\n",
   "                internal = path_context.internal\n",
   "                    extension = splitname[1]\n",
   "                    extension = ''\n",
   "                if base_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][full_name] = PluginPathContext(full_path, internal)\n",
   "                if base_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][full_name] = PluginPathContext(full_path, internal)\n",
   "            self._searched_paths.add(path)\n",
   "                path_with_context = pull_cache[name]\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        if not name.startswith('_'):\n",
   "            alias_name = '_' + name\n",
   "            if alias_name in pull_cache:\n",
   "                path_with_context = pull_cache[alias_name]\n",
   "                if not ignore_deprecated and not os.path.islink(path_with_context.path):\n",
   "                    display.deprecated('%s is kept for backwards compatibility but usage is discouraged. '  # pylint: disable=ansible-deprecated-no-version\n",
   "                                       'The module documentation details page may explain more about this rationale.' % name.lstrip('_'))\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = alias_name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        candidate_fqcr = 'ansible.builtin.{0}'.format(name)\n",
   "        if '.' not in name and AnsibleCollectionRef.is_valid_fqcr(candidate_fqcr):\n",
   "            return self._find_fq_plugin(fq_name=candidate_fqcr, extension=suffix, plugin_load_context=plugin_load_context)\n",
   "        return plugin_load_context.nope('{0} is not eligible for last-chance resolution'.format(name))\n",
   "            return self.find_plugin(name, collection_list=collection_list) is not None\n",
   "            display.debug('has_plugin error: {0}'.format(to_text(ex)))\n",
   "        if name.startswith('ansible_collections.'):\n",
   "            full_name = name\n",
   "            full_name = '.'.join([self.package, name])\n",
   "        if full_name in sys.modules:\n",
   "            return sys.modules[full_name]\n",
   "            if imp is None:\n",
   "                spec = importlib.util.spec_from_file_location(to_native(full_name), to_native(path))\n",
   "                module = importlib.util.module_from_spec(spec)\n",
   "                spec.loader.exec_module(module)\n",
   "                sys.modules[full_name] = module\n",
   "                with open(to_bytes(path), 'rb') as module_file:\n",
   "                    module = imp.load_source(to_native(full_name), to_native(path), module_file)\n",
   "        return module\n",
   "        setattr(obj, '_original_path', path)\n",
   "        setattr(obj, '_load_name', name)\n",
   "        setattr(obj, '_redirected_names', redirected_names or [])\n",
   "        return self.get_with_context(name, *args, **kwargs).object\n",
   "        found_in_cache = True\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        collection_list = kwargs.pop('collection_list', None)\n",
   "        if name in self.aliases:\n",
   "            name = self.aliases[name]\n",
   "        plugin_load_context = self.find_plugin_with_context(name, collection_list=collection_list)\n",
   "        if not plugin_load_context.resolved or not plugin_load_context.plugin_resolved_path:\n",
   "            return get_with_context_result(None, plugin_load_context)\n",
   "        name = plugin_load_context.plugin_resolved_name\n",
   "        path = plugin_load_context.plugin_resolved_path\n",
   "        redirected_names = plugin_load_context.redirect_list or []\n",
   "        if path not in self._module_cache:\n",
   "            self._module_cache[path] = self._load_module_source(name, path)\n",
   "            self._load_config_defs(name, self._module_cache[path], path)\n",
   "            found_in_cache = False\n",
   "        obj = getattr(self._module_cache[path], self.class_name)\n",
   "            module = __import__(self.package, fromlist=[self.base_class])\n",
   "                plugin_class = getattr(module, self.base_class)\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "            if not issubclass(obj, plugin_class):\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "        self._display_plugin_load(self.class_name, name, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "        if not class_only:\n",
   "                instance = object.__new__(obj)\n",
   "                self._update_object(instance, name, path, redirected_names)\n",
   "                obj.__init__(instance, *args, **kwargs)\n",
   "                obj = instance\n",
   "                    return get_with_context_result(None, plugin_load_context)\n",
   "        self._update_object(obj, name, path, redirected_names)\n",
   "        return get_with_context_result(obj, plugin_load_context)\n",
   "            msg = 'Loading %s \\'%s\\' from %s' % (class_name, os.path.basename(name), path)\n",
   "                msg = '%s (searched paths: %s)' % (msg, self.format_paths(searched_paths))\n",
   "            if found_in_cache or class_only:\n",
   "                msg = '%s (found_in_cache=%s, class_only=%s)' % (msg, found_in_cache, class_only)\n",
   "            display.debug(msg)\n",
   "        dedupe = kwargs.pop('_dedupe', True)\n",
   "        path_only = kwargs.pop('path_only', False)\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        if path_only and class_only:\n",
   "        all_matches = []\n",
   "        found_in_cache = True\n",
   "        for i in self._get_paths():\n",
   "            all_matches.extend(glob.glob(os.path.join(i, \"*.py\")))\n",
   "        loaded_modules = set()\n",
   "        for path in sorted(all_matches, key=os.path.basename):\n",
   "            name = os.path.splitext(path)[0]\n",
   "            basename = os.path.basename(name)\n",
   "            if basename == '__init__' or basename in _PLUGIN_FILTERS[self.package]:\n",
   "            if dedupe and basename in loaded_modules:\n",
   "            loaded_modules.add(basename)\n",
   "            if path_only:\n",
   "                yield path\n",
   "            if path not in self._module_cache:\n",
   "                        full_name = '{0}_{1}'.format(abs(hash(path)), basename)\n",
   "                        full_name = basename\n",
   "                    module = self._load_module_source(full_name, path)\n",
   "                    self._load_config_defs(basename, module, path)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                self._module_cache[path] = module\n",
   "                found_in_cache = False\n",
   "                obj = getattr(self._module_cache[path], self.class_name)\n",
   "                display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                module = __import__(self.package, fromlist=[self.base_class])\n",
   "                    plugin_class = getattr(module, self.base_class)\n",
   "                if not issubclass(obj, plugin_class):\n",
   "            self._display_plugin_load(self.class_name, basename, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "            if not class_only:\n",
   "                    obj = obj(*args, **kwargs)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be incomplete: %s\" % (path, to_text(e)))\n",
   "            self._update_object(obj, basename, path)\n",
   "            yield obj\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).find_plugin(name, collection_list=collection_list)\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).get(name, *args, **kwargs)\n",
   "        plugins = list(super(Jinja2Loader, self).all(*args, **kwargs))\n",
   "        plugins.reverse()\n",
   "        return plugins\n",
   "    filters = defaultdict(frozenset)\n",
   "        filter_cfg = '/etc/ansible/plugin_filters.yml'\n",
   "        filter_cfg = C.PLUGIN_FILTERS_CFG\n",
   "    if os.path.exists(filter_cfg):\n",
   "        with open(filter_cfg, 'rb') as f:\n",
   "                filter_data = from_yaml(f.read())\n",
   "                display.warning(u'The plugin filter file, {0} was not parsable.'\n",
   "                                u' Skipping: {1}'.format(filter_cfg, to_text(e)))\n",
   "                return filters\n",
   "            version = filter_data['filter_version']\n",
   "            display.warning(u'The plugin filter file, {0} was invalid.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "            return filters\n",
   "        version = to_text(version)\n",
   "        version = version.strip()\n",
   "        if version == u'1.0':\n",
   "                filters['ansible.modules'] = frozenset(filter_data['module_blacklist'])\n",
   "                display.warning(u'Unable to parse the plugin filter file {0} as'\n",
   "                                u' Skipping.'.format(filter_cfg))\n",
   "                return filters\n",
   "            filters['ansible.plugins.action'] = filters['ansible.modules']\n",
   "            display.warning(u'The plugin filter file, {0} was a version not recognized by this'\n",
   "                            u' version of Ansible. Skipping.'.format(filter_cfg))\n",
   "            display.warning(u'The plugin filter file, {0} does not exist.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "    if 'stat' in filters['ansible.modules']:\n",
   "                           ' from the blacklist.'.format(to_native(filter_cfg)))\n",
   "    return filters\n",
   "    display.vvvv(to_text('Loading collection {0} from {1}'.format(collection_name, collection_path)))\n",
   "        if not _does_collection_support_ansible_version(collection_meta.get('requires_ansible', ''), ansible_version):\n",
   "                display.warning(message)\n",
   "        display.warning('Error parsing collection metadata requires_ansible value from collection {0}: {1}'.format(collection_name, ex))\n",
   "        display.warning('packaging Python module unavailable; unable to validate collection Ansible version requirements')\n",
   "        display.warning('AnsibleCollectionFinder has already been configured')\n"
  ]
 },
 "110": {
  "name": "path",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/loader.py",
  "lineno": "661",
  "column": "12",
  "context": "hed_paths and os.path.isdir(p.path)):\n            path = path_context.path\n            display.debug('trying %s' % path)\n    ",
  "context_lines": "        #       we need to make sure we don't want to add additional directories\n        #       (add_directory()) once we start using the iterator.\n        #       We can use _get_paths_with_context() since add_directory() forces a cache refresh.\n        for path_context in (p for p in self._get_paths_with_context() if p.path not in self._searched_paths and os.path.isdir(p.path)):\n            path = path_context.path\n            display.debug('trying %s' % path)\n            plugin_load_context.load_attempts.append(path)\n            try:\n                full_paths = (os.path.join(path, f) for f in os.listdir(path))\n",
  "slicing": [
   "    imp = None\n",
   "display = Display()\n",
   "get_with_context_result = namedtuple('get_with_context_result', ['object', 'plugin_load_context'])\n",
   "    return [(name, obj) for (name, obj) in globals().items() if isinstance(obj, PluginLoader)]\n",
   "    b_path = os.path.expanduser(to_bytes(path, errors='surrogate_or_strict'))\n",
   "    if os.path.isdir(b_path):\n",
   "        for name, obj in get_all_plugin_loaders():\n",
   "            if obj.subdir:\n",
   "                plugin_path = os.path.join(b_path, to_bytes(obj.subdir))\n",
   "                if os.path.isdir(plugin_path):\n",
   "                    obj.add_directory(to_text(plugin_path))\n",
   "        display.warning(\"Ignoring invalid path provided to plugin path: '%s' is not a directory\" % to_text(path))\n",
   "        shell_type = 'sh'\n",
   "                shell_filename = os.path.basename(executable)\n",
   "                    shell = shell_loader.get(shell_filename)\n",
   "                    shell = None\n",
   "                if shell is None:\n",
   "                    for shell in shell_loader.all():\n",
   "                        if shell_filename in shell.COMPATIBLE_SHELLS:\n",
   "                            shell_type = shell.SHELL_FAMILY\n",
   "    shell = shell_loader.get(shell_type)\n",
   "    if not shell:\n",
   "        raise AnsibleError(\"Could not find the shell plugin required (%s).\" % shell_type)\n",
   "        setattr(shell, 'executable', executable)\n",
   "    return shell\n",
   "    loader = getattr(sys.modules[__name__], '%s_loader' % which_loader)\n",
   "    for path in paths:\n",
   "        loader.add_directory(path, with_subdir=True)\n",
   "        self.path = path\n",
   "        warning_text = deprecation.get('warning_text', None)\n",
   "        removal_date = deprecation.get('removal_date', None)\n",
   "        removal_version = deprecation.get('removal_version', None)\n",
   "        if removal_date is not None:\n",
   "            removal_version = None\n",
   "        if not warning_text:\n",
   "            warning_text = '{0} has been deprecated'.format(name)\n",
   "        display.deprecated(warning_text, date=removal_date, version=removal_version, collection_name=collection_name)\n",
   "        if removal_date:\n",
   "            self.removal_date = removal_date\n",
   "        if removal_version:\n",
   "            self.removal_version = removal_version\n",
   "        self.deprecation_warnings.append(warning_text)\n",
   "        aliases = {} if aliases is None else aliases\n",
   "        self.aliases = aliases\n",
   "            config = [config]\n",
   "        elif not config:\n",
   "            config = []\n",
   "        self.config = config\n",
   "        class_name = data.get('class_name')\n",
   "        package = data.get('package')\n",
   "        config = data.get('config')\n",
   "        subdir = data.get('subdir')\n",
   "        aliases = data.get('aliases')\n",
   "        base_class = data.get('base_class')\n",
   "        PATH_CACHE[class_name] = data.get('PATH_CACHE')\n",
   "        PLUGIN_PATH_CACHE[class_name] = data.get('PLUGIN_PATH_CACHE')\n",
   "        self.__init__(class_name, package, config, subdir, aliases, base_class)\n",
   "        ret = []\n",
   "        for i in paths:\n",
   "            if i not in ret:\n",
   "                ret.append(i)\n",
   "        return os.pathsep.join(ret)\n",
   "        results = []\n",
   "        results.append(dir)\n",
   "        for root, subdirs, files in os.walk(dir, followlinks=True):\n",
   "            if '__init__.py' in files:\n",
   "                for x in subdirs:\n",
   "                    results.append(os.path.join(root, x))\n",
   "        return results\n",
   "            m = __import__(self.package)\n",
   "            parts = self.package.split('.')[1:]\n",
   "            for parent_mod in parts:\n",
   "                m = getattr(m, parent_mod)\n",
   "            self.package_path = os.path.dirname(m.__file__)\n",
   "        if subdirs:\n",
   "        ret = [PluginPathContext(p, False) for p in self._extra_dirs]\n",
   "            for path in self.config:\n",
   "                path = os.path.realpath(os.path.expanduser(path))\n",
   "                if subdirs:\n",
   "                    contents = glob.glob(\"%s/*\" % path) + glob.glob(\"%s/*/*\" % path)\n",
   "                    for c in contents:\n",
   "                        if os.path.isdir(c) and c not in ret:\n",
   "                            ret.append(PluginPathContext(c, False))\n",
   "                if path not in ret:\n",
   "                    ret.append(PluginPathContext(path, False))\n",
   "        ret.extend([PluginPathContext(p, True) for p in self._get_package_paths(subdirs=subdirs)])\n",
   "        ret.sort(key=lambda p: p.path.endswith('/windows'))\n",
   "        self._paths = ret\n",
   "        return ret\n",
   "        paths_with_context = self._get_paths_with_context(subdirs=subdirs)\n",
   "        return [path_with_context.path for path_with_context in paths_with_context]\n",
   "            type_name = get_plugin_class(self.class_name)\n",
   "            if type_name in C.CONFIGURABLE_PLUGINS:\n",
   "                dstring = AnsibleLoader(getattr(module, 'DOCUMENTATION', ''), file_name=path).get_single_data()\n",
   "                if dstring:\n",
   "                    add_fragments(dstring, path, fragment_loader=fragment_loader, is_module=(type_name == 'module'))\n",
   "                if dstring and 'options' in dstring and isinstance(dstring['options'], dict):\n",
   "                    C.config.initialize_plugin_configuration_definitions(type_name, name, dstring['options'])\n",
   "                    display.debug('Loaded config def from plugin (%s/%s)' % (type_name, name))\n",
   "        directory = os.path.realpath(directory)\n",
   "        if directory is not None:\n",
   "                directory = os.path.join(directory, self.subdir)\n",
   "            if directory not in self._extra_dirs:\n",
   "                self._extra_dirs.append(directory)\n",
   "                display.debug('Added %s to loader search path' % (directory))\n",
   "        collection_pkg = import_module(acr.n_python_collection_package_name)\n",
   "        if not collection_pkg:\n",
   "        collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "        if not collection_meta:\n",
   "            subdir_qualified_resource = '.'.join([acr.subdirs, acr.resource])\n",
   "            subdir_qualified_resource = acr.resource\n",
   "        entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource + extension, None)\n",
   "        if not entry:\n",
   "            entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource, None)\n",
   "        return entry\n",
   "        plugin_type = AnsibleCollectionRef.legacy_plugin_dir_to_plugin_type(self.subdir)\n",
   "        acr = AnsibleCollectionRef.from_fqcr(fq_name, plugin_type)\n",
   "        routing_metadata = self._query_collection_routing_meta(acr, plugin_type, extension=extension)\n",
   "        if routing_metadata:\n",
   "            deprecation = routing_metadata.get('deprecation', None)\n",
   "            plugin_load_context.record_deprecation(fq_name, deprecation, acr.collection)\n",
   "            tombstone = routing_metadata.get('tombstone', None)\n",
   "            if tombstone:\n",
   "                removal_date = tombstone.get('removal_date')\n",
   "                removal_version = tombstone.get('removal_version')\n",
   "                warning_text = tombstone.get('warning_text') or '{0} has been removed.'.format(fq_name)\n",
   "                removed_msg = display.get_deprecation_message(msg=warning_text, version=removal_version,\n",
   "                                                              date=removal_date, removed=True,\n",
   "                                                              collection_name=acr.collection)\n",
   "                plugin_load_context.removal_date = removal_date\n",
   "                plugin_load_context.removal_version = removal_version\n",
   "                plugin_load_context.exit_reason = removed_msg\n",
   "                raise AnsiblePluginRemovedError(removed_msg, plugin_load_context=plugin_load_context)\n",
   "            redirect = routing_metadata.get('redirect', None)\n",
   "            if redirect:\n",
   "                display.vv(\"redirecting (type: {0}) {1} to {2}\".format(plugin_type, fq_name, redirect))\n",
   "                return plugin_load_context.redirect(redirect)\n",
   "        n_resource = to_native(acr.resource, errors='strict')\n",
   "        full_name = '{0}.{1}'.format(acr.n_python_package_name, n_resource)\n",
   "            n_resource += extension\n",
   "        pkg = sys.modules.get(acr.n_python_package_name)\n",
   "        if not pkg:\n",
   "                pkg = import_module(acr.n_python_package_name)\n",
   "                return plugin_load_context.nope('Python package {0} not found'.format(acr.n_python_package_name))\n",
   "        pkg_path = os.path.dirname(pkg.__file__)\n",
   "        n_resource_path = os.path.join(pkg_path, n_resource)\n",
   "        if os.path.exists(n_resource_path):\n",
   "                full_name, to_text(n_resource_path), acr.collection, 'found exact match for {0} in {1}'.format(full_name, acr.collection))\n",
   "            return plugin_load_context.nope('no match for {0} in {1}'.format(to_text(n_resource), acr.collection))\n",
   "        found_files = [f\n",
   "                       for f in glob.iglob(os.path.join(pkg_path, n_resource) + '.*')\n",
   "                       if os.path.isfile(f) and not f.endswith(C.MODULE_IGNORE_EXTS)]\n",
   "        if not found_files:\n",
   "            return plugin_load_context.nope('failed fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        if len(found_files) > 1:\n",
   "            full_name, to_text(found_files[0]), acr.collection, 'found fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        result = self.find_plugin_with_context(name, mod_type, ignore_deprecated, check_aliases, collection_list)\n",
   "        if result.resolved and result.plugin_resolved_path:\n",
   "            return result.plugin_resolved_path\n",
   "        plugin_load_context = PluginLoadContext()\n",
   "        plugin_load_context.original_name = name\n",
   "            result = self._resolve_plugin_step(name, mod_type, ignore_deprecated, check_aliases, collection_list, plugin_load_context=plugin_load_context)\n",
   "            if result.pending_redirect:\n",
   "                if result.pending_redirect in result.redirect_list:\n",
   "                    raise AnsiblePluginCircularRedirect('plugin redirect loop resolving {0} (path: {1})'.format(result.original_name, result.redirect_list))\n",
   "                name = result.pending_redirect\n",
   "                result.pending_redirect = None\n",
   "                plugin_load_context = result\n",
   "        if plugin_load_context.error_list:\n",
   "            display.warning(\"errors were encountered during the plugin load for {0}:\\n{1}\".format(name, plugin_load_context.error_list))\n",
   "        return plugin_load_context\n",
   "        if not plugin_load_context:\n",
   "        plugin_load_context.redirect_list.append(name)\n",
   "        plugin_load_context.resolved = False\n",
   "        if name in _PLUGIN_FILTERS[self.package]:\n",
   "            plugin_load_context.exit_reason = '{0} matched a defined plugin filter'.format(name)\n",
   "            return plugin_load_context\n",
   "            suffix = mod_type\n",
   "            suffix = '.py'\n",
   "            suffix = ''\n",
   "        if (AnsibleCollectionRef.is_valid_fqcr(name) or collection_list) and not name.startswith('Ansible'):\n",
   "            if '.' in name or not collection_list:\n",
   "                candidates = [name]\n",
   "                candidates = ['{0}.{1}'.format(c, name) for c in collection_list]\n",
   "            for candidate_name in candidates:\n",
   "                    plugin_load_context.load_attempts.append(candidate_name)\n",
   "                    if candidate_name.startswith('ansible.legacy'):\n",
   "                        plugin_load_context = self._find_plugin_legacy(name.replace('ansible.legacy.', '', 1),\n",
   "                                                                       plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "                        plugin_load_context = self._find_fq_plugin(candidate_name, suffix, plugin_load_context=plugin_load_context)\n",
   "                        if candidate_name != plugin_load_context.original_name and candidate_name not in plugin_load_context.redirect_list:\n",
   "                            plugin_load_context.redirect_list.append(candidate_name)\n",
   "                    if plugin_load_context.resolved or plugin_load_context.pending_redirect:  # if we got an answer or need to chase down a redirect, return\n",
   "                        return plugin_load_context\n",
   "                    plugin_load_context.import_error_list.append(ie)\n",
   "                    plugin_load_context.error_list.append(to_native(ex))\n",
   "            if plugin_load_context.error_list:\n",
   "                display.debug(msg='plugin lookup for {0} failed; errors: {1}'.format(name, '; '.join(plugin_load_context.error_list)))\n",
   "            plugin_load_context.exit_reason = 'no matches found for {0}'.format(name)\n",
   "            return plugin_load_context\n",
   "        return self._find_plugin_legacy(name, plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "        plugin_load_context.resolved = False\n",
   "            name = self.aliases.get(name, name)\n",
   "        pull_cache = self._plugin_path_cache[suffix]\n",
   "            path_with_context = pull_cache[name]\n",
   "            plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "            plugin_load_context.plugin_resolved_name = name\n",
   "            plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "            plugin_load_context.resolved = True\n",
   "            return plugin_load_context\n",
   "        for path_context in (p for p in self._get_paths_with_context() if p.path not in self._searched_paths and os.path.isdir(p.path)):\n",
   "            path = path_context.path\n",
   "            display.debug('trying %s' % path)\n",
   "            plugin_load_context.load_attempts.append(path)\n",
   "                full_paths = (os.path.join(path, f) for f in os.listdir(path))\n",
   "                display.warning(\"Error accessing plugin paths: %s\" % to_text(e))\n",
   "            for full_path in (f for f in full_paths if os.path.isfile(f) and not f.endswith('__init__.py')):\n",
   "                full_name = os.path.basename(full_path)\n",
   "                if any(full_path.endswith(x) for x in C.MODULE_IGNORE_EXTS):\n",
   "                splitname = os.path.splitext(full_name)\n",
   "                base_name = splitname[0]\n",
   "                internal = path_context.internal\n",
   "                    extension = splitname[1]\n",
   "                    extension = ''\n",
   "                if base_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][full_name] = PluginPathContext(full_path, internal)\n",
   "                if base_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][full_name] = PluginPathContext(full_path, internal)\n",
   "            self._searched_paths.add(path)\n",
   "                path_with_context = pull_cache[name]\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        if not name.startswith('_'):\n",
   "            alias_name = '_' + name\n",
   "            if alias_name in pull_cache:\n",
   "                path_with_context = pull_cache[alias_name]\n",
   "                if not ignore_deprecated and not os.path.islink(path_with_context.path):\n",
   "                    display.deprecated('%s is kept for backwards compatibility but usage is discouraged. '  # pylint: disable=ansible-deprecated-no-version\n",
   "                                       'The module documentation details page may explain more about this rationale.' % name.lstrip('_'))\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = alias_name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        candidate_fqcr = 'ansible.builtin.{0}'.format(name)\n",
   "        if '.' not in name and AnsibleCollectionRef.is_valid_fqcr(candidate_fqcr):\n",
   "            return self._find_fq_plugin(fq_name=candidate_fqcr, extension=suffix, plugin_load_context=plugin_load_context)\n",
   "        return plugin_load_context.nope('{0} is not eligible for last-chance resolution'.format(name))\n",
   "            return self.find_plugin(name, collection_list=collection_list) is not None\n",
   "            display.debug('has_plugin error: {0}'.format(to_text(ex)))\n",
   "        if name.startswith('ansible_collections.'):\n",
   "            full_name = name\n",
   "            full_name = '.'.join([self.package, name])\n",
   "        if full_name in sys.modules:\n",
   "            return sys.modules[full_name]\n",
   "            if imp is None:\n",
   "                spec = importlib.util.spec_from_file_location(to_native(full_name), to_native(path))\n",
   "                module = importlib.util.module_from_spec(spec)\n",
   "                spec.loader.exec_module(module)\n",
   "                sys.modules[full_name] = module\n",
   "                with open(to_bytes(path), 'rb') as module_file:\n",
   "                    module = imp.load_source(to_native(full_name), to_native(path), module_file)\n",
   "        return module\n",
   "        setattr(obj, '_original_path', path)\n",
   "        setattr(obj, '_load_name', name)\n",
   "        setattr(obj, '_redirected_names', redirected_names or [])\n",
   "        return self.get_with_context(name, *args, **kwargs).object\n",
   "        found_in_cache = True\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        collection_list = kwargs.pop('collection_list', None)\n",
   "        if name in self.aliases:\n",
   "            name = self.aliases[name]\n",
   "        plugin_load_context = self.find_plugin_with_context(name, collection_list=collection_list)\n",
   "        if not plugin_load_context.resolved or not plugin_load_context.plugin_resolved_path:\n",
   "            return get_with_context_result(None, plugin_load_context)\n",
   "        name = plugin_load_context.plugin_resolved_name\n",
   "        path = plugin_load_context.plugin_resolved_path\n",
   "        redirected_names = plugin_load_context.redirect_list or []\n",
   "        if path not in self._module_cache:\n",
   "            self._module_cache[path] = self._load_module_source(name, path)\n",
   "            self._load_config_defs(name, self._module_cache[path], path)\n",
   "            found_in_cache = False\n",
   "        obj = getattr(self._module_cache[path], self.class_name)\n",
   "            module = __import__(self.package, fromlist=[self.base_class])\n",
   "                plugin_class = getattr(module, self.base_class)\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "            if not issubclass(obj, plugin_class):\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "        self._display_plugin_load(self.class_name, name, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "        if not class_only:\n",
   "                instance = object.__new__(obj)\n",
   "                self._update_object(instance, name, path, redirected_names)\n",
   "                obj.__init__(instance, *args, **kwargs)\n",
   "                obj = instance\n",
   "                    return get_with_context_result(None, plugin_load_context)\n",
   "        self._update_object(obj, name, path, redirected_names)\n",
   "        return get_with_context_result(obj, plugin_load_context)\n",
   "            msg = 'Loading %s \\'%s\\' from %s' % (class_name, os.path.basename(name), path)\n",
   "                msg = '%s (searched paths: %s)' % (msg, self.format_paths(searched_paths))\n",
   "            if found_in_cache or class_only:\n",
   "                msg = '%s (found_in_cache=%s, class_only=%s)' % (msg, found_in_cache, class_only)\n",
   "            display.debug(msg)\n",
   "        dedupe = kwargs.pop('_dedupe', True)\n",
   "        path_only = kwargs.pop('path_only', False)\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        if path_only and class_only:\n",
   "        all_matches = []\n",
   "        found_in_cache = True\n",
   "        for i in self._get_paths():\n",
   "            all_matches.extend(glob.glob(os.path.join(i, \"*.py\")))\n",
   "        loaded_modules = set()\n",
   "        for path in sorted(all_matches, key=os.path.basename):\n",
   "            name = os.path.splitext(path)[0]\n",
   "            basename = os.path.basename(name)\n",
   "            if basename == '__init__' or basename in _PLUGIN_FILTERS[self.package]:\n",
   "            if dedupe and basename in loaded_modules:\n",
   "            loaded_modules.add(basename)\n",
   "            if path_only:\n",
   "                yield path\n",
   "            if path not in self._module_cache:\n",
   "                        full_name = '{0}_{1}'.format(abs(hash(path)), basename)\n",
   "                        full_name = basename\n",
   "                    module = self._load_module_source(full_name, path)\n",
   "                    self._load_config_defs(basename, module, path)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                self._module_cache[path] = module\n",
   "                found_in_cache = False\n",
   "                obj = getattr(self._module_cache[path], self.class_name)\n",
   "                display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                module = __import__(self.package, fromlist=[self.base_class])\n",
   "                    plugin_class = getattr(module, self.base_class)\n",
   "                if not issubclass(obj, plugin_class):\n",
   "            self._display_plugin_load(self.class_name, basename, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "            if not class_only:\n",
   "                    obj = obj(*args, **kwargs)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be incomplete: %s\" % (path, to_text(e)))\n",
   "            self._update_object(obj, basename, path)\n",
   "            yield obj\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).find_plugin(name, collection_list=collection_list)\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).get(name, *args, **kwargs)\n",
   "        plugins = list(super(Jinja2Loader, self).all(*args, **kwargs))\n",
   "        plugins.reverse()\n",
   "        return plugins\n",
   "    filters = defaultdict(frozenset)\n",
   "        filter_cfg = '/etc/ansible/plugin_filters.yml'\n",
   "        filter_cfg = C.PLUGIN_FILTERS_CFG\n",
   "    if os.path.exists(filter_cfg):\n",
   "        with open(filter_cfg, 'rb') as f:\n",
   "                filter_data = from_yaml(f.read())\n",
   "                display.warning(u'The plugin filter file, {0} was not parsable.'\n",
   "                                u' Skipping: {1}'.format(filter_cfg, to_text(e)))\n",
   "                return filters\n",
   "            version = filter_data['filter_version']\n",
   "            display.warning(u'The plugin filter file, {0} was invalid.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "            return filters\n",
   "        version = to_text(version)\n",
   "        version = version.strip()\n",
   "        if version == u'1.0':\n",
   "                filters['ansible.modules'] = frozenset(filter_data['module_blacklist'])\n",
   "                display.warning(u'Unable to parse the plugin filter file {0} as'\n",
   "                                u' Skipping.'.format(filter_cfg))\n",
   "                return filters\n",
   "            filters['ansible.plugins.action'] = filters['ansible.modules']\n",
   "            display.warning(u'The plugin filter file, {0} was a version not recognized by this'\n",
   "                            u' version of Ansible. Skipping.'.format(filter_cfg))\n",
   "            display.warning(u'The plugin filter file, {0} does not exist.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "    if 'stat' in filters['ansible.modules']:\n",
   "                           ' from the blacklist.'.format(to_native(filter_cfg)))\n",
   "    return filters\n",
   "    display.vvvv(to_text('Loading collection {0} from {1}'.format(collection_name, collection_path)))\n",
   "        if not _does_collection_support_ansible_version(collection_meta.get('requires_ansible', ''), ansible_version):\n",
   "                display.warning(message)\n",
   "        display.warning('Error parsing collection metadata requires_ansible value from collection {0}: {1}'.format(collection_name, ex))\n",
   "        display.warning('packaging Python module unavailable; unable to validate collection Ansible version requirements')\n",
   "        display.warning('AnsibleCollectionFinder has already been configured')\n"
  ]
 },
 "111": {
  "name": "full_name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/loader.py",
  "lineno": "670",
  "column": "16",
  "context": "d not f.endswith('__init__.py')):\n                full_name = os.path.basename(full_path)\n\n                # HACK: We have no way of executi",
  "context_lines": "                full_paths = (os.path.join(path, f) for f in os.listdir(path))\n            except OSError as e:\n                display.warning(\"Error accessing plugin paths: %s\" % to_text(e))\n\n            for full_path in (f for f in full_paths if os.path.isfile(f) and not f.endswith('__init__.py')):\n                full_name = os.path.basename(full_path)\n\n                # HACK: We have no way of executing python byte compiled files as ansible modules so specifically exclude them\n                # FIXME: I believe this is only correct for modules and module_utils.\n                # For all other plugins we want .pyc and .pyo should be valid\n",
  "slicing": [
   "    imp = None\n",
   "display = Display()\n",
   "get_with_context_result = namedtuple('get_with_context_result', ['object', 'plugin_load_context'])\n",
   "    return [(name, obj) for (name, obj) in globals().items() if isinstance(obj, PluginLoader)]\n",
   "    b_path = os.path.expanduser(to_bytes(path, errors='surrogate_or_strict'))\n",
   "    if os.path.isdir(b_path):\n",
   "        for name, obj in get_all_plugin_loaders():\n",
   "            if obj.subdir:\n",
   "                plugin_path = os.path.join(b_path, to_bytes(obj.subdir))\n",
   "                if os.path.isdir(plugin_path):\n",
   "                    obj.add_directory(to_text(plugin_path))\n",
   "        display.warning(\"Ignoring invalid path provided to plugin path: '%s' is not a directory\" % to_text(path))\n",
   "        shell_type = 'sh'\n",
   "                shell_filename = os.path.basename(executable)\n",
   "                    shell = shell_loader.get(shell_filename)\n",
   "                    shell = None\n",
   "                if shell is None:\n",
   "                    for shell in shell_loader.all():\n",
   "                        if shell_filename in shell.COMPATIBLE_SHELLS:\n",
   "                            shell_type = shell.SHELL_FAMILY\n",
   "    shell = shell_loader.get(shell_type)\n",
   "    if not shell:\n",
   "        raise AnsibleError(\"Could not find the shell plugin required (%s).\" % shell_type)\n",
   "        setattr(shell, 'executable', executable)\n",
   "    return shell\n",
   "    loader = getattr(sys.modules[__name__], '%s_loader' % which_loader)\n",
   "    for path in paths:\n",
   "        loader.add_directory(path, with_subdir=True)\n",
   "        self.path = path\n",
   "        warning_text = deprecation.get('warning_text', None)\n",
   "        removal_date = deprecation.get('removal_date', None)\n",
   "        removal_version = deprecation.get('removal_version', None)\n",
   "        if removal_date is not None:\n",
   "            removal_version = None\n",
   "        if not warning_text:\n",
   "            warning_text = '{0} has been deprecated'.format(name)\n",
   "        display.deprecated(warning_text, date=removal_date, version=removal_version, collection_name=collection_name)\n",
   "        if removal_date:\n",
   "            self.removal_date = removal_date\n",
   "        if removal_version:\n",
   "            self.removal_version = removal_version\n",
   "        self.deprecation_warnings.append(warning_text)\n",
   "        aliases = {} if aliases is None else aliases\n",
   "        self.aliases = aliases\n",
   "            config = [config]\n",
   "        elif not config:\n",
   "            config = []\n",
   "        self.config = config\n",
   "        class_name = data.get('class_name')\n",
   "        package = data.get('package')\n",
   "        config = data.get('config')\n",
   "        subdir = data.get('subdir')\n",
   "        aliases = data.get('aliases')\n",
   "        base_class = data.get('base_class')\n",
   "        PATH_CACHE[class_name] = data.get('PATH_CACHE')\n",
   "        PLUGIN_PATH_CACHE[class_name] = data.get('PLUGIN_PATH_CACHE')\n",
   "        self.__init__(class_name, package, config, subdir, aliases, base_class)\n",
   "        ret = []\n",
   "        for i in paths:\n",
   "            if i not in ret:\n",
   "                ret.append(i)\n",
   "        return os.pathsep.join(ret)\n",
   "        results = []\n",
   "        results.append(dir)\n",
   "        for root, subdirs, files in os.walk(dir, followlinks=True):\n",
   "            if '__init__.py' in files:\n",
   "                for x in subdirs:\n",
   "                    results.append(os.path.join(root, x))\n",
   "        return results\n",
   "            m = __import__(self.package)\n",
   "            parts = self.package.split('.')[1:]\n",
   "            for parent_mod in parts:\n",
   "                m = getattr(m, parent_mod)\n",
   "            self.package_path = os.path.dirname(m.__file__)\n",
   "        if subdirs:\n",
   "        ret = [PluginPathContext(p, False) for p in self._extra_dirs]\n",
   "            for path in self.config:\n",
   "                path = os.path.realpath(os.path.expanduser(path))\n",
   "                if subdirs:\n",
   "                    contents = glob.glob(\"%s/*\" % path) + glob.glob(\"%s/*/*\" % path)\n",
   "                    for c in contents:\n",
   "                        if os.path.isdir(c) and c not in ret:\n",
   "                            ret.append(PluginPathContext(c, False))\n",
   "                if path not in ret:\n",
   "                    ret.append(PluginPathContext(path, False))\n",
   "        ret.extend([PluginPathContext(p, True) for p in self._get_package_paths(subdirs=subdirs)])\n",
   "        ret.sort(key=lambda p: p.path.endswith('/windows'))\n",
   "        self._paths = ret\n",
   "        return ret\n",
   "        paths_with_context = self._get_paths_with_context(subdirs=subdirs)\n",
   "        return [path_with_context.path for path_with_context in paths_with_context]\n",
   "            type_name = get_plugin_class(self.class_name)\n",
   "            if type_name in C.CONFIGURABLE_PLUGINS:\n",
   "                dstring = AnsibleLoader(getattr(module, 'DOCUMENTATION', ''), file_name=path).get_single_data()\n",
   "                if dstring:\n",
   "                    add_fragments(dstring, path, fragment_loader=fragment_loader, is_module=(type_name == 'module'))\n",
   "                if dstring and 'options' in dstring and isinstance(dstring['options'], dict):\n",
   "                    C.config.initialize_plugin_configuration_definitions(type_name, name, dstring['options'])\n",
   "                    display.debug('Loaded config def from plugin (%s/%s)' % (type_name, name))\n",
   "        directory = os.path.realpath(directory)\n",
   "        if directory is not None:\n",
   "                directory = os.path.join(directory, self.subdir)\n",
   "            if directory not in self._extra_dirs:\n",
   "                self._extra_dirs.append(directory)\n",
   "                display.debug('Added %s to loader search path' % (directory))\n",
   "        collection_pkg = import_module(acr.n_python_collection_package_name)\n",
   "        if not collection_pkg:\n",
   "        collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "        if not collection_meta:\n",
   "            subdir_qualified_resource = '.'.join([acr.subdirs, acr.resource])\n",
   "            subdir_qualified_resource = acr.resource\n",
   "        entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource + extension, None)\n",
   "        if not entry:\n",
   "            entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource, None)\n",
   "        return entry\n",
   "        plugin_type = AnsibleCollectionRef.legacy_plugin_dir_to_plugin_type(self.subdir)\n",
   "        acr = AnsibleCollectionRef.from_fqcr(fq_name, plugin_type)\n",
   "        routing_metadata = self._query_collection_routing_meta(acr, plugin_type, extension=extension)\n",
   "        if routing_metadata:\n",
   "            deprecation = routing_metadata.get('deprecation', None)\n",
   "            plugin_load_context.record_deprecation(fq_name, deprecation, acr.collection)\n",
   "            tombstone = routing_metadata.get('tombstone', None)\n",
   "            if tombstone:\n",
   "                removal_date = tombstone.get('removal_date')\n",
   "                removal_version = tombstone.get('removal_version')\n",
   "                warning_text = tombstone.get('warning_text') or '{0} has been removed.'.format(fq_name)\n",
   "                removed_msg = display.get_deprecation_message(msg=warning_text, version=removal_version,\n",
   "                                                              date=removal_date, removed=True,\n",
   "                                                              collection_name=acr.collection)\n",
   "                plugin_load_context.removal_date = removal_date\n",
   "                plugin_load_context.removal_version = removal_version\n",
   "                plugin_load_context.exit_reason = removed_msg\n",
   "                raise AnsiblePluginRemovedError(removed_msg, plugin_load_context=plugin_load_context)\n",
   "            redirect = routing_metadata.get('redirect', None)\n",
   "            if redirect:\n",
   "                display.vv(\"redirecting (type: {0}) {1} to {2}\".format(plugin_type, fq_name, redirect))\n",
   "                return plugin_load_context.redirect(redirect)\n",
   "        n_resource = to_native(acr.resource, errors='strict')\n",
   "        full_name = '{0}.{1}'.format(acr.n_python_package_name, n_resource)\n",
   "            n_resource += extension\n",
   "        pkg = sys.modules.get(acr.n_python_package_name)\n",
   "        if not pkg:\n",
   "                pkg = import_module(acr.n_python_package_name)\n",
   "                return plugin_load_context.nope('Python package {0} not found'.format(acr.n_python_package_name))\n",
   "        pkg_path = os.path.dirname(pkg.__file__)\n",
   "        n_resource_path = os.path.join(pkg_path, n_resource)\n",
   "        if os.path.exists(n_resource_path):\n",
   "                full_name, to_text(n_resource_path), acr.collection, 'found exact match for {0} in {1}'.format(full_name, acr.collection))\n",
   "            return plugin_load_context.nope('no match for {0} in {1}'.format(to_text(n_resource), acr.collection))\n",
   "        found_files = [f\n",
   "                       for f in glob.iglob(os.path.join(pkg_path, n_resource) + '.*')\n",
   "                       if os.path.isfile(f) and not f.endswith(C.MODULE_IGNORE_EXTS)]\n",
   "        if not found_files:\n",
   "            return plugin_load_context.nope('failed fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        if len(found_files) > 1:\n",
   "            full_name, to_text(found_files[0]), acr.collection, 'found fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        result = self.find_plugin_with_context(name, mod_type, ignore_deprecated, check_aliases, collection_list)\n",
   "        if result.resolved and result.plugin_resolved_path:\n",
   "            return result.plugin_resolved_path\n",
   "        plugin_load_context = PluginLoadContext()\n",
   "        plugin_load_context.original_name = name\n",
   "            result = self._resolve_plugin_step(name, mod_type, ignore_deprecated, check_aliases, collection_list, plugin_load_context=plugin_load_context)\n",
   "            if result.pending_redirect:\n",
   "                if result.pending_redirect in result.redirect_list:\n",
   "                    raise AnsiblePluginCircularRedirect('plugin redirect loop resolving {0} (path: {1})'.format(result.original_name, result.redirect_list))\n",
   "                name = result.pending_redirect\n",
   "                result.pending_redirect = None\n",
   "                plugin_load_context = result\n",
   "        if plugin_load_context.error_list:\n",
   "            display.warning(\"errors were encountered during the plugin load for {0}:\\n{1}\".format(name, plugin_load_context.error_list))\n",
   "        return plugin_load_context\n",
   "        if not plugin_load_context:\n",
   "        plugin_load_context.redirect_list.append(name)\n",
   "        plugin_load_context.resolved = False\n",
   "        if name in _PLUGIN_FILTERS[self.package]:\n",
   "            plugin_load_context.exit_reason = '{0} matched a defined plugin filter'.format(name)\n",
   "            return plugin_load_context\n",
   "            suffix = mod_type\n",
   "            suffix = '.py'\n",
   "            suffix = ''\n",
   "        if (AnsibleCollectionRef.is_valid_fqcr(name) or collection_list) and not name.startswith('Ansible'):\n",
   "            if '.' in name or not collection_list:\n",
   "                candidates = [name]\n",
   "                candidates = ['{0}.{1}'.format(c, name) for c in collection_list]\n",
   "            for candidate_name in candidates:\n",
   "                    plugin_load_context.load_attempts.append(candidate_name)\n",
   "                    if candidate_name.startswith('ansible.legacy'):\n",
   "                        plugin_load_context = self._find_plugin_legacy(name.replace('ansible.legacy.', '', 1),\n",
   "                                                                       plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "                        plugin_load_context = self._find_fq_plugin(candidate_name, suffix, plugin_load_context=plugin_load_context)\n",
   "                        if candidate_name != plugin_load_context.original_name and candidate_name not in plugin_load_context.redirect_list:\n",
   "                            plugin_load_context.redirect_list.append(candidate_name)\n",
   "                    if plugin_load_context.resolved or plugin_load_context.pending_redirect:  # if we got an answer or need to chase down a redirect, return\n",
   "                        return plugin_load_context\n",
   "                    plugin_load_context.import_error_list.append(ie)\n",
   "                    plugin_load_context.error_list.append(to_native(ex))\n",
   "            if plugin_load_context.error_list:\n",
   "                display.debug(msg='plugin lookup for {0} failed; errors: {1}'.format(name, '; '.join(plugin_load_context.error_list)))\n",
   "            plugin_load_context.exit_reason = 'no matches found for {0}'.format(name)\n",
   "            return plugin_load_context\n",
   "        return self._find_plugin_legacy(name, plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "        plugin_load_context.resolved = False\n",
   "            name = self.aliases.get(name, name)\n",
   "        pull_cache = self._plugin_path_cache[suffix]\n",
   "            path_with_context = pull_cache[name]\n",
   "            plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "            plugin_load_context.plugin_resolved_name = name\n",
   "            plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "            plugin_load_context.resolved = True\n",
   "            return plugin_load_context\n",
   "        for path_context in (p for p in self._get_paths_with_context() if p.path not in self._searched_paths and os.path.isdir(p.path)):\n",
   "            path = path_context.path\n",
   "            display.debug('trying %s' % path)\n",
   "            plugin_load_context.load_attempts.append(path)\n",
   "                full_paths = (os.path.join(path, f) for f in os.listdir(path))\n",
   "                display.warning(\"Error accessing plugin paths: %s\" % to_text(e))\n",
   "            for full_path in (f for f in full_paths if os.path.isfile(f) and not f.endswith('__init__.py')):\n",
   "                full_name = os.path.basename(full_path)\n",
   "                if any(full_path.endswith(x) for x in C.MODULE_IGNORE_EXTS):\n",
   "                splitname = os.path.splitext(full_name)\n",
   "                base_name = splitname[0]\n",
   "                internal = path_context.internal\n",
   "                    extension = splitname[1]\n",
   "                    extension = ''\n",
   "                if base_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][full_name] = PluginPathContext(full_path, internal)\n",
   "                if base_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][full_name] = PluginPathContext(full_path, internal)\n",
   "            self._searched_paths.add(path)\n",
   "                path_with_context = pull_cache[name]\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        if not name.startswith('_'):\n",
   "            alias_name = '_' + name\n",
   "            if alias_name in pull_cache:\n",
   "                path_with_context = pull_cache[alias_name]\n",
   "                if not ignore_deprecated and not os.path.islink(path_with_context.path):\n",
   "                    display.deprecated('%s is kept for backwards compatibility but usage is discouraged. '  # pylint: disable=ansible-deprecated-no-version\n",
   "                                       'The module documentation details page may explain more about this rationale.' % name.lstrip('_'))\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = alias_name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        candidate_fqcr = 'ansible.builtin.{0}'.format(name)\n",
   "        if '.' not in name and AnsibleCollectionRef.is_valid_fqcr(candidate_fqcr):\n",
   "            return self._find_fq_plugin(fq_name=candidate_fqcr, extension=suffix, plugin_load_context=plugin_load_context)\n",
   "        return plugin_load_context.nope('{0} is not eligible for last-chance resolution'.format(name))\n",
   "            return self.find_plugin(name, collection_list=collection_list) is not None\n",
   "            display.debug('has_plugin error: {0}'.format(to_text(ex)))\n",
   "        if name.startswith('ansible_collections.'):\n",
   "            full_name = name\n",
   "            full_name = '.'.join([self.package, name])\n",
   "        if full_name in sys.modules:\n",
   "            return sys.modules[full_name]\n",
   "            if imp is None:\n",
   "                spec = importlib.util.spec_from_file_location(to_native(full_name), to_native(path))\n",
   "                module = importlib.util.module_from_spec(spec)\n",
   "                spec.loader.exec_module(module)\n",
   "                sys.modules[full_name] = module\n",
   "                with open(to_bytes(path), 'rb') as module_file:\n",
   "                    module = imp.load_source(to_native(full_name), to_native(path), module_file)\n",
   "        return module\n",
   "        setattr(obj, '_original_path', path)\n",
   "        setattr(obj, '_load_name', name)\n",
   "        setattr(obj, '_redirected_names', redirected_names or [])\n",
   "        return self.get_with_context(name, *args, **kwargs).object\n",
   "        found_in_cache = True\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        collection_list = kwargs.pop('collection_list', None)\n",
   "        if name in self.aliases:\n",
   "            name = self.aliases[name]\n",
   "        plugin_load_context = self.find_plugin_with_context(name, collection_list=collection_list)\n",
   "        if not plugin_load_context.resolved or not plugin_load_context.plugin_resolved_path:\n",
   "            return get_with_context_result(None, plugin_load_context)\n",
   "        name = plugin_load_context.plugin_resolved_name\n",
   "        path = plugin_load_context.plugin_resolved_path\n",
   "        redirected_names = plugin_load_context.redirect_list or []\n",
   "        if path not in self._module_cache:\n",
   "            self._module_cache[path] = self._load_module_source(name, path)\n",
   "            self._load_config_defs(name, self._module_cache[path], path)\n",
   "            found_in_cache = False\n",
   "        obj = getattr(self._module_cache[path], self.class_name)\n",
   "            module = __import__(self.package, fromlist=[self.base_class])\n",
   "                plugin_class = getattr(module, self.base_class)\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "            if not issubclass(obj, plugin_class):\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "        self._display_plugin_load(self.class_name, name, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "        if not class_only:\n",
   "                instance = object.__new__(obj)\n",
   "                self._update_object(instance, name, path, redirected_names)\n",
   "                obj.__init__(instance, *args, **kwargs)\n",
   "                obj = instance\n",
   "                    return get_with_context_result(None, plugin_load_context)\n",
   "        self._update_object(obj, name, path, redirected_names)\n",
   "        return get_with_context_result(obj, plugin_load_context)\n",
   "            msg = 'Loading %s \\'%s\\' from %s' % (class_name, os.path.basename(name), path)\n",
   "                msg = '%s (searched paths: %s)' % (msg, self.format_paths(searched_paths))\n",
   "            if found_in_cache or class_only:\n",
   "                msg = '%s (found_in_cache=%s, class_only=%s)' % (msg, found_in_cache, class_only)\n",
   "            display.debug(msg)\n",
   "        dedupe = kwargs.pop('_dedupe', True)\n",
   "        path_only = kwargs.pop('path_only', False)\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        if path_only and class_only:\n",
   "        all_matches = []\n",
   "        found_in_cache = True\n",
   "        for i in self._get_paths():\n",
   "            all_matches.extend(glob.glob(os.path.join(i, \"*.py\")))\n",
   "        loaded_modules = set()\n",
   "        for path in sorted(all_matches, key=os.path.basename):\n",
   "            name = os.path.splitext(path)[0]\n",
   "            basename = os.path.basename(name)\n",
   "            if basename == '__init__' or basename in _PLUGIN_FILTERS[self.package]:\n",
   "            if dedupe and basename in loaded_modules:\n",
   "            loaded_modules.add(basename)\n",
   "            if path_only:\n",
   "                yield path\n",
   "            if path not in self._module_cache:\n",
   "                        full_name = '{0}_{1}'.format(abs(hash(path)), basename)\n",
   "                        full_name = basename\n",
   "                    module = self._load_module_source(full_name, path)\n",
   "                    self._load_config_defs(basename, module, path)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                self._module_cache[path] = module\n",
   "                found_in_cache = False\n",
   "                obj = getattr(self._module_cache[path], self.class_name)\n",
   "                display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                module = __import__(self.package, fromlist=[self.base_class])\n",
   "                    plugin_class = getattr(module, self.base_class)\n",
   "                if not issubclass(obj, plugin_class):\n",
   "            self._display_plugin_load(self.class_name, basename, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "            if not class_only:\n",
   "                    obj = obj(*args, **kwargs)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be incomplete: %s\" % (path, to_text(e)))\n",
   "            self._update_object(obj, basename, path)\n",
   "            yield obj\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).find_plugin(name, collection_list=collection_list)\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).get(name, *args, **kwargs)\n",
   "        plugins = list(super(Jinja2Loader, self).all(*args, **kwargs))\n",
   "        plugins.reverse()\n",
   "        return plugins\n",
   "    filters = defaultdict(frozenset)\n",
   "        filter_cfg = '/etc/ansible/plugin_filters.yml'\n",
   "        filter_cfg = C.PLUGIN_FILTERS_CFG\n",
   "    if os.path.exists(filter_cfg):\n",
   "        with open(filter_cfg, 'rb') as f:\n",
   "                filter_data = from_yaml(f.read())\n",
   "                display.warning(u'The plugin filter file, {0} was not parsable.'\n",
   "                                u' Skipping: {1}'.format(filter_cfg, to_text(e)))\n",
   "                return filters\n",
   "            version = filter_data['filter_version']\n",
   "            display.warning(u'The plugin filter file, {0} was invalid.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "            return filters\n",
   "        version = to_text(version)\n",
   "        version = version.strip()\n",
   "        if version == u'1.0':\n",
   "                filters['ansible.modules'] = frozenset(filter_data['module_blacklist'])\n",
   "                display.warning(u'Unable to parse the plugin filter file {0} as'\n",
   "                                u' Skipping.'.format(filter_cfg))\n",
   "                return filters\n",
   "            filters['ansible.plugins.action'] = filters['ansible.modules']\n",
   "            display.warning(u'The plugin filter file, {0} was a version not recognized by this'\n",
   "                            u' version of Ansible. Skipping.'.format(filter_cfg))\n",
   "            display.warning(u'The plugin filter file, {0} does not exist.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "    if 'stat' in filters['ansible.modules']:\n",
   "                           ' from the blacklist.'.format(to_native(filter_cfg)))\n",
   "    return filters\n",
   "    display.vvvv(to_text('Loading collection {0} from {1}'.format(collection_name, collection_path)))\n",
   "        if not _does_collection_support_ansible_version(collection_meta.get('requires_ansible', ''), ansible_version):\n",
   "                display.warning(message)\n",
   "        display.warning('Error parsing collection metadata requires_ansible value from collection {0}: {1}'.format(collection_name, ex))\n",
   "        display.warning('packaging Python module unavailable; unable to validate collection Ansible version requirements')\n",
   "        display.warning('AnsibleCollectionFinder has already been configured')\n"
  ]
 },
 "112": {
  "name": "splitname",
  "type": "tuple",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/loader.py",
  "lineno": "678",
  "column": "16",
  "context": "S):\n                    continue\n\n                splitname = os.path.splitext(full_name)\n                base_name = splitname[0]\n         ",
  "context_lines": "                # FIXME: I believe this is only correct for modules and module_utils.\n                # For all other plugins we want .pyc and .pyo should be valid\n                if any(full_path.endswith(x) for x in C.MODULE_IGNORE_EXTS):\n                    continue\n\n                splitname = os.path.splitext(full_name)\n                base_name = splitname[0]\n                internal = path_context.internal\n                try:\n                    extension = splitname[1]\n",
  "slicing": [
   "    imp = None\n",
   "display = Display()\n",
   "get_with_context_result = namedtuple('get_with_context_result', ['object', 'plugin_load_context'])\n",
   "    return [(name, obj) for (name, obj) in globals().items() if isinstance(obj, PluginLoader)]\n",
   "    b_path = os.path.expanduser(to_bytes(path, errors='surrogate_or_strict'))\n",
   "    if os.path.isdir(b_path):\n",
   "        for name, obj in get_all_plugin_loaders():\n",
   "            if obj.subdir:\n",
   "                plugin_path = os.path.join(b_path, to_bytes(obj.subdir))\n",
   "                if os.path.isdir(plugin_path):\n",
   "                    obj.add_directory(to_text(plugin_path))\n",
   "        display.warning(\"Ignoring invalid path provided to plugin path: '%s' is not a directory\" % to_text(path))\n",
   "        shell_type = 'sh'\n",
   "                shell_filename = os.path.basename(executable)\n",
   "                    shell = shell_loader.get(shell_filename)\n",
   "                    shell = None\n",
   "                if shell is None:\n",
   "                    for shell in shell_loader.all():\n",
   "                        if shell_filename in shell.COMPATIBLE_SHELLS:\n",
   "                            shell_type = shell.SHELL_FAMILY\n",
   "    shell = shell_loader.get(shell_type)\n",
   "    if not shell:\n",
   "        raise AnsibleError(\"Could not find the shell plugin required (%s).\" % shell_type)\n",
   "        setattr(shell, 'executable', executable)\n",
   "    return shell\n",
   "    loader = getattr(sys.modules[__name__], '%s_loader' % which_loader)\n",
   "    for path in paths:\n",
   "        loader.add_directory(path, with_subdir=True)\n",
   "        self.path = path\n",
   "        warning_text = deprecation.get('warning_text', None)\n",
   "        removal_date = deprecation.get('removal_date', None)\n",
   "        removal_version = deprecation.get('removal_version', None)\n",
   "        if removal_date is not None:\n",
   "            removal_version = None\n",
   "        if not warning_text:\n",
   "            warning_text = '{0} has been deprecated'.format(name)\n",
   "        display.deprecated(warning_text, date=removal_date, version=removal_version, collection_name=collection_name)\n",
   "        if removal_date:\n",
   "            self.removal_date = removal_date\n",
   "        if removal_version:\n",
   "            self.removal_version = removal_version\n",
   "        self.deprecation_warnings.append(warning_text)\n",
   "        aliases = {} if aliases is None else aliases\n",
   "        self.aliases = aliases\n",
   "            config = [config]\n",
   "        elif not config:\n",
   "            config = []\n",
   "        self.config = config\n",
   "        class_name = data.get('class_name')\n",
   "        package = data.get('package')\n",
   "        config = data.get('config')\n",
   "        subdir = data.get('subdir')\n",
   "        aliases = data.get('aliases')\n",
   "        base_class = data.get('base_class')\n",
   "        PATH_CACHE[class_name] = data.get('PATH_CACHE')\n",
   "        PLUGIN_PATH_CACHE[class_name] = data.get('PLUGIN_PATH_CACHE')\n",
   "        self.__init__(class_name, package, config, subdir, aliases, base_class)\n",
   "        ret = []\n",
   "        for i in paths:\n",
   "            if i not in ret:\n",
   "                ret.append(i)\n",
   "        return os.pathsep.join(ret)\n",
   "        results = []\n",
   "        results.append(dir)\n",
   "        for root, subdirs, files in os.walk(dir, followlinks=True):\n",
   "            if '__init__.py' in files:\n",
   "                for x in subdirs:\n",
   "                    results.append(os.path.join(root, x))\n",
   "        return results\n",
   "            m = __import__(self.package)\n",
   "            parts = self.package.split('.')[1:]\n",
   "            for parent_mod in parts:\n",
   "                m = getattr(m, parent_mod)\n",
   "            self.package_path = os.path.dirname(m.__file__)\n",
   "        if subdirs:\n",
   "        ret = [PluginPathContext(p, False) for p in self._extra_dirs]\n",
   "            for path in self.config:\n",
   "                path = os.path.realpath(os.path.expanduser(path))\n",
   "                if subdirs:\n",
   "                    contents = glob.glob(\"%s/*\" % path) + glob.glob(\"%s/*/*\" % path)\n",
   "                    for c in contents:\n",
   "                        if os.path.isdir(c) and c not in ret:\n",
   "                            ret.append(PluginPathContext(c, False))\n",
   "                if path not in ret:\n",
   "                    ret.append(PluginPathContext(path, False))\n",
   "        ret.extend([PluginPathContext(p, True) for p in self._get_package_paths(subdirs=subdirs)])\n",
   "        ret.sort(key=lambda p: p.path.endswith('/windows'))\n",
   "        self._paths = ret\n",
   "        return ret\n",
   "        paths_with_context = self._get_paths_with_context(subdirs=subdirs)\n",
   "        return [path_with_context.path for path_with_context in paths_with_context]\n",
   "            type_name = get_plugin_class(self.class_name)\n",
   "            if type_name in C.CONFIGURABLE_PLUGINS:\n",
   "                dstring = AnsibleLoader(getattr(module, 'DOCUMENTATION', ''), file_name=path).get_single_data()\n",
   "                if dstring:\n",
   "                    add_fragments(dstring, path, fragment_loader=fragment_loader, is_module=(type_name == 'module'))\n",
   "                if dstring and 'options' in dstring and isinstance(dstring['options'], dict):\n",
   "                    C.config.initialize_plugin_configuration_definitions(type_name, name, dstring['options'])\n",
   "                    display.debug('Loaded config def from plugin (%s/%s)' % (type_name, name))\n",
   "        directory = os.path.realpath(directory)\n",
   "        if directory is not None:\n",
   "                directory = os.path.join(directory, self.subdir)\n",
   "            if directory not in self._extra_dirs:\n",
   "                self._extra_dirs.append(directory)\n",
   "                display.debug('Added %s to loader search path' % (directory))\n",
   "        collection_pkg = import_module(acr.n_python_collection_package_name)\n",
   "        if not collection_pkg:\n",
   "        collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "        if not collection_meta:\n",
   "            subdir_qualified_resource = '.'.join([acr.subdirs, acr.resource])\n",
   "            subdir_qualified_resource = acr.resource\n",
   "        entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource + extension, None)\n",
   "        if not entry:\n",
   "            entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource, None)\n",
   "        return entry\n",
   "        plugin_type = AnsibleCollectionRef.legacy_plugin_dir_to_plugin_type(self.subdir)\n",
   "        acr = AnsibleCollectionRef.from_fqcr(fq_name, plugin_type)\n",
   "        routing_metadata = self._query_collection_routing_meta(acr, plugin_type, extension=extension)\n",
   "        if routing_metadata:\n",
   "            deprecation = routing_metadata.get('deprecation', None)\n",
   "            plugin_load_context.record_deprecation(fq_name, deprecation, acr.collection)\n",
   "            tombstone = routing_metadata.get('tombstone', None)\n",
   "            if tombstone:\n",
   "                removal_date = tombstone.get('removal_date')\n",
   "                removal_version = tombstone.get('removal_version')\n",
   "                warning_text = tombstone.get('warning_text') or '{0} has been removed.'.format(fq_name)\n",
   "                removed_msg = display.get_deprecation_message(msg=warning_text, version=removal_version,\n",
   "                                                              date=removal_date, removed=True,\n",
   "                                                              collection_name=acr.collection)\n",
   "                plugin_load_context.removal_date = removal_date\n",
   "                plugin_load_context.removal_version = removal_version\n",
   "                plugin_load_context.exit_reason = removed_msg\n",
   "                raise AnsiblePluginRemovedError(removed_msg, plugin_load_context=plugin_load_context)\n",
   "            redirect = routing_metadata.get('redirect', None)\n",
   "            if redirect:\n",
   "                display.vv(\"redirecting (type: {0}) {1} to {2}\".format(plugin_type, fq_name, redirect))\n",
   "                return plugin_load_context.redirect(redirect)\n",
   "        n_resource = to_native(acr.resource, errors='strict')\n",
   "        full_name = '{0}.{1}'.format(acr.n_python_package_name, n_resource)\n",
   "            n_resource += extension\n",
   "        pkg = sys.modules.get(acr.n_python_package_name)\n",
   "        if not pkg:\n",
   "                pkg = import_module(acr.n_python_package_name)\n",
   "                return plugin_load_context.nope('Python package {0} not found'.format(acr.n_python_package_name))\n",
   "        pkg_path = os.path.dirname(pkg.__file__)\n",
   "        n_resource_path = os.path.join(pkg_path, n_resource)\n",
   "        if os.path.exists(n_resource_path):\n",
   "                full_name, to_text(n_resource_path), acr.collection, 'found exact match for {0} in {1}'.format(full_name, acr.collection))\n",
   "            return plugin_load_context.nope('no match for {0} in {1}'.format(to_text(n_resource), acr.collection))\n",
   "        found_files = [f\n",
   "                       for f in glob.iglob(os.path.join(pkg_path, n_resource) + '.*')\n",
   "                       if os.path.isfile(f) and not f.endswith(C.MODULE_IGNORE_EXTS)]\n",
   "        if not found_files:\n",
   "            return plugin_load_context.nope('failed fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        if len(found_files) > 1:\n",
   "            full_name, to_text(found_files[0]), acr.collection, 'found fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        result = self.find_plugin_with_context(name, mod_type, ignore_deprecated, check_aliases, collection_list)\n",
   "        if result.resolved and result.plugin_resolved_path:\n",
   "            return result.plugin_resolved_path\n",
   "        plugin_load_context = PluginLoadContext()\n",
   "        plugin_load_context.original_name = name\n",
   "            result = self._resolve_plugin_step(name, mod_type, ignore_deprecated, check_aliases, collection_list, plugin_load_context=plugin_load_context)\n",
   "            if result.pending_redirect:\n",
   "                if result.pending_redirect in result.redirect_list:\n",
   "                    raise AnsiblePluginCircularRedirect('plugin redirect loop resolving {0} (path: {1})'.format(result.original_name, result.redirect_list))\n",
   "                name = result.pending_redirect\n",
   "                result.pending_redirect = None\n",
   "                plugin_load_context = result\n",
   "        if plugin_load_context.error_list:\n",
   "            display.warning(\"errors were encountered during the plugin load for {0}:\\n{1}\".format(name, plugin_load_context.error_list))\n",
   "        return plugin_load_context\n",
   "        if not plugin_load_context:\n",
   "        plugin_load_context.redirect_list.append(name)\n",
   "        plugin_load_context.resolved = False\n",
   "        if name in _PLUGIN_FILTERS[self.package]:\n",
   "            plugin_load_context.exit_reason = '{0} matched a defined plugin filter'.format(name)\n",
   "            return plugin_load_context\n",
   "            suffix = mod_type\n",
   "            suffix = '.py'\n",
   "            suffix = ''\n",
   "        if (AnsibleCollectionRef.is_valid_fqcr(name) or collection_list) and not name.startswith('Ansible'):\n",
   "            if '.' in name or not collection_list:\n",
   "                candidates = [name]\n",
   "                candidates = ['{0}.{1}'.format(c, name) for c in collection_list]\n",
   "            for candidate_name in candidates:\n",
   "                    plugin_load_context.load_attempts.append(candidate_name)\n",
   "                    if candidate_name.startswith('ansible.legacy'):\n",
   "                        plugin_load_context = self._find_plugin_legacy(name.replace('ansible.legacy.', '', 1),\n",
   "                                                                       plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "                        plugin_load_context = self._find_fq_plugin(candidate_name, suffix, plugin_load_context=plugin_load_context)\n",
   "                        if candidate_name != plugin_load_context.original_name and candidate_name not in plugin_load_context.redirect_list:\n",
   "                            plugin_load_context.redirect_list.append(candidate_name)\n",
   "                    if plugin_load_context.resolved or plugin_load_context.pending_redirect:  # if we got an answer or need to chase down a redirect, return\n",
   "                        return plugin_load_context\n",
   "                    plugin_load_context.import_error_list.append(ie)\n",
   "                    plugin_load_context.error_list.append(to_native(ex))\n",
   "            if plugin_load_context.error_list:\n",
   "                display.debug(msg='plugin lookup for {0} failed; errors: {1}'.format(name, '; '.join(plugin_load_context.error_list)))\n",
   "            plugin_load_context.exit_reason = 'no matches found for {0}'.format(name)\n",
   "            return plugin_load_context\n",
   "        return self._find_plugin_legacy(name, plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "        plugin_load_context.resolved = False\n",
   "            name = self.aliases.get(name, name)\n",
   "        pull_cache = self._plugin_path_cache[suffix]\n",
   "            path_with_context = pull_cache[name]\n",
   "            plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "            plugin_load_context.plugin_resolved_name = name\n",
   "            plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "            plugin_load_context.resolved = True\n",
   "            return plugin_load_context\n",
   "        for path_context in (p for p in self._get_paths_with_context() if p.path not in self._searched_paths and os.path.isdir(p.path)):\n",
   "            path = path_context.path\n",
   "            display.debug('trying %s' % path)\n",
   "            plugin_load_context.load_attempts.append(path)\n",
   "                full_paths = (os.path.join(path, f) for f in os.listdir(path))\n",
   "                display.warning(\"Error accessing plugin paths: %s\" % to_text(e))\n",
   "            for full_path in (f for f in full_paths if os.path.isfile(f) and not f.endswith('__init__.py')):\n",
   "                full_name = os.path.basename(full_path)\n",
   "                if any(full_path.endswith(x) for x in C.MODULE_IGNORE_EXTS):\n",
   "                splitname = os.path.splitext(full_name)\n",
   "                base_name = splitname[0]\n",
   "                internal = path_context.internal\n",
   "                    extension = splitname[1]\n",
   "                    extension = ''\n",
   "                if base_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][full_name] = PluginPathContext(full_path, internal)\n",
   "                if base_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][full_name] = PluginPathContext(full_path, internal)\n",
   "            self._searched_paths.add(path)\n",
   "                path_with_context = pull_cache[name]\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        if not name.startswith('_'):\n",
   "            alias_name = '_' + name\n",
   "            if alias_name in pull_cache:\n",
   "                path_with_context = pull_cache[alias_name]\n",
   "                if not ignore_deprecated and not os.path.islink(path_with_context.path):\n",
   "                    display.deprecated('%s is kept for backwards compatibility but usage is discouraged. '  # pylint: disable=ansible-deprecated-no-version\n",
   "                                       'The module documentation details page may explain more about this rationale.' % name.lstrip('_'))\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = alias_name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        candidate_fqcr = 'ansible.builtin.{0}'.format(name)\n",
   "        if '.' not in name and AnsibleCollectionRef.is_valid_fqcr(candidate_fqcr):\n",
   "            return self._find_fq_plugin(fq_name=candidate_fqcr, extension=suffix, plugin_load_context=plugin_load_context)\n",
   "        return plugin_load_context.nope('{0} is not eligible for last-chance resolution'.format(name))\n",
   "            return self.find_plugin(name, collection_list=collection_list) is not None\n",
   "            display.debug('has_plugin error: {0}'.format(to_text(ex)))\n",
   "        if name.startswith('ansible_collections.'):\n",
   "            full_name = name\n",
   "            full_name = '.'.join([self.package, name])\n",
   "        if full_name in sys.modules:\n",
   "            return sys.modules[full_name]\n",
   "            if imp is None:\n",
   "                spec = importlib.util.spec_from_file_location(to_native(full_name), to_native(path))\n",
   "                module = importlib.util.module_from_spec(spec)\n",
   "                spec.loader.exec_module(module)\n",
   "                sys.modules[full_name] = module\n",
   "                with open(to_bytes(path), 'rb') as module_file:\n",
   "                    module = imp.load_source(to_native(full_name), to_native(path), module_file)\n",
   "        return module\n",
   "        setattr(obj, '_original_path', path)\n",
   "        setattr(obj, '_load_name', name)\n",
   "        setattr(obj, '_redirected_names', redirected_names or [])\n",
   "        return self.get_with_context(name, *args, **kwargs).object\n",
   "        found_in_cache = True\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        collection_list = kwargs.pop('collection_list', None)\n",
   "        if name in self.aliases:\n",
   "            name = self.aliases[name]\n",
   "        plugin_load_context = self.find_plugin_with_context(name, collection_list=collection_list)\n",
   "        if not plugin_load_context.resolved or not plugin_load_context.plugin_resolved_path:\n",
   "            return get_with_context_result(None, plugin_load_context)\n",
   "        name = plugin_load_context.plugin_resolved_name\n",
   "        path = plugin_load_context.plugin_resolved_path\n",
   "        redirected_names = plugin_load_context.redirect_list or []\n",
   "        if path not in self._module_cache:\n",
   "            self._module_cache[path] = self._load_module_source(name, path)\n",
   "            self._load_config_defs(name, self._module_cache[path], path)\n",
   "            found_in_cache = False\n",
   "        obj = getattr(self._module_cache[path], self.class_name)\n",
   "            module = __import__(self.package, fromlist=[self.base_class])\n",
   "                plugin_class = getattr(module, self.base_class)\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "            if not issubclass(obj, plugin_class):\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "        self._display_plugin_load(self.class_name, name, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "        if not class_only:\n",
   "                instance = object.__new__(obj)\n",
   "                self._update_object(instance, name, path, redirected_names)\n",
   "                obj.__init__(instance, *args, **kwargs)\n",
   "                obj = instance\n",
   "                    return get_with_context_result(None, plugin_load_context)\n",
   "        self._update_object(obj, name, path, redirected_names)\n",
   "        return get_with_context_result(obj, plugin_load_context)\n",
   "            msg = 'Loading %s \\'%s\\' from %s' % (class_name, os.path.basename(name), path)\n",
   "                msg = '%s (searched paths: %s)' % (msg, self.format_paths(searched_paths))\n",
   "            if found_in_cache or class_only:\n",
   "                msg = '%s (found_in_cache=%s, class_only=%s)' % (msg, found_in_cache, class_only)\n",
   "            display.debug(msg)\n",
   "        dedupe = kwargs.pop('_dedupe', True)\n",
   "        path_only = kwargs.pop('path_only', False)\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        if path_only and class_only:\n",
   "        all_matches = []\n",
   "        found_in_cache = True\n",
   "        for i in self._get_paths():\n",
   "            all_matches.extend(glob.glob(os.path.join(i, \"*.py\")))\n",
   "        loaded_modules = set()\n",
   "        for path in sorted(all_matches, key=os.path.basename):\n",
   "            name = os.path.splitext(path)[0]\n",
   "            basename = os.path.basename(name)\n",
   "            if basename == '__init__' or basename in _PLUGIN_FILTERS[self.package]:\n",
   "            if dedupe and basename in loaded_modules:\n",
   "            loaded_modules.add(basename)\n",
   "            if path_only:\n",
   "                yield path\n",
   "            if path not in self._module_cache:\n",
   "                        full_name = '{0}_{1}'.format(abs(hash(path)), basename)\n",
   "                        full_name = basename\n",
   "                    module = self._load_module_source(full_name, path)\n",
   "                    self._load_config_defs(basename, module, path)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                self._module_cache[path] = module\n",
   "                found_in_cache = False\n",
   "                obj = getattr(self._module_cache[path], self.class_name)\n",
   "                display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                module = __import__(self.package, fromlist=[self.base_class])\n",
   "                    plugin_class = getattr(module, self.base_class)\n",
   "                if not issubclass(obj, plugin_class):\n",
   "            self._display_plugin_load(self.class_name, basename, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "            if not class_only:\n",
   "                    obj = obj(*args, **kwargs)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be incomplete: %s\" % (path, to_text(e)))\n",
   "            self._update_object(obj, basename, path)\n",
   "            yield obj\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).find_plugin(name, collection_list=collection_list)\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).get(name, *args, **kwargs)\n",
   "        plugins = list(super(Jinja2Loader, self).all(*args, **kwargs))\n",
   "        plugins.reverse()\n",
   "        return plugins\n",
   "    filters = defaultdict(frozenset)\n",
   "        filter_cfg = '/etc/ansible/plugin_filters.yml'\n",
   "        filter_cfg = C.PLUGIN_FILTERS_CFG\n",
   "    if os.path.exists(filter_cfg):\n",
   "        with open(filter_cfg, 'rb') as f:\n",
   "                filter_data = from_yaml(f.read())\n",
   "                display.warning(u'The plugin filter file, {0} was not parsable.'\n",
   "                                u' Skipping: {1}'.format(filter_cfg, to_text(e)))\n",
   "                return filters\n",
   "            version = filter_data['filter_version']\n",
   "            display.warning(u'The plugin filter file, {0} was invalid.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "            return filters\n",
   "        version = to_text(version)\n",
   "        version = version.strip()\n",
   "        if version == u'1.0':\n",
   "                filters['ansible.modules'] = frozenset(filter_data['module_blacklist'])\n",
   "                display.warning(u'Unable to parse the plugin filter file {0} as'\n",
   "                                u' Skipping.'.format(filter_cfg))\n",
   "                return filters\n",
   "            filters['ansible.plugins.action'] = filters['ansible.modules']\n",
   "            display.warning(u'The plugin filter file, {0} was a version not recognized by this'\n",
   "                            u' version of Ansible. Skipping.'.format(filter_cfg))\n",
   "            display.warning(u'The plugin filter file, {0} does not exist.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "    if 'stat' in filters['ansible.modules']:\n",
   "                           ' from the blacklist.'.format(to_native(filter_cfg)))\n",
   "    return filters\n",
   "    display.vvvv(to_text('Loading collection {0} from {1}'.format(collection_name, collection_path)))\n",
   "        if not _does_collection_support_ansible_version(collection_meta.get('requires_ansible', ''), ansible_version):\n",
   "                display.warning(message)\n",
   "        display.warning('Error parsing collection metadata requires_ansible value from collection {0}: {1}'.format(collection_name, ex))\n",
   "        display.warning('packaging Python module unavailable; unable to validate collection Ansible version requirements')\n",
   "        display.warning('AnsibleCollectionFinder has already been configured')\n"
  ]
 },
 "113": {
  "name": "internal",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/loader.py",
  "lineno": "680",
  "column": "16",
  "context": "         base_name = splitname[0]\n                internal = path_context.internal\n                try:\n                    extension",
  "context_lines": "                if any(full_path.endswith(x) for x in C.MODULE_IGNORE_EXTS):\n                    continue\n\n                splitname = os.path.splitext(full_name)\n                base_name = splitname[0]\n                internal = path_context.internal\n                try:\n                    extension = splitname[1]\n                except IndexError:\n                    extension = ''\n\n",
  "slicing": [
   "    imp = None\n",
   "display = Display()\n",
   "get_with_context_result = namedtuple('get_with_context_result', ['object', 'plugin_load_context'])\n",
   "    return [(name, obj) for (name, obj) in globals().items() if isinstance(obj, PluginLoader)]\n",
   "    b_path = os.path.expanduser(to_bytes(path, errors='surrogate_or_strict'))\n",
   "    if os.path.isdir(b_path):\n",
   "        for name, obj in get_all_plugin_loaders():\n",
   "            if obj.subdir:\n",
   "                plugin_path = os.path.join(b_path, to_bytes(obj.subdir))\n",
   "                if os.path.isdir(plugin_path):\n",
   "                    obj.add_directory(to_text(plugin_path))\n",
   "        display.warning(\"Ignoring invalid path provided to plugin path: '%s' is not a directory\" % to_text(path))\n",
   "        shell_type = 'sh'\n",
   "                shell_filename = os.path.basename(executable)\n",
   "                    shell = shell_loader.get(shell_filename)\n",
   "                    shell = None\n",
   "                if shell is None:\n",
   "                    for shell in shell_loader.all():\n",
   "                        if shell_filename in shell.COMPATIBLE_SHELLS:\n",
   "                            shell_type = shell.SHELL_FAMILY\n",
   "    shell = shell_loader.get(shell_type)\n",
   "    if not shell:\n",
   "        raise AnsibleError(\"Could not find the shell plugin required (%s).\" % shell_type)\n",
   "        setattr(shell, 'executable', executable)\n",
   "    return shell\n",
   "    loader = getattr(sys.modules[__name__], '%s_loader' % which_loader)\n",
   "    for path in paths:\n",
   "        loader.add_directory(path, with_subdir=True)\n",
   "        self.path = path\n",
   "        warning_text = deprecation.get('warning_text', None)\n",
   "        removal_date = deprecation.get('removal_date', None)\n",
   "        removal_version = deprecation.get('removal_version', None)\n",
   "        if removal_date is not None:\n",
   "            removal_version = None\n",
   "        if not warning_text:\n",
   "            warning_text = '{0} has been deprecated'.format(name)\n",
   "        display.deprecated(warning_text, date=removal_date, version=removal_version, collection_name=collection_name)\n",
   "        if removal_date:\n",
   "            self.removal_date = removal_date\n",
   "        if removal_version:\n",
   "            self.removal_version = removal_version\n",
   "        self.deprecation_warnings.append(warning_text)\n",
   "        aliases = {} if aliases is None else aliases\n",
   "        self.aliases = aliases\n",
   "            config = [config]\n",
   "        elif not config:\n",
   "            config = []\n",
   "        self.config = config\n",
   "        class_name = data.get('class_name')\n",
   "        package = data.get('package')\n",
   "        config = data.get('config')\n",
   "        subdir = data.get('subdir')\n",
   "        aliases = data.get('aliases')\n",
   "        base_class = data.get('base_class')\n",
   "        PATH_CACHE[class_name] = data.get('PATH_CACHE')\n",
   "        PLUGIN_PATH_CACHE[class_name] = data.get('PLUGIN_PATH_CACHE')\n",
   "        self.__init__(class_name, package, config, subdir, aliases, base_class)\n",
   "        ret = []\n",
   "        for i in paths:\n",
   "            if i not in ret:\n",
   "                ret.append(i)\n",
   "        return os.pathsep.join(ret)\n",
   "        results = []\n",
   "        results.append(dir)\n",
   "        for root, subdirs, files in os.walk(dir, followlinks=True):\n",
   "            if '__init__.py' in files:\n",
   "                for x in subdirs:\n",
   "                    results.append(os.path.join(root, x))\n",
   "        return results\n",
   "            m = __import__(self.package)\n",
   "            parts = self.package.split('.')[1:]\n",
   "            for parent_mod in parts:\n",
   "                m = getattr(m, parent_mod)\n",
   "            self.package_path = os.path.dirname(m.__file__)\n",
   "        if subdirs:\n",
   "        ret = [PluginPathContext(p, False) for p in self._extra_dirs]\n",
   "            for path in self.config:\n",
   "                path = os.path.realpath(os.path.expanduser(path))\n",
   "                if subdirs:\n",
   "                    contents = glob.glob(\"%s/*\" % path) + glob.glob(\"%s/*/*\" % path)\n",
   "                    for c in contents:\n",
   "                        if os.path.isdir(c) and c not in ret:\n",
   "                            ret.append(PluginPathContext(c, False))\n",
   "                if path not in ret:\n",
   "                    ret.append(PluginPathContext(path, False))\n",
   "        ret.extend([PluginPathContext(p, True) for p in self._get_package_paths(subdirs=subdirs)])\n",
   "        ret.sort(key=lambda p: p.path.endswith('/windows'))\n",
   "        self._paths = ret\n",
   "        return ret\n",
   "        paths_with_context = self._get_paths_with_context(subdirs=subdirs)\n",
   "        return [path_with_context.path for path_with_context in paths_with_context]\n",
   "            type_name = get_plugin_class(self.class_name)\n",
   "            if type_name in C.CONFIGURABLE_PLUGINS:\n",
   "                dstring = AnsibleLoader(getattr(module, 'DOCUMENTATION', ''), file_name=path).get_single_data()\n",
   "                if dstring:\n",
   "                    add_fragments(dstring, path, fragment_loader=fragment_loader, is_module=(type_name == 'module'))\n",
   "                if dstring and 'options' in dstring and isinstance(dstring['options'], dict):\n",
   "                    C.config.initialize_plugin_configuration_definitions(type_name, name, dstring['options'])\n",
   "                    display.debug('Loaded config def from plugin (%s/%s)' % (type_name, name))\n",
   "        directory = os.path.realpath(directory)\n",
   "        if directory is not None:\n",
   "                directory = os.path.join(directory, self.subdir)\n",
   "            if directory not in self._extra_dirs:\n",
   "                self._extra_dirs.append(directory)\n",
   "                display.debug('Added %s to loader search path' % (directory))\n",
   "        collection_pkg = import_module(acr.n_python_collection_package_name)\n",
   "        if not collection_pkg:\n",
   "        collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "        if not collection_meta:\n",
   "            subdir_qualified_resource = '.'.join([acr.subdirs, acr.resource])\n",
   "            subdir_qualified_resource = acr.resource\n",
   "        entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource + extension, None)\n",
   "        if not entry:\n",
   "            entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource, None)\n",
   "        return entry\n",
   "        plugin_type = AnsibleCollectionRef.legacy_plugin_dir_to_plugin_type(self.subdir)\n",
   "        acr = AnsibleCollectionRef.from_fqcr(fq_name, plugin_type)\n",
   "        routing_metadata = self._query_collection_routing_meta(acr, plugin_type, extension=extension)\n",
   "        if routing_metadata:\n",
   "            deprecation = routing_metadata.get('deprecation', None)\n",
   "            plugin_load_context.record_deprecation(fq_name, deprecation, acr.collection)\n",
   "            tombstone = routing_metadata.get('tombstone', None)\n",
   "            if tombstone:\n",
   "                removal_date = tombstone.get('removal_date')\n",
   "                removal_version = tombstone.get('removal_version')\n",
   "                warning_text = tombstone.get('warning_text') or '{0} has been removed.'.format(fq_name)\n",
   "                removed_msg = display.get_deprecation_message(msg=warning_text, version=removal_version,\n",
   "                                                              date=removal_date, removed=True,\n",
   "                                                              collection_name=acr.collection)\n",
   "                plugin_load_context.removal_date = removal_date\n",
   "                plugin_load_context.removal_version = removal_version\n",
   "                plugin_load_context.exit_reason = removed_msg\n",
   "                raise AnsiblePluginRemovedError(removed_msg, plugin_load_context=plugin_load_context)\n",
   "            redirect = routing_metadata.get('redirect', None)\n",
   "            if redirect:\n",
   "                display.vv(\"redirecting (type: {0}) {1} to {2}\".format(plugin_type, fq_name, redirect))\n",
   "                return plugin_load_context.redirect(redirect)\n",
   "        n_resource = to_native(acr.resource, errors='strict')\n",
   "        full_name = '{0}.{1}'.format(acr.n_python_package_name, n_resource)\n",
   "            n_resource += extension\n",
   "        pkg = sys.modules.get(acr.n_python_package_name)\n",
   "        if not pkg:\n",
   "                pkg = import_module(acr.n_python_package_name)\n",
   "                return plugin_load_context.nope('Python package {0} not found'.format(acr.n_python_package_name))\n",
   "        pkg_path = os.path.dirname(pkg.__file__)\n",
   "        n_resource_path = os.path.join(pkg_path, n_resource)\n",
   "        if os.path.exists(n_resource_path):\n",
   "                full_name, to_text(n_resource_path), acr.collection, 'found exact match for {0} in {1}'.format(full_name, acr.collection))\n",
   "            return plugin_load_context.nope('no match for {0} in {1}'.format(to_text(n_resource), acr.collection))\n",
   "        found_files = [f\n",
   "                       for f in glob.iglob(os.path.join(pkg_path, n_resource) + '.*')\n",
   "                       if os.path.isfile(f) and not f.endswith(C.MODULE_IGNORE_EXTS)]\n",
   "        if not found_files:\n",
   "            return plugin_load_context.nope('failed fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        if len(found_files) > 1:\n",
   "            full_name, to_text(found_files[0]), acr.collection, 'found fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        result = self.find_plugin_with_context(name, mod_type, ignore_deprecated, check_aliases, collection_list)\n",
   "        if result.resolved and result.plugin_resolved_path:\n",
   "            return result.plugin_resolved_path\n",
   "        plugin_load_context = PluginLoadContext()\n",
   "        plugin_load_context.original_name = name\n",
   "            result = self._resolve_plugin_step(name, mod_type, ignore_deprecated, check_aliases, collection_list, plugin_load_context=plugin_load_context)\n",
   "            if result.pending_redirect:\n",
   "                if result.pending_redirect in result.redirect_list:\n",
   "                    raise AnsiblePluginCircularRedirect('plugin redirect loop resolving {0} (path: {1})'.format(result.original_name, result.redirect_list))\n",
   "                name = result.pending_redirect\n",
   "                result.pending_redirect = None\n",
   "                plugin_load_context = result\n",
   "        if plugin_load_context.error_list:\n",
   "            display.warning(\"errors were encountered during the plugin load for {0}:\\n{1}\".format(name, plugin_load_context.error_list))\n",
   "        return plugin_load_context\n",
   "        if not plugin_load_context:\n",
   "        plugin_load_context.redirect_list.append(name)\n",
   "        plugin_load_context.resolved = False\n",
   "        if name in _PLUGIN_FILTERS[self.package]:\n",
   "            plugin_load_context.exit_reason = '{0} matched a defined plugin filter'.format(name)\n",
   "            return plugin_load_context\n",
   "            suffix = mod_type\n",
   "            suffix = '.py'\n",
   "            suffix = ''\n",
   "        if (AnsibleCollectionRef.is_valid_fqcr(name) or collection_list) and not name.startswith('Ansible'):\n",
   "            if '.' in name or not collection_list:\n",
   "                candidates = [name]\n",
   "                candidates = ['{0}.{1}'.format(c, name) for c in collection_list]\n",
   "            for candidate_name in candidates:\n",
   "                    plugin_load_context.load_attempts.append(candidate_name)\n",
   "                    if candidate_name.startswith('ansible.legacy'):\n",
   "                        plugin_load_context = self._find_plugin_legacy(name.replace('ansible.legacy.', '', 1),\n",
   "                                                                       plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "                        plugin_load_context = self._find_fq_plugin(candidate_name, suffix, plugin_load_context=plugin_load_context)\n",
   "                        if candidate_name != plugin_load_context.original_name and candidate_name not in plugin_load_context.redirect_list:\n",
   "                            plugin_load_context.redirect_list.append(candidate_name)\n",
   "                    if plugin_load_context.resolved or plugin_load_context.pending_redirect:  # if we got an answer or need to chase down a redirect, return\n",
   "                        return plugin_load_context\n",
   "                    plugin_load_context.import_error_list.append(ie)\n",
   "                    plugin_load_context.error_list.append(to_native(ex))\n",
   "            if plugin_load_context.error_list:\n",
   "                display.debug(msg='plugin lookup for {0} failed; errors: {1}'.format(name, '; '.join(plugin_load_context.error_list)))\n",
   "            plugin_load_context.exit_reason = 'no matches found for {0}'.format(name)\n",
   "            return plugin_load_context\n",
   "        return self._find_plugin_legacy(name, plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "        plugin_load_context.resolved = False\n",
   "            name = self.aliases.get(name, name)\n",
   "        pull_cache = self._plugin_path_cache[suffix]\n",
   "            path_with_context = pull_cache[name]\n",
   "            plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "            plugin_load_context.plugin_resolved_name = name\n",
   "            plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "            plugin_load_context.resolved = True\n",
   "            return plugin_load_context\n",
   "        for path_context in (p for p in self._get_paths_with_context() if p.path not in self._searched_paths and os.path.isdir(p.path)):\n",
   "            path = path_context.path\n",
   "            display.debug('trying %s' % path)\n",
   "            plugin_load_context.load_attempts.append(path)\n",
   "                full_paths = (os.path.join(path, f) for f in os.listdir(path))\n",
   "                display.warning(\"Error accessing plugin paths: %s\" % to_text(e))\n",
   "            for full_path in (f for f in full_paths if os.path.isfile(f) and not f.endswith('__init__.py')):\n",
   "                full_name = os.path.basename(full_path)\n",
   "                if any(full_path.endswith(x) for x in C.MODULE_IGNORE_EXTS):\n",
   "                splitname = os.path.splitext(full_name)\n",
   "                base_name = splitname[0]\n",
   "                internal = path_context.internal\n",
   "                    extension = splitname[1]\n",
   "                    extension = ''\n",
   "                if base_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][full_name] = PluginPathContext(full_path, internal)\n",
   "                if base_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][full_name] = PluginPathContext(full_path, internal)\n",
   "            self._searched_paths.add(path)\n",
   "                path_with_context = pull_cache[name]\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        if not name.startswith('_'):\n",
   "            alias_name = '_' + name\n",
   "            if alias_name in pull_cache:\n",
   "                path_with_context = pull_cache[alias_name]\n",
   "                if not ignore_deprecated and not os.path.islink(path_with_context.path):\n",
   "                    display.deprecated('%s is kept for backwards compatibility but usage is discouraged. '  # pylint: disable=ansible-deprecated-no-version\n",
   "                                       'The module documentation details page may explain more about this rationale.' % name.lstrip('_'))\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = alias_name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        candidate_fqcr = 'ansible.builtin.{0}'.format(name)\n",
   "        if '.' not in name and AnsibleCollectionRef.is_valid_fqcr(candidate_fqcr):\n",
   "            return self._find_fq_plugin(fq_name=candidate_fqcr, extension=suffix, plugin_load_context=plugin_load_context)\n",
   "        return plugin_load_context.nope('{0} is not eligible for last-chance resolution'.format(name))\n",
   "            return self.find_plugin(name, collection_list=collection_list) is not None\n",
   "            display.debug('has_plugin error: {0}'.format(to_text(ex)))\n",
   "        if name.startswith('ansible_collections.'):\n",
   "            full_name = name\n",
   "            full_name = '.'.join([self.package, name])\n",
   "        if full_name in sys.modules:\n",
   "            return sys.modules[full_name]\n",
   "            if imp is None:\n",
   "                spec = importlib.util.spec_from_file_location(to_native(full_name), to_native(path))\n",
   "                module = importlib.util.module_from_spec(spec)\n",
   "                spec.loader.exec_module(module)\n",
   "                sys.modules[full_name] = module\n",
   "                with open(to_bytes(path), 'rb') as module_file:\n",
   "                    module = imp.load_source(to_native(full_name), to_native(path), module_file)\n",
   "        return module\n",
   "        setattr(obj, '_original_path', path)\n",
   "        setattr(obj, '_load_name', name)\n",
   "        setattr(obj, '_redirected_names', redirected_names or [])\n",
   "        return self.get_with_context(name, *args, **kwargs).object\n",
   "        found_in_cache = True\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        collection_list = kwargs.pop('collection_list', None)\n",
   "        if name in self.aliases:\n",
   "            name = self.aliases[name]\n",
   "        plugin_load_context = self.find_plugin_with_context(name, collection_list=collection_list)\n",
   "        if not plugin_load_context.resolved or not plugin_load_context.plugin_resolved_path:\n",
   "            return get_with_context_result(None, plugin_load_context)\n",
   "        name = plugin_load_context.plugin_resolved_name\n",
   "        path = plugin_load_context.plugin_resolved_path\n",
   "        redirected_names = plugin_load_context.redirect_list or []\n",
   "        if path not in self._module_cache:\n",
   "            self._module_cache[path] = self._load_module_source(name, path)\n",
   "            self._load_config_defs(name, self._module_cache[path], path)\n",
   "            found_in_cache = False\n",
   "        obj = getattr(self._module_cache[path], self.class_name)\n",
   "            module = __import__(self.package, fromlist=[self.base_class])\n",
   "                plugin_class = getattr(module, self.base_class)\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "            if not issubclass(obj, plugin_class):\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "        self._display_plugin_load(self.class_name, name, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "        if not class_only:\n",
   "                instance = object.__new__(obj)\n",
   "                self._update_object(instance, name, path, redirected_names)\n",
   "                obj.__init__(instance, *args, **kwargs)\n",
   "                obj = instance\n",
   "                    return get_with_context_result(None, plugin_load_context)\n",
   "        self._update_object(obj, name, path, redirected_names)\n",
   "        return get_with_context_result(obj, plugin_load_context)\n",
   "            msg = 'Loading %s \\'%s\\' from %s' % (class_name, os.path.basename(name), path)\n",
   "                msg = '%s (searched paths: %s)' % (msg, self.format_paths(searched_paths))\n",
   "            if found_in_cache or class_only:\n",
   "                msg = '%s (found_in_cache=%s, class_only=%s)' % (msg, found_in_cache, class_only)\n",
   "            display.debug(msg)\n",
   "        dedupe = kwargs.pop('_dedupe', True)\n",
   "        path_only = kwargs.pop('path_only', False)\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        if path_only and class_only:\n",
   "        all_matches = []\n",
   "        found_in_cache = True\n",
   "        for i in self._get_paths():\n",
   "            all_matches.extend(glob.glob(os.path.join(i, \"*.py\")))\n",
   "        loaded_modules = set()\n",
   "        for path in sorted(all_matches, key=os.path.basename):\n",
   "            name = os.path.splitext(path)[0]\n",
   "            basename = os.path.basename(name)\n",
   "            if basename == '__init__' or basename in _PLUGIN_FILTERS[self.package]:\n",
   "            if dedupe and basename in loaded_modules:\n",
   "            loaded_modules.add(basename)\n",
   "            if path_only:\n",
   "                yield path\n",
   "            if path not in self._module_cache:\n",
   "                        full_name = '{0}_{1}'.format(abs(hash(path)), basename)\n",
   "                        full_name = basename\n",
   "                    module = self._load_module_source(full_name, path)\n",
   "                    self._load_config_defs(basename, module, path)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                self._module_cache[path] = module\n",
   "                found_in_cache = False\n",
   "                obj = getattr(self._module_cache[path], self.class_name)\n",
   "                display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                module = __import__(self.package, fromlist=[self.base_class])\n",
   "                    plugin_class = getattr(module, self.base_class)\n",
   "                if not issubclass(obj, plugin_class):\n",
   "            self._display_plugin_load(self.class_name, basename, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "            if not class_only:\n",
   "                    obj = obj(*args, **kwargs)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be incomplete: %s\" % (path, to_text(e)))\n",
   "            self._update_object(obj, basename, path)\n",
   "            yield obj\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).find_plugin(name, collection_list=collection_list)\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).get(name, *args, **kwargs)\n",
   "        plugins = list(super(Jinja2Loader, self).all(*args, **kwargs))\n",
   "        plugins.reverse()\n",
   "        return plugins\n",
   "    filters = defaultdict(frozenset)\n",
   "        filter_cfg = '/etc/ansible/plugin_filters.yml'\n",
   "        filter_cfg = C.PLUGIN_FILTERS_CFG\n",
   "    if os.path.exists(filter_cfg):\n",
   "        with open(filter_cfg, 'rb') as f:\n",
   "                filter_data = from_yaml(f.read())\n",
   "                display.warning(u'The plugin filter file, {0} was not parsable.'\n",
   "                                u' Skipping: {1}'.format(filter_cfg, to_text(e)))\n",
   "                return filters\n",
   "            version = filter_data['filter_version']\n",
   "            display.warning(u'The plugin filter file, {0} was invalid.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "            return filters\n",
   "        version = to_text(version)\n",
   "        version = version.strip()\n",
   "        if version == u'1.0':\n",
   "                filters['ansible.modules'] = frozenset(filter_data['module_blacklist'])\n",
   "                display.warning(u'Unable to parse the plugin filter file {0} as'\n",
   "                                u' Skipping.'.format(filter_cfg))\n",
   "                return filters\n",
   "            filters['ansible.plugins.action'] = filters['ansible.modules']\n",
   "            display.warning(u'The plugin filter file, {0} was a version not recognized by this'\n",
   "                            u' version of Ansible. Skipping.'.format(filter_cfg))\n",
   "            display.warning(u'The plugin filter file, {0} does not exist.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "    if 'stat' in filters['ansible.modules']:\n",
   "                           ' from the blacklist.'.format(to_native(filter_cfg)))\n",
   "    return filters\n",
   "    display.vvvv(to_text('Loading collection {0} from {1}'.format(collection_name, collection_path)))\n",
   "        if not _does_collection_support_ansible_version(collection_meta.get('requires_ansible', ''), ansible_version):\n",
   "                display.warning(message)\n",
   "        display.warning('Error parsing collection metadata requires_ansible value from collection {0}: {1}'.format(collection_name, ex))\n",
   "        display.warning('packaging Python module unavailable; unable to validate collection Ansible version requirements')\n",
   "        display.warning('AnsibleCollectionFinder has already been configured')\n"
  ]
 },
 "114": {
  "name": "__contains__",
  "type": "function",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/plugins/loader.py",
  "lineno": "745",
  "column": "4",
  "context": "'has_plugin error: {0}'.format(to_text(ex)))\n\n    __contains__ = has_plugin\n\n    def _load_module_source(self, name, path):\n\n ",
  "context_lines": "            if isinstance(ex, AnsibleError):\n                raise\n            # log and continue, likely an innocuous type/package loading failure in collections import\n            display.debug('has_plugin error: {0}'.format(to_text(ex)))\n\n    __contains__ = has_plugin\n\n    def _load_module_source(self, name, path):\n\n        # avoid collisions across plugins\n        if name.startswith('ansible_collections.'):\n",
  "slicing": [
   "    imp = None\n",
   "display = Display()\n",
   "get_with_context_result = namedtuple('get_with_context_result', ['object', 'plugin_load_context'])\n",
   "    return [(name, obj) for (name, obj) in globals().items() if isinstance(obj, PluginLoader)]\n",
   "    b_path = os.path.expanduser(to_bytes(path, errors='surrogate_or_strict'))\n",
   "    if os.path.isdir(b_path):\n",
   "        for name, obj in get_all_plugin_loaders():\n",
   "            if obj.subdir:\n",
   "                plugin_path = os.path.join(b_path, to_bytes(obj.subdir))\n",
   "                if os.path.isdir(plugin_path):\n",
   "                    obj.add_directory(to_text(plugin_path))\n",
   "        display.warning(\"Ignoring invalid path provided to plugin path: '%s' is not a directory\" % to_text(path))\n",
   "        shell_type = 'sh'\n",
   "                shell_filename = os.path.basename(executable)\n",
   "                    shell = shell_loader.get(shell_filename)\n",
   "                    shell = None\n",
   "                if shell is None:\n",
   "                    for shell in shell_loader.all():\n",
   "                        if shell_filename in shell.COMPATIBLE_SHELLS:\n",
   "                            shell_type = shell.SHELL_FAMILY\n",
   "    shell = shell_loader.get(shell_type)\n",
   "    if not shell:\n",
   "        raise AnsibleError(\"Could not find the shell plugin required (%s).\" % shell_type)\n",
   "        setattr(shell, 'executable', executable)\n",
   "    return shell\n",
   "    loader = getattr(sys.modules[__name__], '%s_loader' % which_loader)\n",
   "    for path in paths:\n",
   "        loader.add_directory(path, with_subdir=True)\n",
   "        self.path = path\n",
   "        warning_text = deprecation.get('warning_text', None)\n",
   "        removal_date = deprecation.get('removal_date', None)\n",
   "        removal_version = deprecation.get('removal_version', None)\n",
   "        if removal_date is not None:\n",
   "            removal_version = None\n",
   "        if not warning_text:\n",
   "            warning_text = '{0} has been deprecated'.format(name)\n",
   "        display.deprecated(warning_text, date=removal_date, version=removal_version, collection_name=collection_name)\n",
   "        if removal_date:\n",
   "            self.removal_date = removal_date\n",
   "        if removal_version:\n",
   "            self.removal_version = removal_version\n",
   "        self.deprecation_warnings.append(warning_text)\n",
   "        aliases = {} if aliases is None else aliases\n",
   "        self.aliases = aliases\n",
   "            config = [config]\n",
   "        elif not config:\n",
   "            config = []\n",
   "        self.config = config\n",
   "        class_name = data.get('class_name')\n",
   "        package = data.get('package')\n",
   "        config = data.get('config')\n",
   "        subdir = data.get('subdir')\n",
   "        aliases = data.get('aliases')\n",
   "        base_class = data.get('base_class')\n",
   "        PATH_CACHE[class_name] = data.get('PATH_CACHE')\n",
   "        PLUGIN_PATH_CACHE[class_name] = data.get('PLUGIN_PATH_CACHE')\n",
   "        self.__init__(class_name, package, config, subdir, aliases, base_class)\n",
   "        ret = []\n",
   "        for i in paths:\n",
   "            if i not in ret:\n",
   "                ret.append(i)\n",
   "        return os.pathsep.join(ret)\n",
   "        results = []\n",
   "        results.append(dir)\n",
   "        for root, subdirs, files in os.walk(dir, followlinks=True):\n",
   "            if '__init__.py' in files:\n",
   "                for x in subdirs:\n",
   "                    results.append(os.path.join(root, x))\n",
   "        return results\n",
   "            m = __import__(self.package)\n",
   "            parts = self.package.split('.')[1:]\n",
   "            for parent_mod in parts:\n",
   "                m = getattr(m, parent_mod)\n",
   "            self.package_path = os.path.dirname(m.__file__)\n",
   "        if subdirs:\n",
   "        ret = [PluginPathContext(p, False) for p in self._extra_dirs]\n",
   "            for path in self.config:\n",
   "                path = os.path.realpath(os.path.expanduser(path))\n",
   "                if subdirs:\n",
   "                    contents = glob.glob(\"%s/*\" % path) + glob.glob(\"%s/*/*\" % path)\n",
   "                    for c in contents:\n",
   "                        if os.path.isdir(c) and c not in ret:\n",
   "                            ret.append(PluginPathContext(c, False))\n",
   "                if path not in ret:\n",
   "                    ret.append(PluginPathContext(path, False))\n",
   "        ret.extend([PluginPathContext(p, True) for p in self._get_package_paths(subdirs=subdirs)])\n",
   "        ret.sort(key=lambda p: p.path.endswith('/windows'))\n",
   "        self._paths = ret\n",
   "        return ret\n",
   "        paths_with_context = self._get_paths_with_context(subdirs=subdirs)\n",
   "        return [path_with_context.path for path_with_context in paths_with_context]\n",
   "            type_name = get_plugin_class(self.class_name)\n",
   "            if type_name in C.CONFIGURABLE_PLUGINS:\n",
   "                dstring = AnsibleLoader(getattr(module, 'DOCUMENTATION', ''), file_name=path).get_single_data()\n",
   "                if dstring:\n",
   "                    add_fragments(dstring, path, fragment_loader=fragment_loader, is_module=(type_name == 'module'))\n",
   "                if dstring and 'options' in dstring and isinstance(dstring['options'], dict):\n",
   "                    C.config.initialize_plugin_configuration_definitions(type_name, name, dstring['options'])\n",
   "                    display.debug('Loaded config def from plugin (%s/%s)' % (type_name, name))\n",
   "        directory = os.path.realpath(directory)\n",
   "        if directory is not None:\n",
   "                directory = os.path.join(directory, self.subdir)\n",
   "            if directory not in self._extra_dirs:\n",
   "                self._extra_dirs.append(directory)\n",
   "                display.debug('Added %s to loader search path' % (directory))\n",
   "        collection_pkg = import_module(acr.n_python_collection_package_name)\n",
   "        if not collection_pkg:\n",
   "        collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "        if not collection_meta:\n",
   "            subdir_qualified_resource = '.'.join([acr.subdirs, acr.resource])\n",
   "            subdir_qualified_resource = acr.resource\n",
   "        entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource + extension, None)\n",
   "        if not entry:\n",
   "            entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource, None)\n",
   "        return entry\n",
   "        plugin_type = AnsibleCollectionRef.legacy_plugin_dir_to_plugin_type(self.subdir)\n",
   "        acr = AnsibleCollectionRef.from_fqcr(fq_name, plugin_type)\n",
   "        routing_metadata = self._query_collection_routing_meta(acr, plugin_type, extension=extension)\n",
   "        if routing_metadata:\n",
   "            deprecation = routing_metadata.get('deprecation', None)\n",
   "            plugin_load_context.record_deprecation(fq_name, deprecation, acr.collection)\n",
   "            tombstone = routing_metadata.get('tombstone', None)\n",
   "            if tombstone:\n",
   "                removal_date = tombstone.get('removal_date')\n",
   "                removal_version = tombstone.get('removal_version')\n",
   "                warning_text = tombstone.get('warning_text') or '{0} has been removed.'.format(fq_name)\n",
   "                removed_msg = display.get_deprecation_message(msg=warning_text, version=removal_version,\n",
   "                                                              date=removal_date, removed=True,\n",
   "                                                              collection_name=acr.collection)\n",
   "                plugin_load_context.removal_date = removal_date\n",
   "                plugin_load_context.removal_version = removal_version\n",
   "                plugin_load_context.exit_reason = removed_msg\n",
   "                raise AnsiblePluginRemovedError(removed_msg, plugin_load_context=plugin_load_context)\n",
   "            redirect = routing_metadata.get('redirect', None)\n",
   "            if redirect:\n",
   "                display.vv(\"redirecting (type: {0}) {1} to {2}\".format(plugin_type, fq_name, redirect))\n",
   "                return plugin_load_context.redirect(redirect)\n",
   "        n_resource = to_native(acr.resource, errors='strict')\n",
   "        full_name = '{0}.{1}'.format(acr.n_python_package_name, n_resource)\n",
   "            n_resource += extension\n",
   "        pkg = sys.modules.get(acr.n_python_package_name)\n",
   "        if not pkg:\n",
   "                pkg = import_module(acr.n_python_package_name)\n",
   "                return plugin_load_context.nope('Python package {0} not found'.format(acr.n_python_package_name))\n",
   "        pkg_path = os.path.dirname(pkg.__file__)\n",
   "        n_resource_path = os.path.join(pkg_path, n_resource)\n",
   "        if os.path.exists(n_resource_path):\n",
   "                full_name, to_text(n_resource_path), acr.collection, 'found exact match for {0} in {1}'.format(full_name, acr.collection))\n",
   "            return plugin_load_context.nope('no match for {0} in {1}'.format(to_text(n_resource), acr.collection))\n",
   "        found_files = [f\n",
   "                       for f in glob.iglob(os.path.join(pkg_path, n_resource) + '.*')\n",
   "                       if os.path.isfile(f) and not f.endswith(C.MODULE_IGNORE_EXTS)]\n",
   "        if not found_files:\n",
   "            return plugin_load_context.nope('failed fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        if len(found_files) > 1:\n",
   "            full_name, to_text(found_files[0]), acr.collection, 'found fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        result = self.find_plugin_with_context(name, mod_type, ignore_deprecated, check_aliases, collection_list)\n",
   "        if result.resolved and result.plugin_resolved_path:\n",
   "            return result.plugin_resolved_path\n",
   "        plugin_load_context = PluginLoadContext()\n",
   "        plugin_load_context.original_name = name\n",
   "            result = self._resolve_plugin_step(name, mod_type, ignore_deprecated, check_aliases, collection_list, plugin_load_context=plugin_load_context)\n",
   "            if result.pending_redirect:\n",
   "                if result.pending_redirect in result.redirect_list:\n",
   "                    raise AnsiblePluginCircularRedirect('plugin redirect loop resolving {0} (path: {1})'.format(result.original_name, result.redirect_list))\n",
   "                name = result.pending_redirect\n",
   "                result.pending_redirect = None\n",
   "                plugin_load_context = result\n",
   "        if plugin_load_context.error_list:\n",
   "            display.warning(\"errors were encountered during the plugin load for {0}:\\n{1}\".format(name, plugin_load_context.error_list))\n",
   "        return plugin_load_context\n",
   "        if not plugin_load_context:\n",
   "        plugin_load_context.redirect_list.append(name)\n",
   "        plugin_load_context.resolved = False\n",
   "        if name in _PLUGIN_FILTERS[self.package]:\n",
   "            plugin_load_context.exit_reason = '{0} matched a defined plugin filter'.format(name)\n",
   "            return plugin_load_context\n",
   "            suffix = mod_type\n",
   "            suffix = '.py'\n",
   "            suffix = ''\n",
   "        if (AnsibleCollectionRef.is_valid_fqcr(name) or collection_list) and not name.startswith('Ansible'):\n",
   "            if '.' in name or not collection_list:\n",
   "                candidates = [name]\n",
   "                candidates = ['{0}.{1}'.format(c, name) for c in collection_list]\n",
   "            for candidate_name in candidates:\n",
   "                    plugin_load_context.load_attempts.append(candidate_name)\n",
   "                    if candidate_name.startswith('ansible.legacy'):\n",
   "                        plugin_load_context = self._find_plugin_legacy(name.replace('ansible.legacy.', '', 1),\n",
   "                                                                       plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "                        plugin_load_context = self._find_fq_plugin(candidate_name, suffix, plugin_load_context=plugin_load_context)\n",
   "                        if candidate_name != plugin_load_context.original_name and candidate_name not in plugin_load_context.redirect_list:\n",
   "                            plugin_load_context.redirect_list.append(candidate_name)\n",
   "                    if plugin_load_context.resolved or plugin_load_context.pending_redirect:  # if we got an answer or need to chase down a redirect, return\n",
   "                        return plugin_load_context\n",
   "                    plugin_load_context.import_error_list.append(ie)\n",
   "                    plugin_load_context.error_list.append(to_native(ex))\n",
   "            if plugin_load_context.error_list:\n",
   "                display.debug(msg='plugin lookup for {0} failed; errors: {1}'.format(name, '; '.join(plugin_load_context.error_list)))\n",
   "            plugin_load_context.exit_reason = 'no matches found for {0}'.format(name)\n",
   "            return plugin_load_context\n",
   "        return self._find_plugin_legacy(name, plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "        plugin_load_context.resolved = False\n",
   "            name = self.aliases.get(name, name)\n",
   "        pull_cache = self._plugin_path_cache[suffix]\n",
   "            path_with_context = pull_cache[name]\n",
   "            plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "            plugin_load_context.plugin_resolved_name = name\n",
   "            plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "            plugin_load_context.resolved = True\n",
   "            return plugin_load_context\n",
   "        for path_context in (p for p in self._get_paths_with_context() if p.path not in self._searched_paths and os.path.isdir(p.path)):\n",
   "            path = path_context.path\n",
   "            display.debug('trying %s' % path)\n",
   "            plugin_load_context.load_attempts.append(path)\n",
   "                full_paths = (os.path.join(path, f) for f in os.listdir(path))\n",
   "                display.warning(\"Error accessing plugin paths: %s\" % to_text(e))\n",
   "            for full_path in (f for f in full_paths if os.path.isfile(f) and not f.endswith('__init__.py')):\n",
   "                full_name = os.path.basename(full_path)\n",
   "                if any(full_path.endswith(x) for x in C.MODULE_IGNORE_EXTS):\n",
   "                splitname = os.path.splitext(full_name)\n",
   "                base_name = splitname[0]\n",
   "                internal = path_context.internal\n",
   "                    extension = splitname[1]\n",
   "                    extension = ''\n",
   "                if base_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][full_name] = PluginPathContext(full_path, internal)\n",
   "                if base_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][full_name] = PluginPathContext(full_path, internal)\n",
   "            self._searched_paths.add(path)\n",
   "                path_with_context = pull_cache[name]\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        if not name.startswith('_'):\n",
   "            alias_name = '_' + name\n",
   "            if alias_name in pull_cache:\n",
   "                path_with_context = pull_cache[alias_name]\n",
   "                if not ignore_deprecated and not os.path.islink(path_with_context.path):\n",
   "                    display.deprecated('%s is kept for backwards compatibility but usage is discouraged. '  # pylint: disable=ansible-deprecated-no-version\n",
   "                                       'The module documentation details page may explain more about this rationale.' % name.lstrip('_'))\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = alias_name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        candidate_fqcr = 'ansible.builtin.{0}'.format(name)\n",
   "        if '.' not in name and AnsibleCollectionRef.is_valid_fqcr(candidate_fqcr):\n",
   "            return self._find_fq_plugin(fq_name=candidate_fqcr, extension=suffix, plugin_load_context=plugin_load_context)\n",
   "        return plugin_load_context.nope('{0} is not eligible for last-chance resolution'.format(name))\n",
   "            return self.find_plugin(name, collection_list=collection_list) is not None\n",
   "            display.debug('has_plugin error: {0}'.format(to_text(ex)))\n",
   "    __contains__ = has_plugin\n",
   "        if name.startswith('ansible_collections.'):\n",
   "            full_name = name\n",
   "            full_name = '.'.join([self.package, name])\n",
   "        if full_name in sys.modules:\n",
   "            return sys.modules[full_name]\n",
   "            if imp is None:\n",
   "                spec = importlib.util.spec_from_file_location(to_native(full_name), to_native(path))\n",
   "                module = importlib.util.module_from_spec(spec)\n",
   "                spec.loader.exec_module(module)\n",
   "                sys.modules[full_name] = module\n",
   "                with open(to_bytes(path), 'rb') as module_file:\n",
   "                    module = imp.load_source(to_native(full_name), to_native(path), module_file)\n",
   "        return module\n",
   "        setattr(obj, '_original_path', path)\n",
   "        setattr(obj, '_load_name', name)\n",
   "        setattr(obj, '_redirected_names', redirected_names or [])\n",
   "        return self.get_with_context(name, *args, **kwargs).object\n",
   "        found_in_cache = True\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        collection_list = kwargs.pop('collection_list', None)\n",
   "        if name in self.aliases:\n",
   "            name = self.aliases[name]\n",
   "        plugin_load_context = self.find_plugin_with_context(name, collection_list=collection_list)\n",
   "        if not plugin_load_context.resolved or not plugin_load_context.plugin_resolved_path:\n",
   "            return get_with_context_result(None, plugin_load_context)\n",
   "        name = plugin_load_context.plugin_resolved_name\n",
   "        path = plugin_load_context.plugin_resolved_path\n",
   "        redirected_names = plugin_load_context.redirect_list or []\n",
   "        if path not in self._module_cache:\n",
   "            self._module_cache[path] = self._load_module_source(name, path)\n",
   "            self._load_config_defs(name, self._module_cache[path], path)\n",
   "            found_in_cache = False\n",
   "        obj = getattr(self._module_cache[path], self.class_name)\n",
   "            module = __import__(self.package, fromlist=[self.base_class])\n",
   "                plugin_class = getattr(module, self.base_class)\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "            if not issubclass(obj, plugin_class):\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "        self._display_plugin_load(self.class_name, name, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "        if not class_only:\n",
   "                instance = object.__new__(obj)\n",
   "                self._update_object(instance, name, path, redirected_names)\n",
   "                obj.__init__(instance, *args, **kwargs)\n",
   "                obj = instance\n",
   "                    return get_with_context_result(None, plugin_load_context)\n",
   "        self._update_object(obj, name, path, redirected_names)\n",
   "        return get_with_context_result(obj, plugin_load_context)\n",
   "            msg = 'Loading %s \\'%s\\' from %s' % (class_name, os.path.basename(name), path)\n",
   "                msg = '%s (searched paths: %s)' % (msg, self.format_paths(searched_paths))\n",
   "            if found_in_cache or class_only:\n",
   "                msg = '%s (found_in_cache=%s, class_only=%s)' % (msg, found_in_cache, class_only)\n",
   "            display.debug(msg)\n",
   "        dedupe = kwargs.pop('_dedupe', True)\n",
   "        path_only = kwargs.pop('path_only', False)\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        if path_only and class_only:\n",
   "        all_matches = []\n",
   "        found_in_cache = True\n",
   "        for i in self._get_paths():\n",
   "            all_matches.extend(glob.glob(os.path.join(i, \"*.py\")))\n",
   "        loaded_modules = set()\n",
   "        for path in sorted(all_matches, key=os.path.basename):\n",
   "            name = os.path.splitext(path)[0]\n",
   "            basename = os.path.basename(name)\n",
   "            if basename == '__init__' or basename in _PLUGIN_FILTERS[self.package]:\n",
   "            if dedupe and basename in loaded_modules:\n",
   "            loaded_modules.add(basename)\n",
   "            if path_only:\n",
   "                yield path\n",
   "            if path not in self._module_cache:\n",
   "                        full_name = '{0}_{1}'.format(abs(hash(path)), basename)\n",
   "                        full_name = basename\n",
   "                    module = self._load_module_source(full_name, path)\n",
   "                    self._load_config_defs(basename, module, path)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                self._module_cache[path] = module\n",
   "                found_in_cache = False\n",
   "                obj = getattr(self._module_cache[path], self.class_name)\n",
   "                display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                module = __import__(self.package, fromlist=[self.base_class])\n",
   "                    plugin_class = getattr(module, self.base_class)\n",
   "                if not issubclass(obj, plugin_class):\n",
   "            self._display_plugin_load(self.class_name, basename, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "            if not class_only:\n",
   "                    obj = obj(*args, **kwargs)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be incomplete: %s\" % (path, to_text(e)))\n",
   "            self._update_object(obj, basename, path)\n",
   "            yield obj\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).find_plugin(name, collection_list=collection_list)\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).get(name, *args, **kwargs)\n",
   "        plugins = list(super(Jinja2Loader, self).all(*args, **kwargs))\n",
   "        plugins.reverse()\n",
   "        return plugins\n",
   "    filters = defaultdict(frozenset)\n",
   "        filter_cfg = '/etc/ansible/plugin_filters.yml'\n",
   "        filter_cfg = C.PLUGIN_FILTERS_CFG\n",
   "    if os.path.exists(filter_cfg):\n",
   "        with open(filter_cfg, 'rb') as f:\n",
   "                filter_data = from_yaml(f.read())\n",
   "                display.warning(u'The plugin filter file, {0} was not parsable.'\n",
   "                                u' Skipping: {1}'.format(filter_cfg, to_text(e)))\n",
   "                return filters\n",
   "            version = filter_data['filter_version']\n",
   "            display.warning(u'The plugin filter file, {0} was invalid.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "            return filters\n",
   "        version = to_text(version)\n",
   "        version = version.strip()\n",
   "        if version == u'1.0':\n",
   "                filters['ansible.modules'] = frozenset(filter_data['module_blacklist'])\n",
   "                display.warning(u'Unable to parse the plugin filter file {0} as'\n",
   "                                u' Skipping.'.format(filter_cfg))\n",
   "                return filters\n",
   "            filters['ansible.plugins.action'] = filters['ansible.modules']\n",
   "            display.warning(u'The plugin filter file, {0} was a version not recognized by this'\n",
   "                            u' version of Ansible. Skipping.'.format(filter_cfg))\n",
   "            display.warning(u'The plugin filter file, {0} does not exist.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "    if 'stat' in filters['ansible.modules']:\n",
   "                           ' from the blacklist.'.format(to_native(filter_cfg)))\n",
   "    return filters\n",
   "    display.vvvv(to_text('Loading collection {0} from {1}'.format(collection_name, collection_path)))\n",
   "        if not _does_collection_support_ansible_version(collection_meta.get('requires_ansible', ''), ansible_version):\n",
   "                display.warning(message)\n",
   "        display.warning('Error parsing collection metadata requires_ansible value from collection {0}: {1}'.format(collection_name, ex))\n",
   "        display.warning('packaging Python module unavailable; unable to validate collection Ansible version requirements')\n",
   "        display.warning('AnsibleCollectionFinder has already been configured')\n"
  ]
 },
 "115": {
  "name": "spec",
  "type": "_frozen_importlib.ModuleSpec",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/loader.py",
  "lineno": "762",
  "column": "16",
  "context": "ning)\n            if imp is None:\n                spec = importlib.util.spec_from_file_location(to_native(full_name), to_native(path))\n                module = importlib.util.module_fro",
  "context_lines": "            return sys.modules[full_name]\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", RuntimeWarning)\n            if imp is None:\n                spec = importlib.util.spec_from_file_location(to_native(full_name), to_native(path))\n                module = importlib.util.module_from_spec(spec)\n                spec.loader.exec_module(module)\n                sys.modules[full_name] = module\n            else:\n",
  "slicing": [
   "    imp = None\n",
   "display = Display()\n",
   "get_with_context_result = namedtuple('get_with_context_result', ['object', 'plugin_load_context'])\n",
   "    return [(name, obj) for (name, obj) in globals().items() if isinstance(obj, PluginLoader)]\n",
   "    b_path = os.path.expanduser(to_bytes(path, errors='surrogate_or_strict'))\n",
   "    if os.path.isdir(b_path):\n",
   "        for name, obj in get_all_plugin_loaders():\n",
   "            if obj.subdir:\n",
   "                plugin_path = os.path.join(b_path, to_bytes(obj.subdir))\n",
   "                if os.path.isdir(plugin_path):\n",
   "                    obj.add_directory(to_text(plugin_path))\n",
   "        display.warning(\"Ignoring invalid path provided to plugin path: '%s' is not a directory\" % to_text(path))\n",
   "        shell_type = 'sh'\n",
   "                shell_filename = os.path.basename(executable)\n",
   "                    shell = shell_loader.get(shell_filename)\n",
   "                    shell = None\n",
   "                if shell is None:\n",
   "                    for shell in shell_loader.all():\n",
   "                        if shell_filename in shell.COMPATIBLE_SHELLS:\n",
   "                            shell_type = shell.SHELL_FAMILY\n",
   "    shell = shell_loader.get(shell_type)\n",
   "    if not shell:\n",
   "        raise AnsibleError(\"Could not find the shell plugin required (%s).\" % shell_type)\n",
   "        setattr(shell, 'executable', executable)\n",
   "    return shell\n",
   "    loader = getattr(sys.modules[__name__], '%s_loader' % which_loader)\n",
   "    for path in paths:\n",
   "        loader.add_directory(path, with_subdir=True)\n",
   "        self.path = path\n",
   "        warning_text = deprecation.get('warning_text', None)\n",
   "        removal_date = deprecation.get('removal_date', None)\n",
   "        removal_version = deprecation.get('removal_version', None)\n",
   "        if removal_date is not None:\n",
   "            removal_version = None\n",
   "        if not warning_text:\n",
   "            warning_text = '{0} has been deprecated'.format(name)\n",
   "        display.deprecated(warning_text, date=removal_date, version=removal_version, collection_name=collection_name)\n",
   "        if removal_date:\n",
   "            self.removal_date = removal_date\n",
   "        if removal_version:\n",
   "            self.removal_version = removal_version\n",
   "        self.deprecation_warnings.append(warning_text)\n",
   "        aliases = {} if aliases is None else aliases\n",
   "        self.aliases = aliases\n",
   "            config = [config]\n",
   "        elif not config:\n",
   "            config = []\n",
   "        self.config = config\n",
   "        class_name = data.get('class_name')\n",
   "        package = data.get('package')\n",
   "        config = data.get('config')\n",
   "        subdir = data.get('subdir')\n",
   "        aliases = data.get('aliases')\n",
   "        base_class = data.get('base_class')\n",
   "        PATH_CACHE[class_name] = data.get('PATH_CACHE')\n",
   "        PLUGIN_PATH_CACHE[class_name] = data.get('PLUGIN_PATH_CACHE')\n",
   "        self.__init__(class_name, package, config, subdir, aliases, base_class)\n",
   "        ret = []\n",
   "        for i in paths:\n",
   "            if i not in ret:\n",
   "                ret.append(i)\n",
   "        return os.pathsep.join(ret)\n",
   "        results = []\n",
   "        results.append(dir)\n",
   "        for root, subdirs, files in os.walk(dir, followlinks=True):\n",
   "            if '__init__.py' in files:\n",
   "                for x in subdirs:\n",
   "                    results.append(os.path.join(root, x))\n",
   "        return results\n",
   "            m = __import__(self.package)\n",
   "            parts = self.package.split('.')[1:]\n",
   "            for parent_mod in parts:\n",
   "                m = getattr(m, parent_mod)\n",
   "            self.package_path = os.path.dirname(m.__file__)\n",
   "        if subdirs:\n",
   "        ret = [PluginPathContext(p, False) for p in self._extra_dirs]\n",
   "            for path in self.config:\n",
   "                path = os.path.realpath(os.path.expanduser(path))\n",
   "                if subdirs:\n",
   "                    contents = glob.glob(\"%s/*\" % path) + glob.glob(\"%s/*/*\" % path)\n",
   "                    for c in contents:\n",
   "                        if os.path.isdir(c) and c not in ret:\n",
   "                            ret.append(PluginPathContext(c, False))\n",
   "                if path not in ret:\n",
   "                    ret.append(PluginPathContext(path, False))\n",
   "        ret.extend([PluginPathContext(p, True) for p in self._get_package_paths(subdirs=subdirs)])\n",
   "        ret.sort(key=lambda p: p.path.endswith('/windows'))\n",
   "        self._paths = ret\n",
   "        return ret\n",
   "        paths_with_context = self._get_paths_with_context(subdirs=subdirs)\n",
   "        return [path_with_context.path for path_with_context in paths_with_context]\n",
   "            type_name = get_plugin_class(self.class_name)\n",
   "            if type_name in C.CONFIGURABLE_PLUGINS:\n",
   "                dstring = AnsibleLoader(getattr(module, 'DOCUMENTATION', ''), file_name=path).get_single_data()\n",
   "                if dstring:\n",
   "                    add_fragments(dstring, path, fragment_loader=fragment_loader, is_module=(type_name == 'module'))\n",
   "                if dstring and 'options' in dstring and isinstance(dstring['options'], dict):\n",
   "                    C.config.initialize_plugin_configuration_definitions(type_name, name, dstring['options'])\n",
   "                    display.debug('Loaded config def from plugin (%s/%s)' % (type_name, name))\n",
   "        directory = os.path.realpath(directory)\n",
   "        if directory is not None:\n",
   "                directory = os.path.join(directory, self.subdir)\n",
   "            if directory not in self._extra_dirs:\n",
   "                self._extra_dirs.append(directory)\n",
   "                display.debug('Added %s to loader search path' % (directory))\n",
   "        collection_pkg = import_module(acr.n_python_collection_package_name)\n",
   "        if not collection_pkg:\n",
   "        collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "        if not collection_meta:\n",
   "            subdir_qualified_resource = '.'.join([acr.subdirs, acr.resource])\n",
   "            subdir_qualified_resource = acr.resource\n",
   "        entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource + extension, None)\n",
   "        if not entry:\n",
   "            entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource, None)\n",
   "        return entry\n",
   "        plugin_type = AnsibleCollectionRef.legacy_plugin_dir_to_plugin_type(self.subdir)\n",
   "        acr = AnsibleCollectionRef.from_fqcr(fq_name, plugin_type)\n",
   "        routing_metadata = self._query_collection_routing_meta(acr, plugin_type, extension=extension)\n",
   "        if routing_metadata:\n",
   "            deprecation = routing_metadata.get('deprecation', None)\n",
   "            plugin_load_context.record_deprecation(fq_name, deprecation, acr.collection)\n",
   "            tombstone = routing_metadata.get('tombstone', None)\n",
   "            if tombstone:\n",
   "                removal_date = tombstone.get('removal_date')\n",
   "                removal_version = tombstone.get('removal_version')\n",
   "                warning_text = tombstone.get('warning_text') or '{0} has been removed.'.format(fq_name)\n",
   "                removed_msg = display.get_deprecation_message(msg=warning_text, version=removal_version,\n",
   "                                                              date=removal_date, removed=True,\n",
   "                                                              collection_name=acr.collection)\n",
   "                plugin_load_context.removal_date = removal_date\n",
   "                plugin_load_context.removal_version = removal_version\n",
   "                plugin_load_context.exit_reason = removed_msg\n",
   "                raise AnsiblePluginRemovedError(removed_msg, plugin_load_context=plugin_load_context)\n",
   "            redirect = routing_metadata.get('redirect', None)\n",
   "            if redirect:\n",
   "                display.vv(\"redirecting (type: {0}) {1} to {2}\".format(plugin_type, fq_name, redirect))\n",
   "                return plugin_load_context.redirect(redirect)\n",
   "        n_resource = to_native(acr.resource, errors='strict')\n",
   "        full_name = '{0}.{1}'.format(acr.n_python_package_name, n_resource)\n",
   "            n_resource += extension\n",
   "        pkg = sys.modules.get(acr.n_python_package_name)\n",
   "        if not pkg:\n",
   "                pkg = import_module(acr.n_python_package_name)\n",
   "                return plugin_load_context.nope('Python package {0} not found'.format(acr.n_python_package_name))\n",
   "        pkg_path = os.path.dirname(pkg.__file__)\n",
   "        n_resource_path = os.path.join(pkg_path, n_resource)\n",
   "        if os.path.exists(n_resource_path):\n",
   "                full_name, to_text(n_resource_path), acr.collection, 'found exact match for {0} in {1}'.format(full_name, acr.collection))\n",
   "            return plugin_load_context.nope('no match for {0} in {1}'.format(to_text(n_resource), acr.collection))\n",
   "        found_files = [f\n",
   "                       for f in glob.iglob(os.path.join(pkg_path, n_resource) + '.*')\n",
   "                       if os.path.isfile(f) and not f.endswith(C.MODULE_IGNORE_EXTS)]\n",
   "        if not found_files:\n",
   "            return plugin_load_context.nope('failed fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        if len(found_files) > 1:\n",
   "            full_name, to_text(found_files[0]), acr.collection, 'found fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        result = self.find_plugin_with_context(name, mod_type, ignore_deprecated, check_aliases, collection_list)\n",
   "        if result.resolved and result.plugin_resolved_path:\n",
   "            return result.plugin_resolved_path\n",
   "        plugin_load_context = PluginLoadContext()\n",
   "        plugin_load_context.original_name = name\n",
   "            result = self._resolve_plugin_step(name, mod_type, ignore_deprecated, check_aliases, collection_list, plugin_load_context=plugin_load_context)\n",
   "            if result.pending_redirect:\n",
   "                if result.pending_redirect in result.redirect_list:\n",
   "                    raise AnsiblePluginCircularRedirect('plugin redirect loop resolving {0} (path: {1})'.format(result.original_name, result.redirect_list))\n",
   "                name = result.pending_redirect\n",
   "                result.pending_redirect = None\n",
   "                plugin_load_context = result\n",
   "        if plugin_load_context.error_list:\n",
   "            display.warning(\"errors were encountered during the plugin load for {0}:\\n{1}\".format(name, plugin_load_context.error_list))\n",
   "        return plugin_load_context\n",
   "        if not plugin_load_context:\n",
   "        plugin_load_context.redirect_list.append(name)\n",
   "        plugin_load_context.resolved = False\n",
   "        if name in _PLUGIN_FILTERS[self.package]:\n",
   "            plugin_load_context.exit_reason = '{0} matched a defined plugin filter'.format(name)\n",
   "            return plugin_load_context\n",
   "            suffix = mod_type\n",
   "            suffix = '.py'\n",
   "            suffix = ''\n",
   "        if (AnsibleCollectionRef.is_valid_fqcr(name) or collection_list) and not name.startswith('Ansible'):\n",
   "            if '.' in name or not collection_list:\n",
   "                candidates = [name]\n",
   "                candidates = ['{0}.{1}'.format(c, name) for c in collection_list]\n",
   "            for candidate_name in candidates:\n",
   "                    plugin_load_context.load_attempts.append(candidate_name)\n",
   "                    if candidate_name.startswith('ansible.legacy'):\n",
   "                        plugin_load_context = self._find_plugin_legacy(name.replace('ansible.legacy.', '', 1),\n",
   "                                                                       plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "                        plugin_load_context = self._find_fq_plugin(candidate_name, suffix, plugin_load_context=plugin_load_context)\n",
   "                        if candidate_name != plugin_load_context.original_name and candidate_name not in plugin_load_context.redirect_list:\n",
   "                            plugin_load_context.redirect_list.append(candidate_name)\n",
   "                    if plugin_load_context.resolved or plugin_load_context.pending_redirect:  # if we got an answer or need to chase down a redirect, return\n",
   "                        return plugin_load_context\n",
   "                    plugin_load_context.import_error_list.append(ie)\n",
   "                    plugin_load_context.error_list.append(to_native(ex))\n",
   "            if plugin_load_context.error_list:\n",
   "                display.debug(msg='plugin lookup for {0} failed; errors: {1}'.format(name, '; '.join(plugin_load_context.error_list)))\n",
   "            plugin_load_context.exit_reason = 'no matches found for {0}'.format(name)\n",
   "            return plugin_load_context\n",
   "        return self._find_plugin_legacy(name, plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "        plugin_load_context.resolved = False\n",
   "            name = self.aliases.get(name, name)\n",
   "        pull_cache = self._plugin_path_cache[suffix]\n",
   "            path_with_context = pull_cache[name]\n",
   "            plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "            plugin_load_context.plugin_resolved_name = name\n",
   "            plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "            plugin_load_context.resolved = True\n",
   "            return plugin_load_context\n",
   "        for path_context in (p for p in self._get_paths_with_context() if p.path not in self._searched_paths and os.path.isdir(p.path)):\n",
   "            path = path_context.path\n",
   "            display.debug('trying %s' % path)\n",
   "            plugin_load_context.load_attempts.append(path)\n",
   "                full_paths = (os.path.join(path, f) for f in os.listdir(path))\n",
   "                display.warning(\"Error accessing plugin paths: %s\" % to_text(e))\n",
   "            for full_path in (f for f in full_paths if os.path.isfile(f) and not f.endswith('__init__.py')):\n",
   "                full_name = os.path.basename(full_path)\n",
   "                if any(full_path.endswith(x) for x in C.MODULE_IGNORE_EXTS):\n",
   "                splitname = os.path.splitext(full_name)\n",
   "                base_name = splitname[0]\n",
   "                internal = path_context.internal\n",
   "                    extension = splitname[1]\n",
   "                    extension = ''\n",
   "                if base_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][full_name] = PluginPathContext(full_path, internal)\n",
   "                if base_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][full_name] = PluginPathContext(full_path, internal)\n",
   "            self._searched_paths.add(path)\n",
   "                path_with_context = pull_cache[name]\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        if not name.startswith('_'):\n",
   "            alias_name = '_' + name\n",
   "            if alias_name in pull_cache:\n",
   "                path_with_context = pull_cache[alias_name]\n",
   "                if not ignore_deprecated and not os.path.islink(path_with_context.path):\n",
   "                    display.deprecated('%s is kept for backwards compatibility but usage is discouraged. '  # pylint: disable=ansible-deprecated-no-version\n",
   "                                       'The module documentation details page may explain more about this rationale.' % name.lstrip('_'))\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = alias_name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        candidate_fqcr = 'ansible.builtin.{0}'.format(name)\n",
   "        if '.' not in name and AnsibleCollectionRef.is_valid_fqcr(candidate_fqcr):\n",
   "            return self._find_fq_plugin(fq_name=candidate_fqcr, extension=suffix, plugin_load_context=plugin_load_context)\n",
   "        return plugin_load_context.nope('{0} is not eligible for last-chance resolution'.format(name))\n",
   "            return self.find_plugin(name, collection_list=collection_list) is not None\n",
   "            display.debug('has_plugin error: {0}'.format(to_text(ex)))\n",
   "        if name.startswith('ansible_collections.'):\n",
   "            full_name = name\n",
   "            full_name = '.'.join([self.package, name])\n",
   "        if full_name in sys.modules:\n",
   "            return sys.modules[full_name]\n",
   "            if imp is None:\n",
   "                spec = importlib.util.spec_from_file_location(to_native(full_name), to_native(path))\n",
   "                module = importlib.util.module_from_spec(spec)\n",
   "                spec.loader.exec_module(module)\n",
   "                sys.modules[full_name] = module\n",
   "                with open(to_bytes(path), 'rb') as module_file:\n",
   "                    module = imp.load_source(to_native(full_name), to_native(path), module_file)\n",
   "        return module\n",
   "        setattr(obj, '_original_path', path)\n",
   "        setattr(obj, '_load_name', name)\n",
   "        setattr(obj, '_redirected_names', redirected_names or [])\n",
   "        return self.get_with_context(name, *args, **kwargs).object\n",
   "        found_in_cache = True\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        collection_list = kwargs.pop('collection_list', None)\n",
   "        if name in self.aliases:\n",
   "            name = self.aliases[name]\n",
   "        plugin_load_context = self.find_plugin_with_context(name, collection_list=collection_list)\n",
   "        if not plugin_load_context.resolved or not plugin_load_context.plugin_resolved_path:\n",
   "            return get_with_context_result(None, plugin_load_context)\n",
   "        name = plugin_load_context.plugin_resolved_name\n",
   "        path = plugin_load_context.plugin_resolved_path\n",
   "        redirected_names = plugin_load_context.redirect_list or []\n",
   "        if path not in self._module_cache:\n",
   "            self._module_cache[path] = self._load_module_source(name, path)\n",
   "            self._load_config_defs(name, self._module_cache[path], path)\n",
   "            found_in_cache = False\n",
   "        obj = getattr(self._module_cache[path], self.class_name)\n",
   "            module = __import__(self.package, fromlist=[self.base_class])\n",
   "                plugin_class = getattr(module, self.base_class)\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "            if not issubclass(obj, plugin_class):\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "        self._display_plugin_load(self.class_name, name, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "        if not class_only:\n",
   "                instance = object.__new__(obj)\n",
   "                self._update_object(instance, name, path, redirected_names)\n",
   "                obj.__init__(instance, *args, **kwargs)\n",
   "                obj = instance\n",
   "                    return get_with_context_result(None, plugin_load_context)\n",
   "        self._update_object(obj, name, path, redirected_names)\n",
   "        return get_with_context_result(obj, plugin_load_context)\n",
   "            msg = 'Loading %s \\'%s\\' from %s' % (class_name, os.path.basename(name), path)\n",
   "                msg = '%s (searched paths: %s)' % (msg, self.format_paths(searched_paths))\n",
   "            if found_in_cache or class_only:\n",
   "                msg = '%s (found_in_cache=%s, class_only=%s)' % (msg, found_in_cache, class_only)\n",
   "            display.debug(msg)\n",
   "        dedupe = kwargs.pop('_dedupe', True)\n",
   "        path_only = kwargs.pop('path_only', False)\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        if path_only and class_only:\n",
   "        all_matches = []\n",
   "        found_in_cache = True\n",
   "        for i in self._get_paths():\n",
   "            all_matches.extend(glob.glob(os.path.join(i, \"*.py\")))\n",
   "        loaded_modules = set()\n",
   "        for path in sorted(all_matches, key=os.path.basename):\n",
   "            name = os.path.splitext(path)[0]\n",
   "            basename = os.path.basename(name)\n",
   "            if basename == '__init__' or basename in _PLUGIN_FILTERS[self.package]:\n",
   "            if dedupe and basename in loaded_modules:\n",
   "            loaded_modules.add(basename)\n",
   "            if path_only:\n",
   "                yield path\n",
   "            if path not in self._module_cache:\n",
   "                        full_name = '{0}_{1}'.format(abs(hash(path)), basename)\n",
   "                        full_name = basename\n",
   "                    module = self._load_module_source(full_name, path)\n",
   "                    self._load_config_defs(basename, module, path)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                self._module_cache[path] = module\n",
   "                found_in_cache = False\n",
   "                obj = getattr(self._module_cache[path], self.class_name)\n",
   "                display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                module = __import__(self.package, fromlist=[self.base_class])\n",
   "                    plugin_class = getattr(module, self.base_class)\n",
   "                if not issubclass(obj, plugin_class):\n",
   "            self._display_plugin_load(self.class_name, basename, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "            if not class_only:\n",
   "                    obj = obj(*args, **kwargs)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be incomplete: %s\" % (path, to_text(e)))\n",
   "            self._update_object(obj, basename, path)\n",
   "            yield obj\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).find_plugin(name, collection_list=collection_list)\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).get(name, *args, **kwargs)\n",
   "        plugins = list(super(Jinja2Loader, self).all(*args, **kwargs))\n",
   "        plugins.reverse()\n",
   "        return plugins\n",
   "    filters = defaultdict(frozenset)\n",
   "        filter_cfg = '/etc/ansible/plugin_filters.yml'\n",
   "        filter_cfg = C.PLUGIN_FILTERS_CFG\n",
   "    if os.path.exists(filter_cfg):\n",
   "        with open(filter_cfg, 'rb') as f:\n",
   "                filter_data = from_yaml(f.read())\n",
   "                display.warning(u'The plugin filter file, {0} was not parsable.'\n",
   "                                u' Skipping: {1}'.format(filter_cfg, to_text(e)))\n",
   "                return filters\n",
   "            version = filter_data['filter_version']\n",
   "            display.warning(u'The plugin filter file, {0} was invalid.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "            return filters\n",
   "        version = to_text(version)\n",
   "        version = version.strip()\n",
   "        if version == u'1.0':\n",
   "                filters['ansible.modules'] = frozenset(filter_data['module_blacklist'])\n",
   "                display.warning(u'Unable to parse the plugin filter file {0} as'\n",
   "                                u' Skipping.'.format(filter_cfg))\n",
   "                return filters\n",
   "            filters['ansible.plugins.action'] = filters['ansible.modules']\n",
   "            display.warning(u'The plugin filter file, {0} was a version not recognized by this'\n",
   "                            u' version of Ansible. Skipping.'.format(filter_cfg))\n",
   "            display.warning(u'The plugin filter file, {0} does not exist.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "    if 'stat' in filters['ansible.modules']:\n",
   "                           ' from the blacklist.'.format(to_native(filter_cfg)))\n",
   "    return filters\n",
   "    display.vvvv(to_text('Loading collection {0} from {1}'.format(collection_name, collection_path)))\n",
   "        if not _does_collection_support_ansible_version(collection_meta.get('requires_ansible', ''), ansible_version):\n",
   "                display.warning(message)\n",
   "        display.warning('Error parsing collection metadata requires_ansible value from collection {0}: {1}'.format(collection_name, ex))\n",
   "        display.warning('packaging Python module unavailable; unable to validate collection Ansible version requirements')\n",
   "        display.warning('AnsibleCollectionFinder has already been configured')\n"
  ]
 },
 "116": {
  "name": "module",
  "type": "module",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/loader.py",
  "lineno": "763",
  "column": "16",
  "context": "tive(full_name), to_native(path))\n                module = importlib.util.module_from_spec(spec)\n                spec.loader.exec_module(module)\n  ",
  "context_lines": "        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", RuntimeWarning)\n            if imp is None:\n                spec = importlib.util.spec_from_file_location(to_native(full_name), to_native(path))\n                module = importlib.util.module_from_spec(spec)\n                spec.loader.exec_module(module)\n                sys.modules[full_name] = module\n            else:\n                with open(to_bytes(path), 'rb') as module_file:\n",
  "slicing": [
   "    imp = None\n",
   "display = Display()\n",
   "get_with_context_result = namedtuple('get_with_context_result', ['object', 'plugin_load_context'])\n",
   "    return [(name, obj) for (name, obj) in globals().items() if isinstance(obj, PluginLoader)]\n",
   "    b_path = os.path.expanduser(to_bytes(path, errors='surrogate_or_strict'))\n",
   "    if os.path.isdir(b_path):\n",
   "        for name, obj in get_all_plugin_loaders():\n",
   "            if obj.subdir:\n",
   "                plugin_path = os.path.join(b_path, to_bytes(obj.subdir))\n",
   "                if os.path.isdir(plugin_path):\n",
   "                    obj.add_directory(to_text(plugin_path))\n",
   "        display.warning(\"Ignoring invalid path provided to plugin path: '%s' is not a directory\" % to_text(path))\n",
   "        shell_type = 'sh'\n",
   "                shell_filename = os.path.basename(executable)\n",
   "                    shell = shell_loader.get(shell_filename)\n",
   "                    shell = None\n",
   "                if shell is None:\n",
   "                    for shell in shell_loader.all():\n",
   "                        if shell_filename in shell.COMPATIBLE_SHELLS:\n",
   "                            shell_type = shell.SHELL_FAMILY\n",
   "    shell = shell_loader.get(shell_type)\n",
   "    if not shell:\n",
   "        raise AnsibleError(\"Could not find the shell plugin required (%s).\" % shell_type)\n",
   "        setattr(shell, 'executable', executable)\n",
   "    return shell\n",
   "    loader = getattr(sys.modules[__name__], '%s_loader' % which_loader)\n",
   "    for path in paths:\n",
   "        loader.add_directory(path, with_subdir=True)\n",
   "        self.path = path\n",
   "        warning_text = deprecation.get('warning_text', None)\n",
   "        removal_date = deprecation.get('removal_date', None)\n",
   "        removal_version = deprecation.get('removal_version', None)\n",
   "        if removal_date is not None:\n",
   "            removal_version = None\n",
   "        if not warning_text:\n",
   "            warning_text = '{0} has been deprecated'.format(name)\n",
   "        display.deprecated(warning_text, date=removal_date, version=removal_version, collection_name=collection_name)\n",
   "        if removal_date:\n",
   "            self.removal_date = removal_date\n",
   "        if removal_version:\n",
   "            self.removal_version = removal_version\n",
   "        self.deprecation_warnings.append(warning_text)\n",
   "        aliases = {} if aliases is None else aliases\n",
   "        self.aliases = aliases\n",
   "            config = [config]\n",
   "        elif not config:\n",
   "            config = []\n",
   "        self.config = config\n",
   "        class_name = data.get('class_name')\n",
   "        package = data.get('package')\n",
   "        config = data.get('config')\n",
   "        subdir = data.get('subdir')\n",
   "        aliases = data.get('aliases')\n",
   "        base_class = data.get('base_class')\n",
   "        PATH_CACHE[class_name] = data.get('PATH_CACHE')\n",
   "        PLUGIN_PATH_CACHE[class_name] = data.get('PLUGIN_PATH_CACHE')\n",
   "        self.__init__(class_name, package, config, subdir, aliases, base_class)\n",
   "        ret = []\n",
   "        for i in paths:\n",
   "            if i not in ret:\n",
   "                ret.append(i)\n",
   "        return os.pathsep.join(ret)\n",
   "        results = []\n",
   "        results.append(dir)\n",
   "        for root, subdirs, files in os.walk(dir, followlinks=True):\n",
   "            if '__init__.py' in files:\n",
   "                for x in subdirs:\n",
   "                    results.append(os.path.join(root, x))\n",
   "        return results\n",
   "            m = __import__(self.package)\n",
   "            parts = self.package.split('.')[1:]\n",
   "            for parent_mod in parts:\n",
   "                m = getattr(m, parent_mod)\n",
   "            self.package_path = os.path.dirname(m.__file__)\n",
   "        if subdirs:\n",
   "        ret = [PluginPathContext(p, False) for p in self._extra_dirs]\n",
   "            for path in self.config:\n",
   "                path = os.path.realpath(os.path.expanduser(path))\n",
   "                if subdirs:\n",
   "                    contents = glob.glob(\"%s/*\" % path) + glob.glob(\"%s/*/*\" % path)\n",
   "                    for c in contents:\n",
   "                        if os.path.isdir(c) and c not in ret:\n",
   "                            ret.append(PluginPathContext(c, False))\n",
   "                if path not in ret:\n",
   "                    ret.append(PluginPathContext(path, False))\n",
   "        ret.extend([PluginPathContext(p, True) for p in self._get_package_paths(subdirs=subdirs)])\n",
   "        ret.sort(key=lambda p: p.path.endswith('/windows'))\n",
   "        self._paths = ret\n",
   "        return ret\n",
   "        paths_with_context = self._get_paths_with_context(subdirs=subdirs)\n",
   "        return [path_with_context.path for path_with_context in paths_with_context]\n",
   "            type_name = get_plugin_class(self.class_name)\n",
   "            if type_name in C.CONFIGURABLE_PLUGINS:\n",
   "                dstring = AnsibleLoader(getattr(module, 'DOCUMENTATION', ''), file_name=path).get_single_data()\n",
   "                if dstring:\n",
   "                    add_fragments(dstring, path, fragment_loader=fragment_loader, is_module=(type_name == 'module'))\n",
   "                if dstring and 'options' in dstring and isinstance(dstring['options'], dict):\n",
   "                    C.config.initialize_plugin_configuration_definitions(type_name, name, dstring['options'])\n",
   "                    display.debug('Loaded config def from plugin (%s/%s)' % (type_name, name))\n",
   "        directory = os.path.realpath(directory)\n",
   "        if directory is not None:\n",
   "                directory = os.path.join(directory, self.subdir)\n",
   "            if directory not in self._extra_dirs:\n",
   "                self._extra_dirs.append(directory)\n",
   "                display.debug('Added %s to loader search path' % (directory))\n",
   "        collection_pkg = import_module(acr.n_python_collection_package_name)\n",
   "        if not collection_pkg:\n",
   "        collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "        if not collection_meta:\n",
   "            subdir_qualified_resource = '.'.join([acr.subdirs, acr.resource])\n",
   "            subdir_qualified_resource = acr.resource\n",
   "        entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource + extension, None)\n",
   "        if not entry:\n",
   "            entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource, None)\n",
   "        return entry\n",
   "        plugin_type = AnsibleCollectionRef.legacy_plugin_dir_to_plugin_type(self.subdir)\n",
   "        acr = AnsibleCollectionRef.from_fqcr(fq_name, plugin_type)\n",
   "        routing_metadata = self._query_collection_routing_meta(acr, plugin_type, extension=extension)\n",
   "        if routing_metadata:\n",
   "            deprecation = routing_metadata.get('deprecation', None)\n",
   "            plugin_load_context.record_deprecation(fq_name, deprecation, acr.collection)\n",
   "            tombstone = routing_metadata.get('tombstone', None)\n",
   "            if tombstone:\n",
   "                removal_date = tombstone.get('removal_date')\n",
   "                removal_version = tombstone.get('removal_version')\n",
   "                warning_text = tombstone.get('warning_text') or '{0} has been removed.'.format(fq_name)\n",
   "                removed_msg = display.get_deprecation_message(msg=warning_text, version=removal_version,\n",
   "                                                              date=removal_date, removed=True,\n",
   "                                                              collection_name=acr.collection)\n",
   "                plugin_load_context.removal_date = removal_date\n",
   "                plugin_load_context.removal_version = removal_version\n",
   "                plugin_load_context.exit_reason = removed_msg\n",
   "                raise AnsiblePluginRemovedError(removed_msg, plugin_load_context=plugin_load_context)\n",
   "            redirect = routing_metadata.get('redirect', None)\n",
   "            if redirect:\n",
   "                display.vv(\"redirecting (type: {0}) {1} to {2}\".format(plugin_type, fq_name, redirect))\n",
   "                return plugin_load_context.redirect(redirect)\n",
   "        n_resource = to_native(acr.resource, errors='strict')\n",
   "        full_name = '{0}.{1}'.format(acr.n_python_package_name, n_resource)\n",
   "            n_resource += extension\n",
   "        pkg = sys.modules.get(acr.n_python_package_name)\n",
   "        if not pkg:\n",
   "                pkg = import_module(acr.n_python_package_name)\n",
   "                return plugin_load_context.nope('Python package {0} not found'.format(acr.n_python_package_name))\n",
   "        pkg_path = os.path.dirname(pkg.__file__)\n",
   "        n_resource_path = os.path.join(pkg_path, n_resource)\n",
   "        if os.path.exists(n_resource_path):\n",
   "                full_name, to_text(n_resource_path), acr.collection, 'found exact match for {0} in {1}'.format(full_name, acr.collection))\n",
   "            return plugin_load_context.nope('no match for {0} in {1}'.format(to_text(n_resource), acr.collection))\n",
   "        found_files = [f\n",
   "                       for f in glob.iglob(os.path.join(pkg_path, n_resource) + '.*')\n",
   "                       if os.path.isfile(f) and not f.endswith(C.MODULE_IGNORE_EXTS)]\n",
   "        if not found_files:\n",
   "            return plugin_load_context.nope('failed fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        if len(found_files) > 1:\n",
   "            full_name, to_text(found_files[0]), acr.collection, 'found fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        result = self.find_plugin_with_context(name, mod_type, ignore_deprecated, check_aliases, collection_list)\n",
   "        if result.resolved and result.plugin_resolved_path:\n",
   "            return result.plugin_resolved_path\n",
   "        plugin_load_context = PluginLoadContext()\n",
   "        plugin_load_context.original_name = name\n",
   "            result = self._resolve_plugin_step(name, mod_type, ignore_deprecated, check_aliases, collection_list, plugin_load_context=plugin_load_context)\n",
   "            if result.pending_redirect:\n",
   "                if result.pending_redirect in result.redirect_list:\n",
   "                    raise AnsiblePluginCircularRedirect('plugin redirect loop resolving {0} (path: {1})'.format(result.original_name, result.redirect_list))\n",
   "                name = result.pending_redirect\n",
   "                result.pending_redirect = None\n",
   "                plugin_load_context = result\n",
   "        if plugin_load_context.error_list:\n",
   "            display.warning(\"errors were encountered during the plugin load for {0}:\\n{1}\".format(name, plugin_load_context.error_list))\n",
   "        return plugin_load_context\n",
   "        if not plugin_load_context:\n",
   "        plugin_load_context.redirect_list.append(name)\n",
   "        plugin_load_context.resolved = False\n",
   "        if name in _PLUGIN_FILTERS[self.package]:\n",
   "            plugin_load_context.exit_reason = '{0} matched a defined plugin filter'.format(name)\n",
   "            return plugin_load_context\n",
   "            suffix = mod_type\n",
   "            suffix = '.py'\n",
   "            suffix = ''\n",
   "        if (AnsibleCollectionRef.is_valid_fqcr(name) or collection_list) and not name.startswith('Ansible'):\n",
   "            if '.' in name or not collection_list:\n",
   "                candidates = [name]\n",
   "                candidates = ['{0}.{1}'.format(c, name) for c in collection_list]\n",
   "            for candidate_name in candidates:\n",
   "                    plugin_load_context.load_attempts.append(candidate_name)\n",
   "                    if candidate_name.startswith('ansible.legacy'):\n",
   "                        plugin_load_context = self._find_plugin_legacy(name.replace('ansible.legacy.', '', 1),\n",
   "                                                                       plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "                        plugin_load_context = self._find_fq_plugin(candidate_name, suffix, plugin_load_context=plugin_load_context)\n",
   "                        if candidate_name != plugin_load_context.original_name and candidate_name not in plugin_load_context.redirect_list:\n",
   "                            plugin_load_context.redirect_list.append(candidate_name)\n",
   "                    if plugin_load_context.resolved or plugin_load_context.pending_redirect:  # if we got an answer or need to chase down a redirect, return\n",
   "                        return plugin_load_context\n",
   "                    plugin_load_context.import_error_list.append(ie)\n",
   "                    plugin_load_context.error_list.append(to_native(ex))\n",
   "            if plugin_load_context.error_list:\n",
   "                display.debug(msg='plugin lookup for {0} failed; errors: {1}'.format(name, '; '.join(plugin_load_context.error_list)))\n",
   "            plugin_load_context.exit_reason = 'no matches found for {0}'.format(name)\n",
   "            return plugin_load_context\n",
   "        return self._find_plugin_legacy(name, plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "        plugin_load_context.resolved = False\n",
   "            name = self.aliases.get(name, name)\n",
   "        pull_cache = self._plugin_path_cache[suffix]\n",
   "            path_with_context = pull_cache[name]\n",
   "            plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "            plugin_load_context.plugin_resolved_name = name\n",
   "            plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "            plugin_load_context.resolved = True\n",
   "            return plugin_load_context\n",
   "        for path_context in (p for p in self._get_paths_with_context() if p.path not in self._searched_paths and os.path.isdir(p.path)):\n",
   "            path = path_context.path\n",
   "            display.debug('trying %s' % path)\n",
   "            plugin_load_context.load_attempts.append(path)\n",
   "                full_paths = (os.path.join(path, f) for f in os.listdir(path))\n",
   "                display.warning(\"Error accessing plugin paths: %s\" % to_text(e))\n",
   "            for full_path in (f for f in full_paths if os.path.isfile(f) and not f.endswith('__init__.py')):\n",
   "                full_name = os.path.basename(full_path)\n",
   "                if any(full_path.endswith(x) for x in C.MODULE_IGNORE_EXTS):\n",
   "                splitname = os.path.splitext(full_name)\n",
   "                base_name = splitname[0]\n",
   "                internal = path_context.internal\n",
   "                    extension = splitname[1]\n",
   "                    extension = ''\n",
   "                if base_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][full_name] = PluginPathContext(full_path, internal)\n",
   "                if base_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][full_name] = PluginPathContext(full_path, internal)\n",
   "            self._searched_paths.add(path)\n",
   "                path_with_context = pull_cache[name]\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        if not name.startswith('_'):\n",
   "            alias_name = '_' + name\n",
   "            if alias_name in pull_cache:\n",
   "                path_with_context = pull_cache[alias_name]\n",
   "                if not ignore_deprecated and not os.path.islink(path_with_context.path):\n",
   "                    display.deprecated('%s is kept for backwards compatibility but usage is discouraged. '  # pylint: disable=ansible-deprecated-no-version\n",
   "                                       'The module documentation details page may explain more about this rationale.' % name.lstrip('_'))\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = alias_name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        candidate_fqcr = 'ansible.builtin.{0}'.format(name)\n",
   "        if '.' not in name and AnsibleCollectionRef.is_valid_fqcr(candidate_fqcr):\n",
   "            return self._find_fq_plugin(fq_name=candidate_fqcr, extension=suffix, plugin_load_context=plugin_load_context)\n",
   "        return plugin_load_context.nope('{0} is not eligible for last-chance resolution'.format(name))\n",
   "            return self.find_plugin(name, collection_list=collection_list) is not None\n",
   "            display.debug('has_plugin error: {0}'.format(to_text(ex)))\n",
   "        if name.startswith('ansible_collections.'):\n",
   "            full_name = name\n",
   "            full_name = '.'.join([self.package, name])\n",
   "        if full_name in sys.modules:\n",
   "            return sys.modules[full_name]\n",
   "            if imp is None:\n",
   "                spec = importlib.util.spec_from_file_location(to_native(full_name), to_native(path))\n",
   "                module = importlib.util.module_from_spec(spec)\n",
   "                spec.loader.exec_module(module)\n",
   "                sys.modules[full_name] = module\n",
   "                with open(to_bytes(path), 'rb') as module_file:\n",
   "                    module = imp.load_source(to_native(full_name), to_native(path), module_file)\n",
   "        return module\n",
   "        setattr(obj, '_original_path', path)\n",
   "        setattr(obj, '_load_name', name)\n",
   "        setattr(obj, '_redirected_names', redirected_names or [])\n",
   "        return self.get_with_context(name, *args, **kwargs).object\n",
   "        found_in_cache = True\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        collection_list = kwargs.pop('collection_list', None)\n",
   "        if name in self.aliases:\n",
   "            name = self.aliases[name]\n",
   "        plugin_load_context = self.find_plugin_with_context(name, collection_list=collection_list)\n",
   "        if not plugin_load_context.resolved or not plugin_load_context.plugin_resolved_path:\n",
   "            return get_with_context_result(None, plugin_load_context)\n",
   "        name = plugin_load_context.plugin_resolved_name\n",
   "        path = plugin_load_context.plugin_resolved_path\n",
   "        redirected_names = plugin_load_context.redirect_list or []\n",
   "        if path not in self._module_cache:\n",
   "            self._module_cache[path] = self._load_module_source(name, path)\n",
   "            self._load_config_defs(name, self._module_cache[path], path)\n",
   "            found_in_cache = False\n",
   "        obj = getattr(self._module_cache[path], self.class_name)\n",
   "            module = __import__(self.package, fromlist=[self.base_class])\n",
   "                plugin_class = getattr(module, self.base_class)\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "            if not issubclass(obj, plugin_class):\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "        self._display_plugin_load(self.class_name, name, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "        if not class_only:\n",
   "                instance = object.__new__(obj)\n",
   "                self._update_object(instance, name, path, redirected_names)\n",
   "                obj.__init__(instance, *args, **kwargs)\n",
   "                obj = instance\n",
   "                    return get_with_context_result(None, plugin_load_context)\n",
   "        self._update_object(obj, name, path, redirected_names)\n",
   "        return get_with_context_result(obj, plugin_load_context)\n",
   "            msg = 'Loading %s \\'%s\\' from %s' % (class_name, os.path.basename(name), path)\n",
   "                msg = '%s (searched paths: %s)' % (msg, self.format_paths(searched_paths))\n",
   "            if found_in_cache or class_only:\n",
   "                msg = '%s (found_in_cache=%s, class_only=%s)' % (msg, found_in_cache, class_only)\n",
   "            display.debug(msg)\n",
   "        dedupe = kwargs.pop('_dedupe', True)\n",
   "        path_only = kwargs.pop('path_only', False)\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        if path_only and class_only:\n",
   "        all_matches = []\n",
   "        found_in_cache = True\n",
   "        for i in self._get_paths():\n",
   "            all_matches.extend(glob.glob(os.path.join(i, \"*.py\")))\n",
   "        loaded_modules = set()\n",
   "        for path in sorted(all_matches, key=os.path.basename):\n",
   "            name = os.path.splitext(path)[0]\n",
   "            basename = os.path.basename(name)\n",
   "            if basename == '__init__' or basename in _PLUGIN_FILTERS[self.package]:\n",
   "            if dedupe and basename in loaded_modules:\n",
   "            loaded_modules.add(basename)\n",
   "            if path_only:\n",
   "                yield path\n",
   "            if path not in self._module_cache:\n",
   "                        full_name = '{0}_{1}'.format(abs(hash(path)), basename)\n",
   "                        full_name = basename\n",
   "                    module = self._load_module_source(full_name, path)\n",
   "                    self._load_config_defs(basename, module, path)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                self._module_cache[path] = module\n",
   "                found_in_cache = False\n",
   "                obj = getattr(self._module_cache[path], self.class_name)\n",
   "                display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                module = __import__(self.package, fromlist=[self.base_class])\n",
   "                    plugin_class = getattr(module, self.base_class)\n",
   "                if not issubclass(obj, plugin_class):\n",
   "            self._display_plugin_load(self.class_name, basename, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "            if not class_only:\n",
   "                    obj = obj(*args, **kwargs)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be incomplete: %s\" % (path, to_text(e)))\n",
   "            self._update_object(obj, basename, path)\n",
   "            yield obj\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).find_plugin(name, collection_list=collection_list)\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).get(name, *args, **kwargs)\n",
   "        plugins = list(super(Jinja2Loader, self).all(*args, **kwargs))\n",
   "        plugins.reverse()\n",
   "        return plugins\n",
   "    filters = defaultdict(frozenset)\n",
   "        filter_cfg = '/etc/ansible/plugin_filters.yml'\n",
   "        filter_cfg = C.PLUGIN_FILTERS_CFG\n",
   "    if os.path.exists(filter_cfg):\n",
   "        with open(filter_cfg, 'rb') as f:\n",
   "                filter_data = from_yaml(f.read())\n",
   "                display.warning(u'The plugin filter file, {0} was not parsable.'\n",
   "                                u' Skipping: {1}'.format(filter_cfg, to_text(e)))\n",
   "                return filters\n",
   "            version = filter_data['filter_version']\n",
   "            display.warning(u'The plugin filter file, {0} was invalid.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "            return filters\n",
   "        version = to_text(version)\n",
   "        version = version.strip()\n",
   "        if version == u'1.0':\n",
   "                filters['ansible.modules'] = frozenset(filter_data['module_blacklist'])\n",
   "                display.warning(u'Unable to parse the plugin filter file {0} as'\n",
   "                                u' Skipping.'.format(filter_cfg))\n",
   "                return filters\n",
   "            filters['ansible.plugins.action'] = filters['ansible.modules']\n",
   "            display.warning(u'The plugin filter file, {0} was a version not recognized by this'\n",
   "                            u' version of Ansible. Skipping.'.format(filter_cfg))\n",
   "            display.warning(u'The plugin filter file, {0} does not exist.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "    if 'stat' in filters['ansible.modules']:\n",
   "                           ' from the blacklist.'.format(to_native(filter_cfg)))\n",
   "    return filters\n",
   "    display.vvvv(to_text('Loading collection {0} from {1}'.format(collection_name, collection_path)))\n",
   "        if not _does_collection_support_ansible_version(collection_meta.get('requires_ansible', ''), ansible_version):\n",
   "                display.warning(message)\n",
   "        display.warning('Error parsing collection metadata requires_ansible value from collection {0}: {1}'.format(collection_name, ex))\n",
   "        display.warning('packaging Python module unavailable; unable to validate collection Ansible version requirements')\n",
   "        display.warning('AnsibleCollectionFinder has already been configured')\n"
  ]
 },
 "117": {
  "name": "full_name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/loader.py",
  "lineno": "753",
  "column": "12",
  "context": "       full_name = name\n        else:\n            full_name = '.'.join([self.package, name])\n\n        if full_name in sys.modules:\n            ",
  "context_lines": "        # avoid collisions across plugins\n        if name.startswith('ansible_collections.'):\n            full_name = name\n        else:\n            full_name = '.'.join([self.package, name])\n\n        if full_name in sys.modules:\n            # Avoids double loading, See https://github.com/ansible/ansible/issues/13110\n            return sys.modules[full_name]\n\n",
  "slicing": [
   "    imp = None\n",
   "display = Display()\n",
   "get_with_context_result = namedtuple('get_with_context_result', ['object', 'plugin_load_context'])\n",
   "    return [(name, obj) for (name, obj) in globals().items() if isinstance(obj, PluginLoader)]\n",
   "    b_path = os.path.expanduser(to_bytes(path, errors='surrogate_or_strict'))\n",
   "    if os.path.isdir(b_path):\n",
   "        for name, obj in get_all_plugin_loaders():\n",
   "            if obj.subdir:\n",
   "                plugin_path = os.path.join(b_path, to_bytes(obj.subdir))\n",
   "                if os.path.isdir(plugin_path):\n",
   "                    obj.add_directory(to_text(plugin_path))\n",
   "        display.warning(\"Ignoring invalid path provided to plugin path: '%s' is not a directory\" % to_text(path))\n",
   "        shell_type = 'sh'\n",
   "                shell_filename = os.path.basename(executable)\n",
   "                    shell = shell_loader.get(shell_filename)\n",
   "                    shell = None\n",
   "                if shell is None:\n",
   "                    for shell in shell_loader.all():\n",
   "                        if shell_filename in shell.COMPATIBLE_SHELLS:\n",
   "                            shell_type = shell.SHELL_FAMILY\n",
   "    shell = shell_loader.get(shell_type)\n",
   "    if not shell:\n",
   "        raise AnsibleError(\"Could not find the shell plugin required (%s).\" % shell_type)\n",
   "        setattr(shell, 'executable', executable)\n",
   "    return shell\n",
   "    loader = getattr(sys.modules[__name__], '%s_loader' % which_loader)\n",
   "    for path in paths:\n",
   "        loader.add_directory(path, with_subdir=True)\n",
   "        self.path = path\n",
   "        warning_text = deprecation.get('warning_text', None)\n",
   "        removal_date = deprecation.get('removal_date', None)\n",
   "        removal_version = deprecation.get('removal_version', None)\n",
   "        if removal_date is not None:\n",
   "            removal_version = None\n",
   "        if not warning_text:\n",
   "            warning_text = '{0} has been deprecated'.format(name)\n",
   "        display.deprecated(warning_text, date=removal_date, version=removal_version, collection_name=collection_name)\n",
   "        if removal_date:\n",
   "            self.removal_date = removal_date\n",
   "        if removal_version:\n",
   "            self.removal_version = removal_version\n",
   "        self.deprecation_warnings.append(warning_text)\n",
   "        aliases = {} if aliases is None else aliases\n",
   "        self.aliases = aliases\n",
   "            config = [config]\n",
   "        elif not config:\n",
   "            config = []\n",
   "        self.config = config\n",
   "        class_name = data.get('class_name')\n",
   "        package = data.get('package')\n",
   "        config = data.get('config')\n",
   "        subdir = data.get('subdir')\n",
   "        aliases = data.get('aliases')\n",
   "        base_class = data.get('base_class')\n",
   "        PATH_CACHE[class_name] = data.get('PATH_CACHE')\n",
   "        PLUGIN_PATH_CACHE[class_name] = data.get('PLUGIN_PATH_CACHE')\n",
   "        self.__init__(class_name, package, config, subdir, aliases, base_class)\n",
   "        ret = []\n",
   "        for i in paths:\n",
   "            if i not in ret:\n",
   "                ret.append(i)\n",
   "        return os.pathsep.join(ret)\n",
   "        results = []\n",
   "        results.append(dir)\n",
   "        for root, subdirs, files in os.walk(dir, followlinks=True):\n",
   "            if '__init__.py' in files:\n",
   "                for x in subdirs:\n",
   "                    results.append(os.path.join(root, x))\n",
   "        return results\n",
   "            m = __import__(self.package)\n",
   "            parts = self.package.split('.')[1:]\n",
   "            for parent_mod in parts:\n",
   "                m = getattr(m, parent_mod)\n",
   "            self.package_path = os.path.dirname(m.__file__)\n",
   "        if subdirs:\n",
   "        ret = [PluginPathContext(p, False) for p in self._extra_dirs]\n",
   "            for path in self.config:\n",
   "                path = os.path.realpath(os.path.expanduser(path))\n",
   "                if subdirs:\n",
   "                    contents = glob.glob(\"%s/*\" % path) + glob.glob(\"%s/*/*\" % path)\n",
   "                    for c in contents:\n",
   "                        if os.path.isdir(c) and c not in ret:\n",
   "                            ret.append(PluginPathContext(c, False))\n",
   "                if path not in ret:\n",
   "                    ret.append(PluginPathContext(path, False))\n",
   "        ret.extend([PluginPathContext(p, True) for p in self._get_package_paths(subdirs=subdirs)])\n",
   "        ret.sort(key=lambda p: p.path.endswith('/windows'))\n",
   "        self._paths = ret\n",
   "        return ret\n",
   "        paths_with_context = self._get_paths_with_context(subdirs=subdirs)\n",
   "        return [path_with_context.path for path_with_context in paths_with_context]\n",
   "            type_name = get_plugin_class(self.class_name)\n",
   "            if type_name in C.CONFIGURABLE_PLUGINS:\n",
   "                dstring = AnsibleLoader(getattr(module, 'DOCUMENTATION', ''), file_name=path).get_single_data()\n",
   "                if dstring:\n",
   "                    add_fragments(dstring, path, fragment_loader=fragment_loader, is_module=(type_name == 'module'))\n",
   "                if dstring and 'options' in dstring and isinstance(dstring['options'], dict):\n",
   "                    C.config.initialize_plugin_configuration_definitions(type_name, name, dstring['options'])\n",
   "                    display.debug('Loaded config def from plugin (%s/%s)' % (type_name, name))\n",
   "        directory = os.path.realpath(directory)\n",
   "        if directory is not None:\n",
   "                directory = os.path.join(directory, self.subdir)\n",
   "            if directory not in self._extra_dirs:\n",
   "                self._extra_dirs.append(directory)\n",
   "                display.debug('Added %s to loader search path' % (directory))\n",
   "        collection_pkg = import_module(acr.n_python_collection_package_name)\n",
   "        if not collection_pkg:\n",
   "        collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "        if not collection_meta:\n",
   "            subdir_qualified_resource = '.'.join([acr.subdirs, acr.resource])\n",
   "            subdir_qualified_resource = acr.resource\n",
   "        entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource + extension, None)\n",
   "        if not entry:\n",
   "            entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource, None)\n",
   "        return entry\n",
   "        plugin_type = AnsibleCollectionRef.legacy_plugin_dir_to_plugin_type(self.subdir)\n",
   "        acr = AnsibleCollectionRef.from_fqcr(fq_name, plugin_type)\n",
   "        routing_metadata = self._query_collection_routing_meta(acr, plugin_type, extension=extension)\n",
   "        if routing_metadata:\n",
   "            deprecation = routing_metadata.get('deprecation', None)\n",
   "            plugin_load_context.record_deprecation(fq_name, deprecation, acr.collection)\n",
   "            tombstone = routing_metadata.get('tombstone', None)\n",
   "            if tombstone:\n",
   "                removal_date = tombstone.get('removal_date')\n",
   "                removal_version = tombstone.get('removal_version')\n",
   "                warning_text = tombstone.get('warning_text') or '{0} has been removed.'.format(fq_name)\n",
   "                removed_msg = display.get_deprecation_message(msg=warning_text, version=removal_version,\n",
   "                                                              date=removal_date, removed=True,\n",
   "                                                              collection_name=acr.collection)\n",
   "                plugin_load_context.removal_date = removal_date\n",
   "                plugin_load_context.removal_version = removal_version\n",
   "                plugin_load_context.exit_reason = removed_msg\n",
   "                raise AnsiblePluginRemovedError(removed_msg, plugin_load_context=plugin_load_context)\n",
   "            redirect = routing_metadata.get('redirect', None)\n",
   "            if redirect:\n",
   "                display.vv(\"redirecting (type: {0}) {1} to {2}\".format(plugin_type, fq_name, redirect))\n",
   "                return plugin_load_context.redirect(redirect)\n",
   "        n_resource = to_native(acr.resource, errors='strict')\n",
   "        full_name = '{0}.{1}'.format(acr.n_python_package_name, n_resource)\n",
   "            n_resource += extension\n",
   "        pkg = sys.modules.get(acr.n_python_package_name)\n",
   "        if not pkg:\n",
   "                pkg = import_module(acr.n_python_package_name)\n",
   "                return plugin_load_context.nope('Python package {0} not found'.format(acr.n_python_package_name))\n",
   "        pkg_path = os.path.dirname(pkg.__file__)\n",
   "        n_resource_path = os.path.join(pkg_path, n_resource)\n",
   "        if os.path.exists(n_resource_path):\n",
   "                full_name, to_text(n_resource_path), acr.collection, 'found exact match for {0} in {1}'.format(full_name, acr.collection))\n",
   "            return plugin_load_context.nope('no match for {0} in {1}'.format(to_text(n_resource), acr.collection))\n",
   "        found_files = [f\n",
   "                       for f in glob.iglob(os.path.join(pkg_path, n_resource) + '.*')\n",
   "                       if os.path.isfile(f) and not f.endswith(C.MODULE_IGNORE_EXTS)]\n",
   "        if not found_files:\n",
   "            return plugin_load_context.nope('failed fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        if len(found_files) > 1:\n",
   "            full_name, to_text(found_files[0]), acr.collection, 'found fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        result = self.find_plugin_with_context(name, mod_type, ignore_deprecated, check_aliases, collection_list)\n",
   "        if result.resolved and result.plugin_resolved_path:\n",
   "            return result.plugin_resolved_path\n",
   "        plugin_load_context = PluginLoadContext()\n",
   "        plugin_load_context.original_name = name\n",
   "            result = self._resolve_plugin_step(name, mod_type, ignore_deprecated, check_aliases, collection_list, plugin_load_context=plugin_load_context)\n",
   "            if result.pending_redirect:\n",
   "                if result.pending_redirect in result.redirect_list:\n",
   "                    raise AnsiblePluginCircularRedirect('plugin redirect loop resolving {0} (path: {1})'.format(result.original_name, result.redirect_list))\n",
   "                name = result.pending_redirect\n",
   "                result.pending_redirect = None\n",
   "                plugin_load_context = result\n",
   "        if plugin_load_context.error_list:\n",
   "            display.warning(\"errors were encountered during the plugin load for {0}:\\n{1}\".format(name, plugin_load_context.error_list))\n",
   "        return plugin_load_context\n",
   "        if not plugin_load_context:\n",
   "        plugin_load_context.redirect_list.append(name)\n",
   "        plugin_load_context.resolved = False\n",
   "        if name in _PLUGIN_FILTERS[self.package]:\n",
   "            plugin_load_context.exit_reason = '{0} matched a defined plugin filter'.format(name)\n",
   "            return plugin_load_context\n",
   "            suffix = mod_type\n",
   "            suffix = '.py'\n",
   "            suffix = ''\n",
   "        if (AnsibleCollectionRef.is_valid_fqcr(name) or collection_list) and not name.startswith('Ansible'):\n",
   "            if '.' in name or not collection_list:\n",
   "                candidates = [name]\n",
   "                candidates = ['{0}.{1}'.format(c, name) for c in collection_list]\n",
   "            for candidate_name in candidates:\n",
   "                    plugin_load_context.load_attempts.append(candidate_name)\n",
   "                    if candidate_name.startswith('ansible.legacy'):\n",
   "                        plugin_load_context = self._find_plugin_legacy(name.replace('ansible.legacy.', '', 1),\n",
   "                                                                       plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "                        plugin_load_context = self._find_fq_plugin(candidate_name, suffix, plugin_load_context=plugin_load_context)\n",
   "                        if candidate_name != plugin_load_context.original_name and candidate_name not in plugin_load_context.redirect_list:\n",
   "                            plugin_load_context.redirect_list.append(candidate_name)\n",
   "                    if plugin_load_context.resolved or plugin_load_context.pending_redirect:  # if we got an answer or need to chase down a redirect, return\n",
   "                        return plugin_load_context\n",
   "                    plugin_load_context.import_error_list.append(ie)\n",
   "                    plugin_load_context.error_list.append(to_native(ex))\n",
   "            if plugin_load_context.error_list:\n",
   "                display.debug(msg='plugin lookup for {0} failed; errors: {1}'.format(name, '; '.join(plugin_load_context.error_list)))\n",
   "            plugin_load_context.exit_reason = 'no matches found for {0}'.format(name)\n",
   "            return plugin_load_context\n",
   "        return self._find_plugin_legacy(name, plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "        plugin_load_context.resolved = False\n",
   "            name = self.aliases.get(name, name)\n",
   "        pull_cache = self._plugin_path_cache[suffix]\n",
   "            path_with_context = pull_cache[name]\n",
   "            plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "            plugin_load_context.plugin_resolved_name = name\n",
   "            plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "            plugin_load_context.resolved = True\n",
   "            return plugin_load_context\n",
   "        for path_context in (p for p in self._get_paths_with_context() if p.path not in self._searched_paths and os.path.isdir(p.path)):\n",
   "            path = path_context.path\n",
   "            display.debug('trying %s' % path)\n",
   "            plugin_load_context.load_attempts.append(path)\n",
   "                full_paths = (os.path.join(path, f) for f in os.listdir(path))\n",
   "                display.warning(\"Error accessing plugin paths: %s\" % to_text(e))\n",
   "            for full_path in (f for f in full_paths if os.path.isfile(f) and not f.endswith('__init__.py')):\n",
   "                full_name = os.path.basename(full_path)\n",
   "                if any(full_path.endswith(x) for x in C.MODULE_IGNORE_EXTS):\n",
   "                splitname = os.path.splitext(full_name)\n",
   "                base_name = splitname[0]\n",
   "                internal = path_context.internal\n",
   "                    extension = splitname[1]\n",
   "                    extension = ''\n",
   "                if base_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][full_name] = PluginPathContext(full_path, internal)\n",
   "                if base_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][full_name] = PluginPathContext(full_path, internal)\n",
   "            self._searched_paths.add(path)\n",
   "                path_with_context = pull_cache[name]\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        if not name.startswith('_'):\n",
   "            alias_name = '_' + name\n",
   "            if alias_name in pull_cache:\n",
   "                path_with_context = pull_cache[alias_name]\n",
   "                if not ignore_deprecated and not os.path.islink(path_with_context.path):\n",
   "                    display.deprecated('%s is kept for backwards compatibility but usage is discouraged. '  # pylint: disable=ansible-deprecated-no-version\n",
   "                                       'The module documentation details page may explain more about this rationale.' % name.lstrip('_'))\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = alias_name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        candidate_fqcr = 'ansible.builtin.{0}'.format(name)\n",
   "        if '.' not in name and AnsibleCollectionRef.is_valid_fqcr(candidate_fqcr):\n",
   "            return self._find_fq_plugin(fq_name=candidate_fqcr, extension=suffix, plugin_load_context=plugin_load_context)\n",
   "        return plugin_load_context.nope('{0} is not eligible for last-chance resolution'.format(name))\n",
   "            return self.find_plugin(name, collection_list=collection_list) is not None\n",
   "            display.debug('has_plugin error: {0}'.format(to_text(ex)))\n",
   "        if name.startswith('ansible_collections.'):\n",
   "            full_name = name\n",
   "            full_name = '.'.join([self.package, name])\n",
   "        if full_name in sys.modules:\n",
   "            return sys.modules[full_name]\n",
   "            if imp is None:\n",
   "                spec = importlib.util.spec_from_file_location(to_native(full_name), to_native(path))\n",
   "                module = importlib.util.module_from_spec(spec)\n",
   "                spec.loader.exec_module(module)\n",
   "                sys.modules[full_name] = module\n",
   "                with open(to_bytes(path), 'rb') as module_file:\n",
   "                    module = imp.load_source(to_native(full_name), to_native(path), module_file)\n",
   "        return module\n",
   "        setattr(obj, '_original_path', path)\n",
   "        setattr(obj, '_load_name', name)\n",
   "        setattr(obj, '_redirected_names', redirected_names or [])\n",
   "        return self.get_with_context(name, *args, **kwargs).object\n",
   "        found_in_cache = True\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        collection_list = kwargs.pop('collection_list', None)\n",
   "        if name in self.aliases:\n",
   "            name = self.aliases[name]\n",
   "        plugin_load_context = self.find_plugin_with_context(name, collection_list=collection_list)\n",
   "        if not plugin_load_context.resolved or not plugin_load_context.plugin_resolved_path:\n",
   "            return get_with_context_result(None, plugin_load_context)\n",
   "        name = plugin_load_context.plugin_resolved_name\n",
   "        path = plugin_load_context.plugin_resolved_path\n",
   "        redirected_names = plugin_load_context.redirect_list or []\n",
   "        if path not in self._module_cache:\n",
   "            self._module_cache[path] = self._load_module_source(name, path)\n",
   "            self._load_config_defs(name, self._module_cache[path], path)\n",
   "            found_in_cache = False\n",
   "        obj = getattr(self._module_cache[path], self.class_name)\n",
   "            module = __import__(self.package, fromlist=[self.base_class])\n",
   "                plugin_class = getattr(module, self.base_class)\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "            if not issubclass(obj, plugin_class):\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "        self._display_plugin_load(self.class_name, name, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "        if not class_only:\n",
   "                instance = object.__new__(obj)\n",
   "                self._update_object(instance, name, path, redirected_names)\n",
   "                obj.__init__(instance, *args, **kwargs)\n",
   "                obj = instance\n",
   "                    return get_with_context_result(None, plugin_load_context)\n",
   "        self._update_object(obj, name, path, redirected_names)\n",
   "        return get_with_context_result(obj, plugin_load_context)\n",
   "            msg = 'Loading %s \\'%s\\' from %s' % (class_name, os.path.basename(name), path)\n",
   "                msg = '%s (searched paths: %s)' % (msg, self.format_paths(searched_paths))\n",
   "            if found_in_cache or class_only:\n",
   "                msg = '%s (found_in_cache=%s, class_only=%s)' % (msg, found_in_cache, class_only)\n",
   "            display.debug(msg)\n",
   "        dedupe = kwargs.pop('_dedupe', True)\n",
   "        path_only = kwargs.pop('path_only', False)\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        if path_only and class_only:\n",
   "        all_matches = []\n",
   "        found_in_cache = True\n",
   "        for i in self._get_paths():\n",
   "            all_matches.extend(glob.glob(os.path.join(i, \"*.py\")))\n",
   "        loaded_modules = set()\n",
   "        for path in sorted(all_matches, key=os.path.basename):\n",
   "            name = os.path.splitext(path)[0]\n",
   "            basename = os.path.basename(name)\n",
   "            if basename == '__init__' or basename in _PLUGIN_FILTERS[self.package]:\n",
   "            if dedupe and basename in loaded_modules:\n",
   "            loaded_modules.add(basename)\n",
   "            if path_only:\n",
   "                yield path\n",
   "            if path not in self._module_cache:\n",
   "                        full_name = '{0}_{1}'.format(abs(hash(path)), basename)\n",
   "                        full_name = basename\n",
   "                    module = self._load_module_source(full_name, path)\n",
   "                    self._load_config_defs(basename, module, path)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                self._module_cache[path] = module\n",
   "                found_in_cache = False\n",
   "                obj = getattr(self._module_cache[path], self.class_name)\n",
   "                display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                module = __import__(self.package, fromlist=[self.base_class])\n",
   "                    plugin_class = getattr(module, self.base_class)\n",
   "                if not issubclass(obj, plugin_class):\n",
   "            self._display_plugin_load(self.class_name, basename, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "            if not class_only:\n",
   "                    obj = obj(*args, **kwargs)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be incomplete: %s\" % (path, to_text(e)))\n",
   "            self._update_object(obj, basename, path)\n",
   "            yield obj\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).find_plugin(name, collection_list=collection_list)\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).get(name, *args, **kwargs)\n",
   "        plugins = list(super(Jinja2Loader, self).all(*args, **kwargs))\n",
   "        plugins.reverse()\n",
   "        return plugins\n",
   "    filters = defaultdict(frozenset)\n",
   "        filter_cfg = '/etc/ansible/plugin_filters.yml'\n",
   "        filter_cfg = C.PLUGIN_FILTERS_CFG\n",
   "    if os.path.exists(filter_cfg):\n",
   "        with open(filter_cfg, 'rb') as f:\n",
   "                filter_data = from_yaml(f.read())\n",
   "                display.warning(u'The plugin filter file, {0} was not parsable.'\n",
   "                                u' Skipping: {1}'.format(filter_cfg, to_text(e)))\n",
   "                return filters\n",
   "            version = filter_data['filter_version']\n",
   "            display.warning(u'The plugin filter file, {0} was invalid.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "            return filters\n",
   "        version = to_text(version)\n",
   "        version = version.strip()\n",
   "        if version == u'1.0':\n",
   "                filters['ansible.modules'] = frozenset(filter_data['module_blacklist'])\n",
   "                display.warning(u'Unable to parse the plugin filter file {0} as'\n",
   "                                u' Skipping.'.format(filter_cfg))\n",
   "                return filters\n",
   "            filters['ansible.plugins.action'] = filters['ansible.modules']\n",
   "            display.warning(u'The plugin filter file, {0} was a version not recognized by this'\n",
   "                            u' version of Ansible. Skipping.'.format(filter_cfg))\n",
   "            display.warning(u'The plugin filter file, {0} does not exist.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "    if 'stat' in filters['ansible.modules']:\n",
   "                           ' from the blacklist.'.format(to_native(filter_cfg)))\n",
   "    return filters\n",
   "    display.vvvv(to_text('Loading collection {0} from {1}'.format(collection_name, collection_path)))\n",
   "        if not _does_collection_support_ansible_version(collection_meta.get('requires_ansible', ''), ansible_version):\n",
   "                display.warning(message)\n",
   "        display.warning('Error parsing collection metadata requires_ansible value from collection {0}: {1}'.format(collection_name, ex))\n",
   "        display.warning('packaging Python module unavailable; unable to validate collection Ansible version requirements')\n",
   "        display.warning('AnsibleCollectionFinder has already been configured')\n"
  ]
 },
 "118": {
  "name": "class_only",
  "type": "bool",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/loader.py",
  "lineno": "786",
  "column": "8",
  "context": "uments '''\n\n        found_in_cache = True\n        class_only = kwargs.pop('class_only', False)\n        collection_list = kwargs.pop('collection_l",
  "context_lines": "        return self.get_with_context(name, *args, **kwargs).object\n\n    def get_with_context(self, name, *args, **kwargs):\n        ''' instantiates a plugin of the given name using arguments '''\n\n        found_in_cache = True\n        class_only = kwargs.pop('class_only', False)\n        collection_list = kwargs.pop('collection_list', None)\n        if name in self.aliases:\n            name = self.aliases[name]\n        plugin_load_context = self.find_plugin_with_context(name, collection_list=collection_list)\n",
  "slicing": [
   "    imp = None\n",
   "display = Display()\n",
   "get_with_context_result = namedtuple('get_with_context_result', ['object', 'plugin_load_context'])\n",
   "    return [(name, obj) for (name, obj) in globals().items() if isinstance(obj, PluginLoader)]\n",
   "    b_path = os.path.expanduser(to_bytes(path, errors='surrogate_or_strict'))\n",
   "    if os.path.isdir(b_path):\n",
   "        for name, obj in get_all_plugin_loaders():\n",
   "            if obj.subdir:\n",
   "                plugin_path = os.path.join(b_path, to_bytes(obj.subdir))\n",
   "                if os.path.isdir(plugin_path):\n",
   "                    obj.add_directory(to_text(plugin_path))\n",
   "        display.warning(\"Ignoring invalid path provided to plugin path: '%s' is not a directory\" % to_text(path))\n",
   "        shell_type = 'sh'\n",
   "                shell_filename = os.path.basename(executable)\n",
   "                    shell = shell_loader.get(shell_filename)\n",
   "                    shell = None\n",
   "                if shell is None:\n",
   "                    for shell in shell_loader.all():\n",
   "                        if shell_filename in shell.COMPATIBLE_SHELLS:\n",
   "                            shell_type = shell.SHELL_FAMILY\n",
   "    shell = shell_loader.get(shell_type)\n",
   "    if not shell:\n",
   "        raise AnsibleError(\"Could not find the shell plugin required (%s).\" % shell_type)\n",
   "        setattr(shell, 'executable', executable)\n",
   "    return shell\n",
   "    loader = getattr(sys.modules[__name__], '%s_loader' % which_loader)\n",
   "    for path in paths:\n",
   "        loader.add_directory(path, with_subdir=True)\n",
   "        self.path = path\n",
   "        warning_text = deprecation.get('warning_text', None)\n",
   "        removal_date = deprecation.get('removal_date', None)\n",
   "        removal_version = deprecation.get('removal_version', None)\n",
   "        if removal_date is not None:\n",
   "            removal_version = None\n",
   "        if not warning_text:\n",
   "            warning_text = '{0} has been deprecated'.format(name)\n",
   "        display.deprecated(warning_text, date=removal_date, version=removal_version, collection_name=collection_name)\n",
   "        if removal_date:\n",
   "            self.removal_date = removal_date\n",
   "        if removal_version:\n",
   "            self.removal_version = removal_version\n",
   "        self.deprecation_warnings.append(warning_text)\n",
   "        aliases = {} if aliases is None else aliases\n",
   "        self.aliases = aliases\n",
   "            config = [config]\n",
   "        elif not config:\n",
   "            config = []\n",
   "        self.config = config\n",
   "        class_name = data.get('class_name')\n",
   "        package = data.get('package')\n",
   "        config = data.get('config')\n",
   "        subdir = data.get('subdir')\n",
   "        aliases = data.get('aliases')\n",
   "        base_class = data.get('base_class')\n",
   "        PATH_CACHE[class_name] = data.get('PATH_CACHE')\n",
   "        PLUGIN_PATH_CACHE[class_name] = data.get('PLUGIN_PATH_CACHE')\n",
   "        self.__init__(class_name, package, config, subdir, aliases, base_class)\n",
   "        ret = []\n",
   "        for i in paths:\n",
   "            if i not in ret:\n",
   "                ret.append(i)\n",
   "        return os.pathsep.join(ret)\n",
   "        results = []\n",
   "        results.append(dir)\n",
   "        for root, subdirs, files in os.walk(dir, followlinks=True):\n",
   "            if '__init__.py' in files:\n",
   "                for x in subdirs:\n",
   "                    results.append(os.path.join(root, x))\n",
   "        return results\n",
   "            m = __import__(self.package)\n",
   "            parts = self.package.split('.')[1:]\n",
   "            for parent_mod in parts:\n",
   "                m = getattr(m, parent_mod)\n",
   "            self.package_path = os.path.dirname(m.__file__)\n",
   "        if subdirs:\n",
   "        ret = [PluginPathContext(p, False) for p in self._extra_dirs]\n",
   "            for path in self.config:\n",
   "                path = os.path.realpath(os.path.expanduser(path))\n",
   "                if subdirs:\n",
   "                    contents = glob.glob(\"%s/*\" % path) + glob.glob(\"%s/*/*\" % path)\n",
   "                    for c in contents:\n",
   "                        if os.path.isdir(c) and c not in ret:\n",
   "                            ret.append(PluginPathContext(c, False))\n",
   "                if path not in ret:\n",
   "                    ret.append(PluginPathContext(path, False))\n",
   "        ret.extend([PluginPathContext(p, True) for p in self._get_package_paths(subdirs=subdirs)])\n",
   "        ret.sort(key=lambda p: p.path.endswith('/windows'))\n",
   "        self._paths = ret\n",
   "        return ret\n",
   "        paths_with_context = self._get_paths_with_context(subdirs=subdirs)\n",
   "        return [path_with_context.path for path_with_context in paths_with_context]\n",
   "            type_name = get_plugin_class(self.class_name)\n",
   "            if type_name in C.CONFIGURABLE_PLUGINS:\n",
   "                dstring = AnsibleLoader(getattr(module, 'DOCUMENTATION', ''), file_name=path).get_single_data()\n",
   "                if dstring:\n",
   "                    add_fragments(dstring, path, fragment_loader=fragment_loader, is_module=(type_name == 'module'))\n",
   "                if dstring and 'options' in dstring and isinstance(dstring['options'], dict):\n",
   "                    C.config.initialize_plugin_configuration_definitions(type_name, name, dstring['options'])\n",
   "                    display.debug('Loaded config def from plugin (%s/%s)' % (type_name, name))\n",
   "        directory = os.path.realpath(directory)\n",
   "        if directory is not None:\n",
   "                directory = os.path.join(directory, self.subdir)\n",
   "            if directory not in self._extra_dirs:\n",
   "                self._extra_dirs.append(directory)\n",
   "                display.debug('Added %s to loader search path' % (directory))\n",
   "        collection_pkg = import_module(acr.n_python_collection_package_name)\n",
   "        if not collection_pkg:\n",
   "        collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "        if not collection_meta:\n",
   "            subdir_qualified_resource = '.'.join([acr.subdirs, acr.resource])\n",
   "            subdir_qualified_resource = acr.resource\n",
   "        entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource + extension, None)\n",
   "        if not entry:\n",
   "            entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource, None)\n",
   "        return entry\n",
   "        plugin_type = AnsibleCollectionRef.legacy_plugin_dir_to_plugin_type(self.subdir)\n",
   "        acr = AnsibleCollectionRef.from_fqcr(fq_name, plugin_type)\n",
   "        routing_metadata = self._query_collection_routing_meta(acr, plugin_type, extension=extension)\n",
   "        if routing_metadata:\n",
   "            deprecation = routing_metadata.get('deprecation', None)\n",
   "            plugin_load_context.record_deprecation(fq_name, deprecation, acr.collection)\n",
   "            tombstone = routing_metadata.get('tombstone', None)\n",
   "            if tombstone:\n",
   "                removal_date = tombstone.get('removal_date')\n",
   "                removal_version = tombstone.get('removal_version')\n",
   "                warning_text = tombstone.get('warning_text') or '{0} has been removed.'.format(fq_name)\n",
   "                removed_msg = display.get_deprecation_message(msg=warning_text, version=removal_version,\n",
   "                                                              date=removal_date, removed=True,\n",
   "                                                              collection_name=acr.collection)\n",
   "                plugin_load_context.removal_date = removal_date\n",
   "                plugin_load_context.removal_version = removal_version\n",
   "                plugin_load_context.exit_reason = removed_msg\n",
   "                raise AnsiblePluginRemovedError(removed_msg, plugin_load_context=plugin_load_context)\n",
   "            redirect = routing_metadata.get('redirect', None)\n",
   "            if redirect:\n",
   "                display.vv(\"redirecting (type: {0}) {1} to {2}\".format(plugin_type, fq_name, redirect))\n",
   "                return plugin_load_context.redirect(redirect)\n",
   "        n_resource = to_native(acr.resource, errors='strict')\n",
   "        full_name = '{0}.{1}'.format(acr.n_python_package_name, n_resource)\n",
   "            n_resource += extension\n",
   "        pkg = sys.modules.get(acr.n_python_package_name)\n",
   "        if not pkg:\n",
   "                pkg = import_module(acr.n_python_package_name)\n",
   "                return plugin_load_context.nope('Python package {0} not found'.format(acr.n_python_package_name))\n",
   "        pkg_path = os.path.dirname(pkg.__file__)\n",
   "        n_resource_path = os.path.join(pkg_path, n_resource)\n",
   "        if os.path.exists(n_resource_path):\n",
   "                full_name, to_text(n_resource_path), acr.collection, 'found exact match for {0} in {1}'.format(full_name, acr.collection))\n",
   "            return plugin_load_context.nope('no match for {0} in {1}'.format(to_text(n_resource), acr.collection))\n",
   "        found_files = [f\n",
   "                       for f in glob.iglob(os.path.join(pkg_path, n_resource) + '.*')\n",
   "                       if os.path.isfile(f) and not f.endswith(C.MODULE_IGNORE_EXTS)]\n",
   "        if not found_files:\n",
   "            return plugin_load_context.nope('failed fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        if len(found_files) > 1:\n",
   "            full_name, to_text(found_files[0]), acr.collection, 'found fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        result = self.find_plugin_with_context(name, mod_type, ignore_deprecated, check_aliases, collection_list)\n",
   "        if result.resolved and result.plugin_resolved_path:\n",
   "            return result.plugin_resolved_path\n",
   "        plugin_load_context = PluginLoadContext()\n",
   "        plugin_load_context.original_name = name\n",
   "            result = self._resolve_plugin_step(name, mod_type, ignore_deprecated, check_aliases, collection_list, plugin_load_context=plugin_load_context)\n",
   "            if result.pending_redirect:\n",
   "                if result.pending_redirect in result.redirect_list:\n",
   "                    raise AnsiblePluginCircularRedirect('plugin redirect loop resolving {0} (path: {1})'.format(result.original_name, result.redirect_list))\n",
   "                name = result.pending_redirect\n",
   "                result.pending_redirect = None\n",
   "                plugin_load_context = result\n",
   "        if plugin_load_context.error_list:\n",
   "            display.warning(\"errors were encountered during the plugin load for {0}:\\n{1}\".format(name, plugin_load_context.error_list))\n",
   "        return plugin_load_context\n",
   "        if not plugin_load_context:\n",
   "        plugin_load_context.redirect_list.append(name)\n",
   "        plugin_load_context.resolved = False\n",
   "        if name in _PLUGIN_FILTERS[self.package]:\n",
   "            plugin_load_context.exit_reason = '{0} matched a defined plugin filter'.format(name)\n",
   "            return plugin_load_context\n",
   "            suffix = mod_type\n",
   "            suffix = '.py'\n",
   "            suffix = ''\n",
   "        if (AnsibleCollectionRef.is_valid_fqcr(name) or collection_list) and not name.startswith('Ansible'):\n",
   "            if '.' in name or not collection_list:\n",
   "                candidates = [name]\n",
   "                candidates = ['{0}.{1}'.format(c, name) for c in collection_list]\n",
   "            for candidate_name in candidates:\n",
   "                    plugin_load_context.load_attempts.append(candidate_name)\n",
   "                    if candidate_name.startswith('ansible.legacy'):\n",
   "                        plugin_load_context = self._find_plugin_legacy(name.replace('ansible.legacy.', '', 1),\n",
   "                                                                       plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "                        plugin_load_context = self._find_fq_plugin(candidate_name, suffix, plugin_load_context=plugin_load_context)\n",
   "                        if candidate_name != plugin_load_context.original_name and candidate_name not in plugin_load_context.redirect_list:\n",
   "                            plugin_load_context.redirect_list.append(candidate_name)\n",
   "                    if plugin_load_context.resolved or plugin_load_context.pending_redirect:  # if we got an answer or need to chase down a redirect, return\n",
   "                        return plugin_load_context\n",
   "                    plugin_load_context.import_error_list.append(ie)\n",
   "                    plugin_load_context.error_list.append(to_native(ex))\n",
   "            if plugin_load_context.error_list:\n",
   "                display.debug(msg='plugin lookup for {0} failed; errors: {1}'.format(name, '; '.join(plugin_load_context.error_list)))\n",
   "            plugin_load_context.exit_reason = 'no matches found for {0}'.format(name)\n",
   "            return plugin_load_context\n",
   "        return self._find_plugin_legacy(name, plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "        plugin_load_context.resolved = False\n",
   "            name = self.aliases.get(name, name)\n",
   "        pull_cache = self._plugin_path_cache[suffix]\n",
   "            path_with_context = pull_cache[name]\n",
   "            plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "            plugin_load_context.plugin_resolved_name = name\n",
   "            plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "            plugin_load_context.resolved = True\n",
   "            return plugin_load_context\n",
   "        for path_context in (p for p in self._get_paths_with_context() if p.path not in self._searched_paths and os.path.isdir(p.path)):\n",
   "            path = path_context.path\n",
   "            display.debug('trying %s' % path)\n",
   "            plugin_load_context.load_attempts.append(path)\n",
   "                full_paths = (os.path.join(path, f) for f in os.listdir(path))\n",
   "                display.warning(\"Error accessing plugin paths: %s\" % to_text(e))\n",
   "            for full_path in (f for f in full_paths if os.path.isfile(f) and not f.endswith('__init__.py')):\n",
   "                full_name = os.path.basename(full_path)\n",
   "                if any(full_path.endswith(x) for x in C.MODULE_IGNORE_EXTS):\n",
   "                splitname = os.path.splitext(full_name)\n",
   "                base_name = splitname[0]\n",
   "                internal = path_context.internal\n",
   "                    extension = splitname[1]\n",
   "                    extension = ''\n",
   "                if base_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][full_name] = PluginPathContext(full_path, internal)\n",
   "                if base_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][full_name] = PluginPathContext(full_path, internal)\n",
   "            self._searched_paths.add(path)\n",
   "                path_with_context = pull_cache[name]\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        if not name.startswith('_'):\n",
   "            alias_name = '_' + name\n",
   "            if alias_name in pull_cache:\n",
   "                path_with_context = pull_cache[alias_name]\n",
   "                if not ignore_deprecated and not os.path.islink(path_with_context.path):\n",
   "                    display.deprecated('%s is kept for backwards compatibility but usage is discouraged. '  # pylint: disable=ansible-deprecated-no-version\n",
   "                                       'The module documentation details page may explain more about this rationale.' % name.lstrip('_'))\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = alias_name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        candidate_fqcr = 'ansible.builtin.{0}'.format(name)\n",
   "        if '.' not in name and AnsibleCollectionRef.is_valid_fqcr(candidate_fqcr):\n",
   "            return self._find_fq_plugin(fq_name=candidate_fqcr, extension=suffix, plugin_load_context=plugin_load_context)\n",
   "        return plugin_load_context.nope('{0} is not eligible for last-chance resolution'.format(name))\n",
   "            return self.find_plugin(name, collection_list=collection_list) is not None\n",
   "            display.debug('has_plugin error: {0}'.format(to_text(ex)))\n",
   "        if name.startswith('ansible_collections.'):\n",
   "            full_name = name\n",
   "            full_name = '.'.join([self.package, name])\n",
   "        if full_name in sys.modules:\n",
   "            return sys.modules[full_name]\n",
   "            if imp is None:\n",
   "                spec = importlib.util.spec_from_file_location(to_native(full_name), to_native(path))\n",
   "                module = importlib.util.module_from_spec(spec)\n",
   "                spec.loader.exec_module(module)\n",
   "                sys.modules[full_name] = module\n",
   "                with open(to_bytes(path), 'rb') as module_file:\n",
   "                    module = imp.load_source(to_native(full_name), to_native(path), module_file)\n",
   "        return module\n",
   "        setattr(obj, '_original_path', path)\n",
   "        setattr(obj, '_load_name', name)\n",
   "        setattr(obj, '_redirected_names', redirected_names or [])\n",
   "        return self.get_with_context(name, *args, **kwargs).object\n",
   "        found_in_cache = True\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        collection_list = kwargs.pop('collection_list', None)\n",
   "        if name in self.aliases:\n",
   "            name = self.aliases[name]\n",
   "        plugin_load_context = self.find_plugin_with_context(name, collection_list=collection_list)\n",
   "        if not plugin_load_context.resolved or not plugin_load_context.plugin_resolved_path:\n",
   "            return get_with_context_result(None, plugin_load_context)\n",
   "        name = plugin_load_context.plugin_resolved_name\n",
   "        path = plugin_load_context.plugin_resolved_path\n",
   "        redirected_names = plugin_load_context.redirect_list or []\n",
   "        if path not in self._module_cache:\n",
   "            self._module_cache[path] = self._load_module_source(name, path)\n",
   "            self._load_config_defs(name, self._module_cache[path], path)\n",
   "            found_in_cache = False\n",
   "        obj = getattr(self._module_cache[path], self.class_name)\n",
   "            module = __import__(self.package, fromlist=[self.base_class])\n",
   "                plugin_class = getattr(module, self.base_class)\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "            if not issubclass(obj, plugin_class):\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "        self._display_plugin_load(self.class_name, name, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "        if not class_only:\n",
   "                instance = object.__new__(obj)\n",
   "                self._update_object(instance, name, path, redirected_names)\n",
   "                obj.__init__(instance, *args, **kwargs)\n",
   "                obj = instance\n",
   "                    return get_with_context_result(None, plugin_load_context)\n",
   "        self._update_object(obj, name, path, redirected_names)\n",
   "        return get_with_context_result(obj, plugin_load_context)\n",
   "            msg = 'Loading %s \\'%s\\' from %s' % (class_name, os.path.basename(name), path)\n",
   "                msg = '%s (searched paths: %s)' % (msg, self.format_paths(searched_paths))\n",
   "            if found_in_cache or class_only:\n",
   "                msg = '%s (found_in_cache=%s, class_only=%s)' % (msg, found_in_cache, class_only)\n",
   "            display.debug(msg)\n",
   "        dedupe = kwargs.pop('_dedupe', True)\n",
   "        path_only = kwargs.pop('path_only', False)\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        if path_only and class_only:\n",
   "        all_matches = []\n",
   "        found_in_cache = True\n",
   "        for i in self._get_paths():\n",
   "            all_matches.extend(glob.glob(os.path.join(i, \"*.py\")))\n",
   "        loaded_modules = set()\n",
   "        for path in sorted(all_matches, key=os.path.basename):\n",
   "            name = os.path.splitext(path)[0]\n",
   "            basename = os.path.basename(name)\n",
   "            if basename == '__init__' or basename in _PLUGIN_FILTERS[self.package]:\n",
   "            if dedupe and basename in loaded_modules:\n",
   "            loaded_modules.add(basename)\n",
   "            if path_only:\n",
   "                yield path\n",
   "            if path not in self._module_cache:\n",
   "                        full_name = '{0}_{1}'.format(abs(hash(path)), basename)\n",
   "                        full_name = basename\n",
   "                    module = self._load_module_source(full_name, path)\n",
   "                    self._load_config_defs(basename, module, path)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                self._module_cache[path] = module\n",
   "                found_in_cache = False\n",
   "                obj = getattr(self._module_cache[path], self.class_name)\n",
   "                display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                module = __import__(self.package, fromlist=[self.base_class])\n",
   "                    plugin_class = getattr(module, self.base_class)\n",
   "                if not issubclass(obj, plugin_class):\n",
   "            self._display_plugin_load(self.class_name, basename, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "            if not class_only:\n",
   "                    obj = obj(*args, **kwargs)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be incomplete: %s\" % (path, to_text(e)))\n",
   "            self._update_object(obj, basename, path)\n",
   "            yield obj\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).find_plugin(name, collection_list=collection_list)\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).get(name, *args, **kwargs)\n",
   "        plugins = list(super(Jinja2Loader, self).all(*args, **kwargs))\n",
   "        plugins.reverse()\n",
   "        return plugins\n",
   "    filters = defaultdict(frozenset)\n",
   "        filter_cfg = '/etc/ansible/plugin_filters.yml'\n",
   "        filter_cfg = C.PLUGIN_FILTERS_CFG\n",
   "    if os.path.exists(filter_cfg):\n",
   "        with open(filter_cfg, 'rb') as f:\n",
   "                filter_data = from_yaml(f.read())\n",
   "                display.warning(u'The plugin filter file, {0} was not parsable.'\n",
   "                                u' Skipping: {1}'.format(filter_cfg, to_text(e)))\n",
   "                return filters\n",
   "            version = filter_data['filter_version']\n",
   "            display.warning(u'The plugin filter file, {0} was invalid.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "            return filters\n",
   "        version = to_text(version)\n",
   "        version = version.strip()\n",
   "        if version == u'1.0':\n",
   "                filters['ansible.modules'] = frozenset(filter_data['module_blacklist'])\n",
   "                display.warning(u'Unable to parse the plugin filter file {0} as'\n",
   "                                u' Skipping.'.format(filter_cfg))\n",
   "                return filters\n",
   "            filters['ansible.plugins.action'] = filters['ansible.modules']\n",
   "            display.warning(u'The plugin filter file, {0} was a version not recognized by this'\n",
   "                            u' version of Ansible. Skipping.'.format(filter_cfg))\n",
   "            display.warning(u'The plugin filter file, {0} does not exist.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "    if 'stat' in filters['ansible.modules']:\n",
   "                           ' from the blacklist.'.format(to_native(filter_cfg)))\n",
   "    return filters\n",
   "    display.vvvv(to_text('Loading collection {0} from {1}'.format(collection_name, collection_path)))\n",
   "        if not _does_collection_support_ansible_version(collection_meta.get('requires_ansible', ''), ansible_version):\n",
   "                display.warning(message)\n",
   "        display.warning('Error parsing collection metadata requires_ansible value from collection {0}: {1}'.format(collection_name, ex))\n",
   "        display.warning('packaging Python module unavailable; unable to validate collection Ansible version requirements')\n",
   "        display.warning('AnsibleCollectionFinder has already been configured')\n"
  ]
 },
 "119": {
  "name": "collection_list",
  "type": "NoneType",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/loader.py",
  "lineno": "787",
  "column": "8",
  "context": "ss_only = kwargs.pop('class_only', False)\n        collection_list = kwargs.pop('collection_list', None)\n        if name in self.aliases:\n            name ",
  "context_lines": "    def get_with_context(self, name, *args, **kwargs):\n        ''' instantiates a plugin of the given name using arguments '''\n\n        found_in_cache = True\n        class_only = kwargs.pop('class_only', False)\n        collection_list = kwargs.pop('collection_list', None)\n        if name in self.aliases:\n            name = self.aliases[name]\n        plugin_load_context = self.find_plugin_with_context(name, collection_list=collection_list)\n        if not plugin_load_context.resolved or not plugin_load_context.plugin_resolved_path:\n",
  "slicing": [
   "    imp = None\n",
   "display = Display()\n",
   "get_with_context_result = namedtuple('get_with_context_result', ['object', 'plugin_load_context'])\n",
   "    return [(name, obj) for (name, obj) in globals().items() if isinstance(obj, PluginLoader)]\n",
   "    b_path = os.path.expanduser(to_bytes(path, errors='surrogate_or_strict'))\n",
   "    if os.path.isdir(b_path):\n",
   "        for name, obj in get_all_plugin_loaders():\n",
   "            if obj.subdir:\n",
   "                plugin_path = os.path.join(b_path, to_bytes(obj.subdir))\n",
   "                if os.path.isdir(plugin_path):\n",
   "                    obj.add_directory(to_text(plugin_path))\n",
   "        display.warning(\"Ignoring invalid path provided to plugin path: '%s' is not a directory\" % to_text(path))\n",
   "        shell_type = 'sh'\n",
   "                shell_filename = os.path.basename(executable)\n",
   "                    shell = shell_loader.get(shell_filename)\n",
   "                    shell = None\n",
   "                if shell is None:\n",
   "                    for shell in shell_loader.all():\n",
   "                        if shell_filename in shell.COMPATIBLE_SHELLS:\n",
   "                            shell_type = shell.SHELL_FAMILY\n",
   "    shell = shell_loader.get(shell_type)\n",
   "    if not shell:\n",
   "        raise AnsibleError(\"Could not find the shell plugin required (%s).\" % shell_type)\n",
   "        setattr(shell, 'executable', executable)\n",
   "    return shell\n",
   "    loader = getattr(sys.modules[__name__], '%s_loader' % which_loader)\n",
   "    for path in paths:\n",
   "        loader.add_directory(path, with_subdir=True)\n",
   "        self.path = path\n",
   "        warning_text = deprecation.get('warning_text', None)\n",
   "        removal_date = deprecation.get('removal_date', None)\n",
   "        removal_version = deprecation.get('removal_version', None)\n",
   "        if removal_date is not None:\n",
   "            removal_version = None\n",
   "        if not warning_text:\n",
   "            warning_text = '{0} has been deprecated'.format(name)\n",
   "        display.deprecated(warning_text, date=removal_date, version=removal_version, collection_name=collection_name)\n",
   "        if removal_date:\n",
   "            self.removal_date = removal_date\n",
   "        if removal_version:\n",
   "            self.removal_version = removal_version\n",
   "        self.deprecation_warnings.append(warning_text)\n",
   "        aliases = {} if aliases is None else aliases\n",
   "        self.aliases = aliases\n",
   "            config = [config]\n",
   "        elif not config:\n",
   "            config = []\n",
   "        self.config = config\n",
   "        class_name = data.get('class_name')\n",
   "        package = data.get('package')\n",
   "        config = data.get('config')\n",
   "        subdir = data.get('subdir')\n",
   "        aliases = data.get('aliases')\n",
   "        base_class = data.get('base_class')\n",
   "        PATH_CACHE[class_name] = data.get('PATH_CACHE')\n",
   "        PLUGIN_PATH_CACHE[class_name] = data.get('PLUGIN_PATH_CACHE')\n",
   "        self.__init__(class_name, package, config, subdir, aliases, base_class)\n",
   "        ret = []\n",
   "        for i in paths:\n",
   "            if i not in ret:\n",
   "                ret.append(i)\n",
   "        return os.pathsep.join(ret)\n",
   "        results = []\n",
   "        results.append(dir)\n",
   "        for root, subdirs, files in os.walk(dir, followlinks=True):\n",
   "            if '__init__.py' in files:\n",
   "                for x in subdirs:\n",
   "                    results.append(os.path.join(root, x))\n",
   "        return results\n",
   "            m = __import__(self.package)\n",
   "            parts = self.package.split('.')[1:]\n",
   "            for parent_mod in parts:\n",
   "                m = getattr(m, parent_mod)\n",
   "            self.package_path = os.path.dirname(m.__file__)\n",
   "        if subdirs:\n",
   "        ret = [PluginPathContext(p, False) for p in self._extra_dirs]\n",
   "            for path in self.config:\n",
   "                path = os.path.realpath(os.path.expanduser(path))\n",
   "                if subdirs:\n",
   "                    contents = glob.glob(\"%s/*\" % path) + glob.glob(\"%s/*/*\" % path)\n",
   "                    for c in contents:\n",
   "                        if os.path.isdir(c) and c not in ret:\n",
   "                            ret.append(PluginPathContext(c, False))\n",
   "                if path not in ret:\n",
   "                    ret.append(PluginPathContext(path, False))\n",
   "        ret.extend([PluginPathContext(p, True) for p in self._get_package_paths(subdirs=subdirs)])\n",
   "        ret.sort(key=lambda p: p.path.endswith('/windows'))\n",
   "        self._paths = ret\n",
   "        return ret\n",
   "        paths_with_context = self._get_paths_with_context(subdirs=subdirs)\n",
   "        return [path_with_context.path for path_with_context in paths_with_context]\n",
   "            type_name = get_plugin_class(self.class_name)\n",
   "            if type_name in C.CONFIGURABLE_PLUGINS:\n",
   "                dstring = AnsibleLoader(getattr(module, 'DOCUMENTATION', ''), file_name=path).get_single_data()\n",
   "                if dstring:\n",
   "                    add_fragments(dstring, path, fragment_loader=fragment_loader, is_module=(type_name == 'module'))\n",
   "                if dstring and 'options' in dstring and isinstance(dstring['options'], dict):\n",
   "                    C.config.initialize_plugin_configuration_definitions(type_name, name, dstring['options'])\n",
   "                    display.debug('Loaded config def from plugin (%s/%s)' % (type_name, name))\n",
   "        directory = os.path.realpath(directory)\n",
   "        if directory is not None:\n",
   "                directory = os.path.join(directory, self.subdir)\n",
   "            if directory not in self._extra_dirs:\n",
   "                self._extra_dirs.append(directory)\n",
   "                display.debug('Added %s to loader search path' % (directory))\n",
   "        collection_pkg = import_module(acr.n_python_collection_package_name)\n",
   "        if not collection_pkg:\n",
   "        collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "        if not collection_meta:\n",
   "            subdir_qualified_resource = '.'.join([acr.subdirs, acr.resource])\n",
   "            subdir_qualified_resource = acr.resource\n",
   "        entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource + extension, None)\n",
   "        if not entry:\n",
   "            entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource, None)\n",
   "        return entry\n",
   "        plugin_type = AnsibleCollectionRef.legacy_plugin_dir_to_plugin_type(self.subdir)\n",
   "        acr = AnsibleCollectionRef.from_fqcr(fq_name, plugin_type)\n",
   "        routing_metadata = self._query_collection_routing_meta(acr, plugin_type, extension=extension)\n",
   "        if routing_metadata:\n",
   "            deprecation = routing_metadata.get('deprecation', None)\n",
   "            plugin_load_context.record_deprecation(fq_name, deprecation, acr.collection)\n",
   "            tombstone = routing_metadata.get('tombstone', None)\n",
   "            if tombstone:\n",
   "                removal_date = tombstone.get('removal_date')\n",
   "                removal_version = tombstone.get('removal_version')\n",
   "                warning_text = tombstone.get('warning_text') or '{0} has been removed.'.format(fq_name)\n",
   "                removed_msg = display.get_deprecation_message(msg=warning_text, version=removal_version,\n",
   "                                                              date=removal_date, removed=True,\n",
   "                                                              collection_name=acr.collection)\n",
   "                plugin_load_context.removal_date = removal_date\n",
   "                plugin_load_context.removal_version = removal_version\n",
   "                plugin_load_context.exit_reason = removed_msg\n",
   "                raise AnsiblePluginRemovedError(removed_msg, plugin_load_context=plugin_load_context)\n",
   "            redirect = routing_metadata.get('redirect', None)\n",
   "            if redirect:\n",
   "                display.vv(\"redirecting (type: {0}) {1} to {2}\".format(plugin_type, fq_name, redirect))\n",
   "                return plugin_load_context.redirect(redirect)\n",
   "        n_resource = to_native(acr.resource, errors='strict')\n",
   "        full_name = '{0}.{1}'.format(acr.n_python_package_name, n_resource)\n",
   "            n_resource += extension\n",
   "        pkg = sys.modules.get(acr.n_python_package_name)\n",
   "        if not pkg:\n",
   "                pkg = import_module(acr.n_python_package_name)\n",
   "                return plugin_load_context.nope('Python package {0} not found'.format(acr.n_python_package_name))\n",
   "        pkg_path = os.path.dirname(pkg.__file__)\n",
   "        n_resource_path = os.path.join(pkg_path, n_resource)\n",
   "        if os.path.exists(n_resource_path):\n",
   "                full_name, to_text(n_resource_path), acr.collection, 'found exact match for {0} in {1}'.format(full_name, acr.collection))\n",
   "            return plugin_load_context.nope('no match for {0} in {1}'.format(to_text(n_resource), acr.collection))\n",
   "        found_files = [f\n",
   "                       for f in glob.iglob(os.path.join(pkg_path, n_resource) + '.*')\n",
   "                       if os.path.isfile(f) and not f.endswith(C.MODULE_IGNORE_EXTS)]\n",
   "        if not found_files:\n",
   "            return plugin_load_context.nope('failed fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        if len(found_files) > 1:\n",
   "            full_name, to_text(found_files[0]), acr.collection, 'found fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        result = self.find_plugin_with_context(name, mod_type, ignore_deprecated, check_aliases, collection_list)\n",
   "        if result.resolved and result.plugin_resolved_path:\n",
   "            return result.plugin_resolved_path\n",
   "        plugin_load_context = PluginLoadContext()\n",
   "        plugin_load_context.original_name = name\n",
   "            result = self._resolve_plugin_step(name, mod_type, ignore_deprecated, check_aliases, collection_list, plugin_load_context=plugin_load_context)\n",
   "            if result.pending_redirect:\n",
   "                if result.pending_redirect in result.redirect_list:\n",
   "                    raise AnsiblePluginCircularRedirect('plugin redirect loop resolving {0} (path: {1})'.format(result.original_name, result.redirect_list))\n",
   "                name = result.pending_redirect\n",
   "                result.pending_redirect = None\n",
   "                plugin_load_context = result\n",
   "        if plugin_load_context.error_list:\n",
   "            display.warning(\"errors were encountered during the plugin load for {0}:\\n{1}\".format(name, plugin_load_context.error_list))\n",
   "        return plugin_load_context\n",
   "        if not plugin_load_context:\n",
   "        plugin_load_context.redirect_list.append(name)\n",
   "        plugin_load_context.resolved = False\n",
   "        if name in _PLUGIN_FILTERS[self.package]:\n",
   "            plugin_load_context.exit_reason = '{0} matched a defined plugin filter'.format(name)\n",
   "            return plugin_load_context\n",
   "            suffix = mod_type\n",
   "            suffix = '.py'\n",
   "            suffix = ''\n",
   "        if (AnsibleCollectionRef.is_valid_fqcr(name) or collection_list) and not name.startswith('Ansible'):\n",
   "            if '.' in name or not collection_list:\n",
   "                candidates = [name]\n",
   "                candidates = ['{0}.{1}'.format(c, name) for c in collection_list]\n",
   "            for candidate_name in candidates:\n",
   "                    plugin_load_context.load_attempts.append(candidate_name)\n",
   "                    if candidate_name.startswith('ansible.legacy'):\n",
   "                        plugin_load_context = self._find_plugin_legacy(name.replace('ansible.legacy.', '', 1),\n",
   "                                                                       plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "                        plugin_load_context = self._find_fq_plugin(candidate_name, suffix, plugin_load_context=plugin_load_context)\n",
   "                        if candidate_name != plugin_load_context.original_name and candidate_name not in plugin_load_context.redirect_list:\n",
   "                            plugin_load_context.redirect_list.append(candidate_name)\n",
   "                    if plugin_load_context.resolved or plugin_load_context.pending_redirect:  # if we got an answer or need to chase down a redirect, return\n",
   "                        return plugin_load_context\n",
   "                    plugin_load_context.import_error_list.append(ie)\n",
   "                    plugin_load_context.error_list.append(to_native(ex))\n",
   "            if plugin_load_context.error_list:\n",
   "                display.debug(msg='plugin lookup for {0} failed; errors: {1}'.format(name, '; '.join(plugin_load_context.error_list)))\n",
   "            plugin_load_context.exit_reason = 'no matches found for {0}'.format(name)\n",
   "            return plugin_load_context\n",
   "        return self._find_plugin_legacy(name, plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "        plugin_load_context.resolved = False\n",
   "            name = self.aliases.get(name, name)\n",
   "        pull_cache = self._plugin_path_cache[suffix]\n",
   "            path_with_context = pull_cache[name]\n",
   "            plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "            plugin_load_context.plugin_resolved_name = name\n",
   "            plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "            plugin_load_context.resolved = True\n",
   "            return plugin_load_context\n",
   "        for path_context in (p for p in self._get_paths_with_context() if p.path not in self._searched_paths and os.path.isdir(p.path)):\n",
   "            path = path_context.path\n",
   "            display.debug('trying %s' % path)\n",
   "            plugin_load_context.load_attempts.append(path)\n",
   "                full_paths = (os.path.join(path, f) for f in os.listdir(path))\n",
   "                display.warning(\"Error accessing plugin paths: %s\" % to_text(e))\n",
   "            for full_path in (f for f in full_paths if os.path.isfile(f) and not f.endswith('__init__.py')):\n",
   "                full_name = os.path.basename(full_path)\n",
   "                if any(full_path.endswith(x) for x in C.MODULE_IGNORE_EXTS):\n",
   "                splitname = os.path.splitext(full_name)\n",
   "                base_name = splitname[0]\n",
   "                internal = path_context.internal\n",
   "                    extension = splitname[1]\n",
   "                    extension = ''\n",
   "                if base_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][full_name] = PluginPathContext(full_path, internal)\n",
   "                if base_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][full_name] = PluginPathContext(full_path, internal)\n",
   "            self._searched_paths.add(path)\n",
   "                path_with_context = pull_cache[name]\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        if not name.startswith('_'):\n",
   "            alias_name = '_' + name\n",
   "            if alias_name in pull_cache:\n",
   "                path_with_context = pull_cache[alias_name]\n",
   "                if not ignore_deprecated and not os.path.islink(path_with_context.path):\n",
   "                    display.deprecated('%s is kept for backwards compatibility but usage is discouraged. '  # pylint: disable=ansible-deprecated-no-version\n",
   "                                       'The module documentation details page may explain more about this rationale.' % name.lstrip('_'))\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = alias_name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        candidate_fqcr = 'ansible.builtin.{0}'.format(name)\n",
   "        if '.' not in name and AnsibleCollectionRef.is_valid_fqcr(candidate_fqcr):\n",
   "            return self._find_fq_plugin(fq_name=candidate_fqcr, extension=suffix, plugin_load_context=plugin_load_context)\n",
   "        return plugin_load_context.nope('{0} is not eligible for last-chance resolution'.format(name))\n",
   "            return self.find_plugin(name, collection_list=collection_list) is not None\n",
   "            display.debug('has_plugin error: {0}'.format(to_text(ex)))\n",
   "        if name.startswith('ansible_collections.'):\n",
   "            full_name = name\n",
   "            full_name = '.'.join([self.package, name])\n",
   "        if full_name in sys.modules:\n",
   "            return sys.modules[full_name]\n",
   "            if imp is None:\n",
   "                spec = importlib.util.spec_from_file_location(to_native(full_name), to_native(path))\n",
   "                module = importlib.util.module_from_spec(spec)\n",
   "                spec.loader.exec_module(module)\n",
   "                sys.modules[full_name] = module\n",
   "                with open(to_bytes(path), 'rb') as module_file:\n",
   "                    module = imp.load_source(to_native(full_name), to_native(path), module_file)\n",
   "        return module\n",
   "        setattr(obj, '_original_path', path)\n",
   "        setattr(obj, '_load_name', name)\n",
   "        setattr(obj, '_redirected_names', redirected_names or [])\n",
   "        return self.get_with_context(name, *args, **kwargs).object\n",
   "        found_in_cache = True\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        collection_list = kwargs.pop('collection_list', None)\n",
   "        if name in self.aliases:\n",
   "            name = self.aliases[name]\n",
   "        plugin_load_context = self.find_plugin_with_context(name, collection_list=collection_list)\n",
   "        if not plugin_load_context.resolved or not plugin_load_context.plugin_resolved_path:\n",
   "            return get_with_context_result(None, plugin_load_context)\n",
   "        name = plugin_load_context.plugin_resolved_name\n",
   "        path = plugin_load_context.plugin_resolved_path\n",
   "        redirected_names = plugin_load_context.redirect_list or []\n",
   "        if path not in self._module_cache:\n",
   "            self._module_cache[path] = self._load_module_source(name, path)\n",
   "            self._load_config_defs(name, self._module_cache[path], path)\n",
   "            found_in_cache = False\n",
   "        obj = getattr(self._module_cache[path], self.class_name)\n",
   "            module = __import__(self.package, fromlist=[self.base_class])\n",
   "                plugin_class = getattr(module, self.base_class)\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "            if not issubclass(obj, plugin_class):\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "        self._display_plugin_load(self.class_name, name, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "        if not class_only:\n",
   "                instance = object.__new__(obj)\n",
   "                self._update_object(instance, name, path, redirected_names)\n",
   "                obj.__init__(instance, *args, **kwargs)\n",
   "                obj = instance\n",
   "                    return get_with_context_result(None, plugin_load_context)\n",
   "        self._update_object(obj, name, path, redirected_names)\n",
   "        return get_with_context_result(obj, plugin_load_context)\n",
   "            msg = 'Loading %s \\'%s\\' from %s' % (class_name, os.path.basename(name), path)\n",
   "                msg = '%s (searched paths: %s)' % (msg, self.format_paths(searched_paths))\n",
   "            if found_in_cache or class_only:\n",
   "                msg = '%s (found_in_cache=%s, class_only=%s)' % (msg, found_in_cache, class_only)\n",
   "            display.debug(msg)\n",
   "        dedupe = kwargs.pop('_dedupe', True)\n",
   "        path_only = kwargs.pop('path_only', False)\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        if path_only and class_only:\n",
   "        all_matches = []\n",
   "        found_in_cache = True\n",
   "        for i in self._get_paths():\n",
   "            all_matches.extend(glob.glob(os.path.join(i, \"*.py\")))\n",
   "        loaded_modules = set()\n",
   "        for path in sorted(all_matches, key=os.path.basename):\n",
   "            name = os.path.splitext(path)[0]\n",
   "            basename = os.path.basename(name)\n",
   "            if basename == '__init__' or basename in _PLUGIN_FILTERS[self.package]:\n",
   "            if dedupe and basename in loaded_modules:\n",
   "            loaded_modules.add(basename)\n",
   "            if path_only:\n",
   "                yield path\n",
   "            if path not in self._module_cache:\n",
   "                        full_name = '{0}_{1}'.format(abs(hash(path)), basename)\n",
   "                        full_name = basename\n",
   "                    module = self._load_module_source(full_name, path)\n",
   "                    self._load_config_defs(basename, module, path)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                self._module_cache[path] = module\n",
   "                found_in_cache = False\n",
   "                obj = getattr(self._module_cache[path], self.class_name)\n",
   "                display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                module = __import__(self.package, fromlist=[self.base_class])\n",
   "                    plugin_class = getattr(module, self.base_class)\n",
   "                if not issubclass(obj, plugin_class):\n",
   "            self._display_plugin_load(self.class_name, basename, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "            if not class_only:\n",
   "                    obj = obj(*args, **kwargs)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be incomplete: %s\" % (path, to_text(e)))\n",
   "            self._update_object(obj, basename, path)\n",
   "            yield obj\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).find_plugin(name, collection_list=collection_list)\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).get(name, *args, **kwargs)\n",
   "        plugins = list(super(Jinja2Loader, self).all(*args, **kwargs))\n",
   "        plugins.reverse()\n",
   "        return plugins\n",
   "    filters = defaultdict(frozenset)\n",
   "        filter_cfg = '/etc/ansible/plugin_filters.yml'\n",
   "        filter_cfg = C.PLUGIN_FILTERS_CFG\n",
   "    if os.path.exists(filter_cfg):\n",
   "        with open(filter_cfg, 'rb') as f:\n",
   "                filter_data = from_yaml(f.read())\n",
   "                display.warning(u'The plugin filter file, {0} was not parsable.'\n",
   "                                u' Skipping: {1}'.format(filter_cfg, to_text(e)))\n",
   "                return filters\n",
   "            version = filter_data['filter_version']\n",
   "            display.warning(u'The plugin filter file, {0} was invalid.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "            return filters\n",
   "        version = to_text(version)\n",
   "        version = version.strip()\n",
   "        if version == u'1.0':\n",
   "                filters['ansible.modules'] = frozenset(filter_data['module_blacklist'])\n",
   "                display.warning(u'Unable to parse the plugin filter file {0} as'\n",
   "                                u' Skipping.'.format(filter_cfg))\n",
   "                return filters\n",
   "            filters['ansible.plugins.action'] = filters['ansible.modules']\n",
   "            display.warning(u'The plugin filter file, {0} was a version not recognized by this'\n",
   "                            u' version of Ansible. Skipping.'.format(filter_cfg))\n",
   "            display.warning(u'The plugin filter file, {0} does not exist.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "    if 'stat' in filters['ansible.modules']:\n",
   "                           ' from the blacklist.'.format(to_native(filter_cfg)))\n",
   "    return filters\n",
   "    display.vvvv(to_text('Loading collection {0} from {1}'.format(collection_name, collection_path)))\n",
   "        if not _does_collection_support_ansible_version(collection_meta.get('requires_ansible', ''), ansible_version):\n",
   "                display.warning(message)\n",
   "        display.warning('Error parsing collection metadata requires_ansible value from collection {0}: {1}'.format(collection_name, ex))\n",
   "        display.warning('packaging Python module unavailable; unable to validate collection Ansible version requirements')\n",
   "        display.warning('AnsibleCollectionFinder has already been configured')\n"
  ]
 },
 "120": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/loader.py",
  "lineno": "795",
  "column": "8",
  "context": "ontext_result(None, plugin_load_context)\n\n        name = plugin_load_context.plugin_resolved_name\n        path = plugin_load_context.plugin_resolved",
  "context_lines": "        plugin_load_context = self.find_plugin_with_context(name, collection_list=collection_list)\n        if not plugin_load_context.resolved or not plugin_load_context.plugin_resolved_path:\n            # FIXME: this is probably an error (eg removed plugin)\n            return get_with_context_result(None, plugin_load_context)\n\n        name = plugin_load_context.plugin_resolved_name\n        path = plugin_load_context.plugin_resolved_path\n        redirected_names = plugin_load_context.redirect_list or []\n\n        if path not in self._module_cache:\n            self._module_cache[path] = self._load_module_source(name, path)\n",
  "slicing": [
   "    imp = None\n",
   "display = Display()\n",
   "get_with_context_result = namedtuple('get_with_context_result', ['object', 'plugin_load_context'])\n",
   "    return [(name, obj) for (name, obj) in globals().items() if isinstance(obj, PluginLoader)]\n",
   "    b_path = os.path.expanduser(to_bytes(path, errors='surrogate_or_strict'))\n",
   "    if os.path.isdir(b_path):\n",
   "        for name, obj in get_all_plugin_loaders():\n",
   "            if obj.subdir:\n",
   "                plugin_path = os.path.join(b_path, to_bytes(obj.subdir))\n",
   "                if os.path.isdir(plugin_path):\n",
   "                    obj.add_directory(to_text(plugin_path))\n",
   "        display.warning(\"Ignoring invalid path provided to plugin path: '%s' is not a directory\" % to_text(path))\n",
   "        shell_type = 'sh'\n",
   "                shell_filename = os.path.basename(executable)\n",
   "                    shell = shell_loader.get(shell_filename)\n",
   "                    shell = None\n",
   "                if shell is None:\n",
   "                    for shell in shell_loader.all():\n",
   "                        if shell_filename in shell.COMPATIBLE_SHELLS:\n",
   "                            shell_type = shell.SHELL_FAMILY\n",
   "    shell = shell_loader.get(shell_type)\n",
   "    if not shell:\n",
   "        raise AnsibleError(\"Could not find the shell plugin required (%s).\" % shell_type)\n",
   "        setattr(shell, 'executable', executable)\n",
   "    return shell\n",
   "    loader = getattr(sys.modules[__name__], '%s_loader' % which_loader)\n",
   "    for path in paths:\n",
   "        loader.add_directory(path, with_subdir=True)\n",
   "        self.path = path\n",
   "        warning_text = deprecation.get('warning_text', None)\n",
   "        removal_date = deprecation.get('removal_date', None)\n",
   "        removal_version = deprecation.get('removal_version', None)\n",
   "        if removal_date is not None:\n",
   "            removal_version = None\n",
   "        if not warning_text:\n",
   "            warning_text = '{0} has been deprecated'.format(name)\n",
   "        display.deprecated(warning_text, date=removal_date, version=removal_version, collection_name=collection_name)\n",
   "        if removal_date:\n",
   "            self.removal_date = removal_date\n",
   "        if removal_version:\n",
   "            self.removal_version = removal_version\n",
   "        self.deprecation_warnings.append(warning_text)\n",
   "        aliases = {} if aliases is None else aliases\n",
   "        self.aliases = aliases\n",
   "            config = [config]\n",
   "        elif not config:\n",
   "            config = []\n",
   "        self.config = config\n",
   "        class_name = data.get('class_name')\n",
   "        package = data.get('package')\n",
   "        config = data.get('config')\n",
   "        subdir = data.get('subdir')\n",
   "        aliases = data.get('aliases')\n",
   "        base_class = data.get('base_class')\n",
   "        PATH_CACHE[class_name] = data.get('PATH_CACHE')\n",
   "        PLUGIN_PATH_CACHE[class_name] = data.get('PLUGIN_PATH_CACHE')\n",
   "        self.__init__(class_name, package, config, subdir, aliases, base_class)\n",
   "        ret = []\n",
   "        for i in paths:\n",
   "            if i not in ret:\n",
   "                ret.append(i)\n",
   "        return os.pathsep.join(ret)\n",
   "        results = []\n",
   "        results.append(dir)\n",
   "        for root, subdirs, files in os.walk(dir, followlinks=True):\n",
   "            if '__init__.py' in files:\n",
   "                for x in subdirs:\n",
   "                    results.append(os.path.join(root, x))\n",
   "        return results\n",
   "            m = __import__(self.package)\n",
   "            parts = self.package.split('.')[1:]\n",
   "            for parent_mod in parts:\n",
   "                m = getattr(m, parent_mod)\n",
   "            self.package_path = os.path.dirname(m.__file__)\n",
   "        if subdirs:\n",
   "        ret = [PluginPathContext(p, False) for p in self._extra_dirs]\n",
   "            for path in self.config:\n",
   "                path = os.path.realpath(os.path.expanduser(path))\n",
   "                if subdirs:\n",
   "                    contents = glob.glob(\"%s/*\" % path) + glob.glob(\"%s/*/*\" % path)\n",
   "                    for c in contents:\n",
   "                        if os.path.isdir(c) and c not in ret:\n",
   "                            ret.append(PluginPathContext(c, False))\n",
   "                if path not in ret:\n",
   "                    ret.append(PluginPathContext(path, False))\n",
   "        ret.extend([PluginPathContext(p, True) for p in self._get_package_paths(subdirs=subdirs)])\n",
   "        ret.sort(key=lambda p: p.path.endswith('/windows'))\n",
   "        self._paths = ret\n",
   "        return ret\n",
   "        paths_with_context = self._get_paths_with_context(subdirs=subdirs)\n",
   "        return [path_with_context.path for path_with_context in paths_with_context]\n",
   "            type_name = get_plugin_class(self.class_name)\n",
   "            if type_name in C.CONFIGURABLE_PLUGINS:\n",
   "                dstring = AnsibleLoader(getattr(module, 'DOCUMENTATION', ''), file_name=path).get_single_data()\n",
   "                if dstring:\n",
   "                    add_fragments(dstring, path, fragment_loader=fragment_loader, is_module=(type_name == 'module'))\n",
   "                if dstring and 'options' in dstring and isinstance(dstring['options'], dict):\n",
   "                    C.config.initialize_plugin_configuration_definitions(type_name, name, dstring['options'])\n",
   "                    display.debug('Loaded config def from plugin (%s/%s)' % (type_name, name))\n",
   "        directory = os.path.realpath(directory)\n",
   "        if directory is not None:\n",
   "                directory = os.path.join(directory, self.subdir)\n",
   "            if directory not in self._extra_dirs:\n",
   "                self._extra_dirs.append(directory)\n",
   "                display.debug('Added %s to loader search path' % (directory))\n",
   "        collection_pkg = import_module(acr.n_python_collection_package_name)\n",
   "        if not collection_pkg:\n",
   "        collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "        if not collection_meta:\n",
   "            subdir_qualified_resource = '.'.join([acr.subdirs, acr.resource])\n",
   "            subdir_qualified_resource = acr.resource\n",
   "        entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource + extension, None)\n",
   "        if not entry:\n",
   "            entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource, None)\n",
   "        return entry\n",
   "        plugin_type = AnsibleCollectionRef.legacy_plugin_dir_to_plugin_type(self.subdir)\n",
   "        acr = AnsibleCollectionRef.from_fqcr(fq_name, plugin_type)\n",
   "        routing_metadata = self._query_collection_routing_meta(acr, plugin_type, extension=extension)\n",
   "        if routing_metadata:\n",
   "            deprecation = routing_metadata.get('deprecation', None)\n",
   "            plugin_load_context.record_deprecation(fq_name, deprecation, acr.collection)\n",
   "            tombstone = routing_metadata.get('tombstone', None)\n",
   "            if tombstone:\n",
   "                removal_date = tombstone.get('removal_date')\n",
   "                removal_version = tombstone.get('removal_version')\n",
   "                warning_text = tombstone.get('warning_text') or '{0} has been removed.'.format(fq_name)\n",
   "                removed_msg = display.get_deprecation_message(msg=warning_text, version=removal_version,\n",
   "                                                              date=removal_date, removed=True,\n",
   "                                                              collection_name=acr.collection)\n",
   "                plugin_load_context.removal_date = removal_date\n",
   "                plugin_load_context.removal_version = removal_version\n",
   "                plugin_load_context.exit_reason = removed_msg\n",
   "                raise AnsiblePluginRemovedError(removed_msg, plugin_load_context=plugin_load_context)\n",
   "            redirect = routing_metadata.get('redirect', None)\n",
   "            if redirect:\n",
   "                display.vv(\"redirecting (type: {0}) {1} to {2}\".format(plugin_type, fq_name, redirect))\n",
   "                return plugin_load_context.redirect(redirect)\n",
   "        n_resource = to_native(acr.resource, errors='strict')\n",
   "        full_name = '{0}.{1}'.format(acr.n_python_package_name, n_resource)\n",
   "            n_resource += extension\n",
   "        pkg = sys.modules.get(acr.n_python_package_name)\n",
   "        if not pkg:\n",
   "                pkg = import_module(acr.n_python_package_name)\n",
   "                return plugin_load_context.nope('Python package {0} not found'.format(acr.n_python_package_name))\n",
   "        pkg_path = os.path.dirname(pkg.__file__)\n",
   "        n_resource_path = os.path.join(pkg_path, n_resource)\n",
   "        if os.path.exists(n_resource_path):\n",
   "                full_name, to_text(n_resource_path), acr.collection, 'found exact match for {0} in {1}'.format(full_name, acr.collection))\n",
   "            return plugin_load_context.nope('no match for {0} in {1}'.format(to_text(n_resource), acr.collection))\n",
   "        found_files = [f\n",
   "                       for f in glob.iglob(os.path.join(pkg_path, n_resource) + '.*')\n",
   "                       if os.path.isfile(f) and not f.endswith(C.MODULE_IGNORE_EXTS)]\n",
   "        if not found_files:\n",
   "            return plugin_load_context.nope('failed fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        if len(found_files) > 1:\n",
   "            full_name, to_text(found_files[0]), acr.collection, 'found fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        result = self.find_plugin_with_context(name, mod_type, ignore_deprecated, check_aliases, collection_list)\n",
   "        if result.resolved and result.plugin_resolved_path:\n",
   "            return result.plugin_resolved_path\n",
   "        plugin_load_context = PluginLoadContext()\n",
   "        plugin_load_context.original_name = name\n",
   "            result = self._resolve_plugin_step(name, mod_type, ignore_deprecated, check_aliases, collection_list, plugin_load_context=plugin_load_context)\n",
   "            if result.pending_redirect:\n",
   "                if result.pending_redirect in result.redirect_list:\n",
   "                    raise AnsiblePluginCircularRedirect('plugin redirect loop resolving {0} (path: {1})'.format(result.original_name, result.redirect_list))\n",
   "                name = result.pending_redirect\n",
   "                result.pending_redirect = None\n",
   "                plugin_load_context = result\n",
   "        if plugin_load_context.error_list:\n",
   "            display.warning(\"errors were encountered during the plugin load for {0}:\\n{1}\".format(name, plugin_load_context.error_list))\n",
   "        return plugin_load_context\n",
   "        if not plugin_load_context:\n",
   "        plugin_load_context.redirect_list.append(name)\n",
   "        plugin_load_context.resolved = False\n",
   "        if name in _PLUGIN_FILTERS[self.package]:\n",
   "            plugin_load_context.exit_reason = '{0} matched a defined plugin filter'.format(name)\n",
   "            return plugin_load_context\n",
   "            suffix = mod_type\n",
   "            suffix = '.py'\n",
   "            suffix = ''\n",
   "        if (AnsibleCollectionRef.is_valid_fqcr(name) or collection_list) and not name.startswith('Ansible'):\n",
   "            if '.' in name or not collection_list:\n",
   "                candidates = [name]\n",
   "                candidates = ['{0}.{1}'.format(c, name) for c in collection_list]\n",
   "            for candidate_name in candidates:\n",
   "                    plugin_load_context.load_attempts.append(candidate_name)\n",
   "                    if candidate_name.startswith('ansible.legacy'):\n",
   "                        plugin_load_context = self._find_plugin_legacy(name.replace('ansible.legacy.', '', 1),\n",
   "                                                                       plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "                        plugin_load_context = self._find_fq_plugin(candidate_name, suffix, plugin_load_context=plugin_load_context)\n",
   "                        if candidate_name != plugin_load_context.original_name and candidate_name not in plugin_load_context.redirect_list:\n",
   "                            plugin_load_context.redirect_list.append(candidate_name)\n",
   "                    if plugin_load_context.resolved or plugin_load_context.pending_redirect:  # if we got an answer or need to chase down a redirect, return\n",
   "                        return plugin_load_context\n",
   "                    plugin_load_context.import_error_list.append(ie)\n",
   "                    plugin_load_context.error_list.append(to_native(ex))\n",
   "            if plugin_load_context.error_list:\n",
   "                display.debug(msg='plugin lookup for {0} failed; errors: {1}'.format(name, '; '.join(plugin_load_context.error_list)))\n",
   "            plugin_load_context.exit_reason = 'no matches found for {0}'.format(name)\n",
   "            return plugin_load_context\n",
   "        return self._find_plugin_legacy(name, plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "        plugin_load_context.resolved = False\n",
   "            name = self.aliases.get(name, name)\n",
   "        pull_cache = self._plugin_path_cache[suffix]\n",
   "            path_with_context = pull_cache[name]\n",
   "            plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "            plugin_load_context.plugin_resolved_name = name\n",
   "            plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "            plugin_load_context.resolved = True\n",
   "            return plugin_load_context\n",
   "        for path_context in (p for p in self._get_paths_with_context() if p.path not in self._searched_paths and os.path.isdir(p.path)):\n",
   "            path = path_context.path\n",
   "            display.debug('trying %s' % path)\n",
   "            plugin_load_context.load_attempts.append(path)\n",
   "                full_paths = (os.path.join(path, f) for f in os.listdir(path))\n",
   "                display.warning(\"Error accessing plugin paths: %s\" % to_text(e))\n",
   "            for full_path in (f for f in full_paths if os.path.isfile(f) and not f.endswith('__init__.py')):\n",
   "                full_name = os.path.basename(full_path)\n",
   "                if any(full_path.endswith(x) for x in C.MODULE_IGNORE_EXTS):\n",
   "                splitname = os.path.splitext(full_name)\n",
   "                base_name = splitname[0]\n",
   "                internal = path_context.internal\n",
   "                    extension = splitname[1]\n",
   "                    extension = ''\n",
   "                if base_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][full_name] = PluginPathContext(full_path, internal)\n",
   "                if base_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][full_name] = PluginPathContext(full_path, internal)\n",
   "            self._searched_paths.add(path)\n",
   "                path_with_context = pull_cache[name]\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        if not name.startswith('_'):\n",
   "            alias_name = '_' + name\n",
   "            if alias_name in pull_cache:\n",
   "                path_with_context = pull_cache[alias_name]\n",
   "                if not ignore_deprecated and not os.path.islink(path_with_context.path):\n",
   "                    display.deprecated('%s is kept for backwards compatibility but usage is discouraged. '  # pylint: disable=ansible-deprecated-no-version\n",
   "                                       'The module documentation details page may explain more about this rationale.' % name.lstrip('_'))\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = alias_name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        candidate_fqcr = 'ansible.builtin.{0}'.format(name)\n",
   "        if '.' not in name and AnsibleCollectionRef.is_valid_fqcr(candidate_fqcr):\n",
   "            return self._find_fq_plugin(fq_name=candidate_fqcr, extension=suffix, plugin_load_context=plugin_load_context)\n",
   "        return plugin_load_context.nope('{0} is not eligible for last-chance resolution'.format(name))\n",
   "            return self.find_plugin(name, collection_list=collection_list) is not None\n",
   "            display.debug('has_plugin error: {0}'.format(to_text(ex)))\n",
   "        if name.startswith('ansible_collections.'):\n",
   "            full_name = name\n",
   "            full_name = '.'.join([self.package, name])\n",
   "        if full_name in sys.modules:\n",
   "            return sys.modules[full_name]\n",
   "            if imp is None:\n",
   "                spec = importlib.util.spec_from_file_location(to_native(full_name), to_native(path))\n",
   "                module = importlib.util.module_from_spec(spec)\n",
   "                spec.loader.exec_module(module)\n",
   "                sys.modules[full_name] = module\n",
   "                with open(to_bytes(path), 'rb') as module_file:\n",
   "                    module = imp.load_source(to_native(full_name), to_native(path), module_file)\n",
   "        return module\n",
   "        setattr(obj, '_original_path', path)\n",
   "        setattr(obj, '_load_name', name)\n",
   "        setattr(obj, '_redirected_names', redirected_names or [])\n",
   "        return self.get_with_context(name, *args, **kwargs).object\n",
   "        found_in_cache = True\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        collection_list = kwargs.pop('collection_list', None)\n",
   "        if name in self.aliases:\n",
   "            name = self.aliases[name]\n",
   "        plugin_load_context = self.find_plugin_with_context(name, collection_list=collection_list)\n",
   "        if not plugin_load_context.resolved or not plugin_load_context.plugin_resolved_path:\n",
   "            return get_with_context_result(None, plugin_load_context)\n",
   "        name = plugin_load_context.plugin_resolved_name\n",
   "        path = plugin_load_context.plugin_resolved_path\n",
   "        redirected_names = plugin_load_context.redirect_list or []\n",
   "        if path not in self._module_cache:\n",
   "            self._module_cache[path] = self._load_module_source(name, path)\n",
   "            self._load_config_defs(name, self._module_cache[path], path)\n",
   "            found_in_cache = False\n",
   "        obj = getattr(self._module_cache[path], self.class_name)\n",
   "            module = __import__(self.package, fromlist=[self.base_class])\n",
   "                plugin_class = getattr(module, self.base_class)\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "            if not issubclass(obj, plugin_class):\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "        self._display_plugin_load(self.class_name, name, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "        if not class_only:\n",
   "                instance = object.__new__(obj)\n",
   "                self._update_object(instance, name, path, redirected_names)\n",
   "                obj.__init__(instance, *args, **kwargs)\n",
   "                obj = instance\n",
   "                    return get_with_context_result(None, plugin_load_context)\n",
   "        self._update_object(obj, name, path, redirected_names)\n",
   "        return get_with_context_result(obj, plugin_load_context)\n",
   "            msg = 'Loading %s \\'%s\\' from %s' % (class_name, os.path.basename(name), path)\n",
   "                msg = '%s (searched paths: %s)' % (msg, self.format_paths(searched_paths))\n",
   "            if found_in_cache or class_only:\n",
   "                msg = '%s (found_in_cache=%s, class_only=%s)' % (msg, found_in_cache, class_only)\n",
   "            display.debug(msg)\n",
   "        dedupe = kwargs.pop('_dedupe', True)\n",
   "        path_only = kwargs.pop('path_only', False)\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        if path_only and class_only:\n",
   "        all_matches = []\n",
   "        found_in_cache = True\n",
   "        for i in self._get_paths():\n",
   "            all_matches.extend(glob.glob(os.path.join(i, \"*.py\")))\n",
   "        loaded_modules = set()\n",
   "        for path in sorted(all_matches, key=os.path.basename):\n",
   "            name = os.path.splitext(path)[0]\n",
   "            basename = os.path.basename(name)\n",
   "            if basename == '__init__' or basename in _PLUGIN_FILTERS[self.package]:\n",
   "            if dedupe and basename in loaded_modules:\n",
   "            loaded_modules.add(basename)\n",
   "            if path_only:\n",
   "                yield path\n",
   "            if path not in self._module_cache:\n",
   "                        full_name = '{0}_{1}'.format(abs(hash(path)), basename)\n",
   "                        full_name = basename\n",
   "                    module = self._load_module_source(full_name, path)\n",
   "                    self._load_config_defs(basename, module, path)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                self._module_cache[path] = module\n",
   "                found_in_cache = False\n",
   "                obj = getattr(self._module_cache[path], self.class_name)\n",
   "                display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                module = __import__(self.package, fromlist=[self.base_class])\n",
   "                    plugin_class = getattr(module, self.base_class)\n",
   "                if not issubclass(obj, plugin_class):\n",
   "            self._display_plugin_load(self.class_name, basename, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "            if not class_only:\n",
   "                    obj = obj(*args, **kwargs)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be incomplete: %s\" % (path, to_text(e)))\n",
   "            self._update_object(obj, basename, path)\n",
   "            yield obj\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).find_plugin(name, collection_list=collection_list)\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).get(name, *args, **kwargs)\n",
   "        plugins = list(super(Jinja2Loader, self).all(*args, **kwargs))\n",
   "        plugins.reverse()\n",
   "        return plugins\n",
   "    filters = defaultdict(frozenset)\n",
   "        filter_cfg = '/etc/ansible/plugin_filters.yml'\n",
   "        filter_cfg = C.PLUGIN_FILTERS_CFG\n",
   "    if os.path.exists(filter_cfg):\n",
   "        with open(filter_cfg, 'rb') as f:\n",
   "                filter_data = from_yaml(f.read())\n",
   "                display.warning(u'The plugin filter file, {0} was not parsable.'\n",
   "                                u' Skipping: {1}'.format(filter_cfg, to_text(e)))\n",
   "                return filters\n",
   "            version = filter_data['filter_version']\n",
   "            display.warning(u'The plugin filter file, {0} was invalid.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "            return filters\n",
   "        version = to_text(version)\n",
   "        version = version.strip()\n",
   "        if version == u'1.0':\n",
   "                filters['ansible.modules'] = frozenset(filter_data['module_blacklist'])\n",
   "                display.warning(u'Unable to parse the plugin filter file {0} as'\n",
   "                                u' Skipping.'.format(filter_cfg))\n",
   "                return filters\n",
   "            filters['ansible.plugins.action'] = filters['ansible.modules']\n",
   "            display.warning(u'The plugin filter file, {0} was a version not recognized by this'\n",
   "                            u' version of Ansible. Skipping.'.format(filter_cfg))\n",
   "            display.warning(u'The plugin filter file, {0} does not exist.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "    if 'stat' in filters['ansible.modules']:\n",
   "                           ' from the blacklist.'.format(to_native(filter_cfg)))\n",
   "    return filters\n",
   "    display.vvvv(to_text('Loading collection {0} from {1}'.format(collection_name, collection_path)))\n",
   "        if not _does_collection_support_ansible_version(collection_meta.get('requires_ansible', ''), ansible_version):\n",
   "                display.warning(message)\n",
   "        display.warning('Error parsing collection metadata requires_ansible value from collection {0}: {1}'.format(collection_name, ex))\n",
   "        display.warning('packaging Python module unavailable; unable to validate collection Ansible version requirements')\n",
   "        display.warning('AnsibleCollectionFinder has already been configured')\n"
  ]
 },
 "121": {
  "name": "path",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/loader.py",
  "lineno": "796",
  "column": "8",
  "context": " plugin_load_context.plugin_resolved_name\n        path = plugin_load_context.plugin_resolved_path\n        redirected_names = plugin_load_context.red",
  "context_lines": "        if not plugin_load_context.resolved or not plugin_load_context.plugin_resolved_path:\n            # FIXME: this is probably an error (eg removed plugin)\n            return get_with_context_result(None, plugin_load_context)\n\n        name = plugin_load_context.plugin_resolved_name\n        path = plugin_load_context.plugin_resolved_path\n        redirected_names = plugin_load_context.redirect_list or []\n\n        if path not in self._module_cache:\n            self._module_cache[path] = self._load_module_source(name, path)\n            self._load_config_defs(name, self._module_cache[path], path)\n",
  "slicing": [
   "    imp = None\n",
   "display = Display()\n",
   "get_with_context_result = namedtuple('get_with_context_result', ['object', 'plugin_load_context'])\n",
   "    return [(name, obj) for (name, obj) in globals().items() if isinstance(obj, PluginLoader)]\n",
   "    b_path = os.path.expanduser(to_bytes(path, errors='surrogate_or_strict'))\n",
   "    if os.path.isdir(b_path):\n",
   "        for name, obj in get_all_plugin_loaders():\n",
   "            if obj.subdir:\n",
   "                plugin_path = os.path.join(b_path, to_bytes(obj.subdir))\n",
   "                if os.path.isdir(plugin_path):\n",
   "                    obj.add_directory(to_text(plugin_path))\n",
   "        display.warning(\"Ignoring invalid path provided to plugin path: '%s' is not a directory\" % to_text(path))\n",
   "        shell_type = 'sh'\n",
   "                shell_filename = os.path.basename(executable)\n",
   "                    shell = shell_loader.get(shell_filename)\n",
   "                    shell = None\n",
   "                if shell is None:\n",
   "                    for shell in shell_loader.all():\n",
   "                        if shell_filename in shell.COMPATIBLE_SHELLS:\n",
   "                            shell_type = shell.SHELL_FAMILY\n",
   "    shell = shell_loader.get(shell_type)\n",
   "    if not shell:\n",
   "        raise AnsibleError(\"Could not find the shell plugin required (%s).\" % shell_type)\n",
   "        setattr(shell, 'executable', executable)\n",
   "    return shell\n",
   "    loader = getattr(sys.modules[__name__], '%s_loader' % which_loader)\n",
   "    for path in paths:\n",
   "        loader.add_directory(path, with_subdir=True)\n",
   "        self.path = path\n",
   "        warning_text = deprecation.get('warning_text', None)\n",
   "        removal_date = deprecation.get('removal_date', None)\n",
   "        removal_version = deprecation.get('removal_version', None)\n",
   "        if removal_date is not None:\n",
   "            removal_version = None\n",
   "        if not warning_text:\n",
   "            warning_text = '{0} has been deprecated'.format(name)\n",
   "        display.deprecated(warning_text, date=removal_date, version=removal_version, collection_name=collection_name)\n",
   "        if removal_date:\n",
   "            self.removal_date = removal_date\n",
   "        if removal_version:\n",
   "            self.removal_version = removal_version\n",
   "        self.deprecation_warnings.append(warning_text)\n",
   "        aliases = {} if aliases is None else aliases\n",
   "        self.aliases = aliases\n",
   "            config = [config]\n",
   "        elif not config:\n",
   "            config = []\n",
   "        self.config = config\n",
   "        class_name = data.get('class_name')\n",
   "        package = data.get('package')\n",
   "        config = data.get('config')\n",
   "        subdir = data.get('subdir')\n",
   "        aliases = data.get('aliases')\n",
   "        base_class = data.get('base_class')\n",
   "        PATH_CACHE[class_name] = data.get('PATH_CACHE')\n",
   "        PLUGIN_PATH_CACHE[class_name] = data.get('PLUGIN_PATH_CACHE')\n",
   "        self.__init__(class_name, package, config, subdir, aliases, base_class)\n",
   "        ret = []\n",
   "        for i in paths:\n",
   "            if i not in ret:\n",
   "                ret.append(i)\n",
   "        return os.pathsep.join(ret)\n",
   "        results = []\n",
   "        results.append(dir)\n",
   "        for root, subdirs, files in os.walk(dir, followlinks=True):\n",
   "            if '__init__.py' in files:\n",
   "                for x in subdirs:\n",
   "                    results.append(os.path.join(root, x))\n",
   "        return results\n",
   "            m = __import__(self.package)\n",
   "            parts = self.package.split('.')[1:]\n",
   "            for parent_mod in parts:\n",
   "                m = getattr(m, parent_mod)\n",
   "            self.package_path = os.path.dirname(m.__file__)\n",
   "        if subdirs:\n",
   "        ret = [PluginPathContext(p, False) for p in self._extra_dirs]\n",
   "            for path in self.config:\n",
   "                path = os.path.realpath(os.path.expanduser(path))\n",
   "                if subdirs:\n",
   "                    contents = glob.glob(\"%s/*\" % path) + glob.glob(\"%s/*/*\" % path)\n",
   "                    for c in contents:\n",
   "                        if os.path.isdir(c) and c not in ret:\n",
   "                            ret.append(PluginPathContext(c, False))\n",
   "                if path not in ret:\n",
   "                    ret.append(PluginPathContext(path, False))\n",
   "        ret.extend([PluginPathContext(p, True) for p in self._get_package_paths(subdirs=subdirs)])\n",
   "        ret.sort(key=lambda p: p.path.endswith('/windows'))\n",
   "        self._paths = ret\n",
   "        return ret\n",
   "        paths_with_context = self._get_paths_with_context(subdirs=subdirs)\n",
   "        return [path_with_context.path for path_with_context in paths_with_context]\n",
   "            type_name = get_plugin_class(self.class_name)\n",
   "            if type_name in C.CONFIGURABLE_PLUGINS:\n",
   "                dstring = AnsibleLoader(getattr(module, 'DOCUMENTATION', ''), file_name=path).get_single_data()\n",
   "                if dstring:\n",
   "                    add_fragments(dstring, path, fragment_loader=fragment_loader, is_module=(type_name == 'module'))\n",
   "                if dstring and 'options' in dstring and isinstance(dstring['options'], dict):\n",
   "                    C.config.initialize_plugin_configuration_definitions(type_name, name, dstring['options'])\n",
   "                    display.debug('Loaded config def from plugin (%s/%s)' % (type_name, name))\n",
   "        directory = os.path.realpath(directory)\n",
   "        if directory is not None:\n",
   "                directory = os.path.join(directory, self.subdir)\n",
   "            if directory not in self._extra_dirs:\n",
   "                self._extra_dirs.append(directory)\n",
   "                display.debug('Added %s to loader search path' % (directory))\n",
   "        collection_pkg = import_module(acr.n_python_collection_package_name)\n",
   "        if not collection_pkg:\n",
   "        collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "        if not collection_meta:\n",
   "            subdir_qualified_resource = '.'.join([acr.subdirs, acr.resource])\n",
   "            subdir_qualified_resource = acr.resource\n",
   "        entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource + extension, None)\n",
   "        if not entry:\n",
   "            entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource, None)\n",
   "        return entry\n",
   "        plugin_type = AnsibleCollectionRef.legacy_plugin_dir_to_plugin_type(self.subdir)\n",
   "        acr = AnsibleCollectionRef.from_fqcr(fq_name, plugin_type)\n",
   "        routing_metadata = self._query_collection_routing_meta(acr, plugin_type, extension=extension)\n",
   "        if routing_metadata:\n",
   "            deprecation = routing_metadata.get('deprecation', None)\n",
   "            plugin_load_context.record_deprecation(fq_name, deprecation, acr.collection)\n",
   "            tombstone = routing_metadata.get('tombstone', None)\n",
   "            if tombstone:\n",
   "                removal_date = tombstone.get('removal_date')\n",
   "                removal_version = tombstone.get('removal_version')\n",
   "                warning_text = tombstone.get('warning_text') or '{0} has been removed.'.format(fq_name)\n",
   "                removed_msg = display.get_deprecation_message(msg=warning_text, version=removal_version,\n",
   "                                                              date=removal_date, removed=True,\n",
   "                                                              collection_name=acr.collection)\n",
   "                plugin_load_context.removal_date = removal_date\n",
   "                plugin_load_context.removal_version = removal_version\n",
   "                plugin_load_context.exit_reason = removed_msg\n",
   "                raise AnsiblePluginRemovedError(removed_msg, plugin_load_context=plugin_load_context)\n",
   "            redirect = routing_metadata.get('redirect', None)\n",
   "            if redirect:\n",
   "                display.vv(\"redirecting (type: {0}) {1} to {2}\".format(plugin_type, fq_name, redirect))\n",
   "                return plugin_load_context.redirect(redirect)\n",
   "        n_resource = to_native(acr.resource, errors='strict')\n",
   "        full_name = '{0}.{1}'.format(acr.n_python_package_name, n_resource)\n",
   "            n_resource += extension\n",
   "        pkg = sys.modules.get(acr.n_python_package_name)\n",
   "        if not pkg:\n",
   "                pkg = import_module(acr.n_python_package_name)\n",
   "                return plugin_load_context.nope('Python package {0} not found'.format(acr.n_python_package_name))\n",
   "        pkg_path = os.path.dirname(pkg.__file__)\n",
   "        n_resource_path = os.path.join(pkg_path, n_resource)\n",
   "        if os.path.exists(n_resource_path):\n",
   "                full_name, to_text(n_resource_path), acr.collection, 'found exact match for {0} in {1}'.format(full_name, acr.collection))\n",
   "            return plugin_load_context.nope('no match for {0} in {1}'.format(to_text(n_resource), acr.collection))\n",
   "        found_files = [f\n",
   "                       for f in glob.iglob(os.path.join(pkg_path, n_resource) + '.*')\n",
   "                       if os.path.isfile(f) and not f.endswith(C.MODULE_IGNORE_EXTS)]\n",
   "        if not found_files:\n",
   "            return plugin_load_context.nope('failed fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        if len(found_files) > 1:\n",
   "            full_name, to_text(found_files[0]), acr.collection, 'found fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        result = self.find_plugin_with_context(name, mod_type, ignore_deprecated, check_aliases, collection_list)\n",
   "        if result.resolved and result.plugin_resolved_path:\n",
   "            return result.plugin_resolved_path\n",
   "        plugin_load_context = PluginLoadContext()\n",
   "        plugin_load_context.original_name = name\n",
   "            result = self._resolve_plugin_step(name, mod_type, ignore_deprecated, check_aliases, collection_list, plugin_load_context=plugin_load_context)\n",
   "            if result.pending_redirect:\n",
   "                if result.pending_redirect in result.redirect_list:\n",
   "                    raise AnsiblePluginCircularRedirect('plugin redirect loop resolving {0} (path: {1})'.format(result.original_name, result.redirect_list))\n",
   "                name = result.pending_redirect\n",
   "                result.pending_redirect = None\n",
   "                plugin_load_context = result\n",
   "        if plugin_load_context.error_list:\n",
   "            display.warning(\"errors were encountered during the plugin load for {0}:\\n{1}\".format(name, plugin_load_context.error_list))\n",
   "        return plugin_load_context\n",
   "        if not plugin_load_context:\n",
   "        plugin_load_context.redirect_list.append(name)\n",
   "        plugin_load_context.resolved = False\n",
   "        if name in _PLUGIN_FILTERS[self.package]:\n",
   "            plugin_load_context.exit_reason = '{0} matched a defined plugin filter'.format(name)\n",
   "            return plugin_load_context\n",
   "            suffix = mod_type\n",
   "            suffix = '.py'\n",
   "            suffix = ''\n",
   "        if (AnsibleCollectionRef.is_valid_fqcr(name) or collection_list) and not name.startswith('Ansible'):\n",
   "            if '.' in name or not collection_list:\n",
   "                candidates = [name]\n",
   "                candidates = ['{0}.{1}'.format(c, name) for c in collection_list]\n",
   "            for candidate_name in candidates:\n",
   "                    plugin_load_context.load_attempts.append(candidate_name)\n",
   "                    if candidate_name.startswith('ansible.legacy'):\n",
   "                        plugin_load_context = self._find_plugin_legacy(name.replace('ansible.legacy.', '', 1),\n",
   "                                                                       plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "                        plugin_load_context = self._find_fq_plugin(candidate_name, suffix, plugin_load_context=plugin_load_context)\n",
   "                        if candidate_name != plugin_load_context.original_name and candidate_name not in plugin_load_context.redirect_list:\n",
   "                            plugin_load_context.redirect_list.append(candidate_name)\n",
   "                    if plugin_load_context.resolved or plugin_load_context.pending_redirect:  # if we got an answer or need to chase down a redirect, return\n",
   "                        return plugin_load_context\n",
   "                    plugin_load_context.import_error_list.append(ie)\n",
   "                    plugin_load_context.error_list.append(to_native(ex))\n",
   "            if plugin_load_context.error_list:\n",
   "                display.debug(msg='plugin lookup for {0} failed; errors: {1}'.format(name, '; '.join(plugin_load_context.error_list)))\n",
   "            plugin_load_context.exit_reason = 'no matches found for {0}'.format(name)\n",
   "            return plugin_load_context\n",
   "        return self._find_plugin_legacy(name, plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "        plugin_load_context.resolved = False\n",
   "            name = self.aliases.get(name, name)\n",
   "        pull_cache = self._plugin_path_cache[suffix]\n",
   "            path_with_context = pull_cache[name]\n",
   "            plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "            plugin_load_context.plugin_resolved_name = name\n",
   "            plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "            plugin_load_context.resolved = True\n",
   "            return plugin_load_context\n",
   "        for path_context in (p for p in self._get_paths_with_context() if p.path not in self._searched_paths and os.path.isdir(p.path)):\n",
   "            path = path_context.path\n",
   "            display.debug('trying %s' % path)\n",
   "            plugin_load_context.load_attempts.append(path)\n",
   "                full_paths = (os.path.join(path, f) for f in os.listdir(path))\n",
   "                display.warning(\"Error accessing plugin paths: %s\" % to_text(e))\n",
   "            for full_path in (f for f in full_paths if os.path.isfile(f) and not f.endswith('__init__.py')):\n",
   "                full_name = os.path.basename(full_path)\n",
   "                if any(full_path.endswith(x) for x in C.MODULE_IGNORE_EXTS):\n",
   "                splitname = os.path.splitext(full_name)\n",
   "                base_name = splitname[0]\n",
   "                internal = path_context.internal\n",
   "                    extension = splitname[1]\n",
   "                    extension = ''\n",
   "                if base_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][full_name] = PluginPathContext(full_path, internal)\n",
   "                if base_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][full_name] = PluginPathContext(full_path, internal)\n",
   "            self._searched_paths.add(path)\n",
   "                path_with_context = pull_cache[name]\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        if not name.startswith('_'):\n",
   "            alias_name = '_' + name\n",
   "            if alias_name in pull_cache:\n",
   "                path_with_context = pull_cache[alias_name]\n",
   "                if not ignore_deprecated and not os.path.islink(path_with_context.path):\n",
   "                    display.deprecated('%s is kept for backwards compatibility but usage is discouraged. '  # pylint: disable=ansible-deprecated-no-version\n",
   "                                       'The module documentation details page may explain more about this rationale.' % name.lstrip('_'))\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = alias_name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        candidate_fqcr = 'ansible.builtin.{0}'.format(name)\n",
   "        if '.' not in name and AnsibleCollectionRef.is_valid_fqcr(candidate_fqcr):\n",
   "            return self._find_fq_plugin(fq_name=candidate_fqcr, extension=suffix, plugin_load_context=plugin_load_context)\n",
   "        return plugin_load_context.nope('{0} is not eligible for last-chance resolution'.format(name))\n",
   "            return self.find_plugin(name, collection_list=collection_list) is not None\n",
   "            display.debug('has_plugin error: {0}'.format(to_text(ex)))\n",
   "        if name.startswith('ansible_collections.'):\n",
   "            full_name = name\n",
   "            full_name = '.'.join([self.package, name])\n",
   "        if full_name in sys.modules:\n",
   "            return sys.modules[full_name]\n",
   "            if imp is None:\n",
   "                spec = importlib.util.spec_from_file_location(to_native(full_name), to_native(path))\n",
   "                module = importlib.util.module_from_spec(spec)\n",
   "                spec.loader.exec_module(module)\n",
   "                sys.modules[full_name] = module\n",
   "                with open(to_bytes(path), 'rb') as module_file:\n",
   "                    module = imp.load_source(to_native(full_name), to_native(path), module_file)\n",
   "        return module\n",
   "        setattr(obj, '_original_path', path)\n",
   "        setattr(obj, '_load_name', name)\n",
   "        setattr(obj, '_redirected_names', redirected_names or [])\n",
   "        return self.get_with_context(name, *args, **kwargs).object\n",
   "        found_in_cache = True\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        collection_list = kwargs.pop('collection_list', None)\n",
   "        if name in self.aliases:\n",
   "            name = self.aliases[name]\n",
   "        plugin_load_context = self.find_plugin_with_context(name, collection_list=collection_list)\n",
   "        if not plugin_load_context.resolved or not plugin_load_context.plugin_resolved_path:\n",
   "            return get_with_context_result(None, plugin_load_context)\n",
   "        name = plugin_load_context.plugin_resolved_name\n",
   "        path = plugin_load_context.plugin_resolved_path\n",
   "        redirected_names = plugin_load_context.redirect_list or []\n",
   "        if path not in self._module_cache:\n",
   "            self._module_cache[path] = self._load_module_source(name, path)\n",
   "            self._load_config_defs(name, self._module_cache[path], path)\n",
   "            found_in_cache = False\n",
   "        obj = getattr(self._module_cache[path], self.class_name)\n",
   "            module = __import__(self.package, fromlist=[self.base_class])\n",
   "                plugin_class = getattr(module, self.base_class)\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "            if not issubclass(obj, plugin_class):\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "        self._display_plugin_load(self.class_name, name, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "        if not class_only:\n",
   "                instance = object.__new__(obj)\n",
   "                self._update_object(instance, name, path, redirected_names)\n",
   "                obj.__init__(instance, *args, **kwargs)\n",
   "                obj = instance\n",
   "                    return get_with_context_result(None, plugin_load_context)\n",
   "        self._update_object(obj, name, path, redirected_names)\n",
   "        return get_with_context_result(obj, plugin_load_context)\n",
   "            msg = 'Loading %s \\'%s\\' from %s' % (class_name, os.path.basename(name), path)\n",
   "                msg = '%s (searched paths: %s)' % (msg, self.format_paths(searched_paths))\n",
   "            if found_in_cache or class_only:\n",
   "                msg = '%s (found_in_cache=%s, class_only=%s)' % (msg, found_in_cache, class_only)\n",
   "            display.debug(msg)\n",
   "        dedupe = kwargs.pop('_dedupe', True)\n",
   "        path_only = kwargs.pop('path_only', False)\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        if path_only and class_only:\n",
   "        all_matches = []\n",
   "        found_in_cache = True\n",
   "        for i in self._get_paths():\n",
   "            all_matches.extend(glob.glob(os.path.join(i, \"*.py\")))\n",
   "        loaded_modules = set()\n",
   "        for path in sorted(all_matches, key=os.path.basename):\n",
   "            name = os.path.splitext(path)[0]\n",
   "            basename = os.path.basename(name)\n",
   "            if basename == '__init__' or basename in _PLUGIN_FILTERS[self.package]:\n",
   "            if dedupe and basename in loaded_modules:\n",
   "            loaded_modules.add(basename)\n",
   "            if path_only:\n",
   "                yield path\n",
   "            if path not in self._module_cache:\n",
   "                        full_name = '{0}_{1}'.format(abs(hash(path)), basename)\n",
   "                        full_name = basename\n",
   "                    module = self._load_module_source(full_name, path)\n",
   "                    self._load_config_defs(basename, module, path)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                self._module_cache[path] = module\n",
   "                found_in_cache = False\n",
   "                obj = getattr(self._module_cache[path], self.class_name)\n",
   "                display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                module = __import__(self.package, fromlist=[self.base_class])\n",
   "                    plugin_class = getattr(module, self.base_class)\n",
   "                if not issubclass(obj, plugin_class):\n",
   "            self._display_plugin_load(self.class_name, basename, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "            if not class_only:\n",
   "                    obj = obj(*args, **kwargs)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be incomplete: %s\" % (path, to_text(e)))\n",
   "            self._update_object(obj, basename, path)\n",
   "            yield obj\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).find_plugin(name, collection_list=collection_list)\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).get(name, *args, **kwargs)\n",
   "        plugins = list(super(Jinja2Loader, self).all(*args, **kwargs))\n",
   "        plugins.reverse()\n",
   "        return plugins\n",
   "    filters = defaultdict(frozenset)\n",
   "        filter_cfg = '/etc/ansible/plugin_filters.yml'\n",
   "        filter_cfg = C.PLUGIN_FILTERS_CFG\n",
   "    if os.path.exists(filter_cfg):\n",
   "        with open(filter_cfg, 'rb') as f:\n",
   "                filter_data = from_yaml(f.read())\n",
   "                display.warning(u'The plugin filter file, {0} was not parsable.'\n",
   "                                u' Skipping: {1}'.format(filter_cfg, to_text(e)))\n",
   "                return filters\n",
   "            version = filter_data['filter_version']\n",
   "            display.warning(u'The plugin filter file, {0} was invalid.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "            return filters\n",
   "        version = to_text(version)\n",
   "        version = version.strip()\n",
   "        if version == u'1.0':\n",
   "                filters['ansible.modules'] = frozenset(filter_data['module_blacklist'])\n",
   "                display.warning(u'Unable to parse the plugin filter file {0} as'\n",
   "                                u' Skipping.'.format(filter_cfg))\n",
   "                return filters\n",
   "            filters['ansible.plugins.action'] = filters['ansible.modules']\n",
   "            display.warning(u'The plugin filter file, {0} was a version not recognized by this'\n",
   "                            u' version of Ansible. Skipping.'.format(filter_cfg))\n",
   "            display.warning(u'The plugin filter file, {0} does not exist.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "    if 'stat' in filters['ansible.modules']:\n",
   "                           ' from the blacklist.'.format(to_native(filter_cfg)))\n",
   "    return filters\n",
   "    display.vvvv(to_text('Loading collection {0} from {1}'.format(collection_name, collection_path)))\n",
   "        if not _does_collection_support_ansible_version(collection_meta.get('requires_ansible', ''), ansible_version):\n",
   "                display.warning(message)\n",
   "        display.warning('Error parsing collection metadata requires_ansible value from collection {0}: {1}'.format(collection_name, ex))\n",
   "        display.warning('packaging Python module unavailable; unable to validate collection Ansible version requirements')\n",
   "        display.warning('AnsibleCollectionFinder has already been configured')\n"
  ]
 },
 "122": {
  "name": "obj",
  "type": "abc.ABCMeta",
  "class": "imported",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/plugins/loader.py",
  "lineno": "804",
  "column": "8",
  "context": "path)\n            found_in_cache = False\n\n        obj = getattr(self._module_cache[path], self.class_name)\n        if self.base_class:\n            # The impo",
  "context_lines": "        if path not in self._module_cache:\n            self._module_cache[path] = self._load_module_source(name, path)\n            self._load_config_defs(name, self._module_cache[path], path)\n            found_in_cache = False\n\n        obj = getattr(self._module_cache[path], self.class_name)\n        if self.base_class:\n            # The import path is hardcoded and should be the right place,\n            # so we are not expecting an ImportError.\n            module = __import__(self.package, fromlist=[self.base_class])\n",
  "slicing": [
   "    imp = None\n",
   "display = Display()\n",
   "get_with_context_result = namedtuple('get_with_context_result', ['object', 'plugin_load_context'])\n",
   "    return [(name, obj) for (name, obj) in globals().items() if isinstance(obj, PluginLoader)]\n",
   "    b_path = os.path.expanduser(to_bytes(path, errors='surrogate_or_strict'))\n",
   "    if os.path.isdir(b_path):\n",
   "        for name, obj in get_all_plugin_loaders():\n",
   "            if obj.subdir:\n",
   "                plugin_path = os.path.join(b_path, to_bytes(obj.subdir))\n",
   "                if os.path.isdir(plugin_path):\n",
   "                    obj.add_directory(to_text(plugin_path))\n",
   "        display.warning(\"Ignoring invalid path provided to plugin path: '%s' is not a directory\" % to_text(path))\n",
   "        shell_type = 'sh'\n",
   "                shell_filename = os.path.basename(executable)\n",
   "                    shell = shell_loader.get(shell_filename)\n",
   "                    shell = None\n",
   "                if shell is None:\n",
   "                    for shell in shell_loader.all():\n",
   "                        if shell_filename in shell.COMPATIBLE_SHELLS:\n",
   "                            shell_type = shell.SHELL_FAMILY\n",
   "    shell = shell_loader.get(shell_type)\n",
   "    if not shell:\n",
   "        raise AnsibleError(\"Could not find the shell plugin required (%s).\" % shell_type)\n",
   "        setattr(shell, 'executable', executable)\n",
   "    return shell\n",
   "    loader = getattr(sys.modules[__name__], '%s_loader' % which_loader)\n",
   "    for path in paths:\n",
   "        loader.add_directory(path, with_subdir=True)\n",
   "        self.path = path\n",
   "        warning_text = deprecation.get('warning_text', None)\n",
   "        removal_date = deprecation.get('removal_date', None)\n",
   "        removal_version = deprecation.get('removal_version', None)\n",
   "        if removal_date is not None:\n",
   "            removal_version = None\n",
   "        if not warning_text:\n",
   "            warning_text = '{0} has been deprecated'.format(name)\n",
   "        display.deprecated(warning_text, date=removal_date, version=removal_version, collection_name=collection_name)\n",
   "        if removal_date:\n",
   "            self.removal_date = removal_date\n",
   "        if removal_version:\n",
   "            self.removal_version = removal_version\n",
   "        self.deprecation_warnings.append(warning_text)\n",
   "        aliases = {} if aliases is None else aliases\n",
   "        self.aliases = aliases\n",
   "            config = [config]\n",
   "        elif not config:\n",
   "            config = []\n",
   "        self.config = config\n",
   "        class_name = data.get('class_name')\n",
   "        package = data.get('package')\n",
   "        config = data.get('config')\n",
   "        subdir = data.get('subdir')\n",
   "        aliases = data.get('aliases')\n",
   "        base_class = data.get('base_class')\n",
   "        PATH_CACHE[class_name] = data.get('PATH_CACHE')\n",
   "        PLUGIN_PATH_CACHE[class_name] = data.get('PLUGIN_PATH_CACHE')\n",
   "        self.__init__(class_name, package, config, subdir, aliases, base_class)\n",
   "        ret = []\n",
   "        for i in paths:\n",
   "            if i not in ret:\n",
   "                ret.append(i)\n",
   "        return os.pathsep.join(ret)\n",
   "        results = []\n",
   "        results.append(dir)\n",
   "        for root, subdirs, files in os.walk(dir, followlinks=True):\n",
   "            if '__init__.py' in files:\n",
   "                for x in subdirs:\n",
   "                    results.append(os.path.join(root, x))\n",
   "        return results\n",
   "            m = __import__(self.package)\n",
   "            parts = self.package.split('.')[1:]\n",
   "            for parent_mod in parts:\n",
   "                m = getattr(m, parent_mod)\n",
   "            self.package_path = os.path.dirname(m.__file__)\n",
   "        if subdirs:\n",
   "        ret = [PluginPathContext(p, False) for p in self._extra_dirs]\n",
   "            for path in self.config:\n",
   "                path = os.path.realpath(os.path.expanduser(path))\n",
   "                if subdirs:\n",
   "                    contents = glob.glob(\"%s/*\" % path) + glob.glob(\"%s/*/*\" % path)\n",
   "                    for c in contents:\n",
   "                        if os.path.isdir(c) and c not in ret:\n",
   "                            ret.append(PluginPathContext(c, False))\n",
   "                if path not in ret:\n",
   "                    ret.append(PluginPathContext(path, False))\n",
   "        ret.extend([PluginPathContext(p, True) for p in self._get_package_paths(subdirs=subdirs)])\n",
   "        ret.sort(key=lambda p: p.path.endswith('/windows'))\n",
   "        self._paths = ret\n",
   "        return ret\n",
   "        paths_with_context = self._get_paths_with_context(subdirs=subdirs)\n",
   "        return [path_with_context.path for path_with_context in paths_with_context]\n",
   "            type_name = get_plugin_class(self.class_name)\n",
   "            if type_name in C.CONFIGURABLE_PLUGINS:\n",
   "                dstring = AnsibleLoader(getattr(module, 'DOCUMENTATION', ''), file_name=path).get_single_data()\n",
   "                if dstring:\n",
   "                    add_fragments(dstring, path, fragment_loader=fragment_loader, is_module=(type_name == 'module'))\n",
   "                if dstring and 'options' in dstring and isinstance(dstring['options'], dict):\n",
   "                    C.config.initialize_plugin_configuration_definitions(type_name, name, dstring['options'])\n",
   "                    display.debug('Loaded config def from plugin (%s/%s)' % (type_name, name))\n",
   "        directory = os.path.realpath(directory)\n",
   "        if directory is not None:\n",
   "                directory = os.path.join(directory, self.subdir)\n",
   "            if directory not in self._extra_dirs:\n",
   "                self._extra_dirs.append(directory)\n",
   "                display.debug('Added %s to loader search path' % (directory))\n",
   "        collection_pkg = import_module(acr.n_python_collection_package_name)\n",
   "        if not collection_pkg:\n",
   "        collection_meta = getattr(collection_pkg, '_collection_meta', None)\n",
   "        if not collection_meta:\n",
   "            subdir_qualified_resource = '.'.join([acr.subdirs, acr.resource])\n",
   "            subdir_qualified_resource = acr.resource\n",
   "        entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource + extension, None)\n",
   "        if not entry:\n",
   "            entry = collection_meta.get('plugin_routing', {}).get(plugin_type, {}).get(subdir_qualified_resource, None)\n",
   "        return entry\n",
   "        plugin_type = AnsibleCollectionRef.legacy_plugin_dir_to_plugin_type(self.subdir)\n",
   "        acr = AnsibleCollectionRef.from_fqcr(fq_name, plugin_type)\n",
   "        routing_metadata = self._query_collection_routing_meta(acr, plugin_type, extension=extension)\n",
   "        if routing_metadata:\n",
   "            deprecation = routing_metadata.get('deprecation', None)\n",
   "            plugin_load_context.record_deprecation(fq_name, deprecation, acr.collection)\n",
   "            tombstone = routing_metadata.get('tombstone', None)\n",
   "            if tombstone:\n",
   "                removal_date = tombstone.get('removal_date')\n",
   "                removal_version = tombstone.get('removal_version')\n",
   "                warning_text = tombstone.get('warning_text') or '{0} has been removed.'.format(fq_name)\n",
   "                removed_msg = display.get_deprecation_message(msg=warning_text, version=removal_version,\n",
   "                                                              date=removal_date, removed=True,\n",
   "                                                              collection_name=acr.collection)\n",
   "                plugin_load_context.removal_date = removal_date\n",
   "                plugin_load_context.removal_version = removal_version\n",
   "                plugin_load_context.exit_reason = removed_msg\n",
   "                raise AnsiblePluginRemovedError(removed_msg, plugin_load_context=plugin_load_context)\n",
   "            redirect = routing_metadata.get('redirect', None)\n",
   "            if redirect:\n",
   "                display.vv(\"redirecting (type: {0}) {1} to {2}\".format(plugin_type, fq_name, redirect))\n",
   "                return plugin_load_context.redirect(redirect)\n",
   "        n_resource = to_native(acr.resource, errors='strict')\n",
   "        full_name = '{0}.{1}'.format(acr.n_python_package_name, n_resource)\n",
   "            n_resource += extension\n",
   "        pkg = sys.modules.get(acr.n_python_package_name)\n",
   "        if not pkg:\n",
   "                pkg = import_module(acr.n_python_package_name)\n",
   "                return plugin_load_context.nope('Python package {0} not found'.format(acr.n_python_package_name))\n",
   "        pkg_path = os.path.dirname(pkg.__file__)\n",
   "        n_resource_path = os.path.join(pkg_path, n_resource)\n",
   "        if os.path.exists(n_resource_path):\n",
   "                full_name, to_text(n_resource_path), acr.collection, 'found exact match for {0} in {1}'.format(full_name, acr.collection))\n",
   "            return plugin_load_context.nope('no match for {0} in {1}'.format(to_text(n_resource), acr.collection))\n",
   "        found_files = [f\n",
   "                       for f in glob.iglob(os.path.join(pkg_path, n_resource) + '.*')\n",
   "                       if os.path.isfile(f) and not f.endswith(C.MODULE_IGNORE_EXTS)]\n",
   "        if not found_files:\n",
   "            return plugin_load_context.nope('failed fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        if len(found_files) > 1:\n",
   "            full_name, to_text(found_files[0]), acr.collection, 'found fuzzy extension match for {0} in {1}'.format(full_name, acr.collection))\n",
   "        result = self.find_plugin_with_context(name, mod_type, ignore_deprecated, check_aliases, collection_list)\n",
   "        if result.resolved and result.plugin_resolved_path:\n",
   "            return result.plugin_resolved_path\n",
   "        plugin_load_context = PluginLoadContext()\n",
   "        plugin_load_context.original_name = name\n",
   "            result = self._resolve_plugin_step(name, mod_type, ignore_deprecated, check_aliases, collection_list, plugin_load_context=plugin_load_context)\n",
   "            if result.pending_redirect:\n",
   "                if result.pending_redirect in result.redirect_list:\n",
   "                    raise AnsiblePluginCircularRedirect('plugin redirect loop resolving {0} (path: {1})'.format(result.original_name, result.redirect_list))\n",
   "                name = result.pending_redirect\n",
   "                result.pending_redirect = None\n",
   "                plugin_load_context = result\n",
   "        if plugin_load_context.error_list:\n",
   "            display.warning(\"errors were encountered during the plugin load for {0}:\\n{1}\".format(name, plugin_load_context.error_list))\n",
   "        return plugin_load_context\n",
   "        if not plugin_load_context:\n",
   "        plugin_load_context.redirect_list.append(name)\n",
   "        plugin_load_context.resolved = False\n",
   "        if name in _PLUGIN_FILTERS[self.package]:\n",
   "            plugin_load_context.exit_reason = '{0} matched a defined plugin filter'.format(name)\n",
   "            return plugin_load_context\n",
   "            suffix = mod_type\n",
   "            suffix = '.py'\n",
   "            suffix = ''\n",
   "        if (AnsibleCollectionRef.is_valid_fqcr(name) or collection_list) and not name.startswith('Ansible'):\n",
   "            if '.' in name or not collection_list:\n",
   "                candidates = [name]\n",
   "                candidates = ['{0}.{1}'.format(c, name) for c in collection_list]\n",
   "            for candidate_name in candidates:\n",
   "                    plugin_load_context.load_attempts.append(candidate_name)\n",
   "                    if candidate_name.startswith('ansible.legacy'):\n",
   "                        plugin_load_context = self._find_plugin_legacy(name.replace('ansible.legacy.', '', 1),\n",
   "                                                                       plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "                        plugin_load_context = self._find_fq_plugin(candidate_name, suffix, plugin_load_context=plugin_load_context)\n",
   "                        if candidate_name != plugin_load_context.original_name and candidate_name not in plugin_load_context.redirect_list:\n",
   "                            plugin_load_context.redirect_list.append(candidate_name)\n",
   "                    if plugin_load_context.resolved or plugin_load_context.pending_redirect:  # if we got an answer or need to chase down a redirect, return\n",
   "                        return plugin_load_context\n",
   "                    plugin_load_context.import_error_list.append(ie)\n",
   "                    plugin_load_context.error_list.append(to_native(ex))\n",
   "            if plugin_load_context.error_list:\n",
   "                display.debug(msg='plugin lookup for {0} failed; errors: {1}'.format(name, '; '.join(plugin_load_context.error_list)))\n",
   "            plugin_load_context.exit_reason = 'no matches found for {0}'.format(name)\n",
   "            return plugin_load_context\n",
   "        return self._find_plugin_legacy(name, plugin_load_context, ignore_deprecated, check_aliases, suffix)\n",
   "        plugin_load_context.resolved = False\n",
   "            name = self.aliases.get(name, name)\n",
   "        pull_cache = self._plugin_path_cache[suffix]\n",
   "            path_with_context = pull_cache[name]\n",
   "            plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "            plugin_load_context.plugin_resolved_name = name\n",
   "            plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "            plugin_load_context.resolved = True\n",
   "            return plugin_load_context\n",
   "        for path_context in (p for p in self._get_paths_with_context() if p.path not in self._searched_paths and os.path.isdir(p.path)):\n",
   "            path = path_context.path\n",
   "            display.debug('trying %s' % path)\n",
   "            plugin_load_context.load_attempts.append(path)\n",
   "                full_paths = (os.path.join(path, f) for f in os.listdir(path))\n",
   "                display.warning(\"Error accessing plugin paths: %s\" % to_text(e))\n",
   "            for full_path in (f for f in full_paths if os.path.isfile(f) and not f.endswith('__init__.py')):\n",
   "                full_name = os.path.basename(full_path)\n",
   "                if any(full_path.endswith(x) for x in C.MODULE_IGNORE_EXTS):\n",
   "                splitname = os.path.splitext(full_name)\n",
   "                base_name = splitname[0]\n",
   "                internal = path_context.internal\n",
   "                    extension = splitname[1]\n",
   "                    extension = ''\n",
   "                if base_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache['']:\n",
   "                    self._plugin_path_cache[''][full_name] = PluginPathContext(full_path, internal)\n",
   "                if base_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][base_name] = PluginPathContext(full_path, internal)\n",
   "                if full_name not in self._plugin_path_cache[extension]:\n",
   "                    self._plugin_path_cache[extension][full_name] = PluginPathContext(full_path, internal)\n",
   "            self._searched_paths.add(path)\n",
   "                path_with_context = pull_cache[name]\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        if not name.startswith('_'):\n",
   "            alias_name = '_' + name\n",
   "            if alias_name in pull_cache:\n",
   "                path_with_context = pull_cache[alias_name]\n",
   "                if not ignore_deprecated and not os.path.islink(path_with_context.path):\n",
   "                    display.deprecated('%s is kept for backwards compatibility but usage is discouraged. '  # pylint: disable=ansible-deprecated-no-version\n",
   "                                       'The module documentation details page may explain more about this rationale.' % name.lstrip('_'))\n",
   "                plugin_load_context.plugin_resolved_path = path_with_context.path\n",
   "                plugin_load_context.plugin_resolved_name = alias_name\n",
   "                plugin_load_context.plugin_resolved_collection = 'ansible.builtin' if path_with_context.internal else ''\n",
   "                plugin_load_context.resolved = True\n",
   "                return plugin_load_context\n",
   "        candidate_fqcr = 'ansible.builtin.{0}'.format(name)\n",
   "        if '.' not in name and AnsibleCollectionRef.is_valid_fqcr(candidate_fqcr):\n",
   "            return self._find_fq_plugin(fq_name=candidate_fqcr, extension=suffix, plugin_load_context=plugin_load_context)\n",
   "        return plugin_load_context.nope('{0} is not eligible for last-chance resolution'.format(name))\n",
   "            return self.find_plugin(name, collection_list=collection_list) is not None\n",
   "            display.debug('has_plugin error: {0}'.format(to_text(ex)))\n",
   "        if name.startswith('ansible_collections.'):\n",
   "            full_name = name\n",
   "            full_name = '.'.join([self.package, name])\n",
   "        if full_name in sys.modules:\n",
   "            return sys.modules[full_name]\n",
   "            if imp is None:\n",
   "                spec = importlib.util.spec_from_file_location(to_native(full_name), to_native(path))\n",
   "                module = importlib.util.module_from_spec(spec)\n",
   "                spec.loader.exec_module(module)\n",
   "                sys.modules[full_name] = module\n",
   "                with open(to_bytes(path), 'rb') as module_file:\n",
   "                    module = imp.load_source(to_native(full_name), to_native(path), module_file)\n",
   "        return module\n",
   "        setattr(obj, '_original_path', path)\n",
   "        setattr(obj, '_load_name', name)\n",
   "        setattr(obj, '_redirected_names', redirected_names or [])\n",
   "        return self.get_with_context(name, *args, **kwargs).object\n",
   "        found_in_cache = True\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        collection_list = kwargs.pop('collection_list', None)\n",
   "        if name in self.aliases:\n",
   "            name = self.aliases[name]\n",
   "        plugin_load_context = self.find_plugin_with_context(name, collection_list=collection_list)\n",
   "        if not plugin_load_context.resolved or not plugin_load_context.plugin_resolved_path:\n",
   "            return get_with_context_result(None, plugin_load_context)\n",
   "        name = plugin_load_context.plugin_resolved_name\n",
   "        path = plugin_load_context.plugin_resolved_path\n",
   "        redirected_names = plugin_load_context.redirect_list or []\n",
   "        if path not in self._module_cache:\n",
   "            self._module_cache[path] = self._load_module_source(name, path)\n",
   "            self._load_config_defs(name, self._module_cache[path], path)\n",
   "            found_in_cache = False\n",
   "        obj = getattr(self._module_cache[path], self.class_name)\n",
   "            module = __import__(self.package, fromlist=[self.base_class])\n",
   "                plugin_class = getattr(module, self.base_class)\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "            if not issubclass(obj, plugin_class):\n",
   "                return get_with_context_result(None, plugin_load_context)\n",
   "        self._display_plugin_load(self.class_name, name, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "        if not class_only:\n",
   "                instance = object.__new__(obj)\n",
   "                self._update_object(instance, name, path, redirected_names)\n",
   "                obj.__init__(instance, *args, **kwargs)\n",
   "                obj = instance\n",
   "                    return get_with_context_result(None, plugin_load_context)\n",
   "        self._update_object(obj, name, path, redirected_names)\n",
   "        return get_with_context_result(obj, plugin_load_context)\n",
   "            msg = 'Loading %s \\'%s\\' from %s' % (class_name, os.path.basename(name), path)\n",
   "                msg = '%s (searched paths: %s)' % (msg, self.format_paths(searched_paths))\n",
   "            if found_in_cache or class_only:\n",
   "                msg = '%s (found_in_cache=%s, class_only=%s)' % (msg, found_in_cache, class_only)\n",
   "            display.debug(msg)\n",
   "        dedupe = kwargs.pop('_dedupe', True)\n",
   "        path_only = kwargs.pop('path_only', False)\n",
   "        class_only = kwargs.pop('class_only', False)\n",
   "        if path_only and class_only:\n",
   "        all_matches = []\n",
   "        found_in_cache = True\n",
   "        for i in self._get_paths():\n",
   "            all_matches.extend(glob.glob(os.path.join(i, \"*.py\")))\n",
   "        loaded_modules = set()\n",
   "        for path in sorted(all_matches, key=os.path.basename):\n",
   "            name = os.path.splitext(path)[0]\n",
   "            basename = os.path.basename(name)\n",
   "            if basename == '__init__' or basename in _PLUGIN_FILTERS[self.package]:\n",
   "            if dedupe and basename in loaded_modules:\n",
   "            loaded_modules.add(basename)\n",
   "            if path_only:\n",
   "                yield path\n",
   "            if path not in self._module_cache:\n",
   "                        full_name = '{0}_{1}'.format(abs(hash(path)), basename)\n",
   "                        full_name = basename\n",
   "                    module = self._load_module_source(full_name, path)\n",
   "                    self._load_config_defs(basename, module, path)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                self._module_cache[path] = module\n",
   "                found_in_cache = False\n",
   "                obj = getattr(self._module_cache[path], self.class_name)\n",
   "                display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n",
   "                module = __import__(self.package, fromlist=[self.base_class])\n",
   "                    plugin_class = getattr(module, self.base_class)\n",
   "                if not issubclass(obj, plugin_class):\n",
   "            self._display_plugin_load(self.class_name, basename, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n",
   "            if not class_only:\n",
   "                    obj = obj(*args, **kwargs)\n",
   "                    display.warning(\"Skipping plugin (%s) as it seems to be incomplete: %s\" % (path, to_text(e)))\n",
   "            self._update_object(obj, basename, path)\n",
   "            yield obj\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).find_plugin(name, collection_list=collection_list)\n",
   "        if '.' in name:\n",
   "            return super(Jinja2Loader, self).get(name, *args, **kwargs)\n",
   "        plugins = list(super(Jinja2Loader, self).all(*args, **kwargs))\n",
   "        plugins.reverse()\n",
   "        return plugins\n",
   "    filters = defaultdict(frozenset)\n",
   "        filter_cfg = '/etc/ansible/plugin_filters.yml'\n",
   "        filter_cfg = C.PLUGIN_FILTERS_CFG\n",
   "    if os.path.exists(filter_cfg):\n",
   "        with open(filter_cfg, 'rb') as f:\n",
   "                filter_data = from_yaml(f.read())\n",
   "                display.warning(u'The plugin filter file, {0} was not parsable.'\n",
   "                                u' Skipping: {1}'.format(filter_cfg, to_text(e)))\n",
   "                return filters\n",
   "            version = filter_data['filter_version']\n",
   "            display.warning(u'The plugin filter file, {0} was invalid.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "            return filters\n",
   "        version = to_text(version)\n",
   "        version = version.strip()\n",
   "        if version == u'1.0':\n",
   "                filters['ansible.modules'] = frozenset(filter_data['module_blacklist'])\n",
   "                display.warning(u'Unable to parse the plugin filter file {0} as'\n",
   "                                u' Skipping.'.format(filter_cfg))\n",
   "                return filters\n",
   "            filters['ansible.plugins.action'] = filters['ansible.modules']\n",
   "            display.warning(u'The plugin filter file, {0} was a version not recognized by this'\n",
   "                            u' version of Ansible. Skipping.'.format(filter_cfg))\n",
   "            display.warning(u'The plugin filter file, {0} does not exist.'\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "    if 'stat' in filters['ansible.modules']:\n",
   "                           ' from the blacklist.'.format(to_native(filter_cfg)))\n",
   "    return filters\n",
   "    display.vvvv(to_text('Loading collection {0} from {1}'.format(collection_name, collection_path)))\n",
   "        if not _does_collection_support_ansible_version(collection_meta.get('requires_ansible', ''), ansible_version):\n",
   "                display.warning(message)\n",
   "        display.warning('Error parsing collection metadata requires_ansible value from collection {0}: {1}'.format(collection_name, ex))\n",
   "        display.warning('packaging Python module unavailable; unable to validate collection Ansible version requirements')\n",
   "        display.warning('AnsibleCollectionFinder has already been configured')\n"
  ]
 },
 "123": {
  "name": "filter_cfg",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/loader.py",
  "lineno": "1018",
  "column": "8",
  "context": "alse\n    if C.PLUGIN_FILTERS_CFG is None:\n        filter_cfg = '/etc/ansible/plugin_filters.yml'\n    else:\n        filter_cfg = C.PLUGIN_FILTERS_CF",
  "context_lines": "def _load_plugin_filter():\n    filters = defaultdict(frozenset)\n    user_set = False\n    if C.PLUGIN_FILTERS_CFG is None:\n        filter_cfg = '/etc/ansible/plugin_filters.yml'\n    else:\n        filter_cfg = C.PLUGIN_FILTERS_CFG\n        user_set = True\n\n    if os.path.exists(filter_cfg):\n",
  "slicing": [
   "        filter_cfg = '/etc/ansible/plugin_filters.yml'\n",
   "    if os.path.exists(filter_cfg):\n",
   "        with open(filter_cfg, 'rb') as f:\n",
   "                                u' Skipping: {1}'.format(filter_cfg, to_text(e)))\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "                                u' Skipping.'.format(filter_cfg))\n",
   "                            u' version of Ansible. Skipping.'.format(filter_cfg))\n",
   "                            u' Skipping.'.format(filter_cfg))\n",
   "                           ' from the blacklist.'.format(to_native(filter_cfg)))\n"
  ]
 },
 "124": {
  "name": "name",
  "type": "NoneType",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/plugins/become/__init__.py",
  "lineno": "25",
  "column": "4",
  "context": "(length))\n\n\nclass BecomeBase(AnsiblePlugin):\n\n    name = None\n\n    # messages for detecting prompted password is",
  "context_lines": "def _gen_id(length=32):\n    ''' return random string used to identify the current privilege escalation '''\n    return ''.join(choice(ascii_lowercase) for x in range(length))\n\n\nclass BecomeBase(AnsiblePlugin):\n\n    name = None\n\n    # messages for detecting prompted password issues\n    fail = tuple()\n    missing = tuple()\n\n",
  "slicing": "    name = None\n"
 },
 "125": {
  "name": "prompt",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/become/__init__.py",
  "lineno": "36",
  "column": "4",
  "context": "   require_tty = False\n\n    # prompt to match\n    prompt = ''\n\n    def __init__(self):\n        super(BecomeBase,",
  "context_lines": "    # many connection plugins cannot provide tty, set to True if your become\n    # plugin requires a tty, i.e su\n    require_tty = False\n\n    # prompt to match\n    prompt = ''\n\n    def __init__(self):\n        super(BecomeBase, self).__init__()\n        self._id = ''\n",
  "slicing": "    prompt = ''\n"
 },
 "126": {
  "name": "REDO",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/strategy/__init__.py",
  "lineno": "1261",
  "column": "4",
  "context": " next action after an interpreter's exit. \"\"\"\n    REDO = 1\n    CONTINUE = 2\n    EXIT = 3\n\n    def __init__(se",
  "context_lines": "                    if r._host not in self._active_connections:\n                        self._active_connections[r._host] = socket_path\n\n\nclass NextAction(object):\n    \"\"\" The next action after an interpreter's exit. \"\"\"\n    REDO = 1\n    CONTINUE = 2\n    EXIT = 3\n\n    def __init__(self, result=EXIT):\n        self.result = result\n\n\n",
  "slicing": "    REDO = 1\n"
 },
 "127": {
  "name": "CONTINUE",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/strategy/__init__.py",
  "lineno": "1262",
  "column": "4",
  "context": "after an interpreter's exit. \"\"\"\n    REDO = 1\n    CONTINUE = 2\n    EXIT = 3\n\n    def __init__(self, result=EXIT):",
  "context_lines": "                        self._active_connections[r._host] = socket_path\n\n\nclass NextAction(object):\n    \"\"\" The next action after an interpreter's exit. \"\"\"\n    REDO = 1\n    CONTINUE = 2\n    EXIT = 3\n\n    def __init__(self, result=EXIT):\n        self.result = result\n\n\nclass Debugger(cmd.Cmd):\n",
  "slicing": "    CONTINUE = 2\n"
 },
 "128": {
  "name": "EXIT",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/strategy/__init__.py",
  "lineno": "1263",
  "column": "4",
  "context": "ter's exit. \"\"\"\n    REDO = 1\n    CONTINUE = 2\n    EXIT = 3\n\n    def __init__(self, result=EXIT):\n        self",
  "context_lines": "class NextAction(object):\n    \"\"\" The next action after an interpreter's exit. \"\"\"\n    REDO = 1\n    CONTINUE = 2\n    EXIT = 3\n\n    def __init__(self, result=EXIT):\n        self.result = result\n\n\nclass Debugger(cmd.Cmd):\n",
  "slicing": [
   "    EXIT = 3\n",
   "    def __init__(self, result=EXIT):\n"
  ]
 },
 "129": {
  "name": "prompt_continuous",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/strategy/__init__.py",
  "lineno": "1270",
  "column": "4",
  "context": "lf.result = result\n\n\nclass Debugger(cmd.Cmd):\n    prompt_continuous = '> '  # multiple lines\n\n    def __init__(self, task, host, task_vars, pla",
  "context_lines": "    EXIT = 3\n\n    def __init__(self, result=EXIT):\n        self.result = result\n\n\nclass Debugger(cmd.Cmd):\n    prompt_continuous = '> '  # multiple lines\n\n    def __init__(self, task, host, task_vars, play_context, result, next_action):\n        # cmd.Cmd is old-style class\n        cmd.Cmd.__init__(self)\n\n",
  "slicing": "    prompt_continuous = '> '  # multiple lines\n"
 },
 "130": {
  "name": "do_h",
  "type": "function",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/strategy/__init__.py",
  "lineno": "1292",
  "column": "4",
  "context": "  except KeyboardInterrupt:\n            pass\n\n    do_h = cmd.Cmd.do_help\n\n    def do_EOF(self, args):\n        \"\"\"Quit\"\"\"\n  ",
  "context_lines": "        try:\n            cmd.Cmd.cmdloop(self)\n        except KeyboardInterrupt:\n            pass\n\n    do_h = cmd.Cmd.do_help\n\n    def do_EOF(self, args):\n        \"\"\"Quit\"\"\"\n        return self.do_quit(args)\n\n",
  "slicing": [
   "display = Display()\n",
   "ALWAYS_DELEGATE_FACT_PREFIXES = frozenset((\n",
   "_sentinel = StrategySentinel()\n",
   "def post_process_whens(result, task, templar):\n",
   "    cond = None\n",
   "        cond = Conditional(loader=templar._loader)\n",
   "        cond.when = task.changed_when\n",
   "        result['changed'] = cond.evaluate_conditional(templar, templar.available_variables)\n",
   "        if cond is None:\n",
   "            cond = Conditional(loader=templar._loader)\n",
   "        cond.when = task.failed_when\n",
   "        failed_when_result = cond.evaluate_conditional(templar, templar.available_variables)\n",
   "        result['failed_when_result'] = result['failed'] = failed_when_result\n",
   "            result = strategy._final_q.get()\n",
   "            if isinstance(result, StrategySentinel):\n",
   "                if 'listen' in result._task_fields:\n",
   "                    strategy._handler_results.append(result)\n",
   "                    strategy._results.append(result)\n",
   "        status_to_stats_map = (\n",
   "        prev_host_states = iterator._host_states.copy()\n",
   "        results = func(self, iterator, one_pass=one_pass, max_passes=max_passes, do_handlers=do_handlers)\n",
   "        _processed_results = []\n",
   "        for result in results:\n",
   "            task = result._task\n",
   "            host = result._host\n",
   "            _queued_task_args = self._queued_task_cache.pop((host.name, task._uuid), None)\n",
   "            task_vars = _queued_task_args['task_vars']\n",
   "            play_context = _queued_task_args['play_context']\n",
   "                prev_host_state = prev_host_states[host.name]\n",
   "                prev_host_state = iterator.get_host_state(host)\n",
   "            while result.needs_debugger(globally_enabled=self.debugger_active):\n",
   "                next_action = NextAction()\n",
   "                dbg = Debugger(task, host, task_vars, play_context, result, next_action)\n",
   "                dbg.cmdloop()\n",
   "                if next_action.result == NextAction.REDO:\n",
   "                    iterator._host_states[host.name] = prev_host_state\n",
   "                    for method, what in status_to_stats_map:\n",
   "                        if getattr(result, method)():\n",
   "                            self._tqm._stats.decrement(what, host.name)\n",
   "                    self._tqm._stats.decrement('ok', host.name)\n",
   "                    self._queue_task(host, task, task_vars, play_context)\n",
   "                    _processed_results.extend(debug_closure(func)(self, iterator, one_pass))\n",
   "                elif next_action.result == NextAction.CONTINUE:\n",
   "                    _processed_results.append(result)\n",
   "                elif next_action.result == NextAction.EXIT:\n",
   "                _processed_results.append(result)\n",
   "        return _processed_results\n",
   "        self._display = display\n",
   "            _pattern = 'all'\n",
   "            _pattern = play.hosts or 'all'\n",
   "        self._hosts_cache_all = [h.name for h in self._inventory.get_hosts(pattern=_pattern, ignore_restrictions=True)]\n",
   "        self._hosts_cache = [h.name for h in self._inventory.get_hosts(play.hosts, order=play.order)]\n",
   "        for sock in itervalues(self._active_connections):\n",
   "                conn = Connection(sock)\n",
   "                conn.reset()\n",
   "                display.debug(\"got an error while closing persistent connection: %s\" % e)\n",
   "        self._final_q.put(_sentinel)\n",
   "        for host in self._hosts_cache:\n",
   "            if host not in self._tqm._unreachable_hosts:\n",
   "                    iterator.get_next_task_for_host(self._inventory.hosts[host])\n",
   "                    iterator.get_next_task_for_host(self._inventory.get_host(host))\n",
   "        failed_hosts = iterator.get_failed_hosts()\n",
   "        unreachable_hosts = self._tqm._unreachable_hosts.keys()\n",
   "        display.debug(\"running handlers\")\n",
   "        handler_result = self.run_handlers(iterator, play_context)\n",
   "        if isinstance(handler_result, bool) and not handler_result:\n",
   "            result |= self._tqm.RUN_ERROR\n",
   "        elif not handler_result:\n",
   "            result |= handler_result\n",
   "        failed_hosts = set(failed_hosts).union(iterator.get_failed_hosts())\n",
   "        unreachable_hosts = set(unreachable_hosts).union(self._tqm._unreachable_hosts.keys())\n",
   "        if not isinstance(result, bool) and result != self._tqm.RUN_OK:\n",
   "            return result\n",
   "        elif len(unreachable_hosts) > 0:\n",
   "        elif len(failed_hosts) > 0:\n",
   "        ignore = set(self._tqm._failed_hosts).union(self._tqm._unreachable_hosts)\n",
   "        return [host for host in self._hosts_cache if host not in ignore]\n",
   "        return [host for host in self._hosts_cache if host in self._tqm._failed_hosts]\n",
   "        display.debug(\"entering _queue_task() for %s/%s\" % (host.name, task.action))\n",
   "        if task.action not in action_write_locks.action_write_locks:\n",
   "            display.debug('Creating lock for %s' % task.action)\n",
   "            action_write_locks.action_write_locks[task.action] = Lock()\n",
   "        templar = Templar(loader=self._loader, variables=task_vars)\n",
   "            throttle = int(templar.template(task.throttle))\n",
   "            raise AnsibleError(\"Failed to convert the throttle value to an integer.\", obj=task._ds, orig_exc=e)\n",
   "            rewind_point = len(self._workers)\n",
   "            if throttle > 0 and self.ALLOW_BASE_THROTTLING:\n",
   "                if task.run_once:\n",
   "                    display.debug(\"Ignoring 'throttle' as 'run_once' is also set for '%s'\" % task.get_name())\n",
   "                    if throttle <= rewind_point:\n",
   "                        display.debug(\"task: %s, throttle: %d\" % (task.get_name(), throttle))\n",
   "                        rewind_point = throttle\n",
   "            queued = False\n",
   "            starting_worker = self._cur_worker\n",
   "                if self._cur_worker >= rewind_point:\n",
   "                worker_prc = self._workers[self._cur_worker]\n",
   "                if worker_prc is None or not worker_prc.is_alive():\n",
   "                    self._queued_task_cache[(host.name, task._uuid)] = {\n",
   "                        'host': host,\n",
   "                        'task': task,\n",
   "                        'task_vars': task_vars,\n",
   "                        'play_context': play_context\n",
   "                    worker_prc = WorkerProcess(self._final_q, task_vars, host, task, play_context, self._loader, self._variable_manager, plugin_loader)\n",
   "                    self._workers[self._cur_worker] = worker_prc\n",
   "                    self._tqm.send_callback('v2_runner_on_start', host, task)\n",
   "                    worker_prc.start()\n",
   "                    display.debug(\"worker is %d (out of %d available)\" % (self._cur_worker + 1, len(self._workers)))\n",
   "                    queued = True\n",
   "                if self._cur_worker >= rewind_point:\n",
   "                if queued:\n",
   "                elif self._cur_worker == starting_worker:\n",
   "            if isinstance(task, Handler):\n",
   "            display.debug(\"got an error while queuing: %s\" % e)\n",
   "        display.debug(\"exiting _queue_task() for %s/%s\" % (host.name, task.action))\n",
   "        if task.run_once:\n",
   "            host_list = [host for host in self._hosts_cache if host not in self._tqm._unreachable_hosts]\n",
   "            host_list = [task_host.name]\n",
   "        return host_list\n",
   "        host_name = result.get('_ansible_delegated_vars', {}).get('ansible_delegated_host', None)\n",
   "        return [host_name or task.delegate_to]\n",
   "        if task.delegate_to is None:\n",
   "        facts = result['ansible_facts']\n",
   "        always_keys = set()\n",
   "        _add = always_keys.add\n",
   "        for fact_key in facts:\n",
   "            for always_key in ALWAYS_DELEGATE_FACT_PREFIXES:\n",
   "                if fact_key.startswith(always_key):\n",
   "                    _add(fact_key)\n",
   "        if always_keys:\n",
   "            _pop = facts.pop\n",
   "            always_facts = {\n",
   "                'ansible_facts': dict((k, _pop(k)) for k in list(facts) if k in always_keys)\n",
   "            host_list = self.get_delegated_hosts(result, task)\n",
   "            _set_host_facts = self._variable_manager.set_host_facts\n",
   "            for target_host in host_list:\n",
   "                _set_host_facts(target_host, always_facts)\n",
   "        ret_results = []\n",
   "        handler_templar = Templar(self._loader)\n",
   "        def get_original_host(host_name):\n",
   "            host_name = to_text(host_name)\n",
   "            if host_name in self._inventory.hosts:\n",
   "                return self._inventory.hosts[host_name]\n",
   "                return self._inventory.get_host(host_name)\n",
   "        def search_handler_blocks_by_name(handler_name, handler_blocks):\n",
   "            for handler_block in reversed(handler_blocks):\n",
   "                for handler_task in handler_block.block:\n",
   "                    if handler_task.name:\n",
   "                        if not handler_task.cached_name:\n",
   "                            if handler_templar.is_template(handler_task.name):\n",
   "                                handler_templar.available_variables = self._variable_manager.get_vars(play=iterator._play,\n",
   "                                                                                                      task=handler_task,\n",
   "                                handler_task.name = handler_templar.template(handler_task.name)\n",
   "                            handler_task.cached_name = True\n",
   "                            candidates = (\n",
   "                                handler_task.name,\n",
   "                                handler_task.get_name(include_role_fqcn=False),\n",
   "                                handler_task.get_name(include_role_fqcn=True),\n",
   "                            if handler_name in candidates:\n",
   "                                return handler_task\n",
   "        cur_pass = 0\n",
   "                    task_result = self._handler_results.popleft()\n",
   "                    task_result = self._results.popleft()\n",
   "            original_host = get_original_host(task_result._host)\n",
   "            queue_cache_entry = (original_host.name, task_result._task)\n",
   "            found_task = self._queued_task_cache.get(queue_cache_entry)['task']\n",
   "            original_task = found_task.copy(exclude_parent=True, exclude_tasks=True)\n",
   "            original_task._parent = found_task._parent\n",
   "            original_task.from_attrs(task_result._task_fields)\n",
   "            task_result._host = original_host\n",
   "            task_result._task = original_task\n",
   "            if '_ansible_retry' in task_result._result:\n",
   "                self._tqm.send_callback('v2_runner_retry', task_result)\n",
   "            elif '_ansible_item_result' in task_result._result:\n",
   "                if task_result.is_failed() or task_result.is_unreachable():\n",
   "                    self._tqm.send_callback('v2_runner_item_on_failed', task_result)\n",
   "                elif task_result.is_skipped():\n",
   "                    self._tqm.send_callback('v2_runner_item_on_skipped', task_result)\n",
   "                    if 'diff' in task_result._result:\n",
   "                        if self._diff or getattr(original_task, 'diff', False):\n",
   "                            self._tqm.send_callback('v2_on_file_diff', task_result)\n",
   "                    self._tqm.send_callback('v2_runner_item_on_ok', task_result)\n",
   "            role_ran = False\n",
   "            if task_result.is_failed():\n",
   "                role_ran = True\n",
   "                ignore_errors = original_task.ignore_errors\n",
   "                if not ignore_errors:\n",
   "                    display.debug(\"marking %s as failed\" % original_host.name)\n",
   "                    if original_task.run_once:\n",
   "                        for h in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                            if h.name not in self._tqm._unreachable_hosts:\n",
   "                                state, _ = iterator.get_next_task_for_host(h, peek=True)\n",
   "                                iterator.mark_host_failed(h)\n",
   "                                state, new_task = iterator.get_next_task_for_host(h, peek=True)\n",
   "                        iterator.mark_host_failed(original_host)\n",
   "                    state, _ = iterator.get_next_task_for_host(original_host, peek=True)\n",
   "                    if iterator.is_failed(original_host) and state and state.run_state == iterator.ITERATING_COMPLETE:\n",
   "                        self._tqm._failed_hosts[original_host.name] = True\n",
   "                    if state and iterator.get_active_state(state).run_state == iterator.ITERATING_RESCUE:\n",
   "                        self._tqm._stats.increment('rescued', original_host.name)\n",
   "                            original_host.name,\n",
   "                                ansible_failed_task=original_task.serialize(),\n",
   "                                ansible_failed_result=task_result._result,\n",
   "                        self._tqm._stats.increment('failures', original_host.name)\n",
   "                    self._tqm._stats.increment('ok', original_host.name)\n",
   "                    self._tqm._stats.increment('ignored', original_host.name)\n",
   "                    if 'changed' in task_result._result and task_result._result['changed']:\n",
   "                        self._tqm._stats.increment('changed', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_failed', task_result, ignore_errors=ignore_errors)\n",
   "            elif task_result.is_unreachable():\n",
   "                ignore_unreachable = original_task.ignore_unreachable\n",
   "                if not ignore_unreachable:\n",
   "                    self._tqm._unreachable_hosts[original_host.name] = True\n",
   "                    iterator._play._removed_hosts.append(original_host.name)\n",
   "                    self._tqm._stats.increment('skipped', original_host.name)\n",
   "                    task_result._result['skip_reason'] = 'Host %s is unreachable' % original_host.name\n",
   "                self._tqm._stats.increment('dark', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_unreachable', task_result)\n",
   "            elif task_result.is_skipped():\n",
   "                self._tqm._stats.increment('skipped', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_skipped', task_result)\n",
   "                role_ran = True\n",
   "                if original_task.loop:\n",
   "                    result_items = task_result._result.get('results', [])\n",
   "                    result_items = [task_result._result]\n",
   "                for result_item in result_items:\n",
   "                    if '_ansible_notify' in result_item:\n",
   "                        if task_result.is_changed():\n",
   "                            for handler_name in result_item['_ansible_notify']:\n",
   "                                found = False\n",
   "                                target_handler = search_handler_blocks_by_name(handler_name, iterator._play.handlers)\n",
   "                                if target_handler is not None:\n",
   "                                    found = True\n",
   "                                    if target_handler.notify_host(original_host):\n",
   "                                        self._tqm.send_callback('v2_playbook_on_notify', target_handler, original_host)\n",
   "                                for listening_handler_block in iterator._play.handlers:\n",
   "                                    for listening_handler in listening_handler_block.block:\n",
   "                                        listeners = getattr(listening_handler, 'listen', []) or []\n",
   "                                        if not listeners:\n",
   "                                        listeners = listening_handler.get_validated_value(\n",
   "                                            'listen', listening_handler._valid_attrs['listen'], listeners, handler_templar\n",
   "                                        if handler_name not in listeners:\n",
   "                                            found = True\n",
   "                                        if listening_handler.notify_host(original_host):\n",
   "                                            self._tqm.send_callback('v2_playbook_on_notify', listening_handler, original_host)\n",
   "                                if not found:\n",
   "                                    msg = (\"The requested handler '%s' was not found in either the main handlers list nor in the listening \"\n",
   "                                           \"handlers list\" % handler_name)\n",
   "                                        raise AnsibleError(msg)\n",
   "                                        display.warning(msg)\n",
   "                    if 'add_host' in result_item:\n",
   "                        new_host_info = result_item.get('add_host', dict())\n",
   "                        self._add_host(new_host_info, result_item)\n",
   "                        post_process_whens(result_item, original_task, handler_templar)\n",
   "                    elif 'add_group' in result_item:\n",
   "                        self._add_group(original_host, result_item)\n",
   "                        post_process_whens(result_item, original_task, handler_templar)\n",
   "                    if 'ansible_facts' in result_item:\n",
   "                        if original_task.delegate_to is not None and original_task.delegate_facts:\n",
   "                            host_list = self.get_delegated_hosts(result_item, original_task)\n",
   "                            self._set_always_delegated_facts(result_item, original_task)\n",
   "                            host_list = self.get_task_hosts(iterator, original_host, original_task)\n",
   "                        if original_task.action == 'include_vars':\n",
   "                            for (var_name, var_value) in iteritems(result_item['ansible_facts']):\n",
   "                                for target_host in host_list:\n",
   "                                    self._variable_manager.set_host_variable(target_host, var_name, var_value)\n",
   "                            cacheable = result_item.pop('_ansible_facts_cacheable', False)\n",
   "                            for target_host in host_list:\n",
   "                                if original_task.action != 'set_fact' or cacheable:\n",
   "                                    self._variable_manager.set_host_facts(target_host, result_item['ansible_facts'].copy())\n",
   "                                if original_task.action == 'set_fact':\n",
   "                                    self._variable_manager.set_nonpersistent_facts(target_host, result_item['ansible_facts'].copy())\n",
   "                    if 'ansible_stats' in result_item and 'data' in result_item['ansible_stats'] and result_item['ansible_stats']['data']:\n",
   "                        if 'per_host' not in result_item['ansible_stats'] or result_item['ansible_stats']['per_host']:\n",
   "                            host_list = self.get_task_hosts(iterator, original_host, original_task)\n",
   "                            host_list = [None]\n",
   "                        data = result_item['ansible_stats']['data']\n",
   "                        aggregate = 'aggregate' in result_item['ansible_stats'] and result_item['ansible_stats']['aggregate']\n",
   "                        for myhost in host_list:\n",
   "                            for k in data.keys():\n",
   "                                if aggregate:\n",
   "                                    self._tqm._stats.update_custom_stats(k, data[k], myhost)\n",
   "                                    self._tqm._stats.set_custom_stats(k, data[k], myhost)\n",
   "                if 'diff' in task_result._result:\n",
   "                    if self._diff or getattr(original_task, 'diff', False):\n",
   "                        self._tqm.send_callback('v2_on_file_diff', task_result)\n",
   "                if not isinstance(original_task, TaskInclude):\n",
   "                    self._tqm._stats.increment('ok', original_host.name)\n",
   "                    if 'changed' in task_result._result and task_result._result['changed']:\n",
   "                        self._tqm._stats.increment('changed', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_ok', task_result)\n",
   "            if original_task.register:\n",
   "                host_list = self.get_task_hosts(iterator, original_host, original_task)\n",
   "                clean_copy = strip_internal_keys(module_response_deepcopy(task_result._result))\n",
   "                if 'invocation' in clean_copy:\n",
   "                    del clean_copy['invocation']\n",
   "                for target_host in host_list:\n",
   "                    self._variable_manager.set_nonpersistent_facts(target_host, {original_task.register: clean_copy})\n",
   "            if original_host.name in self._blocked_hosts:\n",
   "                del self._blocked_hosts[original_host.name]\n",
   "            if original_task._role is not None and role_ran:  # TODO:  and original_task.action != 'include_role':?\n",
   "                for (entry, role_obj) in iteritems(iterator._play.ROLE_CACHE[original_task._role.get_name()]):\n",
   "                    if role_obj._uuid == original_task._role._uuid:\n",
   "                        role_obj._had_task_run[original_host.name] = True\n",
   "            ret_results.append(task_result)\n",
   "            if one_pass or max_passes is not None and (cur_pass + 1) >= max_passes:\n",
   "            cur_pass += 1\n",
   "        return ret_results\n",
   "        ret_results = []\n",
   "        handler_results = 0\n",
   "        display.debug(\"waiting for handler results...\")\n",
   "               handler_results < len(notified_hosts) and\n",
   "            results = self._process_pending_results(iterator, do_handlers=True)\n",
   "            ret_results.extend(results)\n",
   "            handler_results += len([\n",
   "                r._host for r in results if r._host in notified_hosts and\n",
   "                r.task_name == handler.name])\n",
   "        display.debug(\"no more pending handlers, returning what we have\")\n",
   "        return ret_results\n",
   "        ret_results = []\n",
   "        display.debug(\"waiting for pending results...\")\n",
   "            results = self._process_pending_results(iterator)\n",
   "            ret_results.extend(results)\n",
   "        display.debug(\"no more pending results, returning what we have\")\n",
   "        return ret_results\n",
   "        changed = False\n",
   "            host_name = host_info.get('host_name')\n",
   "            if host_name not in self._inventory.hosts:\n",
   "                self._inventory.add_host(host_name, 'all')\n",
   "                self._hosts_cache_all.append(host_name)\n",
   "                changed = True\n",
   "            new_host = self._inventory.hosts.get(host_name)\n",
   "            new_host_vars = new_host.get_vars()\n",
   "            new_host_combined_vars = combine_vars(new_host_vars, host_info.get('host_vars', dict()))\n",
   "            if new_host_vars != new_host_combined_vars:\n",
   "                new_host.vars = new_host_combined_vars\n",
   "                changed = True\n",
   "            new_groups = host_info.get('groups', [])\n",
   "            for group_name in new_groups:\n",
   "                if group_name not in self._inventory.groups:\n",
   "                    group_name = self._inventory.add_group(group_name)\n",
   "                    changed = True\n",
   "                new_group = self._inventory.groups[group_name]\n",
   "                if new_group.add_host(self._inventory.hosts[host_name]):\n",
   "                    changed = True\n",
   "            if changed:\n",
   "            result_item['changed'] = changed\n",
   "        changed = False\n",
   "        real_host = self._inventory.hosts.get(host.name)\n",
   "        if real_host is None:\n",
   "            if host.name == self._inventory.localhost.name:\n",
   "                real_host = self._inventory.localhost\n",
   "                raise AnsibleError('%s cannot be matched in inventory' % host.name)\n",
   "        group_name = result_item.get('add_group')\n",
   "        parent_group_names = result_item.get('parent_groups', [])\n",
   "        if group_name not in self._inventory.groups:\n",
   "            group_name = self._inventory.add_group(group_name)\n",
   "        for name in parent_group_names:\n",
   "            if name not in self._inventory.groups:\n",
   "                self._inventory.add_group(name)\n",
   "                changed = True\n",
   "        group = self._inventory.groups[group_name]\n",
   "        for parent_group_name in parent_group_names:\n",
   "            parent_group = self._inventory.groups[parent_group_name]\n",
   "            new = parent_group.add_child_group(group)\n",
   "            if new and not changed:\n",
   "                changed = True\n",
   "        if real_host not in group.get_hosts():\n",
   "            changed = group.add_host(real_host)\n",
   "        if group not in real_host.get_groups():\n",
   "            changed = real_host.add_group(group)\n",
   "        if changed:\n",
   "        result_item['changed'] = changed\n",
   "        ti_copy = included_file._task.copy(exclude_parent=True)\n",
   "        ti_copy._parent = included_file._task._parent\n",
   "        temp_vars = ti_copy.vars.copy()\n",
   "        temp_vars.update(included_file._vars)\n",
   "        ti_copy.vars = temp_vars\n",
   "        return ti_copy\n",
   "        display.debug(\"loading included file: %s\" % included_file._filename)\n",
   "            data = self._loader.load_from_file(included_file._filename)\n",
   "            if data is None:\n",
   "            elif not isinstance(data, list):\n",
   "            ti_copy = self._copy_included_file(included_file)\n",
   "            tags = included_file._task.vars.pop('tags', [])\n",
   "            if isinstance(tags, string_types):\n",
   "                tags = tags.split(',')\n",
   "            if len(tags) > 0:\n",
   "                display.deprecated(\"You should not specify tags in the include parameters. All tags should be specified using the task-level option\",\n",
   "                included_file._task.tags = tags\n",
   "            block_list = load_list_of_blocks(\n",
   "                data,\n",
   "                parent_block=ti_copy.build_parent_block(),\n",
   "            for host in included_file._hosts:\n",
   "                self._tqm._stats.increment('ok', host.name)\n",
   "                reason = \"Could not find or access '%s' on the Ansible Controller.\" % to_text(e.file_name)\n",
   "                reason = to_text(e)\n",
   "            for host in included_file._hosts:\n",
   "                tr = TaskResult(host=host, task=included_file._task, return_data=dict(failed=True, reason=reason))\n",
   "                iterator.mark_host_failed(host)\n",
   "                self._tqm._failed_hosts[host.name] = True\n",
   "                self._tqm._stats.increment('failures', host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_failed', tr)\n",
   "        display.debug(\"done processing included file\")\n",
   "        return block_list\n",
   "        result = self._tqm.RUN_OK\n",
   "        for handler_block in iterator._play.handlers:\n",
   "            for handler in handler_block.block:\n",
   "                if handler.notified_hosts:\n",
   "                    result = self._do_handler_run(handler, handler.get_name(), iterator=iterator, play_context=play_context)\n",
   "                    if not result:\n",
   "        return result\n",
   "            notified_hosts = handler.notified_hosts[:]\n",
   "        failed_hosts = self._filter_notified_failed_hosts(iterator, notified_hosts)\n",
   "        notified_hosts = self._filter_notified_hosts(notified_hosts)\n",
   "        notified_hosts += failed_hosts\n",
   "        if len(notified_hosts) > 0:\n",
   "            saved_name = handler.name\n",
   "            handler.name = handler_name\n",
   "            self._tqm.send_callback('v2_playbook_on_handler_task_start', handler)\n",
   "            handler.name = saved_name\n",
   "        bypass_host_loop = False\n",
   "            action = plugin_loader.action_loader.get(handler.action, class_only=True)\n",
   "            if getattr(action, 'BYPASS_HOST_LOOP', False):\n",
   "                bypass_host_loop = True\n",
   "        host_results = []\n",
   "        for host in notified_hosts:\n",
   "            if not iterator.is_failed(host) or iterator._play.force_handlers:\n",
   "                task_vars = self._variable_manager.get_vars(play=iterator._play, host=host, task=handler,\n",
   "                self.add_tqm_variables(task_vars, play=iterator._play)\n",
   "                templar = Templar(loader=self._loader, variables=task_vars)\n",
   "                if not handler.cached_name:\n",
   "                    handler.name = templar.template(handler.name)\n",
   "                    handler.cached_name = True\n",
   "                self._queue_task(host, handler, task_vars, play_context)\n",
   "                if templar.template(handler.run_once) or bypass_host_loop:\n",
   "        host_results = self._wait_on_handler_results(iterator, handler, notified_hosts)\n",
   "        included_files = IncludedFile.process_include_results(\n",
   "            host_results,\n",
   "        result = True\n",
   "        if len(included_files) > 0:\n",
   "            for included_file in included_files:\n",
   "                    new_blocks = self._load_included_file(included_file, iterator=iterator, is_handler=True)\n",
   "                    for block in new_blocks:\n",
   "                        iterator._play.handlers.append(block)\n",
   "                        for task in block.block:\n",
   "                            task_name = task.get_name()\n",
   "                            display.debug(\"adding task '%s' included in handler '%s'\" % (task_name, handler_name))\n",
   "                            task.notified_hosts = included_file._hosts[:]\n",
   "                            result = self._do_handler_run(\n",
   "                                handler=task,\n",
   "                                handler_name=task_name,\n",
   "                                play_context=play_context,\n",
   "                                notified_hosts=included_file._hosts[:],\n",
   "                            if not result:\n",
   "                    for host in included_file._hosts:\n",
   "                        iterator.mark_host_failed(host)\n",
   "                        self._tqm._failed_hosts[host.name] = True\n",
   "                    display.warning(to_text(e))\n",
   "        handler.notified_hosts = [\n",
   "            h for h in handler.notified_hosts\n",
   "            if h not in notified_hosts]\n",
   "        display.debug(\"done running handlers, result is: %s\" % result)\n",
   "        return result\n",
   "        return notified_hosts[:]\n",
   "        ret = False\n",
   "        msg = u'Perform task: %s ' % task\n",
   "        if host:\n",
   "            msg += u'on %s ' % host\n",
   "        msg += u'(N)o/(y)es/(c)ontinue: '\n",
   "        resp = display.prompt(msg)\n",
   "        if resp.lower() in ['y', 'yes']:\n",
   "            display.debug(\"User ran task\")\n",
   "            ret = True\n",
   "        elif resp.lower() in ['c', 'continue']:\n",
   "            display.debug(\"User ran task and canceled step mode\")\n",
   "            ret = True\n",
   "            display.debug(\"User skipped task\")\n",
   "        display.banner(msg)\n",
   "        return ret\n",
   "        display.warning(\"%s task does not support when conditional\" % task_name)\n",
   "        meta_action = task.args.get('_raw_params')\n",
   "        def _evaluate_conditional(h):\n",
   "            all_vars = self._variable_manager.get_vars(play=iterator._play, host=h, task=task,\n",
   "            templar = Templar(loader=self._loader, variables=all_vars)\n",
   "            return task.evaluate_conditional(templar, all_vars)\n",
   "        skipped = False\n",
   "        msg = ''\n",
   "        if meta_action == 'noop':\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            msg = \"noop\"\n",
   "        elif meta_action == 'flush_handlers':\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            self._flushed_hosts[target_host] = True\n",
   "            self.run_handlers(iterator, play_context)\n",
   "            self._flushed_hosts[target_host] = False\n",
   "            msg = \"ran handlers\"\n",
   "        elif meta_action == 'refresh_inventory' or self.flush_cache:\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            msg = \"inventory successfully refreshed\"\n",
   "        elif meta_action == 'clear_facts':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                for host in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                    hostname = host.get_name()\n",
   "                    self._variable_manager.clear_facts(hostname)\n",
   "                msg = \"facts cleared\"\n",
   "                skipped = True\n",
   "        elif meta_action == 'clear_host_errors':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                for host in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                    self._tqm._failed_hosts.pop(host.name, False)\n",
   "                    self._tqm._unreachable_hosts.pop(host.name, False)\n",
   "                    iterator._host_states[host.name].fail_state = iterator.FAILED_NONE\n",
   "                msg = \"cleared host errors\"\n",
   "                skipped = True\n",
   "        elif meta_action == 'end_play':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                for host in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                    if host.name not in self._tqm._unreachable_hosts:\n",
   "                        iterator._host_states[host.name].run_state = iterator.ITERATING_COMPLETE\n",
   "                msg = \"ending play\"\n",
   "        elif meta_action == 'end_host':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                iterator._host_states[target_host.name].run_state = iterator.ITERATING_COMPLETE\n",
   "                iterator._play._removed_hosts.append(target_host.name)\n",
   "                msg = \"ending play for %s\" % target_host.name\n",
   "                skipped = True\n",
   "                msg = \"end_host conditional evaluated to false, continuing execution for %s\" % target_host.name\n",
   "        elif meta_action == 'reset_connection':\n",
   "            all_vars = self._variable_manager.get_vars(play=iterator._play, host=target_host, task=task,\n",
   "            templar = Templar(loader=self._loader, variables=all_vars)\n",
   "            play_context = play_context.set_task_and_variable_override(task=task, variables=all_vars, templar=templar)\n",
   "            play_context.post_validate(templar=templar)\n",
   "            if not play_context.remote_addr:\n",
   "                play_context.remote_addr = target_host.address\n",
   "            play_context.update_vars(all_vars)\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            if target_host in self._active_connections:\n",
   "                connection = Connection(self._active_connections[target_host])\n",
   "                del self._active_connections[target_host]\n",
   "                connection = plugin_loader.connection_loader.get(play_context.connection, play_context, os.devnull)\n",
   "                play_context.set_attributes_from_plugin(connection)\n",
   "            if connection:\n",
   "                    connection.reset()\n",
   "                    msg = 'reset connection'\n",
   "                    display.debug(\"got an error while closing persistent connection: %s\" % e)\n",
   "                msg = 'no connection, nothing to reset'\n",
   "            raise AnsibleError(\"invalid meta action requested: %s\" % meta_action, obj=task._ds)\n",
   "        result = {'msg': msg}\n",
   "        if skipped:\n",
   "            result['skipped'] = True\n",
   "            result['changed'] = False\n",
   "        display.vv(\"META: %s\" % msg)\n",
   "        return [TaskResult(target_host, task, result)]\n",
   "        hosts_left = []\n",
   "        for host in self._hosts_cache:\n",
   "            if host not in self._tqm._unreachable_hosts:\n",
   "                    hosts_left.append(self._inventory.hosts[host])\n",
   "                    hosts_left.append(self._inventory.get_host(host))\n",
   "        return hosts_left\n",
   "        for r in results:\n",
   "            if 'args' in r._task_fields:\n",
   "                socket_path = r._task_fields['args'].get('_ansible_socket')\n",
   "                if socket_path:\n",
   "                    if r._host not in self._active_connections:\n",
   "                        self._active_connections[r._host] = socket_path\n",
   "    EXIT = 3\n",
   "    def __init__(self, result=EXIT):\n",
   "        self.result = result\n",
   "        self.prompt = '[%s] %s (debug)> ' % (host, task)\n",
   "        self.scope['task'] = task\n",
   "        self.scope['task_vars'] = task_vars\n",
   "        self.scope['host'] = host\n",
   "        self.scope['play_context'] = play_context\n",
   "        self.scope['result'] = result\n",
   "        self.next_action = next_action\n",
   "    do_h = cmd.Cmd.do_help\n",
   "        display.display('User interrupted execution')\n",
   "        templar = Templar(None, shared_loader_obj=None, variables=self.scope['task_vars'])\n",
   "        task = self.scope['task']\n",
   "        task = task.load_data(task._ds)\n",
   "        task.post_validate(templar)\n",
   "        self.scope['task'] = task\n",
   "            display.display('***%s:%s' % (exc_type_name, repr(v)))\n",
   "            display.display(pprint.pformat(result))\n",
   "            display.display('***%s:%s' % (exc_type_name, repr(v)))\n"
  ]
 },
 "131": {
  "name": "do_q",
  "type": "function",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/plugins/strategy/__init__.py",
  "lineno": "1304",
  "column": "4",
  "context": "result = NextAction.EXIT\n        return True\n\n    do_q = do_quit\n\n    def do_continue(self, args):\n        \"\"\"Conti",
  "context_lines": "        \"\"\"Quit\"\"\"\n        display.display('User interrupted execution')\n        self.next_action.result = NextAction.EXIT\n        return True\n\n    do_q = do_quit\n\n    def do_continue(self, args):\n        \"\"\"Continue to next result\"\"\"\n        self.next_action.result = NextAction.CONTINUE\n",
  "slicing": [
   "display = Display()\n",
   "ALWAYS_DELEGATE_FACT_PREFIXES = frozenset((\n",
   "_sentinel = StrategySentinel()\n",
   "def post_process_whens(result, task, templar):\n",
   "    cond = None\n",
   "        cond = Conditional(loader=templar._loader)\n",
   "        cond.when = task.changed_when\n",
   "        result['changed'] = cond.evaluate_conditional(templar, templar.available_variables)\n",
   "        if cond is None:\n",
   "            cond = Conditional(loader=templar._loader)\n",
   "        cond.when = task.failed_when\n",
   "        failed_when_result = cond.evaluate_conditional(templar, templar.available_variables)\n",
   "        result['failed_when_result'] = result['failed'] = failed_when_result\n",
   "            result = strategy._final_q.get()\n",
   "            if isinstance(result, StrategySentinel):\n",
   "                if 'listen' in result._task_fields:\n",
   "                    strategy._handler_results.append(result)\n",
   "                    strategy._results.append(result)\n",
   "        status_to_stats_map = (\n",
   "        prev_host_states = iterator._host_states.copy()\n",
   "        results = func(self, iterator, one_pass=one_pass, max_passes=max_passes, do_handlers=do_handlers)\n",
   "        _processed_results = []\n",
   "        for result in results:\n",
   "            task = result._task\n",
   "            host = result._host\n",
   "            _queued_task_args = self._queued_task_cache.pop((host.name, task._uuid), None)\n",
   "            task_vars = _queued_task_args['task_vars']\n",
   "            play_context = _queued_task_args['play_context']\n",
   "                prev_host_state = prev_host_states[host.name]\n",
   "                prev_host_state = iterator.get_host_state(host)\n",
   "            while result.needs_debugger(globally_enabled=self.debugger_active):\n",
   "                next_action = NextAction()\n",
   "                dbg = Debugger(task, host, task_vars, play_context, result, next_action)\n",
   "                dbg.cmdloop()\n",
   "                if next_action.result == NextAction.REDO:\n",
   "                    iterator._host_states[host.name] = prev_host_state\n",
   "                    for method, what in status_to_stats_map:\n",
   "                        if getattr(result, method)():\n",
   "                            self._tqm._stats.decrement(what, host.name)\n",
   "                    self._tqm._stats.decrement('ok', host.name)\n",
   "                    self._queue_task(host, task, task_vars, play_context)\n",
   "                    _processed_results.extend(debug_closure(func)(self, iterator, one_pass))\n",
   "                elif next_action.result == NextAction.CONTINUE:\n",
   "                    _processed_results.append(result)\n",
   "                elif next_action.result == NextAction.EXIT:\n",
   "                _processed_results.append(result)\n",
   "        return _processed_results\n",
   "        self._display = display\n",
   "            _pattern = 'all'\n",
   "            _pattern = play.hosts or 'all'\n",
   "        self._hosts_cache_all = [h.name for h in self._inventory.get_hosts(pattern=_pattern, ignore_restrictions=True)]\n",
   "        self._hosts_cache = [h.name for h in self._inventory.get_hosts(play.hosts, order=play.order)]\n",
   "        for sock in itervalues(self._active_connections):\n",
   "                conn = Connection(sock)\n",
   "                conn.reset()\n",
   "                display.debug(\"got an error while closing persistent connection: %s\" % e)\n",
   "        self._final_q.put(_sentinel)\n",
   "        for host in self._hosts_cache:\n",
   "            if host not in self._tqm._unreachable_hosts:\n",
   "                    iterator.get_next_task_for_host(self._inventory.hosts[host])\n",
   "                    iterator.get_next_task_for_host(self._inventory.get_host(host))\n",
   "        failed_hosts = iterator.get_failed_hosts()\n",
   "        unreachable_hosts = self._tqm._unreachable_hosts.keys()\n",
   "        display.debug(\"running handlers\")\n",
   "        handler_result = self.run_handlers(iterator, play_context)\n",
   "        if isinstance(handler_result, bool) and not handler_result:\n",
   "            result |= self._tqm.RUN_ERROR\n",
   "        elif not handler_result:\n",
   "            result |= handler_result\n",
   "        failed_hosts = set(failed_hosts).union(iterator.get_failed_hosts())\n",
   "        unreachable_hosts = set(unreachable_hosts).union(self._tqm._unreachable_hosts.keys())\n",
   "        if not isinstance(result, bool) and result != self._tqm.RUN_OK:\n",
   "            return result\n",
   "        elif len(unreachable_hosts) > 0:\n",
   "        elif len(failed_hosts) > 0:\n",
   "        ignore = set(self._tqm._failed_hosts).union(self._tqm._unreachable_hosts)\n",
   "        return [host for host in self._hosts_cache if host not in ignore]\n",
   "        return [host for host in self._hosts_cache if host in self._tqm._failed_hosts]\n",
   "        display.debug(\"entering _queue_task() for %s/%s\" % (host.name, task.action))\n",
   "        if task.action not in action_write_locks.action_write_locks:\n",
   "            display.debug('Creating lock for %s' % task.action)\n",
   "            action_write_locks.action_write_locks[task.action] = Lock()\n",
   "        templar = Templar(loader=self._loader, variables=task_vars)\n",
   "            throttle = int(templar.template(task.throttle))\n",
   "            raise AnsibleError(\"Failed to convert the throttle value to an integer.\", obj=task._ds, orig_exc=e)\n",
   "            rewind_point = len(self._workers)\n",
   "            if throttle > 0 and self.ALLOW_BASE_THROTTLING:\n",
   "                if task.run_once:\n",
   "                    display.debug(\"Ignoring 'throttle' as 'run_once' is also set for '%s'\" % task.get_name())\n",
   "                    if throttle <= rewind_point:\n",
   "                        display.debug(\"task: %s, throttle: %d\" % (task.get_name(), throttle))\n",
   "                        rewind_point = throttle\n",
   "            queued = False\n",
   "            starting_worker = self._cur_worker\n",
   "                if self._cur_worker >= rewind_point:\n",
   "                worker_prc = self._workers[self._cur_worker]\n",
   "                if worker_prc is None or not worker_prc.is_alive():\n",
   "                    self._queued_task_cache[(host.name, task._uuid)] = {\n",
   "                        'host': host,\n",
   "                        'task': task,\n",
   "                        'task_vars': task_vars,\n",
   "                        'play_context': play_context\n",
   "                    worker_prc = WorkerProcess(self._final_q, task_vars, host, task, play_context, self._loader, self._variable_manager, plugin_loader)\n",
   "                    self._workers[self._cur_worker] = worker_prc\n",
   "                    self._tqm.send_callback('v2_runner_on_start', host, task)\n",
   "                    worker_prc.start()\n",
   "                    display.debug(\"worker is %d (out of %d available)\" % (self._cur_worker + 1, len(self._workers)))\n",
   "                    queued = True\n",
   "                if self._cur_worker >= rewind_point:\n",
   "                if queued:\n",
   "                elif self._cur_worker == starting_worker:\n",
   "            if isinstance(task, Handler):\n",
   "            display.debug(\"got an error while queuing: %s\" % e)\n",
   "        display.debug(\"exiting _queue_task() for %s/%s\" % (host.name, task.action))\n",
   "        if task.run_once:\n",
   "            host_list = [host for host in self._hosts_cache if host not in self._tqm._unreachable_hosts]\n",
   "            host_list = [task_host.name]\n",
   "        return host_list\n",
   "        host_name = result.get('_ansible_delegated_vars', {}).get('ansible_delegated_host', None)\n",
   "        return [host_name or task.delegate_to]\n",
   "        if task.delegate_to is None:\n",
   "        facts = result['ansible_facts']\n",
   "        always_keys = set()\n",
   "        _add = always_keys.add\n",
   "        for fact_key in facts:\n",
   "            for always_key in ALWAYS_DELEGATE_FACT_PREFIXES:\n",
   "                if fact_key.startswith(always_key):\n",
   "                    _add(fact_key)\n",
   "        if always_keys:\n",
   "            _pop = facts.pop\n",
   "            always_facts = {\n",
   "                'ansible_facts': dict((k, _pop(k)) for k in list(facts) if k in always_keys)\n",
   "            host_list = self.get_delegated_hosts(result, task)\n",
   "            _set_host_facts = self._variable_manager.set_host_facts\n",
   "            for target_host in host_list:\n",
   "                _set_host_facts(target_host, always_facts)\n",
   "        ret_results = []\n",
   "        handler_templar = Templar(self._loader)\n",
   "        def get_original_host(host_name):\n",
   "            host_name = to_text(host_name)\n",
   "            if host_name in self._inventory.hosts:\n",
   "                return self._inventory.hosts[host_name]\n",
   "                return self._inventory.get_host(host_name)\n",
   "        def search_handler_blocks_by_name(handler_name, handler_blocks):\n",
   "            for handler_block in reversed(handler_blocks):\n",
   "                for handler_task in handler_block.block:\n",
   "                    if handler_task.name:\n",
   "                        if not handler_task.cached_name:\n",
   "                            if handler_templar.is_template(handler_task.name):\n",
   "                                handler_templar.available_variables = self._variable_manager.get_vars(play=iterator._play,\n",
   "                                                                                                      task=handler_task,\n",
   "                                handler_task.name = handler_templar.template(handler_task.name)\n",
   "                            handler_task.cached_name = True\n",
   "                            candidates = (\n",
   "                                handler_task.name,\n",
   "                                handler_task.get_name(include_role_fqcn=False),\n",
   "                                handler_task.get_name(include_role_fqcn=True),\n",
   "                            if handler_name in candidates:\n",
   "                                return handler_task\n",
   "        cur_pass = 0\n",
   "                    task_result = self._handler_results.popleft()\n",
   "                    task_result = self._results.popleft()\n",
   "            original_host = get_original_host(task_result._host)\n",
   "            queue_cache_entry = (original_host.name, task_result._task)\n",
   "            found_task = self._queued_task_cache.get(queue_cache_entry)['task']\n",
   "            original_task = found_task.copy(exclude_parent=True, exclude_tasks=True)\n",
   "            original_task._parent = found_task._parent\n",
   "            original_task.from_attrs(task_result._task_fields)\n",
   "            task_result._host = original_host\n",
   "            task_result._task = original_task\n",
   "            if '_ansible_retry' in task_result._result:\n",
   "                self._tqm.send_callback('v2_runner_retry', task_result)\n",
   "            elif '_ansible_item_result' in task_result._result:\n",
   "                if task_result.is_failed() or task_result.is_unreachable():\n",
   "                    self._tqm.send_callback('v2_runner_item_on_failed', task_result)\n",
   "                elif task_result.is_skipped():\n",
   "                    self._tqm.send_callback('v2_runner_item_on_skipped', task_result)\n",
   "                    if 'diff' in task_result._result:\n",
   "                        if self._diff or getattr(original_task, 'diff', False):\n",
   "                            self._tqm.send_callback('v2_on_file_diff', task_result)\n",
   "                    self._tqm.send_callback('v2_runner_item_on_ok', task_result)\n",
   "            role_ran = False\n",
   "            if task_result.is_failed():\n",
   "                role_ran = True\n",
   "                ignore_errors = original_task.ignore_errors\n",
   "                if not ignore_errors:\n",
   "                    display.debug(\"marking %s as failed\" % original_host.name)\n",
   "                    if original_task.run_once:\n",
   "                        for h in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                            if h.name not in self._tqm._unreachable_hosts:\n",
   "                                state, _ = iterator.get_next_task_for_host(h, peek=True)\n",
   "                                iterator.mark_host_failed(h)\n",
   "                                state, new_task = iterator.get_next_task_for_host(h, peek=True)\n",
   "                        iterator.mark_host_failed(original_host)\n",
   "                    state, _ = iterator.get_next_task_for_host(original_host, peek=True)\n",
   "                    if iterator.is_failed(original_host) and state and state.run_state == iterator.ITERATING_COMPLETE:\n",
   "                        self._tqm._failed_hosts[original_host.name] = True\n",
   "                    if state and iterator.get_active_state(state).run_state == iterator.ITERATING_RESCUE:\n",
   "                        self._tqm._stats.increment('rescued', original_host.name)\n",
   "                            original_host.name,\n",
   "                                ansible_failed_task=original_task.serialize(),\n",
   "                                ansible_failed_result=task_result._result,\n",
   "                        self._tqm._stats.increment('failures', original_host.name)\n",
   "                    self._tqm._stats.increment('ok', original_host.name)\n",
   "                    self._tqm._stats.increment('ignored', original_host.name)\n",
   "                    if 'changed' in task_result._result and task_result._result['changed']:\n",
   "                        self._tqm._stats.increment('changed', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_failed', task_result, ignore_errors=ignore_errors)\n",
   "            elif task_result.is_unreachable():\n",
   "                ignore_unreachable = original_task.ignore_unreachable\n",
   "                if not ignore_unreachable:\n",
   "                    self._tqm._unreachable_hosts[original_host.name] = True\n",
   "                    iterator._play._removed_hosts.append(original_host.name)\n",
   "                    self._tqm._stats.increment('skipped', original_host.name)\n",
   "                    task_result._result['skip_reason'] = 'Host %s is unreachable' % original_host.name\n",
   "                self._tqm._stats.increment('dark', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_unreachable', task_result)\n",
   "            elif task_result.is_skipped():\n",
   "                self._tqm._stats.increment('skipped', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_skipped', task_result)\n",
   "                role_ran = True\n",
   "                if original_task.loop:\n",
   "                    result_items = task_result._result.get('results', [])\n",
   "                    result_items = [task_result._result]\n",
   "                for result_item in result_items:\n",
   "                    if '_ansible_notify' in result_item:\n",
   "                        if task_result.is_changed():\n",
   "                            for handler_name in result_item['_ansible_notify']:\n",
   "                                found = False\n",
   "                                target_handler = search_handler_blocks_by_name(handler_name, iterator._play.handlers)\n",
   "                                if target_handler is not None:\n",
   "                                    found = True\n",
   "                                    if target_handler.notify_host(original_host):\n",
   "                                        self._tqm.send_callback('v2_playbook_on_notify', target_handler, original_host)\n",
   "                                for listening_handler_block in iterator._play.handlers:\n",
   "                                    for listening_handler in listening_handler_block.block:\n",
   "                                        listeners = getattr(listening_handler, 'listen', []) or []\n",
   "                                        if not listeners:\n",
   "                                        listeners = listening_handler.get_validated_value(\n",
   "                                            'listen', listening_handler._valid_attrs['listen'], listeners, handler_templar\n",
   "                                        if handler_name not in listeners:\n",
   "                                            found = True\n",
   "                                        if listening_handler.notify_host(original_host):\n",
   "                                            self._tqm.send_callback('v2_playbook_on_notify', listening_handler, original_host)\n",
   "                                if not found:\n",
   "                                    msg = (\"The requested handler '%s' was not found in either the main handlers list nor in the listening \"\n",
   "                                           \"handlers list\" % handler_name)\n",
   "                                        raise AnsibleError(msg)\n",
   "                                        display.warning(msg)\n",
   "                    if 'add_host' in result_item:\n",
   "                        new_host_info = result_item.get('add_host', dict())\n",
   "                        self._add_host(new_host_info, result_item)\n",
   "                        post_process_whens(result_item, original_task, handler_templar)\n",
   "                    elif 'add_group' in result_item:\n",
   "                        self._add_group(original_host, result_item)\n",
   "                        post_process_whens(result_item, original_task, handler_templar)\n",
   "                    if 'ansible_facts' in result_item:\n",
   "                        if original_task.delegate_to is not None and original_task.delegate_facts:\n",
   "                            host_list = self.get_delegated_hosts(result_item, original_task)\n",
   "                            self._set_always_delegated_facts(result_item, original_task)\n",
   "                            host_list = self.get_task_hosts(iterator, original_host, original_task)\n",
   "                        if original_task.action == 'include_vars':\n",
   "                            for (var_name, var_value) in iteritems(result_item['ansible_facts']):\n",
   "                                for target_host in host_list:\n",
   "                                    self._variable_manager.set_host_variable(target_host, var_name, var_value)\n",
   "                            cacheable = result_item.pop('_ansible_facts_cacheable', False)\n",
   "                            for target_host in host_list:\n",
   "                                if original_task.action != 'set_fact' or cacheable:\n",
   "                                    self._variable_manager.set_host_facts(target_host, result_item['ansible_facts'].copy())\n",
   "                                if original_task.action == 'set_fact':\n",
   "                                    self._variable_manager.set_nonpersistent_facts(target_host, result_item['ansible_facts'].copy())\n",
   "                    if 'ansible_stats' in result_item and 'data' in result_item['ansible_stats'] and result_item['ansible_stats']['data']:\n",
   "                        if 'per_host' not in result_item['ansible_stats'] or result_item['ansible_stats']['per_host']:\n",
   "                            host_list = self.get_task_hosts(iterator, original_host, original_task)\n",
   "                            host_list = [None]\n",
   "                        data = result_item['ansible_stats']['data']\n",
   "                        aggregate = 'aggregate' in result_item['ansible_stats'] and result_item['ansible_stats']['aggregate']\n",
   "                        for myhost in host_list:\n",
   "                            for k in data.keys():\n",
   "                                if aggregate:\n",
   "                                    self._tqm._stats.update_custom_stats(k, data[k], myhost)\n",
   "                                    self._tqm._stats.set_custom_stats(k, data[k], myhost)\n",
   "                if 'diff' in task_result._result:\n",
   "                    if self._diff or getattr(original_task, 'diff', False):\n",
   "                        self._tqm.send_callback('v2_on_file_diff', task_result)\n",
   "                if not isinstance(original_task, TaskInclude):\n",
   "                    self._tqm._stats.increment('ok', original_host.name)\n",
   "                    if 'changed' in task_result._result and task_result._result['changed']:\n",
   "                        self._tqm._stats.increment('changed', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_ok', task_result)\n",
   "            if original_task.register:\n",
   "                host_list = self.get_task_hosts(iterator, original_host, original_task)\n",
   "                clean_copy = strip_internal_keys(module_response_deepcopy(task_result._result))\n",
   "                if 'invocation' in clean_copy:\n",
   "                    del clean_copy['invocation']\n",
   "                for target_host in host_list:\n",
   "                    self._variable_manager.set_nonpersistent_facts(target_host, {original_task.register: clean_copy})\n",
   "            if original_host.name in self._blocked_hosts:\n",
   "                del self._blocked_hosts[original_host.name]\n",
   "            if original_task._role is not None and role_ran:  # TODO:  and original_task.action != 'include_role':?\n",
   "                for (entry, role_obj) in iteritems(iterator._play.ROLE_CACHE[original_task._role.get_name()]):\n",
   "                    if role_obj._uuid == original_task._role._uuid:\n",
   "                        role_obj._had_task_run[original_host.name] = True\n",
   "            ret_results.append(task_result)\n",
   "            if one_pass or max_passes is not None and (cur_pass + 1) >= max_passes:\n",
   "            cur_pass += 1\n",
   "        return ret_results\n",
   "        ret_results = []\n",
   "        handler_results = 0\n",
   "        display.debug(\"waiting for handler results...\")\n",
   "               handler_results < len(notified_hosts) and\n",
   "            results = self._process_pending_results(iterator, do_handlers=True)\n",
   "            ret_results.extend(results)\n",
   "            handler_results += len([\n",
   "                r._host for r in results if r._host in notified_hosts and\n",
   "                r.task_name == handler.name])\n",
   "        display.debug(\"no more pending handlers, returning what we have\")\n",
   "        return ret_results\n",
   "        ret_results = []\n",
   "        display.debug(\"waiting for pending results...\")\n",
   "            results = self._process_pending_results(iterator)\n",
   "            ret_results.extend(results)\n",
   "        display.debug(\"no more pending results, returning what we have\")\n",
   "        return ret_results\n",
   "        changed = False\n",
   "            host_name = host_info.get('host_name')\n",
   "            if host_name not in self._inventory.hosts:\n",
   "                self._inventory.add_host(host_name, 'all')\n",
   "                self._hosts_cache_all.append(host_name)\n",
   "                changed = True\n",
   "            new_host = self._inventory.hosts.get(host_name)\n",
   "            new_host_vars = new_host.get_vars()\n",
   "            new_host_combined_vars = combine_vars(new_host_vars, host_info.get('host_vars', dict()))\n",
   "            if new_host_vars != new_host_combined_vars:\n",
   "                new_host.vars = new_host_combined_vars\n",
   "                changed = True\n",
   "            new_groups = host_info.get('groups', [])\n",
   "            for group_name in new_groups:\n",
   "                if group_name not in self._inventory.groups:\n",
   "                    group_name = self._inventory.add_group(group_name)\n",
   "                    changed = True\n",
   "                new_group = self._inventory.groups[group_name]\n",
   "                if new_group.add_host(self._inventory.hosts[host_name]):\n",
   "                    changed = True\n",
   "            if changed:\n",
   "            result_item['changed'] = changed\n",
   "        changed = False\n",
   "        real_host = self._inventory.hosts.get(host.name)\n",
   "        if real_host is None:\n",
   "            if host.name == self._inventory.localhost.name:\n",
   "                real_host = self._inventory.localhost\n",
   "                raise AnsibleError('%s cannot be matched in inventory' % host.name)\n",
   "        group_name = result_item.get('add_group')\n",
   "        parent_group_names = result_item.get('parent_groups', [])\n",
   "        if group_name not in self._inventory.groups:\n",
   "            group_name = self._inventory.add_group(group_name)\n",
   "        for name in parent_group_names:\n",
   "            if name not in self._inventory.groups:\n",
   "                self._inventory.add_group(name)\n",
   "                changed = True\n",
   "        group = self._inventory.groups[group_name]\n",
   "        for parent_group_name in parent_group_names:\n",
   "            parent_group = self._inventory.groups[parent_group_name]\n",
   "            new = parent_group.add_child_group(group)\n",
   "            if new and not changed:\n",
   "                changed = True\n",
   "        if real_host not in group.get_hosts():\n",
   "            changed = group.add_host(real_host)\n",
   "        if group not in real_host.get_groups():\n",
   "            changed = real_host.add_group(group)\n",
   "        if changed:\n",
   "        result_item['changed'] = changed\n",
   "        ti_copy = included_file._task.copy(exclude_parent=True)\n",
   "        ti_copy._parent = included_file._task._parent\n",
   "        temp_vars = ti_copy.vars.copy()\n",
   "        temp_vars.update(included_file._vars)\n",
   "        ti_copy.vars = temp_vars\n",
   "        return ti_copy\n",
   "        display.debug(\"loading included file: %s\" % included_file._filename)\n",
   "            data = self._loader.load_from_file(included_file._filename)\n",
   "            if data is None:\n",
   "            elif not isinstance(data, list):\n",
   "            ti_copy = self._copy_included_file(included_file)\n",
   "            tags = included_file._task.vars.pop('tags', [])\n",
   "            if isinstance(tags, string_types):\n",
   "                tags = tags.split(',')\n",
   "            if len(tags) > 0:\n",
   "                display.deprecated(\"You should not specify tags in the include parameters. All tags should be specified using the task-level option\",\n",
   "                included_file._task.tags = tags\n",
   "            block_list = load_list_of_blocks(\n",
   "                data,\n",
   "                parent_block=ti_copy.build_parent_block(),\n",
   "            for host in included_file._hosts:\n",
   "                self._tqm._stats.increment('ok', host.name)\n",
   "                reason = \"Could not find or access '%s' on the Ansible Controller.\" % to_text(e.file_name)\n",
   "                reason = to_text(e)\n",
   "            for host in included_file._hosts:\n",
   "                tr = TaskResult(host=host, task=included_file._task, return_data=dict(failed=True, reason=reason))\n",
   "                iterator.mark_host_failed(host)\n",
   "                self._tqm._failed_hosts[host.name] = True\n",
   "                self._tqm._stats.increment('failures', host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_failed', tr)\n",
   "        display.debug(\"done processing included file\")\n",
   "        return block_list\n",
   "        result = self._tqm.RUN_OK\n",
   "        for handler_block in iterator._play.handlers:\n",
   "            for handler in handler_block.block:\n",
   "                if handler.notified_hosts:\n",
   "                    result = self._do_handler_run(handler, handler.get_name(), iterator=iterator, play_context=play_context)\n",
   "                    if not result:\n",
   "        return result\n",
   "            notified_hosts = handler.notified_hosts[:]\n",
   "        failed_hosts = self._filter_notified_failed_hosts(iterator, notified_hosts)\n",
   "        notified_hosts = self._filter_notified_hosts(notified_hosts)\n",
   "        notified_hosts += failed_hosts\n",
   "        if len(notified_hosts) > 0:\n",
   "            saved_name = handler.name\n",
   "            handler.name = handler_name\n",
   "            self._tqm.send_callback('v2_playbook_on_handler_task_start', handler)\n",
   "            handler.name = saved_name\n",
   "        bypass_host_loop = False\n",
   "            action = plugin_loader.action_loader.get(handler.action, class_only=True)\n",
   "            if getattr(action, 'BYPASS_HOST_LOOP', False):\n",
   "                bypass_host_loop = True\n",
   "        host_results = []\n",
   "        for host in notified_hosts:\n",
   "            if not iterator.is_failed(host) or iterator._play.force_handlers:\n",
   "                task_vars = self._variable_manager.get_vars(play=iterator._play, host=host, task=handler,\n",
   "                self.add_tqm_variables(task_vars, play=iterator._play)\n",
   "                templar = Templar(loader=self._loader, variables=task_vars)\n",
   "                if not handler.cached_name:\n",
   "                    handler.name = templar.template(handler.name)\n",
   "                    handler.cached_name = True\n",
   "                self._queue_task(host, handler, task_vars, play_context)\n",
   "                if templar.template(handler.run_once) or bypass_host_loop:\n",
   "        host_results = self._wait_on_handler_results(iterator, handler, notified_hosts)\n",
   "        included_files = IncludedFile.process_include_results(\n",
   "            host_results,\n",
   "        result = True\n",
   "        if len(included_files) > 0:\n",
   "            for included_file in included_files:\n",
   "                    new_blocks = self._load_included_file(included_file, iterator=iterator, is_handler=True)\n",
   "                    for block in new_blocks:\n",
   "                        iterator._play.handlers.append(block)\n",
   "                        for task in block.block:\n",
   "                            task_name = task.get_name()\n",
   "                            display.debug(\"adding task '%s' included in handler '%s'\" % (task_name, handler_name))\n",
   "                            task.notified_hosts = included_file._hosts[:]\n",
   "                            result = self._do_handler_run(\n",
   "                                handler=task,\n",
   "                                handler_name=task_name,\n",
   "                                play_context=play_context,\n",
   "                                notified_hosts=included_file._hosts[:],\n",
   "                            if not result:\n",
   "                    for host in included_file._hosts:\n",
   "                        iterator.mark_host_failed(host)\n",
   "                        self._tqm._failed_hosts[host.name] = True\n",
   "                    display.warning(to_text(e))\n",
   "        handler.notified_hosts = [\n",
   "            h for h in handler.notified_hosts\n",
   "            if h not in notified_hosts]\n",
   "        display.debug(\"done running handlers, result is: %s\" % result)\n",
   "        return result\n",
   "        return notified_hosts[:]\n",
   "        ret = False\n",
   "        msg = u'Perform task: %s ' % task\n",
   "        if host:\n",
   "            msg += u'on %s ' % host\n",
   "        msg += u'(N)o/(y)es/(c)ontinue: '\n",
   "        resp = display.prompt(msg)\n",
   "        if resp.lower() in ['y', 'yes']:\n",
   "            display.debug(\"User ran task\")\n",
   "            ret = True\n",
   "        elif resp.lower() in ['c', 'continue']:\n",
   "            display.debug(\"User ran task and canceled step mode\")\n",
   "            ret = True\n",
   "            display.debug(\"User skipped task\")\n",
   "        display.banner(msg)\n",
   "        return ret\n",
   "        display.warning(\"%s task does not support when conditional\" % task_name)\n",
   "        meta_action = task.args.get('_raw_params')\n",
   "        def _evaluate_conditional(h):\n",
   "            all_vars = self._variable_manager.get_vars(play=iterator._play, host=h, task=task,\n",
   "            templar = Templar(loader=self._loader, variables=all_vars)\n",
   "            return task.evaluate_conditional(templar, all_vars)\n",
   "        skipped = False\n",
   "        msg = ''\n",
   "        if meta_action == 'noop':\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            msg = \"noop\"\n",
   "        elif meta_action == 'flush_handlers':\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            self._flushed_hosts[target_host] = True\n",
   "            self.run_handlers(iterator, play_context)\n",
   "            self._flushed_hosts[target_host] = False\n",
   "            msg = \"ran handlers\"\n",
   "        elif meta_action == 'refresh_inventory' or self.flush_cache:\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            msg = \"inventory successfully refreshed\"\n",
   "        elif meta_action == 'clear_facts':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                for host in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                    hostname = host.get_name()\n",
   "                    self._variable_manager.clear_facts(hostname)\n",
   "                msg = \"facts cleared\"\n",
   "                skipped = True\n",
   "        elif meta_action == 'clear_host_errors':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                for host in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                    self._tqm._failed_hosts.pop(host.name, False)\n",
   "                    self._tqm._unreachable_hosts.pop(host.name, False)\n",
   "                    iterator._host_states[host.name].fail_state = iterator.FAILED_NONE\n",
   "                msg = \"cleared host errors\"\n",
   "                skipped = True\n",
   "        elif meta_action == 'end_play':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                for host in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                    if host.name not in self._tqm._unreachable_hosts:\n",
   "                        iterator._host_states[host.name].run_state = iterator.ITERATING_COMPLETE\n",
   "                msg = \"ending play\"\n",
   "        elif meta_action == 'end_host':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                iterator._host_states[target_host.name].run_state = iterator.ITERATING_COMPLETE\n",
   "                iterator._play._removed_hosts.append(target_host.name)\n",
   "                msg = \"ending play for %s\" % target_host.name\n",
   "                skipped = True\n",
   "                msg = \"end_host conditional evaluated to false, continuing execution for %s\" % target_host.name\n",
   "        elif meta_action == 'reset_connection':\n",
   "            all_vars = self._variable_manager.get_vars(play=iterator._play, host=target_host, task=task,\n",
   "            templar = Templar(loader=self._loader, variables=all_vars)\n",
   "            play_context = play_context.set_task_and_variable_override(task=task, variables=all_vars, templar=templar)\n",
   "            play_context.post_validate(templar=templar)\n",
   "            if not play_context.remote_addr:\n",
   "                play_context.remote_addr = target_host.address\n",
   "            play_context.update_vars(all_vars)\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            if target_host in self._active_connections:\n",
   "                connection = Connection(self._active_connections[target_host])\n",
   "                del self._active_connections[target_host]\n",
   "                connection = plugin_loader.connection_loader.get(play_context.connection, play_context, os.devnull)\n",
   "                play_context.set_attributes_from_plugin(connection)\n",
   "            if connection:\n",
   "                    connection.reset()\n",
   "                    msg = 'reset connection'\n",
   "                    display.debug(\"got an error while closing persistent connection: %s\" % e)\n",
   "                msg = 'no connection, nothing to reset'\n",
   "            raise AnsibleError(\"invalid meta action requested: %s\" % meta_action, obj=task._ds)\n",
   "        result = {'msg': msg}\n",
   "        if skipped:\n",
   "            result['skipped'] = True\n",
   "            result['changed'] = False\n",
   "        display.vv(\"META: %s\" % msg)\n",
   "        return [TaskResult(target_host, task, result)]\n",
   "        hosts_left = []\n",
   "        for host in self._hosts_cache:\n",
   "            if host not in self._tqm._unreachable_hosts:\n",
   "                    hosts_left.append(self._inventory.hosts[host])\n",
   "                    hosts_left.append(self._inventory.get_host(host))\n",
   "        return hosts_left\n",
   "        for r in results:\n",
   "            if 'args' in r._task_fields:\n",
   "                socket_path = r._task_fields['args'].get('_ansible_socket')\n",
   "                if socket_path:\n",
   "                    if r._host not in self._active_connections:\n",
   "                        self._active_connections[r._host] = socket_path\n",
   "    EXIT = 3\n",
   "    def __init__(self, result=EXIT):\n",
   "        self.result = result\n",
   "        self.prompt = '[%s] %s (debug)> ' % (host, task)\n",
   "        self.scope['task'] = task\n",
   "        self.scope['task_vars'] = task_vars\n",
   "        self.scope['host'] = host\n",
   "        self.scope['play_context'] = play_context\n",
   "        self.scope['result'] = result\n",
   "        self.next_action = next_action\n",
   "        display.display('User interrupted execution')\n",
   "    do_q = do_quit\n",
   "        templar = Templar(None, shared_loader_obj=None, variables=self.scope['task_vars'])\n",
   "        task = self.scope['task']\n",
   "        task = task.load_data(task._ds)\n",
   "        task.post_validate(templar)\n",
   "        self.scope['task'] = task\n",
   "            display.display('***%s:%s' % (exc_type_name, repr(v)))\n",
   "            display.display(pprint.pformat(result))\n",
   "            display.display('***%s:%s' % (exc_type_name, repr(v)))\n"
  ]
 },
 "132": {
  "name": "do_c",
  "type": "function",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/plugins/strategy/__init__.py",
  "lineno": "1311",
  "column": "4",
  "context": "lt = NextAction.CONTINUE\n        return True\n\n    do_c = do_continue\n\n    def do_redo(self, args):\n        \"\"\"Schedule ",
  "context_lines": "    def do_continue(self, args):\n        \"\"\"Continue to next result\"\"\"\n        self.next_action.result = NextAction.CONTINUE\n        return True\n\n    do_c = do_continue\n\n    def do_redo(self, args):\n        \"\"\"Schedule task for re-execution. The re-execution may not be the next result\"\"\"\n        self.next_action.result = NextAction.REDO\n",
  "slicing": [
   "display = Display()\n",
   "ALWAYS_DELEGATE_FACT_PREFIXES = frozenset((\n",
   "_sentinel = StrategySentinel()\n",
   "def post_process_whens(result, task, templar):\n",
   "    cond = None\n",
   "        cond = Conditional(loader=templar._loader)\n",
   "        cond.when = task.changed_when\n",
   "        result['changed'] = cond.evaluate_conditional(templar, templar.available_variables)\n",
   "        if cond is None:\n",
   "            cond = Conditional(loader=templar._loader)\n",
   "        cond.when = task.failed_when\n",
   "        failed_when_result = cond.evaluate_conditional(templar, templar.available_variables)\n",
   "        result['failed_when_result'] = result['failed'] = failed_when_result\n",
   "            result = strategy._final_q.get()\n",
   "            if isinstance(result, StrategySentinel):\n",
   "                if 'listen' in result._task_fields:\n",
   "                    strategy._handler_results.append(result)\n",
   "                    strategy._results.append(result)\n",
   "        status_to_stats_map = (\n",
   "        prev_host_states = iterator._host_states.copy()\n",
   "        results = func(self, iterator, one_pass=one_pass, max_passes=max_passes, do_handlers=do_handlers)\n",
   "        _processed_results = []\n",
   "        for result in results:\n",
   "            task = result._task\n",
   "            host = result._host\n",
   "            _queued_task_args = self._queued_task_cache.pop((host.name, task._uuid), None)\n",
   "            task_vars = _queued_task_args['task_vars']\n",
   "            play_context = _queued_task_args['play_context']\n",
   "                prev_host_state = prev_host_states[host.name]\n",
   "                prev_host_state = iterator.get_host_state(host)\n",
   "            while result.needs_debugger(globally_enabled=self.debugger_active):\n",
   "                next_action = NextAction()\n",
   "                dbg = Debugger(task, host, task_vars, play_context, result, next_action)\n",
   "                dbg.cmdloop()\n",
   "                if next_action.result == NextAction.REDO:\n",
   "                    iterator._host_states[host.name] = prev_host_state\n",
   "                    for method, what in status_to_stats_map:\n",
   "                        if getattr(result, method)():\n",
   "                            self._tqm._stats.decrement(what, host.name)\n",
   "                    self._tqm._stats.decrement('ok', host.name)\n",
   "                    self._queue_task(host, task, task_vars, play_context)\n",
   "                    _processed_results.extend(debug_closure(func)(self, iterator, one_pass))\n",
   "                elif next_action.result == NextAction.CONTINUE:\n",
   "                    _processed_results.append(result)\n",
   "                elif next_action.result == NextAction.EXIT:\n",
   "                _processed_results.append(result)\n",
   "        return _processed_results\n",
   "        self._display = display\n",
   "            _pattern = 'all'\n",
   "            _pattern = play.hosts or 'all'\n",
   "        self._hosts_cache_all = [h.name for h in self._inventory.get_hosts(pattern=_pattern, ignore_restrictions=True)]\n",
   "        self._hosts_cache = [h.name for h in self._inventory.get_hosts(play.hosts, order=play.order)]\n",
   "        for sock in itervalues(self._active_connections):\n",
   "                conn = Connection(sock)\n",
   "                conn.reset()\n",
   "                display.debug(\"got an error while closing persistent connection: %s\" % e)\n",
   "        self._final_q.put(_sentinel)\n",
   "        for host in self._hosts_cache:\n",
   "            if host not in self._tqm._unreachable_hosts:\n",
   "                    iterator.get_next_task_for_host(self._inventory.hosts[host])\n",
   "                    iterator.get_next_task_for_host(self._inventory.get_host(host))\n",
   "        failed_hosts = iterator.get_failed_hosts()\n",
   "        unreachable_hosts = self._tqm._unreachable_hosts.keys()\n",
   "        display.debug(\"running handlers\")\n",
   "        handler_result = self.run_handlers(iterator, play_context)\n",
   "        if isinstance(handler_result, bool) and not handler_result:\n",
   "            result |= self._tqm.RUN_ERROR\n",
   "        elif not handler_result:\n",
   "            result |= handler_result\n",
   "        failed_hosts = set(failed_hosts).union(iterator.get_failed_hosts())\n",
   "        unreachable_hosts = set(unreachable_hosts).union(self._tqm._unreachable_hosts.keys())\n",
   "        if not isinstance(result, bool) and result != self._tqm.RUN_OK:\n",
   "            return result\n",
   "        elif len(unreachable_hosts) > 0:\n",
   "        elif len(failed_hosts) > 0:\n",
   "        ignore = set(self._tqm._failed_hosts).union(self._tqm._unreachable_hosts)\n",
   "        return [host for host in self._hosts_cache if host not in ignore]\n",
   "        return [host for host in self._hosts_cache if host in self._tqm._failed_hosts]\n",
   "        display.debug(\"entering _queue_task() for %s/%s\" % (host.name, task.action))\n",
   "        if task.action not in action_write_locks.action_write_locks:\n",
   "            display.debug('Creating lock for %s' % task.action)\n",
   "            action_write_locks.action_write_locks[task.action] = Lock()\n",
   "        templar = Templar(loader=self._loader, variables=task_vars)\n",
   "            throttle = int(templar.template(task.throttle))\n",
   "            raise AnsibleError(\"Failed to convert the throttle value to an integer.\", obj=task._ds, orig_exc=e)\n",
   "            rewind_point = len(self._workers)\n",
   "            if throttle > 0 and self.ALLOW_BASE_THROTTLING:\n",
   "                if task.run_once:\n",
   "                    display.debug(\"Ignoring 'throttle' as 'run_once' is also set for '%s'\" % task.get_name())\n",
   "                    if throttle <= rewind_point:\n",
   "                        display.debug(\"task: %s, throttle: %d\" % (task.get_name(), throttle))\n",
   "                        rewind_point = throttle\n",
   "            queued = False\n",
   "            starting_worker = self._cur_worker\n",
   "                if self._cur_worker >= rewind_point:\n",
   "                worker_prc = self._workers[self._cur_worker]\n",
   "                if worker_prc is None or not worker_prc.is_alive():\n",
   "                    self._queued_task_cache[(host.name, task._uuid)] = {\n",
   "                        'host': host,\n",
   "                        'task': task,\n",
   "                        'task_vars': task_vars,\n",
   "                        'play_context': play_context\n",
   "                    worker_prc = WorkerProcess(self._final_q, task_vars, host, task, play_context, self._loader, self._variable_manager, plugin_loader)\n",
   "                    self._workers[self._cur_worker] = worker_prc\n",
   "                    self._tqm.send_callback('v2_runner_on_start', host, task)\n",
   "                    worker_prc.start()\n",
   "                    display.debug(\"worker is %d (out of %d available)\" % (self._cur_worker + 1, len(self._workers)))\n",
   "                    queued = True\n",
   "                if self._cur_worker >= rewind_point:\n",
   "                if queued:\n",
   "                elif self._cur_worker == starting_worker:\n",
   "            if isinstance(task, Handler):\n",
   "            display.debug(\"got an error while queuing: %s\" % e)\n",
   "        display.debug(\"exiting _queue_task() for %s/%s\" % (host.name, task.action))\n",
   "        if task.run_once:\n",
   "            host_list = [host for host in self._hosts_cache if host not in self._tqm._unreachable_hosts]\n",
   "            host_list = [task_host.name]\n",
   "        return host_list\n",
   "        host_name = result.get('_ansible_delegated_vars', {}).get('ansible_delegated_host', None)\n",
   "        return [host_name or task.delegate_to]\n",
   "        if task.delegate_to is None:\n",
   "        facts = result['ansible_facts']\n",
   "        always_keys = set()\n",
   "        _add = always_keys.add\n",
   "        for fact_key in facts:\n",
   "            for always_key in ALWAYS_DELEGATE_FACT_PREFIXES:\n",
   "                if fact_key.startswith(always_key):\n",
   "                    _add(fact_key)\n",
   "        if always_keys:\n",
   "            _pop = facts.pop\n",
   "            always_facts = {\n",
   "                'ansible_facts': dict((k, _pop(k)) for k in list(facts) if k in always_keys)\n",
   "            host_list = self.get_delegated_hosts(result, task)\n",
   "            _set_host_facts = self._variable_manager.set_host_facts\n",
   "            for target_host in host_list:\n",
   "                _set_host_facts(target_host, always_facts)\n",
   "        ret_results = []\n",
   "        handler_templar = Templar(self._loader)\n",
   "        def get_original_host(host_name):\n",
   "            host_name = to_text(host_name)\n",
   "            if host_name in self._inventory.hosts:\n",
   "                return self._inventory.hosts[host_name]\n",
   "                return self._inventory.get_host(host_name)\n",
   "        def search_handler_blocks_by_name(handler_name, handler_blocks):\n",
   "            for handler_block in reversed(handler_blocks):\n",
   "                for handler_task in handler_block.block:\n",
   "                    if handler_task.name:\n",
   "                        if not handler_task.cached_name:\n",
   "                            if handler_templar.is_template(handler_task.name):\n",
   "                                handler_templar.available_variables = self._variable_manager.get_vars(play=iterator._play,\n",
   "                                                                                                      task=handler_task,\n",
   "                                handler_task.name = handler_templar.template(handler_task.name)\n",
   "                            handler_task.cached_name = True\n",
   "                            candidates = (\n",
   "                                handler_task.name,\n",
   "                                handler_task.get_name(include_role_fqcn=False),\n",
   "                                handler_task.get_name(include_role_fqcn=True),\n",
   "                            if handler_name in candidates:\n",
   "                                return handler_task\n",
   "        cur_pass = 0\n",
   "                    task_result = self._handler_results.popleft()\n",
   "                    task_result = self._results.popleft()\n",
   "            original_host = get_original_host(task_result._host)\n",
   "            queue_cache_entry = (original_host.name, task_result._task)\n",
   "            found_task = self._queued_task_cache.get(queue_cache_entry)['task']\n",
   "            original_task = found_task.copy(exclude_parent=True, exclude_tasks=True)\n",
   "            original_task._parent = found_task._parent\n",
   "            original_task.from_attrs(task_result._task_fields)\n",
   "            task_result._host = original_host\n",
   "            task_result._task = original_task\n",
   "            if '_ansible_retry' in task_result._result:\n",
   "                self._tqm.send_callback('v2_runner_retry', task_result)\n",
   "            elif '_ansible_item_result' in task_result._result:\n",
   "                if task_result.is_failed() or task_result.is_unreachable():\n",
   "                    self._tqm.send_callback('v2_runner_item_on_failed', task_result)\n",
   "                elif task_result.is_skipped():\n",
   "                    self._tqm.send_callback('v2_runner_item_on_skipped', task_result)\n",
   "                    if 'diff' in task_result._result:\n",
   "                        if self._diff or getattr(original_task, 'diff', False):\n",
   "                            self._tqm.send_callback('v2_on_file_diff', task_result)\n",
   "                    self._tqm.send_callback('v2_runner_item_on_ok', task_result)\n",
   "            role_ran = False\n",
   "            if task_result.is_failed():\n",
   "                role_ran = True\n",
   "                ignore_errors = original_task.ignore_errors\n",
   "                if not ignore_errors:\n",
   "                    display.debug(\"marking %s as failed\" % original_host.name)\n",
   "                    if original_task.run_once:\n",
   "                        for h in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                            if h.name not in self._tqm._unreachable_hosts:\n",
   "                                state, _ = iterator.get_next_task_for_host(h, peek=True)\n",
   "                                iterator.mark_host_failed(h)\n",
   "                                state, new_task = iterator.get_next_task_for_host(h, peek=True)\n",
   "                        iterator.mark_host_failed(original_host)\n",
   "                    state, _ = iterator.get_next_task_for_host(original_host, peek=True)\n",
   "                    if iterator.is_failed(original_host) and state and state.run_state == iterator.ITERATING_COMPLETE:\n",
   "                        self._tqm._failed_hosts[original_host.name] = True\n",
   "                    if state and iterator.get_active_state(state).run_state == iterator.ITERATING_RESCUE:\n",
   "                        self._tqm._stats.increment('rescued', original_host.name)\n",
   "                            original_host.name,\n",
   "                                ansible_failed_task=original_task.serialize(),\n",
   "                                ansible_failed_result=task_result._result,\n",
   "                        self._tqm._stats.increment('failures', original_host.name)\n",
   "                    self._tqm._stats.increment('ok', original_host.name)\n",
   "                    self._tqm._stats.increment('ignored', original_host.name)\n",
   "                    if 'changed' in task_result._result and task_result._result['changed']:\n",
   "                        self._tqm._stats.increment('changed', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_failed', task_result, ignore_errors=ignore_errors)\n",
   "            elif task_result.is_unreachable():\n",
   "                ignore_unreachable = original_task.ignore_unreachable\n",
   "                if not ignore_unreachable:\n",
   "                    self._tqm._unreachable_hosts[original_host.name] = True\n",
   "                    iterator._play._removed_hosts.append(original_host.name)\n",
   "                    self._tqm._stats.increment('skipped', original_host.name)\n",
   "                    task_result._result['skip_reason'] = 'Host %s is unreachable' % original_host.name\n",
   "                self._tqm._stats.increment('dark', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_unreachable', task_result)\n",
   "            elif task_result.is_skipped():\n",
   "                self._tqm._stats.increment('skipped', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_skipped', task_result)\n",
   "                role_ran = True\n",
   "                if original_task.loop:\n",
   "                    result_items = task_result._result.get('results', [])\n",
   "                    result_items = [task_result._result]\n",
   "                for result_item in result_items:\n",
   "                    if '_ansible_notify' in result_item:\n",
   "                        if task_result.is_changed():\n",
   "                            for handler_name in result_item['_ansible_notify']:\n",
   "                                found = False\n",
   "                                target_handler = search_handler_blocks_by_name(handler_name, iterator._play.handlers)\n",
   "                                if target_handler is not None:\n",
   "                                    found = True\n",
   "                                    if target_handler.notify_host(original_host):\n",
   "                                        self._tqm.send_callback('v2_playbook_on_notify', target_handler, original_host)\n",
   "                                for listening_handler_block in iterator._play.handlers:\n",
   "                                    for listening_handler in listening_handler_block.block:\n",
   "                                        listeners = getattr(listening_handler, 'listen', []) or []\n",
   "                                        if not listeners:\n",
   "                                        listeners = listening_handler.get_validated_value(\n",
   "                                            'listen', listening_handler._valid_attrs['listen'], listeners, handler_templar\n",
   "                                        if handler_name not in listeners:\n",
   "                                            found = True\n",
   "                                        if listening_handler.notify_host(original_host):\n",
   "                                            self._tqm.send_callback('v2_playbook_on_notify', listening_handler, original_host)\n",
   "                                if not found:\n",
   "                                    msg = (\"The requested handler '%s' was not found in either the main handlers list nor in the listening \"\n",
   "                                           \"handlers list\" % handler_name)\n",
   "                                        raise AnsibleError(msg)\n",
   "                                        display.warning(msg)\n",
   "                    if 'add_host' in result_item:\n",
   "                        new_host_info = result_item.get('add_host', dict())\n",
   "                        self._add_host(new_host_info, result_item)\n",
   "                        post_process_whens(result_item, original_task, handler_templar)\n",
   "                    elif 'add_group' in result_item:\n",
   "                        self._add_group(original_host, result_item)\n",
   "                        post_process_whens(result_item, original_task, handler_templar)\n",
   "                    if 'ansible_facts' in result_item:\n",
   "                        if original_task.delegate_to is not None and original_task.delegate_facts:\n",
   "                            host_list = self.get_delegated_hosts(result_item, original_task)\n",
   "                            self._set_always_delegated_facts(result_item, original_task)\n",
   "                            host_list = self.get_task_hosts(iterator, original_host, original_task)\n",
   "                        if original_task.action == 'include_vars':\n",
   "                            for (var_name, var_value) in iteritems(result_item['ansible_facts']):\n",
   "                                for target_host in host_list:\n",
   "                                    self._variable_manager.set_host_variable(target_host, var_name, var_value)\n",
   "                            cacheable = result_item.pop('_ansible_facts_cacheable', False)\n",
   "                            for target_host in host_list:\n",
   "                                if original_task.action != 'set_fact' or cacheable:\n",
   "                                    self._variable_manager.set_host_facts(target_host, result_item['ansible_facts'].copy())\n",
   "                                if original_task.action == 'set_fact':\n",
   "                                    self._variable_manager.set_nonpersistent_facts(target_host, result_item['ansible_facts'].copy())\n",
   "                    if 'ansible_stats' in result_item and 'data' in result_item['ansible_stats'] and result_item['ansible_stats']['data']:\n",
   "                        if 'per_host' not in result_item['ansible_stats'] or result_item['ansible_stats']['per_host']:\n",
   "                            host_list = self.get_task_hosts(iterator, original_host, original_task)\n",
   "                            host_list = [None]\n",
   "                        data = result_item['ansible_stats']['data']\n",
   "                        aggregate = 'aggregate' in result_item['ansible_stats'] and result_item['ansible_stats']['aggregate']\n",
   "                        for myhost in host_list:\n",
   "                            for k in data.keys():\n",
   "                                if aggregate:\n",
   "                                    self._tqm._stats.update_custom_stats(k, data[k], myhost)\n",
   "                                    self._tqm._stats.set_custom_stats(k, data[k], myhost)\n",
   "                if 'diff' in task_result._result:\n",
   "                    if self._diff or getattr(original_task, 'diff', False):\n",
   "                        self._tqm.send_callback('v2_on_file_diff', task_result)\n",
   "                if not isinstance(original_task, TaskInclude):\n",
   "                    self._tqm._stats.increment('ok', original_host.name)\n",
   "                    if 'changed' in task_result._result and task_result._result['changed']:\n",
   "                        self._tqm._stats.increment('changed', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_ok', task_result)\n",
   "            if original_task.register:\n",
   "                host_list = self.get_task_hosts(iterator, original_host, original_task)\n",
   "                clean_copy = strip_internal_keys(module_response_deepcopy(task_result._result))\n",
   "                if 'invocation' in clean_copy:\n",
   "                    del clean_copy['invocation']\n",
   "                for target_host in host_list:\n",
   "                    self._variable_manager.set_nonpersistent_facts(target_host, {original_task.register: clean_copy})\n",
   "            if original_host.name in self._blocked_hosts:\n",
   "                del self._blocked_hosts[original_host.name]\n",
   "            if original_task._role is not None and role_ran:  # TODO:  and original_task.action != 'include_role':?\n",
   "                for (entry, role_obj) in iteritems(iterator._play.ROLE_CACHE[original_task._role.get_name()]):\n",
   "                    if role_obj._uuid == original_task._role._uuid:\n",
   "                        role_obj._had_task_run[original_host.name] = True\n",
   "            ret_results.append(task_result)\n",
   "            if one_pass or max_passes is not None and (cur_pass + 1) >= max_passes:\n",
   "            cur_pass += 1\n",
   "        return ret_results\n",
   "        ret_results = []\n",
   "        handler_results = 0\n",
   "        display.debug(\"waiting for handler results...\")\n",
   "               handler_results < len(notified_hosts) and\n",
   "            results = self._process_pending_results(iterator, do_handlers=True)\n",
   "            ret_results.extend(results)\n",
   "            handler_results += len([\n",
   "                r._host for r in results if r._host in notified_hosts and\n",
   "                r.task_name == handler.name])\n",
   "        display.debug(\"no more pending handlers, returning what we have\")\n",
   "        return ret_results\n",
   "        ret_results = []\n",
   "        display.debug(\"waiting for pending results...\")\n",
   "            results = self._process_pending_results(iterator)\n",
   "            ret_results.extend(results)\n",
   "        display.debug(\"no more pending results, returning what we have\")\n",
   "        return ret_results\n",
   "        changed = False\n",
   "            host_name = host_info.get('host_name')\n",
   "            if host_name not in self._inventory.hosts:\n",
   "                self._inventory.add_host(host_name, 'all')\n",
   "                self._hosts_cache_all.append(host_name)\n",
   "                changed = True\n",
   "            new_host = self._inventory.hosts.get(host_name)\n",
   "            new_host_vars = new_host.get_vars()\n",
   "            new_host_combined_vars = combine_vars(new_host_vars, host_info.get('host_vars', dict()))\n",
   "            if new_host_vars != new_host_combined_vars:\n",
   "                new_host.vars = new_host_combined_vars\n",
   "                changed = True\n",
   "            new_groups = host_info.get('groups', [])\n",
   "            for group_name in new_groups:\n",
   "                if group_name not in self._inventory.groups:\n",
   "                    group_name = self._inventory.add_group(group_name)\n",
   "                    changed = True\n",
   "                new_group = self._inventory.groups[group_name]\n",
   "                if new_group.add_host(self._inventory.hosts[host_name]):\n",
   "                    changed = True\n",
   "            if changed:\n",
   "            result_item['changed'] = changed\n",
   "        changed = False\n",
   "        real_host = self._inventory.hosts.get(host.name)\n",
   "        if real_host is None:\n",
   "            if host.name == self._inventory.localhost.name:\n",
   "                real_host = self._inventory.localhost\n",
   "                raise AnsibleError('%s cannot be matched in inventory' % host.name)\n",
   "        group_name = result_item.get('add_group')\n",
   "        parent_group_names = result_item.get('parent_groups', [])\n",
   "        if group_name not in self._inventory.groups:\n",
   "            group_name = self._inventory.add_group(group_name)\n",
   "        for name in parent_group_names:\n",
   "            if name not in self._inventory.groups:\n",
   "                self._inventory.add_group(name)\n",
   "                changed = True\n",
   "        group = self._inventory.groups[group_name]\n",
   "        for parent_group_name in parent_group_names:\n",
   "            parent_group = self._inventory.groups[parent_group_name]\n",
   "            new = parent_group.add_child_group(group)\n",
   "            if new and not changed:\n",
   "                changed = True\n",
   "        if real_host not in group.get_hosts():\n",
   "            changed = group.add_host(real_host)\n",
   "        if group not in real_host.get_groups():\n",
   "            changed = real_host.add_group(group)\n",
   "        if changed:\n",
   "        result_item['changed'] = changed\n",
   "        ti_copy = included_file._task.copy(exclude_parent=True)\n",
   "        ti_copy._parent = included_file._task._parent\n",
   "        temp_vars = ti_copy.vars.copy()\n",
   "        temp_vars.update(included_file._vars)\n",
   "        ti_copy.vars = temp_vars\n",
   "        return ti_copy\n",
   "        display.debug(\"loading included file: %s\" % included_file._filename)\n",
   "            data = self._loader.load_from_file(included_file._filename)\n",
   "            if data is None:\n",
   "            elif not isinstance(data, list):\n",
   "            ti_copy = self._copy_included_file(included_file)\n",
   "            tags = included_file._task.vars.pop('tags', [])\n",
   "            if isinstance(tags, string_types):\n",
   "                tags = tags.split(',')\n",
   "            if len(tags) > 0:\n",
   "                display.deprecated(\"You should not specify tags in the include parameters. All tags should be specified using the task-level option\",\n",
   "                included_file._task.tags = tags\n",
   "            block_list = load_list_of_blocks(\n",
   "                data,\n",
   "                parent_block=ti_copy.build_parent_block(),\n",
   "            for host in included_file._hosts:\n",
   "                self._tqm._stats.increment('ok', host.name)\n",
   "                reason = \"Could not find or access '%s' on the Ansible Controller.\" % to_text(e.file_name)\n",
   "                reason = to_text(e)\n",
   "            for host in included_file._hosts:\n",
   "                tr = TaskResult(host=host, task=included_file._task, return_data=dict(failed=True, reason=reason))\n",
   "                iterator.mark_host_failed(host)\n",
   "                self._tqm._failed_hosts[host.name] = True\n",
   "                self._tqm._stats.increment('failures', host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_failed', tr)\n",
   "        display.debug(\"done processing included file\")\n",
   "        return block_list\n",
   "        result = self._tqm.RUN_OK\n",
   "        for handler_block in iterator._play.handlers:\n",
   "            for handler in handler_block.block:\n",
   "                if handler.notified_hosts:\n",
   "                    result = self._do_handler_run(handler, handler.get_name(), iterator=iterator, play_context=play_context)\n",
   "                    if not result:\n",
   "        return result\n",
   "            notified_hosts = handler.notified_hosts[:]\n",
   "        failed_hosts = self._filter_notified_failed_hosts(iterator, notified_hosts)\n",
   "        notified_hosts = self._filter_notified_hosts(notified_hosts)\n",
   "        notified_hosts += failed_hosts\n",
   "        if len(notified_hosts) > 0:\n",
   "            saved_name = handler.name\n",
   "            handler.name = handler_name\n",
   "            self._tqm.send_callback('v2_playbook_on_handler_task_start', handler)\n",
   "            handler.name = saved_name\n",
   "        bypass_host_loop = False\n",
   "            action = plugin_loader.action_loader.get(handler.action, class_only=True)\n",
   "            if getattr(action, 'BYPASS_HOST_LOOP', False):\n",
   "                bypass_host_loop = True\n",
   "        host_results = []\n",
   "        for host in notified_hosts:\n",
   "            if not iterator.is_failed(host) or iterator._play.force_handlers:\n",
   "                task_vars = self._variable_manager.get_vars(play=iterator._play, host=host, task=handler,\n",
   "                self.add_tqm_variables(task_vars, play=iterator._play)\n",
   "                templar = Templar(loader=self._loader, variables=task_vars)\n",
   "                if not handler.cached_name:\n",
   "                    handler.name = templar.template(handler.name)\n",
   "                    handler.cached_name = True\n",
   "                self._queue_task(host, handler, task_vars, play_context)\n",
   "                if templar.template(handler.run_once) or bypass_host_loop:\n",
   "        host_results = self._wait_on_handler_results(iterator, handler, notified_hosts)\n",
   "        included_files = IncludedFile.process_include_results(\n",
   "            host_results,\n",
   "        result = True\n",
   "        if len(included_files) > 0:\n",
   "            for included_file in included_files:\n",
   "                    new_blocks = self._load_included_file(included_file, iterator=iterator, is_handler=True)\n",
   "                    for block in new_blocks:\n",
   "                        iterator._play.handlers.append(block)\n",
   "                        for task in block.block:\n",
   "                            task_name = task.get_name()\n",
   "                            display.debug(\"adding task '%s' included in handler '%s'\" % (task_name, handler_name))\n",
   "                            task.notified_hosts = included_file._hosts[:]\n",
   "                            result = self._do_handler_run(\n",
   "                                handler=task,\n",
   "                                handler_name=task_name,\n",
   "                                play_context=play_context,\n",
   "                                notified_hosts=included_file._hosts[:],\n",
   "                            if not result:\n",
   "                    for host in included_file._hosts:\n",
   "                        iterator.mark_host_failed(host)\n",
   "                        self._tqm._failed_hosts[host.name] = True\n",
   "                    display.warning(to_text(e))\n",
   "        handler.notified_hosts = [\n",
   "            h for h in handler.notified_hosts\n",
   "            if h not in notified_hosts]\n",
   "        display.debug(\"done running handlers, result is: %s\" % result)\n",
   "        return result\n",
   "        return notified_hosts[:]\n",
   "        ret = False\n",
   "        msg = u'Perform task: %s ' % task\n",
   "        if host:\n",
   "            msg += u'on %s ' % host\n",
   "        msg += u'(N)o/(y)es/(c)ontinue: '\n",
   "        resp = display.prompt(msg)\n",
   "        if resp.lower() in ['y', 'yes']:\n",
   "            display.debug(\"User ran task\")\n",
   "            ret = True\n",
   "        elif resp.lower() in ['c', 'continue']:\n",
   "            display.debug(\"User ran task and canceled step mode\")\n",
   "            ret = True\n",
   "            display.debug(\"User skipped task\")\n",
   "        display.banner(msg)\n",
   "        return ret\n",
   "        display.warning(\"%s task does not support when conditional\" % task_name)\n",
   "        meta_action = task.args.get('_raw_params')\n",
   "        def _evaluate_conditional(h):\n",
   "            all_vars = self._variable_manager.get_vars(play=iterator._play, host=h, task=task,\n",
   "            templar = Templar(loader=self._loader, variables=all_vars)\n",
   "            return task.evaluate_conditional(templar, all_vars)\n",
   "        skipped = False\n",
   "        msg = ''\n",
   "        if meta_action == 'noop':\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            msg = \"noop\"\n",
   "        elif meta_action == 'flush_handlers':\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            self._flushed_hosts[target_host] = True\n",
   "            self.run_handlers(iterator, play_context)\n",
   "            self._flushed_hosts[target_host] = False\n",
   "            msg = \"ran handlers\"\n",
   "        elif meta_action == 'refresh_inventory' or self.flush_cache:\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            msg = \"inventory successfully refreshed\"\n",
   "        elif meta_action == 'clear_facts':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                for host in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                    hostname = host.get_name()\n",
   "                    self._variable_manager.clear_facts(hostname)\n",
   "                msg = \"facts cleared\"\n",
   "                skipped = True\n",
   "        elif meta_action == 'clear_host_errors':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                for host in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                    self._tqm._failed_hosts.pop(host.name, False)\n",
   "                    self._tqm._unreachable_hosts.pop(host.name, False)\n",
   "                    iterator._host_states[host.name].fail_state = iterator.FAILED_NONE\n",
   "                msg = \"cleared host errors\"\n",
   "                skipped = True\n",
   "        elif meta_action == 'end_play':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                for host in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                    if host.name not in self._tqm._unreachable_hosts:\n",
   "                        iterator._host_states[host.name].run_state = iterator.ITERATING_COMPLETE\n",
   "                msg = \"ending play\"\n",
   "        elif meta_action == 'end_host':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                iterator._host_states[target_host.name].run_state = iterator.ITERATING_COMPLETE\n",
   "                iterator._play._removed_hosts.append(target_host.name)\n",
   "                msg = \"ending play for %s\" % target_host.name\n",
   "                skipped = True\n",
   "                msg = \"end_host conditional evaluated to false, continuing execution for %s\" % target_host.name\n",
   "        elif meta_action == 'reset_connection':\n",
   "            all_vars = self._variable_manager.get_vars(play=iterator._play, host=target_host, task=task,\n",
   "            templar = Templar(loader=self._loader, variables=all_vars)\n",
   "            play_context = play_context.set_task_and_variable_override(task=task, variables=all_vars, templar=templar)\n",
   "            play_context.post_validate(templar=templar)\n",
   "            if not play_context.remote_addr:\n",
   "                play_context.remote_addr = target_host.address\n",
   "            play_context.update_vars(all_vars)\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            if target_host in self._active_connections:\n",
   "                connection = Connection(self._active_connections[target_host])\n",
   "                del self._active_connections[target_host]\n",
   "                connection = plugin_loader.connection_loader.get(play_context.connection, play_context, os.devnull)\n",
   "                play_context.set_attributes_from_plugin(connection)\n",
   "            if connection:\n",
   "                    connection.reset()\n",
   "                    msg = 'reset connection'\n",
   "                    display.debug(\"got an error while closing persistent connection: %s\" % e)\n",
   "                msg = 'no connection, nothing to reset'\n",
   "            raise AnsibleError(\"invalid meta action requested: %s\" % meta_action, obj=task._ds)\n",
   "        result = {'msg': msg}\n",
   "        if skipped:\n",
   "            result['skipped'] = True\n",
   "            result['changed'] = False\n",
   "        display.vv(\"META: %s\" % msg)\n",
   "        return [TaskResult(target_host, task, result)]\n",
   "        hosts_left = []\n",
   "        for host in self._hosts_cache:\n",
   "            if host not in self._tqm._unreachable_hosts:\n",
   "                    hosts_left.append(self._inventory.hosts[host])\n",
   "                    hosts_left.append(self._inventory.get_host(host))\n",
   "        return hosts_left\n",
   "        for r in results:\n",
   "            if 'args' in r._task_fields:\n",
   "                socket_path = r._task_fields['args'].get('_ansible_socket')\n",
   "                if socket_path:\n",
   "                    if r._host not in self._active_connections:\n",
   "                        self._active_connections[r._host] = socket_path\n",
   "    EXIT = 3\n",
   "    def __init__(self, result=EXIT):\n",
   "        self.result = result\n",
   "        self.prompt = '[%s] %s (debug)> ' % (host, task)\n",
   "        self.scope['task'] = task\n",
   "        self.scope['task_vars'] = task_vars\n",
   "        self.scope['host'] = host\n",
   "        self.scope['play_context'] = play_context\n",
   "        self.scope['result'] = result\n",
   "        self.next_action = next_action\n",
   "        display.display('User interrupted execution')\n",
   "    do_c = do_continue\n",
   "        templar = Templar(None, shared_loader_obj=None, variables=self.scope['task_vars'])\n",
   "        task = self.scope['task']\n",
   "        task = task.load_data(task._ds)\n",
   "        task.post_validate(templar)\n",
   "        self.scope['task'] = task\n",
   "            display.display('***%s:%s' % (exc_type_name, repr(v)))\n",
   "            display.display(pprint.pformat(result))\n",
   "            display.display('***%s:%s' % (exc_type_name, repr(v)))\n"
  ]
 },
 "133": {
  "name": "do_r",
  "type": "function",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/plugins/strategy/__init__.py",
  "lineno": "1318",
  "column": "4",
  "context": "result = NextAction.REDO\n        return True\n\n    do_r = do_redo\n\n    def do_update_task(self, args):\n        \"\"\"Re",
  "context_lines": "    def do_redo(self, args):\n        \"\"\"Schedule task for re-execution. The re-execution may not be the next result\"\"\"\n        self.next_action.result = NextAction.REDO\n        return True\n\n    do_r = do_redo\n\n    def do_update_task(self, args):\n        \"\"\"Recreate the task from ``task._ds``, and template with updated ``task_vars``\"\"\"\n        templar = Templar(None, shared_loader_obj=None, variables=self.scope['task_vars'])\n",
  "slicing": [
   "display = Display()\n",
   "ALWAYS_DELEGATE_FACT_PREFIXES = frozenset((\n",
   "_sentinel = StrategySentinel()\n",
   "def post_process_whens(result, task, templar):\n",
   "    cond = None\n",
   "        cond = Conditional(loader=templar._loader)\n",
   "        cond.when = task.changed_when\n",
   "        result['changed'] = cond.evaluate_conditional(templar, templar.available_variables)\n",
   "        if cond is None:\n",
   "            cond = Conditional(loader=templar._loader)\n",
   "        cond.when = task.failed_when\n",
   "        failed_when_result = cond.evaluate_conditional(templar, templar.available_variables)\n",
   "        result['failed_when_result'] = result['failed'] = failed_when_result\n",
   "            result = strategy._final_q.get()\n",
   "            if isinstance(result, StrategySentinel):\n",
   "                if 'listen' in result._task_fields:\n",
   "                    strategy._handler_results.append(result)\n",
   "                    strategy._results.append(result)\n",
   "        status_to_stats_map = (\n",
   "        prev_host_states = iterator._host_states.copy()\n",
   "        results = func(self, iterator, one_pass=one_pass, max_passes=max_passes, do_handlers=do_handlers)\n",
   "        _processed_results = []\n",
   "        for result in results:\n",
   "            task = result._task\n",
   "            host = result._host\n",
   "            _queued_task_args = self._queued_task_cache.pop((host.name, task._uuid), None)\n",
   "            task_vars = _queued_task_args['task_vars']\n",
   "            play_context = _queued_task_args['play_context']\n",
   "                prev_host_state = prev_host_states[host.name]\n",
   "                prev_host_state = iterator.get_host_state(host)\n",
   "            while result.needs_debugger(globally_enabled=self.debugger_active):\n",
   "                next_action = NextAction()\n",
   "                dbg = Debugger(task, host, task_vars, play_context, result, next_action)\n",
   "                dbg.cmdloop()\n",
   "                if next_action.result == NextAction.REDO:\n",
   "                    iterator._host_states[host.name] = prev_host_state\n",
   "                    for method, what in status_to_stats_map:\n",
   "                        if getattr(result, method)():\n",
   "                            self._tqm._stats.decrement(what, host.name)\n",
   "                    self._tqm._stats.decrement('ok', host.name)\n",
   "                    self._queue_task(host, task, task_vars, play_context)\n",
   "                    _processed_results.extend(debug_closure(func)(self, iterator, one_pass))\n",
   "                elif next_action.result == NextAction.CONTINUE:\n",
   "                    _processed_results.append(result)\n",
   "                elif next_action.result == NextAction.EXIT:\n",
   "                _processed_results.append(result)\n",
   "        return _processed_results\n",
   "        self._display = display\n",
   "            _pattern = 'all'\n",
   "            _pattern = play.hosts or 'all'\n",
   "        self._hosts_cache_all = [h.name for h in self._inventory.get_hosts(pattern=_pattern, ignore_restrictions=True)]\n",
   "        self._hosts_cache = [h.name for h in self._inventory.get_hosts(play.hosts, order=play.order)]\n",
   "        for sock in itervalues(self._active_connections):\n",
   "                conn = Connection(sock)\n",
   "                conn.reset()\n",
   "                display.debug(\"got an error while closing persistent connection: %s\" % e)\n",
   "        self._final_q.put(_sentinel)\n",
   "        for host in self._hosts_cache:\n",
   "            if host not in self._tqm._unreachable_hosts:\n",
   "                    iterator.get_next_task_for_host(self._inventory.hosts[host])\n",
   "                    iterator.get_next_task_for_host(self._inventory.get_host(host))\n",
   "        failed_hosts = iterator.get_failed_hosts()\n",
   "        unreachable_hosts = self._tqm._unreachable_hosts.keys()\n",
   "        display.debug(\"running handlers\")\n",
   "        handler_result = self.run_handlers(iterator, play_context)\n",
   "        if isinstance(handler_result, bool) and not handler_result:\n",
   "            result |= self._tqm.RUN_ERROR\n",
   "        elif not handler_result:\n",
   "            result |= handler_result\n",
   "        failed_hosts = set(failed_hosts).union(iterator.get_failed_hosts())\n",
   "        unreachable_hosts = set(unreachable_hosts).union(self._tqm._unreachable_hosts.keys())\n",
   "        if not isinstance(result, bool) and result != self._tqm.RUN_OK:\n",
   "            return result\n",
   "        elif len(unreachable_hosts) > 0:\n",
   "        elif len(failed_hosts) > 0:\n",
   "        ignore = set(self._tqm._failed_hosts).union(self._tqm._unreachable_hosts)\n",
   "        return [host for host in self._hosts_cache if host not in ignore]\n",
   "        return [host for host in self._hosts_cache if host in self._tqm._failed_hosts]\n",
   "        display.debug(\"entering _queue_task() for %s/%s\" % (host.name, task.action))\n",
   "        if task.action not in action_write_locks.action_write_locks:\n",
   "            display.debug('Creating lock for %s' % task.action)\n",
   "            action_write_locks.action_write_locks[task.action] = Lock()\n",
   "        templar = Templar(loader=self._loader, variables=task_vars)\n",
   "            throttle = int(templar.template(task.throttle))\n",
   "            raise AnsibleError(\"Failed to convert the throttle value to an integer.\", obj=task._ds, orig_exc=e)\n",
   "            rewind_point = len(self._workers)\n",
   "            if throttle > 0 and self.ALLOW_BASE_THROTTLING:\n",
   "                if task.run_once:\n",
   "                    display.debug(\"Ignoring 'throttle' as 'run_once' is also set for '%s'\" % task.get_name())\n",
   "                    if throttle <= rewind_point:\n",
   "                        display.debug(\"task: %s, throttle: %d\" % (task.get_name(), throttle))\n",
   "                        rewind_point = throttle\n",
   "            queued = False\n",
   "            starting_worker = self._cur_worker\n",
   "                if self._cur_worker >= rewind_point:\n",
   "                worker_prc = self._workers[self._cur_worker]\n",
   "                if worker_prc is None or not worker_prc.is_alive():\n",
   "                    self._queued_task_cache[(host.name, task._uuid)] = {\n",
   "                        'host': host,\n",
   "                        'task': task,\n",
   "                        'task_vars': task_vars,\n",
   "                        'play_context': play_context\n",
   "                    worker_prc = WorkerProcess(self._final_q, task_vars, host, task, play_context, self._loader, self._variable_manager, plugin_loader)\n",
   "                    self._workers[self._cur_worker] = worker_prc\n",
   "                    self._tqm.send_callback('v2_runner_on_start', host, task)\n",
   "                    worker_prc.start()\n",
   "                    display.debug(\"worker is %d (out of %d available)\" % (self._cur_worker + 1, len(self._workers)))\n",
   "                    queued = True\n",
   "                if self._cur_worker >= rewind_point:\n",
   "                if queued:\n",
   "                elif self._cur_worker == starting_worker:\n",
   "            if isinstance(task, Handler):\n",
   "            display.debug(\"got an error while queuing: %s\" % e)\n",
   "        display.debug(\"exiting _queue_task() for %s/%s\" % (host.name, task.action))\n",
   "        if task.run_once:\n",
   "            host_list = [host for host in self._hosts_cache if host not in self._tqm._unreachable_hosts]\n",
   "            host_list = [task_host.name]\n",
   "        return host_list\n",
   "        host_name = result.get('_ansible_delegated_vars', {}).get('ansible_delegated_host', None)\n",
   "        return [host_name or task.delegate_to]\n",
   "        if task.delegate_to is None:\n",
   "        facts = result['ansible_facts']\n",
   "        always_keys = set()\n",
   "        _add = always_keys.add\n",
   "        for fact_key in facts:\n",
   "            for always_key in ALWAYS_DELEGATE_FACT_PREFIXES:\n",
   "                if fact_key.startswith(always_key):\n",
   "                    _add(fact_key)\n",
   "        if always_keys:\n",
   "            _pop = facts.pop\n",
   "            always_facts = {\n",
   "                'ansible_facts': dict((k, _pop(k)) for k in list(facts) if k in always_keys)\n",
   "            host_list = self.get_delegated_hosts(result, task)\n",
   "            _set_host_facts = self._variable_manager.set_host_facts\n",
   "            for target_host in host_list:\n",
   "                _set_host_facts(target_host, always_facts)\n",
   "        ret_results = []\n",
   "        handler_templar = Templar(self._loader)\n",
   "        def get_original_host(host_name):\n",
   "            host_name = to_text(host_name)\n",
   "            if host_name in self._inventory.hosts:\n",
   "                return self._inventory.hosts[host_name]\n",
   "                return self._inventory.get_host(host_name)\n",
   "        def search_handler_blocks_by_name(handler_name, handler_blocks):\n",
   "            for handler_block in reversed(handler_blocks):\n",
   "                for handler_task in handler_block.block:\n",
   "                    if handler_task.name:\n",
   "                        if not handler_task.cached_name:\n",
   "                            if handler_templar.is_template(handler_task.name):\n",
   "                                handler_templar.available_variables = self._variable_manager.get_vars(play=iterator._play,\n",
   "                                                                                                      task=handler_task,\n",
   "                                handler_task.name = handler_templar.template(handler_task.name)\n",
   "                            handler_task.cached_name = True\n",
   "                            candidates = (\n",
   "                                handler_task.name,\n",
   "                                handler_task.get_name(include_role_fqcn=False),\n",
   "                                handler_task.get_name(include_role_fqcn=True),\n",
   "                            if handler_name in candidates:\n",
   "                                return handler_task\n",
   "        cur_pass = 0\n",
   "                    task_result = self._handler_results.popleft()\n",
   "                    task_result = self._results.popleft()\n",
   "            original_host = get_original_host(task_result._host)\n",
   "            queue_cache_entry = (original_host.name, task_result._task)\n",
   "            found_task = self._queued_task_cache.get(queue_cache_entry)['task']\n",
   "            original_task = found_task.copy(exclude_parent=True, exclude_tasks=True)\n",
   "            original_task._parent = found_task._parent\n",
   "            original_task.from_attrs(task_result._task_fields)\n",
   "            task_result._host = original_host\n",
   "            task_result._task = original_task\n",
   "            if '_ansible_retry' in task_result._result:\n",
   "                self._tqm.send_callback('v2_runner_retry', task_result)\n",
   "            elif '_ansible_item_result' in task_result._result:\n",
   "                if task_result.is_failed() or task_result.is_unreachable():\n",
   "                    self._tqm.send_callback('v2_runner_item_on_failed', task_result)\n",
   "                elif task_result.is_skipped():\n",
   "                    self._tqm.send_callback('v2_runner_item_on_skipped', task_result)\n",
   "                    if 'diff' in task_result._result:\n",
   "                        if self._diff or getattr(original_task, 'diff', False):\n",
   "                            self._tqm.send_callback('v2_on_file_diff', task_result)\n",
   "                    self._tqm.send_callback('v2_runner_item_on_ok', task_result)\n",
   "            role_ran = False\n",
   "            if task_result.is_failed():\n",
   "                role_ran = True\n",
   "                ignore_errors = original_task.ignore_errors\n",
   "                if not ignore_errors:\n",
   "                    display.debug(\"marking %s as failed\" % original_host.name)\n",
   "                    if original_task.run_once:\n",
   "                        for h in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                            if h.name not in self._tqm._unreachable_hosts:\n",
   "                                state, _ = iterator.get_next_task_for_host(h, peek=True)\n",
   "                                iterator.mark_host_failed(h)\n",
   "                                state, new_task = iterator.get_next_task_for_host(h, peek=True)\n",
   "                        iterator.mark_host_failed(original_host)\n",
   "                    state, _ = iterator.get_next_task_for_host(original_host, peek=True)\n",
   "                    if iterator.is_failed(original_host) and state and state.run_state == iterator.ITERATING_COMPLETE:\n",
   "                        self._tqm._failed_hosts[original_host.name] = True\n",
   "                    if state and iterator.get_active_state(state).run_state == iterator.ITERATING_RESCUE:\n",
   "                        self._tqm._stats.increment('rescued', original_host.name)\n",
   "                            original_host.name,\n",
   "                                ansible_failed_task=original_task.serialize(),\n",
   "                                ansible_failed_result=task_result._result,\n",
   "                        self._tqm._stats.increment('failures', original_host.name)\n",
   "                    self._tqm._stats.increment('ok', original_host.name)\n",
   "                    self._tqm._stats.increment('ignored', original_host.name)\n",
   "                    if 'changed' in task_result._result and task_result._result['changed']:\n",
   "                        self._tqm._stats.increment('changed', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_failed', task_result, ignore_errors=ignore_errors)\n",
   "            elif task_result.is_unreachable():\n",
   "                ignore_unreachable = original_task.ignore_unreachable\n",
   "                if not ignore_unreachable:\n",
   "                    self._tqm._unreachable_hosts[original_host.name] = True\n",
   "                    iterator._play._removed_hosts.append(original_host.name)\n",
   "                    self._tqm._stats.increment('skipped', original_host.name)\n",
   "                    task_result._result['skip_reason'] = 'Host %s is unreachable' % original_host.name\n",
   "                self._tqm._stats.increment('dark', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_unreachable', task_result)\n",
   "            elif task_result.is_skipped():\n",
   "                self._tqm._stats.increment('skipped', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_skipped', task_result)\n",
   "                role_ran = True\n",
   "                if original_task.loop:\n",
   "                    result_items = task_result._result.get('results', [])\n",
   "                    result_items = [task_result._result]\n",
   "                for result_item in result_items:\n",
   "                    if '_ansible_notify' in result_item:\n",
   "                        if task_result.is_changed():\n",
   "                            for handler_name in result_item['_ansible_notify']:\n",
   "                                found = False\n",
   "                                target_handler = search_handler_blocks_by_name(handler_name, iterator._play.handlers)\n",
   "                                if target_handler is not None:\n",
   "                                    found = True\n",
   "                                    if target_handler.notify_host(original_host):\n",
   "                                        self._tqm.send_callback('v2_playbook_on_notify', target_handler, original_host)\n",
   "                                for listening_handler_block in iterator._play.handlers:\n",
   "                                    for listening_handler in listening_handler_block.block:\n",
   "                                        listeners = getattr(listening_handler, 'listen', []) or []\n",
   "                                        if not listeners:\n",
   "                                        listeners = listening_handler.get_validated_value(\n",
   "                                            'listen', listening_handler._valid_attrs['listen'], listeners, handler_templar\n",
   "                                        if handler_name not in listeners:\n",
   "                                            found = True\n",
   "                                        if listening_handler.notify_host(original_host):\n",
   "                                            self._tqm.send_callback('v2_playbook_on_notify', listening_handler, original_host)\n",
   "                                if not found:\n",
   "                                    msg = (\"The requested handler '%s' was not found in either the main handlers list nor in the listening \"\n",
   "                                           \"handlers list\" % handler_name)\n",
   "                                        raise AnsibleError(msg)\n",
   "                                        display.warning(msg)\n",
   "                    if 'add_host' in result_item:\n",
   "                        new_host_info = result_item.get('add_host', dict())\n",
   "                        self._add_host(new_host_info, result_item)\n",
   "                        post_process_whens(result_item, original_task, handler_templar)\n",
   "                    elif 'add_group' in result_item:\n",
   "                        self._add_group(original_host, result_item)\n",
   "                        post_process_whens(result_item, original_task, handler_templar)\n",
   "                    if 'ansible_facts' in result_item:\n",
   "                        if original_task.delegate_to is not None and original_task.delegate_facts:\n",
   "                            host_list = self.get_delegated_hosts(result_item, original_task)\n",
   "                            self._set_always_delegated_facts(result_item, original_task)\n",
   "                            host_list = self.get_task_hosts(iterator, original_host, original_task)\n",
   "                        if original_task.action == 'include_vars':\n",
   "                            for (var_name, var_value) in iteritems(result_item['ansible_facts']):\n",
   "                                for target_host in host_list:\n",
   "                                    self._variable_manager.set_host_variable(target_host, var_name, var_value)\n",
   "                            cacheable = result_item.pop('_ansible_facts_cacheable', False)\n",
   "                            for target_host in host_list:\n",
   "                                if original_task.action != 'set_fact' or cacheable:\n",
   "                                    self._variable_manager.set_host_facts(target_host, result_item['ansible_facts'].copy())\n",
   "                                if original_task.action == 'set_fact':\n",
   "                                    self._variable_manager.set_nonpersistent_facts(target_host, result_item['ansible_facts'].copy())\n",
   "                    if 'ansible_stats' in result_item and 'data' in result_item['ansible_stats'] and result_item['ansible_stats']['data']:\n",
   "                        if 'per_host' not in result_item['ansible_stats'] or result_item['ansible_stats']['per_host']:\n",
   "                            host_list = self.get_task_hosts(iterator, original_host, original_task)\n",
   "                            host_list = [None]\n",
   "                        data = result_item['ansible_stats']['data']\n",
   "                        aggregate = 'aggregate' in result_item['ansible_stats'] and result_item['ansible_stats']['aggregate']\n",
   "                        for myhost in host_list:\n",
   "                            for k in data.keys():\n",
   "                                if aggregate:\n",
   "                                    self._tqm._stats.update_custom_stats(k, data[k], myhost)\n",
   "                                    self._tqm._stats.set_custom_stats(k, data[k], myhost)\n",
   "                if 'diff' in task_result._result:\n",
   "                    if self._diff or getattr(original_task, 'diff', False):\n",
   "                        self._tqm.send_callback('v2_on_file_diff', task_result)\n",
   "                if not isinstance(original_task, TaskInclude):\n",
   "                    self._tqm._stats.increment('ok', original_host.name)\n",
   "                    if 'changed' in task_result._result and task_result._result['changed']:\n",
   "                        self._tqm._stats.increment('changed', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_ok', task_result)\n",
   "            if original_task.register:\n",
   "                host_list = self.get_task_hosts(iterator, original_host, original_task)\n",
   "                clean_copy = strip_internal_keys(module_response_deepcopy(task_result._result))\n",
   "                if 'invocation' in clean_copy:\n",
   "                    del clean_copy['invocation']\n",
   "                for target_host in host_list:\n",
   "                    self._variable_manager.set_nonpersistent_facts(target_host, {original_task.register: clean_copy})\n",
   "            if original_host.name in self._blocked_hosts:\n",
   "                del self._blocked_hosts[original_host.name]\n",
   "            if original_task._role is not None and role_ran:  # TODO:  and original_task.action != 'include_role':?\n",
   "                for (entry, role_obj) in iteritems(iterator._play.ROLE_CACHE[original_task._role.get_name()]):\n",
   "                    if role_obj._uuid == original_task._role._uuid:\n",
   "                        role_obj._had_task_run[original_host.name] = True\n",
   "            ret_results.append(task_result)\n",
   "            if one_pass or max_passes is not None and (cur_pass + 1) >= max_passes:\n",
   "            cur_pass += 1\n",
   "        return ret_results\n",
   "        ret_results = []\n",
   "        handler_results = 0\n",
   "        display.debug(\"waiting for handler results...\")\n",
   "               handler_results < len(notified_hosts) and\n",
   "            results = self._process_pending_results(iterator, do_handlers=True)\n",
   "            ret_results.extend(results)\n",
   "            handler_results += len([\n",
   "                r._host for r in results if r._host in notified_hosts and\n",
   "                r.task_name == handler.name])\n",
   "        display.debug(\"no more pending handlers, returning what we have\")\n",
   "        return ret_results\n",
   "        ret_results = []\n",
   "        display.debug(\"waiting for pending results...\")\n",
   "            results = self._process_pending_results(iterator)\n",
   "            ret_results.extend(results)\n",
   "        display.debug(\"no more pending results, returning what we have\")\n",
   "        return ret_results\n",
   "        changed = False\n",
   "            host_name = host_info.get('host_name')\n",
   "            if host_name not in self._inventory.hosts:\n",
   "                self._inventory.add_host(host_name, 'all')\n",
   "                self._hosts_cache_all.append(host_name)\n",
   "                changed = True\n",
   "            new_host = self._inventory.hosts.get(host_name)\n",
   "            new_host_vars = new_host.get_vars()\n",
   "            new_host_combined_vars = combine_vars(new_host_vars, host_info.get('host_vars', dict()))\n",
   "            if new_host_vars != new_host_combined_vars:\n",
   "                new_host.vars = new_host_combined_vars\n",
   "                changed = True\n",
   "            new_groups = host_info.get('groups', [])\n",
   "            for group_name in new_groups:\n",
   "                if group_name not in self._inventory.groups:\n",
   "                    group_name = self._inventory.add_group(group_name)\n",
   "                    changed = True\n",
   "                new_group = self._inventory.groups[group_name]\n",
   "                if new_group.add_host(self._inventory.hosts[host_name]):\n",
   "                    changed = True\n",
   "            if changed:\n",
   "            result_item['changed'] = changed\n",
   "        changed = False\n",
   "        real_host = self._inventory.hosts.get(host.name)\n",
   "        if real_host is None:\n",
   "            if host.name == self._inventory.localhost.name:\n",
   "                real_host = self._inventory.localhost\n",
   "                raise AnsibleError('%s cannot be matched in inventory' % host.name)\n",
   "        group_name = result_item.get('add_group')\n",
   "        parent_group_names = result_item.get('parent_groups', [])\n",
   "        if group_name not in self._inventory.groups:\n",
   "            group_name = self._inventory.add_group(group_name)\n",
   "        for name in parent_group_names:\n",
   "            if name not in self._inventory.groups:\n",
   "                self._inventory.add_group(name)\n",
   "                changed = True\n",
   "        group = self._inventory.groups[group_name]\n",
   "        for parent_group_name in parent_group_names:\n",
   "            parent_group = self._inventory.groups[parent_group_name]\n",
   "            new = parent_group.add_child_group(group)\n",
   "            if new and not changed:\n",
   "                changed = True\n",
   "        if real_host not in group.get_hosts():\n",
   "            changed = group.add_host(real_host)\n",
   "        if group not in real_host.get_groups():\n",
   "            changed = real_host.add_group(group)\n",
   "        if changed:\n",
   "        result_item['changed'] = changed\n",
   "        ti_copy = included_file._task.copy(exclude_parent=True)\n",
   "        ti_copy._parent = included_file._task._parent\n",
   "        temp_vars = ti_copy.vars.copy()\n",
   "        temp_vars.update(included_file._vars)\n",
   "        ti_copy.vars = temp_vars\n",
   "        return ti_copy\n",
   "        display.debug(\"loading included file: %s\" % included_file._filename)\n",
   "            data = self._loader.load_from_file(included_file._filename)\n",
   "            if data is None:\n",
   "            elif not isinstance(data, list):\n",
   "            ti_copy = self._copy_included_file(included_file)\n",
   "            tags = included_file._task.vars.pop('tags', [])\n",
   "            if isinstance(tags, string_types):\n",
   "                tags = tags.split(',')\n",
   "            if len(tags) > 0:\n",
   "                display.deprecated(\"You should not specify tags in the include parameters. All tags should be specified using the task-level option\",\n",
   "                included_file._task.tags = tags\n",
   "            block_list = load_list_of_blocks(\n",
   "                data,\n",
   "                parent_block=ti_copy.build_parent_block(),\n",
   "            for host in included_file._hosts:\n",
   "                self._tqm._stats.increment('ok', host.name)\n",
   "                reason = \"Could not find or access '%s' on the Ansible Controller.\" % to_text(e.file_name)\n",
   "                reason = to_text(e)\n",
   "            for host in included_file._hosts:\n",
   "                tr = TaskResult(host=host, task=included_file._task, return_data=dict(failed=True, reason=reason))\n",
   "                iterator.mark_host_failed(host)\n",
   "                self._tqm._failed_hosts[host.name] = True\n",
   "                self._tqm._stats.increment('failures', host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_failed', tr)\n",
   "        display.debug(\"done processing included file\")\n",
   "        return block_list\n",
   "        result = self._tqm.RUN_OK\n",
   "        for handler_block in iterator._play.handlers:\n",
   "            for handler in handler_block.block:\n",
   "                if handler.notified_hosts:\n",
   "                    result = self._do_handler_run(handler, handler.get_name(), iterator=iterator, play_context=play_context)\n",
   "                    if not result:\n",
   "        return result\n",
   "            notified_hosts = handler.notified_hosts[:]\n",
   "        failed_hosts = self._filter_notified_failed_hosts(iterator, notified_hosts)\n",
   "        notified_hosts = self._filter_notified_hosts(notified_hosts)\n",
   "        notified_hosts += failed_hosts\n",
   "        if len(notified_hosts) > 0:\n",
   "            saved_name = handler.name\n",
   "            handler.name = handler_name\n",
   "            self._tqm.send_callback('v2_playbook_on_handler_task_start', handler)\n",
   "            handler.name = saved_name\n",
   "        bypass_host_loop = False\n",
   "            action = plugin_loader.action_loader.get(handler.action, class_only=True)\n",
   "            if getattr(action, 'BYPASS_HOST_LOOP', False):\n",
   "                bypass_host_loop = True\n",
   "        host_results = []\n",
   "        for host in notified_hosts:\n",
   "            if not iterator.is_failed(host) or iterator._play.force_handlers:\n",
   "                task_vars = self._variable_manager.get_vars(play=iterator._play, host=host, task=handler,\n",
   "                self.add_tqm_variables(task_vars, play=iterator._play)\n",
   "                templar = Templar(loader=self._loader, variables=task_vars)\n",
   "                if not handler.cached_name:\n",
   "                    handler.name = templar.template(handler.name)\n",
   "                    handler.cached_name = True\n",
   "                self._queue_task(host, handler, task_vars, play_context)\n",
   "                if templar.template(handler.run_once) or bypass_host_loop:\n",
   "        host_results = self._wait_on_handler_results(iterator, handler, notified_hosts)\n",
   "        included_files = IncludedFile.process_include_results(\n",
   "            host_results,\n",
   "        result = True\n",
   "        if len(included_files) > 0:\n",
   "            for included_file in included_files:\n",
   "                    new_blocks = self._load_included_file(included_file, iterator=iterator, is_handler=True)\n",
   "                    for block in new_blocks:\n",
   "                        iterator._play.handlers.append(block)\n",
   "                        for task in block.block:\n",
   "                            task_name = task.get_name()\n",
   "                            display.debug(\"adding task '%s' included in handler '%s'\" % (task_name, handler_name))\n",
   "                            task.notified_hosts = included_file._hosts[:]\n",
   "                            result = self._do_handler_run(\n",
   "                                handler=task,\n",
   "                                handler_name=task_name,\n",
   "                                play_context=play_context,\n",
   "                                notified_hosts=included_file._hosts[:],\n",
   "                            if not result:\n",
   "                    for host in included_file._hosts:\n",
   "                        iterator.mark_host_failed(host)\n",
   "                        self._tqm._failed_hosts[host.name] = True\n",
   "                    display.warning(to_text(e))\n",
   "        handler.notified_hosts = [\n",
   "            h for h in handler.notified_hosts\n",
   "            if h not in notified_hosts]\n",
   "        display.debug(\"done running handlers, result is: %s\" % result)\n",
   "        return result\n",
   "        return notified_hosts[:]\n",
   "        ret = False\n",
   "        msg = u'Perform task: %s ' % task\n",
   "        if host:\n",
   "            msg += u'on %s ' % host\n",
   "        msg += u'(N)o/(y)es/(c)ontinue: '\n",
   "        resp = display.prompt(msg)\n",
   "        if resp.lower() in ['y', 'yes']:\n",
   "            display.debug(\"User ran task\")\n",
   "            ret = True\n",
   "        elif resp.lower() in ['c', 'continue']:\n",
   "            display.debug(\"User ran task and canceled step mode\")\n",
   "            ret = True\n",
   "            display.debug(\"User skipped task\")\n",
   "        display.banner(msg)\n",
   "        return ret\n",
   "        display.warning(\"%s task does not support when conditional\" % task_name)\n",
   "        meta_action = task.args.get('_raw_params')\n",
   "        def _evaluate_conditional(h):\n",
   "            all_vars = self._variable_manager.get_vars(play=iterator._play, host=h, task=task,\n",
   "            templar = Templar(loader=self._loader, variables=all_vars)\n",
   "            return task.evaluate_conditional(templar, all_vars)\n",
   "        skipped = False\n",
   "        msg = ''\n",
   "        if meta_action == 'noop':\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            msg = \"noop\"\n",
   "        elif meta_action == 'flush_handlers':\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            self._flushed_hosts[target_host] = True\n",
   "            self.run_handlers(iterator, play_context)\n",
   "            self._flushed_hosts[target_host] = False\n",
   "            msg = \"ran handlers\"\n",
   "        elif meta_action == 'refresh_inventory' or self.flush_cache:\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            msg = \"inventory successfully refreshed\"\n",
   "        elif meta_action == 'clear_facts':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                for host in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                    hostname = host.get_name()\n",
   "                    self._variable_manager.clear_facts(hostname)\n",
   "                msg = \"facts cleared\"\n",
   "                skipped = True\n",
   "        elif meta_action == 'clear_host_errors':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                for host in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                    self._tqm._failed_hosts.pop(host.name, False)\n",
   "                    self._tqm._unreachable_hosts.pop(host.name, False)\n",
   "                    iterator._host_states[host.name].fail_state = iterator.FAILED_NONE\n",
   "                msg = \"cleared host errors\"\n",
   "                skipped = True\n",
   "        elif meta_action == 'end_play':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                for host in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                    if host.name not in self._tqm._unreachable_hosts:\n",
   "                        iterator._host_states[host.name].run_state = iterator.ITERATING_COMPLETE\n",
   "                msg = \"ending play\"\n",
   "        elif meta_action == 'end_host':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                iterator._host_states[target_host.name].run_state = iterator.ITERATING_COMPLETE\n",
   "                iterator._play._removed_hosts.append(target_host.name)\n",
   "                msg = \"ending play for %s\" % target_host.name\n",
   "                skipped = True\n",
   "                msg = \"end_host conditional evaluated to false, continuing execution for %s\" % target_host.name\n",
   "        elif meta_action == 'reset_connection':\n",
   "            all_vars = self._variable_manager.get_vars(play=iterator._play, host=target_host, task=task,\n",
   "            templar = Templar(loader=self._loader, variables=all_vars)\n",
   "            play_context = play_context.set_task_and_variable_override(task=task, variables=all_vars, templar=templar)\n",
   "            play_context.post_validate(templar=templar)\n",
   "            if not play_context.remote_addr:\n",
   "                play_context.remote_addr = target_host.address\n",
   "            play_context.update_vars(all_vars)\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            if target_host in self._active_connections:\n",
   "                connection = Connection(self._active_connections[target_host])\n",
   "                del self._active_connections[target_host]\n",
   "                connection = plugin_loader.connection_loader.get(play_context.connection, play_context, os.devnull)\n",
   "                play_context.set_attributes_from_plugin(connection)\n",
   "            if connection:\n",
   "                    connection.reset()\n",
   "                    msg = 'reset connection'\n",
   "                    display.debug(\"got an error while closing persistent connection: %s\" % e)\n",
   "                msg = 'no connection, nothing to reset'\n",
   "            raise AnsibleError(\"invalid meta action requested: %s\" % meta_action, obj=task._ds)\n",
   "        result = {'msg': msg}\n",
   "        if skipped:\n",
   "            result['skipped'] = True\n",
   "            result['changed'] = False\n",
   "        display.vv(\"META: %s\" % msg)\n",
   "        return [TaskResult(target_host, task, result)]\n",
   "        hosts_left = []\n",
   "        for host in self._hosts_cache:\n",
   "            if host not in self._tqm._unreachable_hosts:\n",
   "                    hosts_left.append(self._inventory.hosts[host])\n",
   "                    hosts_left.append(self._inventory.get_host(host))\n",
   "        return hosts_left\n",
   "        for r in results:\n",
   "            if 'args' in r._task_fields:\n",
   "                socket_path = r._task_fields['args'].get('_ansible_socket')\n",
   "                if socket_path:\n",
   "                    if r._host not in self._active_connections:\n",
   "                        self._active_connections[r._host] = socket_path\n",
   "    EXIT = 3\n",
   "    def __init__(self, result=EXIT):\n",
   "        self.result = result\n",
   "        self.prompt = '[%s] %s (debug)> ' % (host, task)\n",
   "        self.scope['task'] = task\n",
   "        self.scope['task_vars'] = task_vars\n",
   "        self.scope['host'] = host\n",
   "        self.scope['play_context'] = play_context\n",
   "        self.scope['result'] = result\n",
   "        self.next_action = next_action\n",
   "        display.display('User interrupted execution')\n",
   "    do_r = do_redo\n",
   "        templar = Templar(None, shared_loader_obj=None, variables=self.scope['task_vars'])\n",
   "        task = self.scope['task']\n",
   "        task = task.load_data(task._ds)\n",
   "        task.post_validate(templar)\n",
   "        self.scope['task'] = task\n",
   "            display.display('***%s:%s' % (exc_type_name, repr(v)))\n",
   "            display.display(pprint.pformat(result))\n",
   "            display.display('***%s:%s' % (exc_type_name, repr(v)))\n"
  ]
 },
 "134": {
  "name": "do_u",
  "type": "function",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/plugins/strategy/__init__.py",
  "lineno": "1328",
  "column": "4",
  "context": "e(templar)\n        self.scope['task'] = task\n\n    do_u = do_update_task\n\n    def evaluate(self, args):\n        try:\n      ",
  "context_lines": "        task = self.scope['task']\n        task = task.load_data(task._ds)\n        task.post_validate(templar)\n        self.scope['task'] = task\n\n    do_u = do_update_task\n\n    def evaluate(self, args):\n        try:\n            return eval(args, globals(), self.scope)\n",
  "slicing": [
   "display = Display()\n",
   "ALWAYS_DELEGATE_FACT_PREFIXES = frozenset((\n",
   "_sentinel = StrategySentinel()\n",
   "def post_process_whens(result, task, templar):\n",
   "    cond = None\n",
   "        cond = Conditional(loader=templar._loader)\n",
   "        cond.when = task.changed_when\n",
   "        result['changed'] = cond.evaluate_conditional(templar, templar.available_variables)\n",
   "        if cond is None:\n",
   "            cond = Conditional(loader=templar._loader)\n",
   "        cond.when = task.failed_when\n",
   "        failed_when_result = cond.evaluate_conditional(templar, templar.available_variables)\n",
   "        result['failed_when_result'] = result['failed'] = failed_when_result\n",
   "            result = strategy._final_q.get()\n",
   "            if isinstance(result, StrategySentinel):\n",
   "                if 'listen' in result._task_fields:\n",
   "                    strategy._handler_results.append(result)\n",
   "                    strategy._results.append(result)\n",
   "        status_to_stats_map = (\n",
   "        prev_host_states = iterator._host_states.copy()\n",
   "        results = func(self, iterator, one_pass=one_pass, max_passes=max_passes, do_handlers=do_handlers)\n",
   "        _processed_results = []\n",
   "        for result in results:\n",
   "            task = result._task\n",
   "            host = result._host\n",
   "            _queued_task_args = self._queued_task_cache.pop((host.name, task._uuid), None)\n",
   "            task_vars = _queued_task_args['task_vars']\n",
   "            play_context = _queued_task_args['play_context']\n",
   "                prev_host_state = prev_host_states[host.name]\n",
   "                prev_host_state = iterator.get_host_state(host)\n",
   "            while result.needs_debugger(globally_enabled=self.debugger_active):\n",
   "                next_action = NextAction()\n",
   "                dbg = Debugger(task, host, task_vars, play_context, result, next_action)\n",
   "                dbg.cmdloop()\n",
   "                if next_action.result == NextAction.REDO:\n",
   "                    iterator._host_states[host.name] = prev_host_state\n",
   "                    for method, what in status_to_stats_map:\n",
   "                        if getattr(result, method)():\n",
   "                            self._tqm._stats.decrement(what, host.name)\n",
   "                    self._tqm._stats.decrement('ok', host.name)\n",
   "                    self._queue_task(host, task, task_vars, play_context)\n",
   "                    _processed_results.extend(debug_closure(func)(self, iterator, one_pass))\n",
   "                elif next_action.result == NextAction.CONTINUE:\n",
   "                    _processed_results.append(result)\n",
   "                elif next_action.result == NextAction.EXIT:\n",
   "                _processed_results.append(result)\n",
   "        return _processed_results\n",
   "        self._display = display\n",
   "            _pattern = 'all'\n",
   "            _pattern = play.hosts or 'all'\n",
   "        self._hosts_cache_all = [h.name for h in self._inventory.get_hosts(pattern=_pattern, ignore_restrictions=True)]\n",
   "        self._hosts_cache = [h.name for h in self._inventory.get_hosts(play.hosts, order=play.order)]\n",
   "        for sock in itervalues(self._active_connections):\n",
   "                conn = Connection(sock)\n",
   "                conn.reset()\n",
   "                display.debug(\"got an error while closing persistent connection: %s\" % e)\n",
   "        self._final_q.put(_sentinel)\n",
   "        for host in self._hosts_cache:\n",
   "            if host not in self._tqm._unreachable_hosts:\n",
   "                    iterator.get_next_task_for_host(self._inventory.hosts[host])\n",
   "                    iterator.get_next_task_for_host(self._inventory.get_host(host))\n",
   "        failed_hosts = iterator.get_failed_hosts()\n",
   "        unreachable_hosts = self._tqm._unreachable_hosts.keys()\n",
   "        display.debug(\"running handlers\")\n",
   "        handler_result = self.run_handlers(iterator, play_context)\n",
   "        if isinstance(handler_result, bool) and not handler_result:\n",
   "            result |= self._tqm.RUN_ERROR\n",
   "        elif not handler_result:\n",
   "            result |= handler_result\n",
   "        failed_hosts = set(failed_hosts).union(iterator.get_failed_hosts())\n",
   "        unreachable_hosts = set(unreachable_hosts).union(self._tqm._unreachable_hosts.keys())\n",
   "        if not isinstance(result, bool) and result != self._tqm.RUN_OK:\n",
   "            return result\n",
   "        elif len(unreachable_hosts) > 0:\n",
   "        elif len(failed_hosts) > 0:\n",
   "        ignore = set(self._tqm._failed_hosts).union(self._tqm._unreachable_hosts)\n",
   "        return [host for host in self._hosts_cache if host not in ignore]\n",
   "        return [host for host in self._hosts_cache if host in self._tqm._failed_hosts]\n",
   "        display.debug(\"entering _queue_task() for %s/%s\" % (host.name, task.action))\n",
   "        if task.action not in action_write_locks.action_write_locks:\n",
   "            display.debug('Creating lock for %s' % task.action)\n",
   "            action_write_locks.action_write_locks[task.action] = Lock()\n",
   "        templar = Templar(loader=self._loader, variables=task_vars)\n",
   "            throttle = int(templar.template(task.throttle))\n",
   "            raise AnsibleError(\"Failed to convert the throttle value to an integer.\", obj=task._ds, orig_exc=e)\n",
   "            rewind_point = len(self._workers)\n",
   "            if throttle > 0 and self.ALLOW_BASE_THROTTLING:\n",
   "                if task.run_once:\n",
   "                    display.debug(\"Ignoring 'throttle' as 'run_once' is also set for '%s'\" % task.get_name())\n",
   "                    if throttle <= rewind_point:\n",
   "                        display.debug(\"task: %s, throttle: %d\" % (task.get_name(), throttle))\n",
   "                        rewind_point = throttle\n",
   "            queued = False\n",
   "            starting_worker = self._cur_worker\n",
   "                if self._cur_worker >= rewind_point:\n",
   "                worker_prc = self._workers[self._cur_worker]\n",
   "                if worker_prc is None or not worker_prc.is_alive():\n",
   "                    self._queued_task_cache[(host.name, task._uuid)] = {\n",
   "                        'host': host,\n",
   "                        'task': task,\n",
   "                        'task_vars': task_vars,\n",
   "                        'play_context': play_context\n",
   "                    worker_prc = WorkerProcess(self._final_q, task_vars, host, task, play_context, self._loader, self._variable_manager, plugin_loader)\n",
   "                    self._workers[self._cur_worker] = worker_prc\n",
   "                    self._tqm.send_callback('v2_runner_on_start', host, task)\n",
   "                    worker_prc.start()\n",
   "                    display.debug(\"worker is %d (out of %d available)\" % (self._cur_worker + 1, len(self._workers)))\n",
   "                    queued = True\n",
   "                if self._cur_worker >= rewind_point:\n",
   "                if queued:\n",
   "                elif self._cur_worker == starting_worker:\n",
   "            if isinstance(task, Handler):\n",
   "            display.debug(\"got an error while queuing: %s\" % e)\n",
   "        display.debug(\"exiting _queue_task() for %s/%s\" % (host.name, task.action))\n",
   "        if task.run_once:\n",
   "            host_list = [host for host in self._hosts_cache if host not in self._tqm._unreachable_hosts]\n",
   "            host_list = [task_host.name]\n",
   "        return host_list\n",
   "        host_name = result.get('_ansible_delegated_vars', {}).get('ansible_delegated_host', None)\n",
   "        return [host_name or task.delegate_to]\n",
   "        if task.delegate_to is None:\n",
   "        facts = result['ansible_facts']\n",
   "        always_keys = set()\n",
   "        _add = always_keys.add\n",
   "        for fact_key in facts:\n",
   "            for always_key in ALWAYS_DELEGATE_FACT_PREFIXES:\n",
   "                if fact_key.startswith(always_key):\n",
   "                    _add(fact_key)\n",
   "        if always_keys:\n",
   "            _pop = facts.pop\n",
   "            always_facts = {\n",
   "                'ansible_facts': dict((k, _pop(k)) for k in list(facts) if k in always_keys)\n",
   "            host_list = self.get_delegated_hosts(result, task)\n",
   "            _set_host_facts = self._variable_manager.set_host_facts\n",
   "            for target_host in host_list:\n",
   "                _set_host_facts(target_host, always_facts)\n",
   "        ret_results = []\n",
   "        handler_templar = Templar(self._loader)\n",
   "        def get_original_host(host_name):\n",
   "            host_name = to_text(host_name)\n",
   "            if host_name in self._inventory.hosts:\n",
   "                return self._inventory.hosts[host_name]\n",
   "                return self._inventory.get_host(host_name)\n",
   "        def search_handler_blocks_by_name(handler_name, handler_blocks):\n",
   "            for handler_block in reversed(handler_blocks):\n",
   "                for handler_task in handler_block.block:\n",
   "                    if handler_task.name:\n",
   "                        if not handler_task.cached_name:\n",
   "                            if handler_templar.is_template(handler_task.name):\n",
   "                                handler_templar.available_variables = self._variable_manager.get_vars(play=iterator._play,\n",
   "                                                                                                      task=handler_task,\n",
   "                                handler_task.name = handler_templar.template(handler_task.name)\n",
   "                            handler_task.cached_name = True\n",
   "                            candidates = (\n",
   "                                handler_task.name,\n",
   "                                handler_task.get_name(include_role_fqcn=False),\n",
   "                                handler_task.get_name(include_role_fqcn=True),\n",
   "                            if handler_name in candidates:\n",
   "                                return handler_task\n",
   "        cur_pass = 0\n",
   "                    task_result = self._handler_results.popleft()\n",
   "                    task_result = self._results.popleft()\n",
   "            original_host = get_original_host(task_result._host)\n",
   "            queue_cache_entry = (original_host.name, task_result._task)\n",
   "            found_task = self._queued_task_cache.get(queue_cache_entry)['task']\n",
   "            original_task = found_task.copy(exclude_parent=True, exclude_tasks=True)\n",
   "            original_task._parent = found_task._parent\n",
   "            original_task.from_attrs(task_result._task_fields)\n",
   "            task_result._host = original_host\n",
   "            task_result._task = original_task\n",
   "            if '_ansible_retry' in task_result._result:\n",
   "                self._tqm.send_callback('v2_runner_retry', task_result)\n",
   "            elif '_ansible_item_result' in task_result._result:\n",
   "                if task_result.is_failed() or task_result.is_unreachable():\n",
   "                    self._tqm.send_callback('v2_runner_item_on_failed', task_result)\n",
   "                elif task_result.is_skipped():\n",
   "                    self._tqm.send_callback('v2_runner_item_on_skipped', task_result)\n",
   "                    if 'diff' in task_result._result:\n",
   "                        if self._diff or getattr(original_task, 'diff', False):\n",
   "                            self._tqm.send_callback('v2_on_file_diff', task_result)\n",
   "                    self._tqm.send_callback('v2_runner_item_on_ok', task_result)\n",
   "            role_ran = False\n",
   "            if task_result.is_failed():\n",
   "                role_ran = True\n",
   "                ignore_errors = original_task.ignore_errors\n",
   "                if not ignore_errors:\n",
   "                    display.debug(\"marking %s as failed\" % original_host.name)\n",
   "                    if original_task.run_once:\n",
   "                        for h in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                            if h.name not in self._tqm._unreachable_hosts:\n",
   "                                state, _ = iterator.get_next_task_for_host(h, peek=True)\n",
   "                                iterator.mark_host_failed(h)\n",
   "                                state, new_task = iterator.get_next_task_for_host(h, peek=True)\n",
   "                        iterator.mark_host_failed(original_host)\n",
   "                    state, _ = iterator.get_next_task_for_host(original_host, peek=True)\n",
   "                    if iterator.is_failed(original_host) and state and state.run_state == iterator.ITERATING_COMPLETE:\n",
   "                        self._tqm._failed_hosts[original_host.name] = True\n",
   "                    if state and iterator.get_active_state(state).run_state == iterator.ITERATING_RESCUE:\n",
   "                        self._tqm._stats.increment('rescued', original_host.name)\n",
   "                            original_host.name,\n",
   "                                ansible_failed_task=original_task.serialize(),\n",
   "                                ansible_failed_result=task_result._result,\n",
   "                        self._tqm._stats.increment('failures', original_host.name)\n",
   "                    self._tqm._stats.increment('ok', original_host.name)\n",
   "                    self._tqm._stats.increment('ignored', original_host.name)\n",
   "                    if 'changed' in task_result._result and task_result._result['changed']:\n",
   "                        self._tqm._stats.increment('changed', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_failed', task_result, ignore_errors=ignore_errors)\n",
   "            elif task_result.is_unreachable():\n",
   "                ignore_unreachable = original_task.ignore_unreachable\n",
   "                if not ignore_unreachable:\n",
   "                    self._tqm._unreachable_hosts[original_host.name] = True\n",
   "                    iterator._play._removed_hosts.append(original_host.name)\n",
   "                    self._tqm._stats.increment('skipped', original_host.name)\n",
   "                    task_result._result['skip_reason'] = 'Host %s is unreachable' % original_host.name\n",
   "                self._tqm._stats.increment('dark', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_unreachable', task_result)\n",
   "            elif task_result.is_skipped():\n",
   "                self._tqm._stats.increment('skipped', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_skipped', task_result)\n",
   "                role_ran = True\n",
   "                if original_task.loop:\n",
   "                    result_items = task_result._result.get('results', [])\n",
   "                    result_items = [task_result._result]\n",
   "                for result_item in result_items:\n",
   "                    if '_ansible_notify' in result_item:\n",
   "                        if task_result.is_changed():\n",
   "                            for handler_name in result_item['_ansible_notify']:\n",
   "                                found = False\n",
   "                                target_handler = search_handler_blocks_by_name(handler_name, iterator._play.handlers)\n",
   "                                if target_handler is not None:\n",
   "                                    found = True\n",
   "                                    if target_handler.notify_host(original_host):\n",
   "                                        self._tqm.send_callback('v2_playbook_on_notify', target_handler, original_host)\n",
   "                                for listening_handler_block in iterator._play.handlers:\n",
   "                                    for listening_handler in listening_handler_block.block:\n",
   "                                        listeners = getattr(listening_handler, 'listen', []) or []\n",
   "                                        if not listeners:\n",
   "                                        listeners = listening_handler.get_validated_value(\n",
   "                                            'listen', listening_handler._valid_attrs['listen'], listeners, handler_templar\n",
   "                                        if handler_name not in listeners:\n",
   "                                            found = True\n",
   "                                        if listening_handler.notify_host(original_host):\n",
   "                                            self._tqm.send_callback('v2_playbook_on_notify', listening_handler, original_host)\n",
   "                                if not found:\n",
   "                                    msg = (\"The requested handler '%s' was not found in either the main handlers list nor in the listening \"\n",
   "                                           \"handlers list\" % handler_name)\n",
   "                                        raise AnsibleError(msg)\n",
   "                                        display.warning(msg)\n",
   "                    if 'add_host' in result_item:\n",
   "                        new_host_info = result_item.get('add_host', dict())\n",
   "                        self._add_host(new_host_info, result_item)\n",
   "                        post_process_whens(result_item, original_task, handler_templar)\n",
   "                    elif 'add_group' in result_item:\n",
   "                        self._add_group(original_host, result_item)\n",
   "                        post_process_whens(result_item, original_task, handler_templar)\n",
   "                    if 'ansible_facts' in result_item:\n",
   "                        if original_task.delegate_to is not None and original_task.delegate_facts:\n",
   "                            host_list = self.get_delegated_hosts(result_item, original_task)\n",
   "                            self._set_always_delegated_facts(result_item, original_task)\n",
   "                            host_list = self.get_task_hosts(iterator, original_host, original_task)\n",
   "                        if original_task.action == 'include_vars':\n",
   "                            for (var_name, var_value) in iteritems(result_item['ansible_facts']):\n",
   "                                for target_host in host_list:\n",
   "                                    self._variable_manager.set_host_variable(target_host, var_name, var_value)\n",
   "                            cacheable = result_item.pop('_ansible_facts_cacheable', False)\n",
   "                            for target_host in host_list:\n",
   "                                if original_task.action != 'set_fact' or cacheable:\n",
   "                                    self._variable_manager.set_host_facts(target_host, result_item['ansible_facts'].copy())\n",
   "                                if original_task.action == 'set_fact':\n",
   "                                    self._variable_manager.set_nonpersistent_facts(target_host, result_item['ansible_facts'].copy())\n",
   "                    if 'ansible_stats' in result_item and 'data' in result_item['ansible_stats'] and result_item['ansible_stats']['data']:\n",
   "                        if 'per_host' not in result_item['ansible_stats'] or result_item['ansible_stats']['per_host']:\n",
   "                            host_list = self.get_task_hosts(iterator, original_host, original_task)\n",
   "                            host_list = [None]\n",
   "                        data = result_item['ansible_stats']['data']\n",
   "                        aggregate = 'aggregate' in result_item['ansible_stats'] and result_item['ansible_stats']['aggregate']\n",
   "                        for myhost in host_list:\n",
   "                            for k in data.keys():\n",
   "                                if aggregate:\n",
   "                                    self._tqm._stats.update_custom_stats(k, data[k], myhost)\n",
   "                                    self._tqm._stats.set_custom_stats(k, data[k], myhost)\n",
   "                if 'diff' in task_result._result:\n",
   "                    if self._diff or getattr(original_task, 'diff', False):\n",
   "                        self._tqm.send_callback('v2_on_file_diff', task_result)\n",
   "                if not isinstance(original_task, TaskInclude):\n",
   "                    self._tqm._stats.increment('ok', original_host.name)\n",
   "                    if 'changed' in task_result._result and task_result._result['changed']:\n",
   "                        self._tqm._stats.increment('changed', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_ok', task_result)\n",
   "            if original_task.register:\n",
   "                host_list = self.get_task_hosts(iterator, original_host, original_task)\n",
   "                clean_copy = strip_internal_keys(module_response_deepcopy(task_result._result))\n",
   "                if 'invocation' in clean_copy:\n",
   "                    del clean_copy['invocation']\n",
   "                for target_host in host_list:\n",
   "                    self._variable_manager.set_nonpersistent_facts(target_host, {original_task.register: clean_copy})\n",
   "            if original_host.name in self._blocked_hosts:\n",
   "                del self._blocked_hosts[original_host.name]\n",
   "            if original_task._role is not None and role_ran:  # TODO:  and original_task.action != 'include_role':?\n",
   "                for (entry, role_obj) in iteritems(iterator._play.ROLE_CACHE[original_task._role.get_name()]):\n",
   "                    if role_obj._uuid == original_task._role._uuid:\n",
   "                        role_obj._had_task_run[original_host.name] = True\n",
   "            ret_results.append(task_result)\n",
   "            if one_pass or max_passes is not None and (cur_pass + 1) >= max_passes:\n",
   "            cur_pass += 1\n",
   "        return ret_results\n",
   "        ret_results = []\n",
   "        handler_results = 0\n",
   "        display.debug(\"waiting for handler results...\")\n",
   "               handler_results < len(notified_hosts) and\n",
   "            results = self._process_pending_results(iterator, do_handlers=True)\n",
   "            ret_results.extend(results)\n",
   "            handler_results += len([\n",
   "                r._host for r in results if r._host in notified_hosts and\n",
   "                r.task_name == handler.name])\n",
   "        display.debug(\"no more pending handlers, returning what we have\")\n",
   "        return ret_results\n",
   "        ret_results = []\n",
   "        display.debug(\"waiting for pending results...\")\n",
   "            results = self._process_pending_results(iterator)\n",
   "            ret_results.extend(results)\n",
   "        display.debug(\"no more pending results, returning what we have\")\n",
   "        return ret_results\n",
   "        changed = False\n",
   "            host_name = host_info.get('host_name')\n",
   "            if host_name not in self._inventory.hosts:\n",
   "                self._inventory.add_host(host_name, 'all')\n",
   "                self._hosts_cache_all.append(host_name)\n",
   "                changed = True\n",
   "            new_host = self._inventory.hosts.get(host_name)\n",
   "            new_host_vars = new_host.get_vars()\n",
   "            new_host_combined_vars = combine_vars(new_host_vars, host_info.get('host_vars', dict()))\n",
   "            if new_host_vars != new_host_combined_vars:\n",
   "                new_host.vars = new_host_combined_vars\n",
   "                changed = True\n",
   "            new_groups = host_info.get('groups', [])\n",
   "            for group_name in new_groups:\n",
   "                if group_name not in self._inventory.groups:\n",
   "                    group_name = self._inventory.add_group(group_name)\n",
   "                    changed = True\n",
   "                new_group = self._inventory.groups[group_name]\n",
   "                if new_group.add_host(self._inventory.hosts[host_name]):\n",
   "                    changed = True\n",
   "            if changed:\n",
   "            result_item['changed'] = changed\n",
   "        changed = False\n",
   "        real_host = self._inventory.hosts.get(host.name)\n",
   "        if real_host is None:\n",
   "            if host.name == self._inventory.localhost.name:\n",
   "                real_host = self._inventory.localhost\n",
   "                raise AnsibleError('%s cannot be matched in inventory' % host.name)\n",
   "        group_name = result_item.get('add_group')\n",
   "        parent_group_names = result_item.get('parent_groups', [])\n",
   "        if group_name not in self._inventory.groups:\n",
   "            group_name = self._inventory.add_group(group_name)\n",
   "        for name in parent_group_names:\n",
   "            if name not in self._inventory.groups:\n",
   "                self._inventory.add_group(name)\n",
   "                changed = True\n",
   "        group = self._inventory.groups[group_name]\n",
   "        for parent_group_name in parent_group_names:\n",
   "            parent_group = self._inventory.groups[parent_group_name]\n",
   "            new = parent_group.add_child_group(group)\n",
   "            if new and not changed:\n",
   "                changed = True\n",
   "        if real_host not in group.get_hosts():\n",
   "            changed = group.add_host(real_host)\n",
   "        if group not in real_host.get_groups():\n",
   "            changed = real_host.add_group(group)\n",
   "        if changed:\n",
   "        result_item['changed'] = changed\n",
   "        ti_copy = included_file._task.copy(exclude_parent=True)\n",
   "        ti_copy._parent = included_file._task._parent\n",
   "        temp_vars = ti_copy.vars.copy()\n",
   "        temp_vars.update(included_file._vars)\n",
   "        ti_copy.vars = temp_vars\n",
   "        return ti_copy\n",
   "        display.debug(\"loading included file: %s\" % included_file._filename)\n",
   "            data = self._loader.load_from_file(included_file._filename)\n",
   "            if data is None:\n",
   "            elif not isinstance(data, list):\n",
   "            ti_copy = self._copy_included_file(included_file)\n",
   "            tags = included_file._task.vars.pop('tags', [])\n",
   "            if isinstance(tags, string_types):\n",
   "                tags = tags.split(',')\n",
   "            if len(tags) > 0:\n",
   "                display.deprecated(\"You should not specify tags in the include parameters. All tags should be specified using the task-level option\",\n",
   "                included_file._task.tags = tags\n",
   "            block_list = load_list_of_blocks(\n",
   "                data,\n",
   "                parent_block=ti_copy.build_parent_block(),\n",
   "            for host in included_file._hosts:\n",
   "                self._tqm._stats.increment('ok', host.name)\n",
   "                reason = \"Could not find or access '%s' on the Ansible Controller.\" % to_text(e.file_name)\n",
   "                reason = to_text(e)\n",
   "            for host in included_file._hosts:\n",
   "                tr = TaskResult(host=host, task=included_file._task, return_data=dict(failed=True, reason=reason))\n",
   "                iterator.mark_host_failed(host)\n",
   "                self._tqm._failed_hosts[host.name] = True\n",
   "                self._tqm._stats.increment('failures', host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_failed', tr)\n",
   "        display.debug(\"done processing included file\")\n",
   "        return block_list\n",
   "        result = self._tqm.RUN_OK\n",
   "        for handler_block in iterator._play.handlers:\n",
   "            for handler in handler_block.block:\n",
   "                if handler.notified_hosts:\n",
   "                    result = self._do_handler_run(handler, handler.get_name(), iterator=iterator, play_context=play_context)\n",
   "                    if not result:\n",
   "        return result\n",
   "            notified_hosts = handler.notified_hosts[:]\n",
   "        failed_hosts = self._filter_notified_failed_hosts(iterator, notified_hosts)\n",
   "        notified_hosts = self._filter_notified_hosts(notified_hosts)\n",
   "        notified_hosts += failed_hosts\n",
   "        if len(notified_hosts) > 0:\n",
   "            saved_name = handler.name\n",
   "            handler.name = handler_name\n",
   "            self._tqm.send_callback('v2_playbook_on_handler_task_start', handler)\n",
   "            handler.name = saved_name\n",
   "        bypass_host_loop = False\n",
   "            action = plugin_loader.action_loader.get(handler.action, class_only=True)\n",
   "            if getattr(action, 'BYPASS_HOST_LOOP', False):\n",
   "                bypass_host_loop = True\n",
   "        host_results = []\n",
   "        for host in notified_hosts:\n",
   "            if not iterator.is_failed(host) or iterator._play.force_handlers:\n",
   "                task_vars = self._variable_manager.get_vars(play=iterator._play, host=host, task=handler,\n",
   "                self.add_tqm_variables(task_vars, play=iterator._play)\n",
   "                templar = Templar(loader=self._loader, variables=task_vars)\n",
   "                if not handler.cached_name:\n",
   "                    handler.name = templar.template(handler.name)\n",
   "                    handler.cached_name = True\n",
   "                self._queue_task(host, handler, task_vars, play_context)\n",
   "                if templar.template(handler.run_once) or bypass_host_loop:\n",
   "        host_results = self._wait_on_handler_results(iterator, handler, notified_hosts)\n",
   "        included_files = IncludedFile.process_include_results(\n",
   "            host_results,\n",
   "        result = True\n",
   "        if len(included_files) > 0:\n",
   "            for included_file in included_files:\n",
   "                    new_blocks = self._load_included_file(included_file, iterator=iterator, is_handler=True)\n",
   "                    for block in new_blocks:\n",
   "                        iterator._play.handlers.append(block)\n",
   "                        for task in block.block:\n",
   "                            task_name = task.get_name()\n",
   "                            display.debug(\"adding task '%s' included in handler '%s'\" % (task_name, handler_name))\n",
   "                            task.notified_hosts = included_file._hosts[:]\n",
   "                            result = self._do_handler_run(\n",
   "                                handler=task,\n",
   "                                handler_name=task_name,\n",
   "                                play_context=play_context,\n",
   "                                notified_hosts=included_file._hosts[:],\n",
   "                            if not result:\n",
   "                    for host in included_file._hosts:\n",
   "                        iterator.mark_host_failed(host)\n",
   "                        self._tqm._failed_hosts[host.name] = True\n",
   "                    display.warning(to_text(e))\n",
   "        handler.notified_hosts = [\n",
   "            h for h in handler.notified_hosts\n",
   "            if h not in notified_hosts]\n",
   "        display.debug(\"done running handlers, result is: %s\" % result)\n",
   "        return result\n",
   "        return notified_hosts[:]\n",
   "        ret = False\n",
   "        msg = u'Perform task: %s ' % task\n",
   "        if host:\n",
   "            msg += u'on %s ' % host\n",
   "        msg += u'(N)o/(y)es/(c)ontinue: '\n",
   "        resp = display.prompt(msg)\n",
   "        if resp.lower() in ['y', 'yes']:\n",
   "            display.debug(\"User ran task\")\n",
   "            ret = True\n",
   "        elif resp.lower() in ['c', 'continue']:\n",
   "            display.debug(\"User ran task and canceled step mode\")\n",
   "            ret = True\n",
   "            display.debug(\"User skipped task\")\n",
   "        display.banner(msg)\n",
   "        return ret\n",
   "        display.warning(\"%s task does not support when conditional\" % task_name)\n",
   "        meta_action = task.args.get('_raw_params')\n",
   "        def _evaluate_conditional(h):\n",
   "            all_vars = self._variable_manager.get_vars(play=iterator._play, host=h, task=task,\n",
   "            templar = Templar(loader=self._loader, variables=all_vars)\n",
   "            return task.evaluate_conditional(templar, all_vars)\n",
   "        skipped = False\n",
   "        msg = ''\n",
   "        if meta_action == 'noop':\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            msg = \"noop\"\n",
   "        elif meta_action == 'flush_handlers':\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            self._flushed_hosts[target_host] = True\n",
   "            self.run_handlers(iterator, play_context)\n",
   "            self._flushed_hosts[target_host] = False\n",
   "            msg = \"ran handlers\"\n",
   "        elif meta_action == 'refresh_inventory' or self.flush_cache:\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            msg = \"inventory successfully refreshed\"\n",
   "        elif meta_action == 'clear_facts':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                for host in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                    hostname = host.get_name()\n",
   "                    self._variable_manager.clear_facts(hostname)\n",
   "                msg = \"facts cleared\"\n",
   "                skipped = True\n",
   "        elif meta_action == 'clear_host_errors':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                for host in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                    self._tqm._failed_hosts.pop(host.name, False)\n",
   "                    self._tqm._unreachable_hosts.pop(host.name, False)\n",
   "                    iterator._host_states[host.name].fail_state = iterator.FAILED_NONE\n",
   "                msg = \"cleared host errors\"\n",
   "                skipped = True\n",
   "        elif meta_action == 'end_play':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                for host in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                    if host.name not in self._tqm._unreachable_hosts:\n",
   "                        iterator._host_states[host.name].run_state = iterator.ITERATING_COMPLETE\n",
   "                msg = \"ending play\"\n",
   "        elif meta_action == 'end_host':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                iterator._host_states[target_host.name].run_state = iterator.ITERATING_COMPLETE\n",
   "                iterator._play._removed_hosts.append(target_host.name)\n",
   "                msg = \"ending play for %s\" % target_host.name\n",
   "                skipped = True\n",
   "                msg = \"end_host conditional evaluated to false, continuing execution for %s\" % target_host.name\n",
   "        elif meta_action == 'reset_connection':\n",
   "            all_vars = self._variable_manager.get_vars(play=iterator._play, host=target_host, task=task,\n",
   "            templar = Templar(loader=self._loader, variables=all_vars)\n",
   "            play_context = play_context.set_task_and_variable_override(task=task, variables=all_vars, templar=templar)\n",
   "            play_context.post_validate(templar=templar)\n",
   "            if not play_context.remote_addr:\n",
   "                play_context.remote_addr = target_host.address\n",
   "            play_context.update_vars(all_vars)\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            if target_host in self._active_connections:\n",
   "                connection = Connection(self._active_connections[target_host])\n",
   "                del self._active_connections[target_host]\n",
   "                connection = plugin_loader.connection_loader.get(play_context.connection, play_context, os.devnull)\n",
   "                play_context.set_attributes_from_plugin(connection)\n",
   "            if connection:\n",
   "                    connection.reset()\n",
   "                    msg = 'reset connection'\n",
   "                    display.debug(\"got an error while closing persistent connection: %s\" % e)\n",
   "                msg = 'no connection, nothing to reset'\n",
   "            raise AnsibleError(\"invalid meta action requested: %s\" % meta_action, obj=task._ds)\n",
   "        result = {'msg': msg}\n",
   "        if skipped:\n",
   "            result['skipped'] = True\n",
   "            result['changed'] = False\n",
   "        display.vv(\"META: %s\" % msg)\n",
   "        return [TaskResult(target_host, task, result)]\n",
   "        hosts_left = []\n",
   "        for host in self._hosts_cache:\n",
   "            if host not in self._tqm._unreachable_hosts:\n",
   "                    hosts_left.append(self._inventory.hosts[host])\n",
   "                    hosts_left.append(self._inventory.get_host(host))\n",
   "        return hosts_left\n",
   "        for r in results:\n",
   "            if 'args' in r._task_fields:\n",
   "                socket_path = r._task_fields['args'].get('_ansible_socket')\n",
   "                if socket_path:\n",
   "                    if r._host not in self._active_connections:\n",
   "                        self._active_connections[r._host] = socket_path\n",
   "    EXIT = 3\n",
   "    def __init__(self, result=EXIT):\n",
   "        self.result = result\n",
   "        self.prompt = '[%s] %s (debug)> ' % (host, task)\n",
   "        self.scope['task'] = task\n",
   "        self.scope['task_vars'] = task_vars\n",
   "        self.scope['host'] = host\n",
   "        self.scope['play_context'] = play_context\n",
   "        self.scope['result'] = result\n",
   "        self.next_action = next_action\n",
   "        display.display('User interrupted execution')\n",
   "        templar = Templar(None, shared_loader_obj=None, variables=self.scope['task_vars'])\n",
   "        task = self.scope['task']\n",
   "        task = task.load_data(task._ds)\n",
   "        task.post_validate(templar)\n",
   "        self.scope['task'] = task\n",
   "    do_u = do_update_task\n",
   "            display.display('***%s:%s' % (exc_type_name, repr(v)))\n",
   "            display.display(pprint.pformat(result))\n",
   "            display.display('***%s:%s' % (exc_type_name, repr(v)))\n"
  ]
 },
 "135": {
  "name": "do_p",
  "type": "function",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/plugins/strategy/__init__.py",
  "lineno": "1350",
  "column": "4",
  "context": ")\n        except Exception:\n            pass\n\n    do_p = do_pprint\n\n    def execute(self, args):\n        try:\n       ",
  "context_lines": "            result = self.evaluate(args)\n            display.display(pprint.pformat(result))\n        except Exception:\n            pass\n\n    do_p = do_pprint\n\n    def execute(self, args):\n        try:\n            code = compile(args + '\\n', '<stdin>', 'single')\n",
  "slicing": [
   "display = Display()\n",
   "ALWAYS_DELEGATE_FACT_PREFIXES = frozenset((\n",
   "_sentinel = StrategySentinel()\n",
   "def post_process_whens(result, task, templar):\n",
   "    cond = None\n",
   "        cond = Conditional(loader=templar._loader)\n",
   "        cond.when = task.changed_when\n",
   "        result['changed'] = cond.evaluate_conditional(templar, templar.available_variables)\n",
   "        if cond is None:\n",
   "            cond = Conditional(loader=templar._loader)\n",
   "        cond.when = task.failed_when\n",
   "        failed_when_result = cond.evaluate_conditional(templar, templar.available_variables)\n",
   "        result['failed_when_result'] = result['failed'] = failed_when_result\n",
   "            result = strategy._final_q.get()\n",
   "            if isinstance(result, StrategySentinel):\n",
   "                if 'listen' in result._task_fields:\n",
   "                    strategy._handler_results.append(result)\n",
   "                    strategy._results.append(result)\n",
   "        status_to_stats_map = (\n",
   "        prev_host_states = iterator._host_states.copy()\n",
   "        results = func(self, iterator, one_pass=one_pass, max_passes=max_passes, do_handlers=do_handlers)\n",
   "        _processed_results = []\n",
   "        for result in results:\n",
   "            task = result._task\n",
   "            host = result._host\n",
   "            _queued_task_args = self._queued_task_cache.pop((host.name, task._uuid), None)\n",
   "            task_vars = _queued_task_args['task_vars']\n",
   "            play_context = _queued_task_args['play_context']\n",
   "                prev_host_state = prev_host_states[host.name]\n",
   "                prev_host_state = iterator.get_host_state(host)\n",
   "            while result.needs_debugger(globally_enabled=self.debugger_active):\n",
   "                next_action = NextAction()\n",
   "                dbg = Debugger(task, host, task_vars, play_context, result, next_action)\n",
   "                dbg.cmdloop()\n",
   "                if next_action.result == NextAction.REDO:\n",
   "                    iterator._host_states[host.name] = prev_host_state\n",
   "                    for method, what in status_to_stats_map:\n",
   "                        if getattr(result, method)():\n",
   "                            self._tqm._stats.decrement(what, host.name)\n",
   "                    self._tqm._stats.decrement('ok', host.name)\n",
   "                    self._queue_task(host, task, task_vars, play_context)\n",
   "                    _processed_results.extend(debug_closure(func)(self, iterator, one_pass))\n",
   "                elif next_action.result == NextAction.CONTINUE:\n",
   "                    _processed_results.append(result)\n",
   "                elif next_action.result == NextAction.EXIT:\n",
   "                _processed_results.append(result)\n",
   "        return _processed_results\n",
   "        self._display = display\n",
   "            _pattern = 'all'\n",
   "            _pattern = play.hosts or 'all'\n",
   "        self._hosts_cache_all = [h.name for h in self._inventory.get_hosts(pattern=_pattern, ignore_restrictions=True)]\n",
   "        self._hosts_cache = [h.name for h in self._inventory.get_hosts(play.hosts, order=play.order)]\n",
   "        for sock in itervalues(self._active_connections):\n",
   "                conn = Connection(sock)\n",
   "                conn.reset()\n",
   "                display.debug(\"got an error while closing persistent connection: %s\" % e)\n",
   "        self._final_q.put(_sentinel)\n",
   "        for host in self._hosts_cache:\n",
   "            if host not in self._tqm._unreachable_hosts:\n",
   "                    iterator.get_next_task_for_host(self._inventory.hosts[host])\n",
   "                    iterator.get_next_task_for_host(self._inventory.get_host(host))\n",
   "        failed_hosts = iterator.get_failed_hosts()\n",
   "        unreachable_hosts = self._tqm._unreachable_hosts.keys()\n",
   "        display.debug(\"running handlers\")\n",
   "        handler_result = self.run_handlers(iterator, play_context)\n",
   "        if isinstance(handler_result, bool) and not handler_result:\n",
   "            result |= self._tqm.RUN_ERROR\n",
   "        elif not handler_result:\n",
   "            result |= handler_result\n",
   "        failed_hosts = set(failed_hosts).union(iterator.get_failed_hosts())\n",
   "        unreachable_hosts = set(unreachable_hosts).union(self._tqm._unreachable_hosts.keys())\n",
   "        if not isinstance(result, bool) and result != self._tqm.RUN_OK:\n",
   "            return result\n",
   "        elif len(unreachable_hosts) > 0:\n",
   "        elif len(failed_hosts) > 0:\n",
   "        ignore = set(self._tqm._failed_hosts).union(self._tqm._unreachable_hosts)\n",
   "        return [host for host in self._hosts_cache if host not in ignore]\n",
   "        return [host for host in self._hosts_cache if host in self._tqm._failed_hosts]\n",
   "        display.debug(\"entering _queue_task() for %s/%s\" % (host.name, task.action))\n",
   "        if task.action not in action_write_locks.action_write_locks:\n",
   "            display.debug('Creating lock for %s' % task.action)\n",
   "            action_write_locks.action_write_locks[task.action] = Lock()\n",
   "        templar = Templar(loader=self._loader, variables=task_vars)\n",
   "            throttle = int(templar.template(task.throttle))\n",
   "            raise AnsibleError(\"Failed to convert the throttle value to an integer.\", obj=task._ds, orig_exc=e)\n",
   "            rewind_point = len(self._workers)\n",
   "            if throttle > 0 and self.ALLOW_BASE_THROTTLING:\n",
   "                if task.run_once:\n",
   "                    display.debug(\"Ignoring 'throttle' as 'run_once' is also set for '%s'\" % task.get_name())\n",
   "                    if throttle <= rewind_point:\n",
   "                        display.debug(\"task: %s, throttle: %d\" % (task.get_name(), throttle))\n",
   "                        rewind_point = throttle\n",
   "            queued = False\n",
   "            starting_worker = self._cur_worker\n",
   "                if self._cur_worker >= rewind_point:\n",
   "                worker_prc = self._workers[self._cur_worker]\n",
   "                if worker_prc is None or not worker_prc.is_alive():\n",
   "                    self._queued_task_cache[(host.name, task._uuid)] = {\n",
   "                        'host': host,\n",
   "                        'task': task,\n",
   "                        'task_vars': task_vars,\n",
   "                        'play_context': play_context\n",
   "                    worker_prc = WorkerProcess(self._final_q, task_vars, host, task, play_context, self._loader, self._variable_manager, plugin_loader)\n",
   "                    self._workers[self._cur_worker] = worker_prc\n",
   "                    self._tqm.send_callback('v2_runner_on_start', host, task)\n",
   "                    worker_prc.start()\n",
   "                    display.debug(\"worker is %d (out of %d available)\" % (self._cur_worker + 1, len(self._workers)))\n",
   "                    queued = True\n",
   "                if self._cur_worker >= rewind_point:\n",
   "                if queued:\n",
   "                elif self._cur_worker == starting_worker:\n",
   "            if isinstance(task, Handler):\n",
   "            display.debug(\"got an error while queuing: %s\" % e)\n",
   "        display.debug(\"exiting _queue_task() for %s/%s\" % (host.name, task.action))\n",
   "        if task.run_once:\n",
   "            host_list = [host for host in self._hosts_cache if host not in self._tqm._unreachable_hosts]\n",
   "            host_list = [task_host.name]\n",
   "        return host_list\n",
   "        host_name = result.get('_ansible_delegated_vars', {}).get('ansible_delegated_host', None)\n",
   "        return [host_name or task.delegate_to]\n",
   "        if task.delegate_to is None:\n",
   "        facts = result['ansible_facts']\n",
   "        always_keys = set()\n",
   "        _add = always_keys.add\n",
   "        for fact_key in facts:\n",
   "            for always_key in ALWAYS_DELEGATE_FACT_PREFIXES:\n",
   "                if fact_key.startswith(always_key):\n",
   "                    _add(fact_key)\n",
   "        if always_keys:\n",
   "            _pop = facts.pop\n",
   "            always_facts = {\n",
   "                'ansible_facts': dict((k, _pop(k)) for k in list(facts) if k in always_keys)\n",
   "            host_list = self.get_delegated_hosts(result, task)\n",
   "            _set_host_facts = self._variable_manager.set_host_facts\n",
   "            for target_host in host_list:\n",
   "                _set_host_facts(target_host, always_facts)\n",
   "        ret_results = []\n",
   "        handler_templar = Templar(self._loader)\n",
   "        def get_original_host(host_name):\n",
   "            host_name = to_text(host_name)\n",
   "            if host_name in self._inventory.hosts:\n",
   "                return self._inventory.hosts[host_name]\n",
   "                return self._inventory.get_host(host_name)\n",
   "        def search_handler_blocks_by_name(handler_name, handler_blocks):\n",
   "            for handler_block in reversed(handler_blocks):\n",
   "                for handler_task in handler_block.block:\n",
   "                    if handler_task.name:\n",
   "                        if not handler_task.cached_name:\n",
   "                            if handler_templar.is_template(handler_task.name):\n",
   "                                handler_templar.available_variables = self._variable_manager.get_vars(play=iterator._play,\n",
   "                                                                                                      task=handler_task,\n",
   "                                handler_task.name = handler_templar.template(handler_task.name)\n",
   "                            handler_task.cached_name = True\n",
   "                            candidates = (\n",
   "                                handler_task.name,\n",
   "                                handler_task.get_name(include_role_fqcn=False),\n",
   "                                handler_task.get_name(include_role_fqcn=True),\n",
   "                            if handler_name in candidates:\n",
   "                                return handler_task\n",
   "        cur_pass = 0\n",
   "                    task_result = self._handler_results.popleft()\n",
   "                    task_result = self._results.popleft()\n",
   "            original_host = get_original_host(task_result._host)\n",
   "            queue_cache_entry = (original_host.name, task_result._task)\n",
   "            found_task = self._queued_task_cache.get(queue_cache_entry)['task']\n",
   "            original_task = found_task.copy(exclude_parent=True, exclude_tasks=True)\n",
   "            original_task._parent = found_task._parent\n",
   "            original_task.from_attrs(task_result._task_fields)\n",
   "            task_result._host = original_host\n",
   "            task_result._task = original_task\n",
   "            if '_ansible_retry' in task_result._result:\n",
   "                self._tqm.send_callback('v2_runner_retry', task_result)\n",
   "            elif '_ansible_item_result' in task_result._result:\n",
   "                if task_result.is_failed() or task_result.is_unreachable():\n",
   "                    self._tqm.send_callback('v2_runner_item_on_failed', task_result)\n",
   "                elif task_result.is_skipped():\n",
   "                    self._tqm.send_callback('v2_runner_item_on_skipped', task_result)\n",
   "                    if 'diff' in task_result._result:\n",
   "                        if self._diff or getattr(original_task, 'diff', False):\n",
   "                            self._tqm.send_callback('v2_on_file_diff', task_result)\n",
   "                    self._tqm.send_callback('v2_runner_item_on_ok', task_result)\n",
   "            role_ran = False\n",
   "            if task_result.is_failed():\n",
   "                role_ran = True\n",
   "                ignore_errors = original_task.ignore_errors\n",
   "                if not ignore_errors:\n",
   "                    display.debug(\"marking %s as failed\" % original_host.name)\n",
   "                    if original_task.run_once:\n",
   "                        for h in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                            if h.name not in self._tqm._unreachable_hosts:\n",
   "                                state, _ = iterator.get_next_task_for_host(h, peek=True)\n",
   "                                iterator.mark_host_failed(h)\n",
   "                                state, new_task = iterator.get_next_task_for_host(h, peek=True)\n",
   "                        iterator.mark_host_failed(original_host)\n",
   "                    state, _ = iterator.get_next_task_for_host(original_host, peek=True)\n",
   "                    if iterator.is_failed(original_host) and state and state.run_state == iterator.ITERATING_COMPLETE:\n",
   "                        self._tqm._failed_hosts[original_host.name] = True\n",
   "                    if state and iterator.get_active_state(state).run_state == iterator.ITERATING_RESCUE:\n",
   "                        self._tqm._stats.increment('rescued', original_host.name)\n",
   "                            original_host.name,\n",
   "                                ansible_failed_task=original_task.serialize(),\n",
   "                                ansible_failed_result=task_result._result,\n",
   "                        self._tqm._stats.increment('failures', original_host.name)\n",
   "                    self._tqm._stats.increment('ok', original_host.name)\n",
   "                    self._tqm._stats.increment('ignored', original_host.name)\n",
   "                    if 'changed' in task_result._result and task_result._result['changed']:\n",
   "                        self._tqm._stats.increment('changed', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_failed', task_result, ignore_errors=ignore_errors)\n",
   "            elif task_result.is_unreachable():\n",
   "                ignore_unreachable = original_task.ignore_unreachable\n",
   "                if not ignore_unreachable:\n",
   "                    self._tqm._unreachable_hosts[original_host.name] = True\n",
   "                    iterator._play._removed_hosts.append(original_host.name)\n",
   "                    self._tqm._stats.increment('skipped', original_host.name)\n",
   "                    task_result._result['skip_reason'] = 'Host %s is unreachable' % original_host.name\n",
   "                self._tqm._stats.increment('dark', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_unreachable', task_result)\n",
   "            elif task_result.is_skipped():\n",
   "                self._tqm._stats.increment('skipped', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_skipped', task_result)\n",
   "                role_ran = True\n",
   "                if original_task.loop:\n",
   "                    result_items = task_result._result.get('results', [])\n",
   "                    result_items = [task_result._result]\n",
   "                for result_item in result_items:\n",
   "                    if '_ansible_notify' in result_item:\n",
   "                        if task_result.is_changed():\n",
   "                            for handler_name in result_item['_ansible_notify']:\n",
   "                                found = False\n",
   "                                target_handler = search_handler_blocks_by_name(handler_name, iterator._play.handlers)\n",
   "                                if target_handler is not None:\n",
   "                                    found = True\n",
   "                                    if target_handler.notify_host(original_host):\n",
   "                                        self._tqm.send_callback('v2_playbook_on_notify', target_handler, original_host)\n",
   "                                for listening_handler_block in iterator._play.handlers:\n",
   "                                    for listening_handler in listening_handler_block.block:\n",
   "                                        listeners = getattr(listening_handler, 'listen', []) or []\n",
   "                                        if not listeners:\n",
   "                                        listeners = listening_handler.get_validated_value(\n",
   "                                            'listen', listening_handler._valid_attrs['listen'], listeners, handler_templar\n",
   "                                        if handler_name not in listeners:\n",
   "                                            found = True\n",
   "                                        if listening_handler.notify_host(original_host):\n",
   "                                            self._tqm.send_callback('v2_playbook_on_notify', listening_handler, original_host)\n",
   "                                if not found:\n",
   "                                    msg = (\"The requested handler '%s' was not found in either the main handlers list nor in the listening \"\n",
   "                                           \"handlers list\" % handler_name)\n",
   "                                        raise AnsibleError(msg)\n",
   "                                        display.warning(msg)\n",
   "                    if 'add_host' in result_item:\n",
   "                        new_host_info = result_item.get('add_host', dict())\n",
   "                        self._add_host(new_host_info, result_item)\n",
   "                        post_process_whens(result_item, original_task, handler_templar)\n",
   "                    elif 'add_group' in result_item:\n",
   "                        self._add_group(original_host, result_item)\n",
   "                        post_process_whens(result_item, original_task, handler_templar)\n",
   "                    if 'ansible_facts' in result_item:\n",
   "                        if original_task.delegate_to is not None and original_task.delegate_facts:\n",
   "                            host_list = self.get_delegated_hosts(result_item, original_task)\n",
   "                            self._set_always_delegated_facts(result_item, original_task)\n",
   "                            host_list = self.get_task_hosts(iterator, original_host, original_task)\n",
   "                        if original_task.action == 'include_vars':\n",
   "                            for (var_name, var_value) in iteritems(result_item['ansible_facts']):\n",
   "                                for target_host in host_list:\n",
   "                                    self._variable_manager.set_host_variable(target_host, var_name, var_value)\n",
   "                            cacheable = result_item.pop('_ansible_facts_cacheable', False)\n",
   "                            for target_host in host_list:\n",
   "                                if original_task.action != 'set_fact' or cacheable:\n",
   "                                    self._variable_manager.set_host_facts(target_host, result_item['ansible_facts'].copy())\n",
   "                                if original_task.action == 'set_fact':\n",
   "                                    self._variable_manager.set_nonpersistent_facts(target_host, result_item['ansible_facts'].copy())\n",
   "                    if 'ansible_stats' in result_item and 'data' in result_item['ansible_stats'] and result_item['ansible_stats']['data']:\n",
   "                        if 'per_host' not in result_item['ansible_stats'] or result_item['ansible_stats']['per_host']:\n",
   "                            host_list = self.get_task_hosts(iterator, original_host, original_task)\n",
   "                            host_list = [None]\n",
   "                        data = result_item['ansible_stats']['data']\n",
   "                        aggregate = 'aggregate' in result_item['ansible_stats'] and result_item['ansible_stats']['aggregate']\n",
   "                        for myhost in host_list:\n",
   "                            for k in data.keys():\n",
   "                                if aggregate:\n",
   "                                    self._tqm._stats.update_custom_stats(k, data[k], myhost)\n",
   "                                    self._tqm._stats.set_custom_stats(k, data[k], myhost)\n",
   "                if 'diff' in task_result._result:\n",
   "                    if self._diff or getattr(original_task, 'diff', False):\n",
   "                        self._tqm.send_callback('v2_on_file_diff', task_result)\n",
   "                if not isinstance(original_task, TaskInclude):\n",
   "                    self._tqm._stats.increment('ok', original_host.name)\n",
   "                    if 'changed' in task_result._result and task_result._result['changed']:\n",
   "                        self._tqm._stats.increment('changed', original_host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_ok', task_result)\n",
   "            if original_task.register:\n",
   "                host_list = self.get_task_hosts(iterator, original_host, original_task)\n",
   "                clean_copy = strip_internal_keys(module_response_deepcopy(task_result._result))\n",
   "                if 'invocation' in clean_copy:\n",
   "                    del clean_copy['invocation']\n",
   "                for target_host in host_list:\n",
   "                    self._variable_manager.set_nonpersistent_facts(target_host, {original_task.register: clean_copy})\n",
   "            if original_host.name in self._blocked_hosts:\n",
   "                del self._blocked_hosts[original_host.name]\n",
   "            if original_task._role is not None and role_ran:  # TODO:  and original_task.action != 'include_role':?\n",
   "                for (entry, role_obj) in iteritems(iterator._play.ROLE_CACHE[original_task._role.get_name()]):\n",
   "                    if role_obj._uuid == original_task._role._uuid:\n",
   "                        role_obj._had_task_run[original_host.name] = True\n",
   "            ret_results.append(task_result)\n",
   "            if one_pass or max_passes is not None and (cur_pass + 1) >= max_passes:\n",
   "            cur_pass += 1\n",
   "        return ret_results\n",
   "        ret_results = []\n",
   "        handler_results = 0\n",
   "        display.debug(\"waiting for handler results...\")\n",
   "               handler_results < len(notified_hosts) and\n",
   "            results = self._process_pending_results(iterator, do_handlers=True)\n",
   "            ret_results.extend(results)\n",
   "            handler_results += len([\n",
   "                r._host for r in results if r._host in notified_hosts and\n",
   "                r.task_name == handler.name])\n",
   "        display.debug(\"no more pending handlers, returning what we have\")\n",
   "        return ret_results\n",
   "        ret_results = []\n",
   "        display.debug(\"waiting for pending results...\")\n",
   "            results = self._process_pending_results(iterator)\n",
   "            ret_results.extend(results)\n",
   "        display.debug(\"no more pending results, returning what we have\")\n",
   "        return ret_results\n",
   "        changed = False\n",
   "            host_name = host_info.get('host_name')\n",
   "            if host_name not in self._inventory.hosts:\n",
   "                self._inventory.add_host(host_name, 'all')\n",
   "                self._hosts_cache_all.append(host_name)\n",
   "                changed = True\n",
   "            new_host = self._inventory.hosts.get(host_name)\n",
   "            new_host_vars = new_host.get_vars()\n",
   "            new_host_combined_vars = combine_vars(new_host_vars, host_info.get('host_vars', dict()))\n",
   "            if new_host_vars != new_host_combined_vars:\n",
   "                new_host.vars = new_host_combined_vars\n",
   "                changed = True\n",
   "            new_groups = host_info.get('groups', [])\n",
   "            for group_name in new_groups:\n",
   "                if group_name not in self._inventory.groups:\n",
   "                    group_name = self._inventory.add_group(group_name)\n",
   "                    changed = True\n",
   "                new_group = self._inventory.groups[group_name]\n",
   "                if new_group.add_host(self._inventory.hosts[host_name]):\n",
   "                    changed = True\n",
   "            if changed:\n",
   "            result_item['changed'] = changed\n",
   "        changed = False\n",
   "        real_host = self._inventory.hosts.get(host.name)\n",
   "        if real_host is None:\n",
   "            if host.name == self._inventory.localhost.name:\n",
   "                real_host = self._inventory.localhost\n",
   "                raise AnsibleError('%s cannot be matched in inventory' % host.name)\n",
   "        group_name = result_item.get('add_group')\n",
   "        parent_group_names = result_item.get('parent_groups', [])\n",
   "        if group_name not in self._inventory.groups:\n",
   "            group_name = self._inventory.add_group(group_name)\n",
   "        for name in parent_group_names:\n",
   "            if name not in self._inventory.groups:\n",
   "                self._inventory.add_group(name)\n",
   "                changed = True\n",
   "        group = self._inventory.groups[group_name]\n",
   "        for parent_group_name in parent_group_names:\n",
   "            parent_group = self._inventory.groups[parent_group_name]\n",
   "            new = parent_group.add_child_group(group)\n",
   "            if new and not changed:\n",
   "                changed = True\n",
   "        if real_host not in group.get_hosts():\n",
   "            changed = group.add_host(real_host)\n",
   "        if group not in real_host.get_groups():\n",
   "            changed = real_host.add_group(group)\n",
   "        if changed:\n",
   "        result_item['changed'] = changed\n",
   "        ti_copy = included_file._task.copy(exclude_parent=True)\n",
   "        ti_copy._parent = included_file._task._parent\n",
   "        temp_vars = ti_copy.vars.copy()\n",
   "        temp_vars.update(included_file._vars)\n",
   "        ti_copy.vars = temp_vars\n",
   "        return ti_copy\n",
   "        display.debug(\"loading included file: %s\" % included_file._filename)\n",
   "            data = self._loader.load_from_file(included_file._filename)\n",
   "            if data is None:\n",
   "            elif not isinstance(data, list):\n",
   "            ti_copy = self._copy_included_file(included_file)\n",
   "            tags = included_file._task.vars.pop('tags', [])\n",
   "            if isinstance(tags, string_types):\n",
   "                tags = tags.split(',')\n",
   "            if len(tags) > 0:\n",
   "                display.deprecated(\"You should not specify tags in the include parameters. All tags should be specified using the task-level option\",\n",
   "                included_file._task.tags = tags\n",
   "            block_list = load_list_of_blocks(\n",
   "                data,\n",
   "                parent_block=ti_copy.build_parent_block(),\n",
   "            for host in included_file._hosts:\n",
   "                self._tqm._stats.increment('ok', host.name)\n",
   "                reason = \"Could not find or access '%s' on the Ansible Controller.\" % to_text(e.file_name)\n",
   "                reason = to_text(e)\n",
   "            for host in included_file._hosts:\n",
   "                tr = TaskResult(host=host, task=included_file._task, return_data=dict(failed=True, reason=reason))\n",
   "                iterator.mark_host_failed(host)\n",
   "                self._tqm._failed_hosts[host.name] = True\n",
   "                self._tqm._stats.increment('failures', host.name)\n",
   "                self._tqm.send_callback('v2_runner_on_failed', tr)\n",
   "        display.debug(\"done processing included file\")\n",
   "        return block_list\n",
   "        result = self._tqm.RUN_OK\n",
   "        for handler_block in iterator._play.handlers:\n",
   "            for handler in handler_block.block:\n",
   "                if handler.notified_hosts:\n",
   "                    result = self._do_handler_run(handler, handler.get_name(), iterator=iterator, play_context=play_context)\n",
   "                    if not result:\n",
   "        return result\n",
   "            notified_hosts = handler.notified_hosts[:]\n",
   "        failed_hosts = self._filter_notified_failed_hosts(iterator, notified_hosts)\n",
   "        notified_hosts = self._filter_notified_hosts(notified_hosts)\n",
   "        notified_hosts += failed_hosts\n",
   "        if len(notified_hosts) > 0:\n",
   "            saved_name = handler.name\n",
   "            handler.name = handler_name\n",
   "            self._tqm.send_callback('v2_playbook_on_handler_task_start', handler)\n",
   "            handler.name = saved_name\n",
   "        bypass_host_loop = False\n",
   "            action = plugin_loader.action_loader.get(handler.action, class_only=True)\n",
   "            if getattr(action, 'BYPASS_HOST_LOOP', False):\n",
   "                bypass_host_loop = True\n",
   "        host_results = []\n",
   "        for host in notified_hosts:\n",
   "            if not iterator.is_failed(host) or iterator._play.force_handlers:\n",
   "                task_vars = self._variable_manager.get_vars(play=iterator._play, host=host, task=handler,\n",
   "                self.add_tqm_variables(task_vars, play=iterator._play)\n",
   "                templar = Templar(loader=self._loader, variables=task_vars)\n",
   "                if not handler.cached_name:\n",
   "                    handler.name = templar.template(handler.name)\n",
   "                    handler.cached_name = True\n",
   "                self._queue_task(host, handler, task_vars, play_context)\n",
   "                if templar.template(handler.run_once) or bypass_host_loop:\n",
   "        host_results = self._wait_on_handler_results(iterator, handler, notified_hosts)\n",
   "        included_files = IncludedFile.process_include_results(\n",
   "            host_results,\n",
   "        result = True\n",
   "        if len(included_files) > 0:\n",
   "            for included_file in included_files:\n",
   "                    new_blocks = self._load_included_file(included_file, iterator=iterator, is_handler=True)\n",
   "                    for block in new_blocks:\n",
   "                        iterator._play.handlers.append(block)\n",
   "                        for task in block.block:\n",
   "                            task_name = task.get_name()\n",
   "                            display.debug(\"adding task '%s' included in handler '%s'\" % (task_name, handler_name))\n",
   "                            task.notified_hosts = included_file._hosts[:]\n",
   "                            result = self._do_handler_run(\n",
   "                                handler=task,\n",
   "                                handler_name=task_name,\n",
   "                                play_context=play_context,\n",
   "                                notified_hosts=included_file._hosts[:],\n",
   "                            if not result:\n",
   "                    for host in included_file._hosts:\n",
   "                        iterator.mark_host_failed(host)\n",
   "                        self._tqm._failed_hosts[host.name] = True\n",
   "                    display.warning(to_text(e))\n",
   "        handler.notified_hosts = [\n",
   "            h for h in handler.notified_hosts\n",
   "            if h not in notified_hosts]\n",
   "        display.debug(\"done running handlers, result is: %s\" % result)\n",
   "        return result\n",
   "        return notified_hosts[:]\n",
   "        ret = False\n",
   "        msg = u'Perform task: %s ' % task\n",
   "        if host:\n",
   "            msg += u'on %s ' % host\n",
   "        msg += u'(N)o/(y)es/(c)ontinue: '\n",
   "        resp = display.prompt(msg)\n",
   "        if resp.lower() in ['y', 'yes']:\n",
   "            display.debug(\"User ran task\")\n",
   "            ret = True\n",
   "        elif resp.lower() in ['c', 'continue']:\n",
   "            display.debug(\"User ran task and canceled step mode\")\n",
   "            ret = True\n",
   "            display.debug(\"User skipped task\")\n",
   "        display.banner(msg)\n",
   "        return ret\n",
   "        display.warning(\"%s task does not support when conditional\" % task_name)\n",
   "        meta_action = task.args.get('_raw_params')\n",
   "        def _evaluate_conditional(h):\n",
   "            all_vars = self._variable_manager.get_vars(play=iterator._play, host=h, task=task,\n",
   "            templar = Templar(loader=self._loader, variables=all_vars)\n",
   "            return task.evaluate_conditional(templar, all_vars)\n",
   "        skipped = False\n",
   "        msg = ''\n",
   "        if meta_action == 'noop':\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            msg = \"noop\"\n",
   "        elif meta_action == 'flush_handlers':\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            self._flushed_hosts[target_host] = True\n",
   "            self.run_handlers(iterator, play_context)\n",
   "            self._flushed_hosts[target_host] = False\n",
   "            msg = \"ran handlers\"\n",
   "        elif meta_action == 'refresh_inventory' or self.flush_cache:\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            msg = \"inventory successfully refreshed\"\n",
   "        elif meta_action == 'clear_facts':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                for host in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                    hostname = host.get_name()\n",
   "                    self._variable_manager.clear_facts(hostname)\n",
   "                msg = \"facts cleared\"\n",
   "                skipped = True\n",
   "        elif meta_action == 'clear_host_errors':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                for host in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                    self._tqm._failed_hosts.pop(host.name, False)\n",
   "                    self._tqm._unreachable_hosts.pop(host.name, False)\n",
   "                    iterator._host_states[host.name].fail_state = iterator.FAILED_NONE\n",
   "                msg = \"cleared host errors\"\n",
   "                skipped = True\n",
   "        elif meta_action == 'end_play':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                for host in self._inventory.get_hosts(iterator._play.hosts):\n",
   "                    if host.name not in self._tqm._unreachable_hosts:\n",
   "                        iterator._host_states[host.name].run_state = iterator.ITERATING_COMPLETE\n",
   "                msg = \"ending play\"\n",
   "        elif meta_action == 'end_host':\n",
   "            if _evaluate_conditional(target_host):\n",
   "                iterator._host_states[target_host.name].run_state = iterator.ITERATING_COMPLETE\n",
   "                iterator._play._removed_hosts.append(target_host.name)\n",
   "                msg = \"ending play for %s\" % target_host.name\n",
   "                skipped = True\n",
   "                msg = \"end_host conditional evaluated to false, continuing execution for %s\" % target_host.name\n",
   "        elif meta_action == 'reset_connection':\n",
   "            all_vars = self._variable_manager.get_vars(play=iterator._play, host=target_host, task=task,\n",
   "            templar = Templar(loader=self._loader, variables=all_vars)\n",
   "            play_context = play_context.set_task_and_variable_override(task=task, variables=all_vars, templar=templar)\n",
   "            play_context.post_validate(templar=templar)\n",
   "            if not play_context.remote_addr:\n",
   "                play_context.remote_addr = target_host.address\n",
   "            play_context.update_vars(all_vars)\n",
   "            if task.when:\n",
   "                self._cond_not_supported_warn(meta_action)\n",
   "            if target_host in self._active_connections:\n",
   "                connection = Connection(self._active_connections[target_host])\n",
   "                del self._active_connections[target_host]\n",
   "                connection = plugin_loader.connection_loader.get(play_context.connection, play_context, os.devnull)\n",
   "                play_context.set_attributes_from_plugin(connection)\n",
   "            if connection:\n",
   "                    connection.reset()\n",
   "                    msg = 'reset connection'\n",
   "                    display.debug(\"got an error while closing persistent connection: %s\" % e)\n",
   "                msg = 'no connection, nothing to reset'\n",
   "            raise AnsibleError(\"invalid meta action requested: %s\" % meta_action, obj=task._ds)\n",
   "        result = {'msg': msg}\n",
   "        if skipped:\n",
   "            result['skipped'] = True\n",
   "            result['changed'] = False\n",
   "        display.vv(\"META: %s\" % msg)\n",
   "        return [TaskResult(target_host, task, result)]\n",
   "        hosts_left = []\n",
   "        for host in self._hosts_cache:\n",
   "            if host not in self._tqm._unreachable_hosts:\n",
   "                    hosts_left.append(self._inventory.hosts[host])\n",
   "                    hosts_left.append(self._inventory.get_host(host))\n",
   "        return hosts_left\n",
   "        for r in results:\n",
   "            if 'args' in r._task_fields:\n",
   "                socket_path = r._task_fields['args'].get('_ansible_socket')\n",
   "                if socket_path:\n",
   "                    if r._host not in self._active_connections:\n",
   "                        self._active_connections[r._host] = socket_path\n",
   "    EXIT = 3\n",
   "    def __init__(self, result=EXIT):\n",
   "        self.result = result\n",
   "        self.prompt = '[%s] %s (debug)> ' % (host, task)\n",
   "        self.scope['task'] = task\n",
   "        self.scope['task_vars'] = task_vars\n",
   "        self.scope['host'] = host\n",
   "        self.scope['play_context'] = play_context\n",
   "        self.scope['result'] = result\n",
   "        self.next_action = next_action\n",
   "        display.display('User interrupted execution')\n",
   "        templar = Templar(None, shared_loader_obj=None, variables=self.scope['task_vars'])\n",
   "        task = self.scope['task']\n",
   "        task = task.load_data(task._ds)\n",
   "        task.post_validate(templar)\n",
   "        self.scope['task'] = task\n",
   "            t, v = sys.exc_info()[:2]\n",
   "            if isinstance(t, str):\n",
   "                exc_type_name = t\n",
   "                exc_type_name = t.__name__\n",
   "            display.display('***%s:%s' % (exc_type_name, repr(v)))\n",
   "            result = self.evaluate(args)\n",
   "            display.display(pprint.pformat(result))\n",
   "    do_p = do_pprint\n",
   "            code = compile(args + '\\n', '<stdin>', 'single')\n",
   "            exec(code, globals(), self.scope)\n",
   "            t, v = sys.exc_info()[:2]\n",
   "            if isinstance(t, str):\n",
   "                exc_type_name = t\n",
   "                exc_type_name = t.__name__\n",
   "            display.display('***%s:%s' % (exc_type_name, repr(v)))\n"
  ]
 },
 "136": {
  "name": "noop_task",
  "type": "NoneType",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/plugins/strategy/linear.py",
  "lineno": "51",
  "column": "4",
  "context": "play()\n\n\nclass StrategyModule(StrategyBase):\n\n    noop_task = None\n\n    def _replace_with_noop(self, target):\n       ",
  "context_lines": "from ansible.template import Templar\nfrom ansible.utils.display import Display\n\ndisplay = Display()\n\n\nclass StrategyModule(StrategyBase):\n\n    noop_task = None\n\n    def _replace_with_noop(self, target):\n        if self.noop_task is None:\n            raise AnsibleAssertionError('strategy.linear.StrategyModule.noop_task is None, need Task()')\n\n",
  "slicing": [
   "    noop_task = None\n",
   "        noop_task.action = 'meta'\n",
   "        noop_task.args['_raw_params'] = 'noop'\n",
   "        noop_task.set_loader(iterator._play._loader)\n",
   "                    rvals.append((host, noop_task))\n"
  ]
 },
 "137": {
  "name": "transport",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/connection/winrm.py",
  "lineno": "198",
  "column": "4",
  "context": "    '''WinRM connections over HTTP/HTTPS.'''\n\n    transport = 'winrm'\n    module_implementation_preferences = ('.ps1', '",
  "context_lines": "    HAS_IPADDRESS = False\n\ndisplay = Display()\n\n\nclass Connection(ConnectionBase):\n    '''WinRM connections over HTTP/HTTPS.'''\n\n    transport = 'winrm'\n    module_implementation_preferences = ('.ps1', '.exe', '')\n    allow_executable = False\n    has_pipelining = True\n    allow_extras = True\n\n",
  "slicing": [
   "HAVE_KERBEROS = False\n",
   "    HAVE_KERBEROS = True\n",
   "    HAS_WINRM = True\n",
   "    HAS_WINRM = False\n",
   "    WINRM_IMPORT_ERR = e\n",
   "    HAS_XMLTODICT = True\n",
   "    HAS_XMLTODICT = False\n",
   "    XMLTODICT_IMPORT_ERR = e\n",
   "HAS_PEXPECT = False\n",
   "        argspec = getargspec(pexpect.spawn.__init__)\n",
   "        if 'echo' in argspec.args:\n",
   "            HAS_PEXPECT = True\n",
   "    HAS_IPADDRESS = True\n",
   "    HAS_IPADDRESS = False\n",
   "display = Display()\n",
   "    transport = 'winrm'\n",
   "            transport_selector = ['ssl'] if self._winrm_scheme == 'https' else ['plaintext']\n",
   "            if HAVE_KERBEROS and ((self._winrm_user and '@' in self._winrm_user)):\n",
   "                self._winrm_transport = ['kerberos'] + transport_selector\n",
   "                self._winrm_transport = transport_selector\n",
   "        unsupported_transports = set(self._winrm_transport).difference(self._winrm_supported_authtypes)\n",
   "        if unsupported_transports:\n",
   "                               to_native(list(unsupported_transports), nonstring='simplerepr'))\n",
   "        kinit_mode = self.get_option('kerberos_mode')\n",
   "        if kinit_mode is None:\n",
   "        elif kinit_mode == \"managed\":\n",
   "        elif kinit_mode == \"manual\":\n",
   "        internal_kwarg_mask = set(['self', 'endpoint', 'transport', 'username', 'password', 'scheme', 'path', 'kinit_mode', 'kinit_cmd'])\n",
   "        argspec = getargspec(Protocol.__init__)\n",
   "        supported_winrm_args = set(argspec.args)\n",
   "        supported_winrm_args.update(internal_kwarg_mask)\n",
   "        passed_winrm_args = set([v.replace('ansible_winrm_', '') for v in self.get_option('_extras')])\n",
   "        unsupported_args = passed_winrm_args.difference(supported_winrm_args)\n",
   "        for arg in unsupported_args:\n",
   "            display.warning(\"ansible_winrm_{0} unsupported by pywinrm (is an up-to-date version of pywinrm installed?)\".format(arg))\n",
   "        for arg in passed_winrm_args.difference(internal_kwarg_mask).intersection(supported_winrm_args):\n",
   "            self._winrm_kwargs[arg] = self.get_option('_extras')['ansible_winrm_%s' % arg]\n",
   "            password = \"\"\n",
   "        display.vvvvv(\"creating Kerberos CC at %s\" % self._kerb_ccache.name)\n",
   "        krb5ccname = \"FILE:%s\" % self._kerb_ccache.name\n",
   "        os.environ[\"KRB5CCNAME\"] = krb5ccname\n",
   "        krb5env = dict(KRB5CCNAME=krb5ccname)\n",
   "        kinit_cmdline = [self._kinit_cmd]\n",
   "        kinit_args = self.get_option('kinit_args')\n",
   "        if kinit_args:\n",
   "            kinit_args = [to_text(a) for a in shlex.split(kinit_args) if a.strip()]\n",
   "            kinit_cmdline.extend(kinit_args)\n",
   "            kinit_cmdline.append('-f')\n",
   "        kinit_cmdline.append(principal)\n",
   "        if HAS_PEXPECT:\n",
   "            proc_mechanism = \"pexpect\"\n",
   "            command = kinit_cmdline.pop(0)\n",
   "            password = to_text(password, encoding='utf-8',\n",
   "            display.vvvv(\"calling kinit with pexpect for principal %s\"\n",
   "                child = pexpect.spawn(command, kinit_cmdline, timeout=60,\n",
   "                                      env=krb5env, echo=False)\n",
   "                err_msg = \"Kerberos auth failure when calling kinit cmd \" \\\n",
   "                          \"'%s': %s\" % (command, to_native(err))\n",
   "                raise AnsibleConnectionFailure(err_msg)\n",
   "                child.expect(\".*:\")\n",
   "                child.sendline(password)\n",
   "                display.vvvv(\"kinit with pexpect raised OSError: %s\"\n",
   "            stderr = child.read()\n",
   "            child.wait()\n",
   "            rc = child.exitstatus\n",
   "            proc_mechanism = \"subprocess\"\n",
   "            password = to_bytes(password, encoding='utf-8',\n",
   "            display.vvvv(\"calling kinit with subprocess for principal %s\"\n",
   "                p = subprocess.Popen(kinit_cmdline, stdin=subprocess.PIPE,\n",
   "                                     env=krb5env)\n",
   "                err_msg = \"Kerberos auth failure when calling kinit cmd \" \\\n",
   "                raise AnsibleConnectionFailure(err_msg)\n",
   "            stdout, stderr = p.communicate(password + b'\\n')\n",
   "            rc = p.returncode != 0\n",
   "        if rc != 0:\n",
   "            exp_msg = to_native(stderr.strip())\n",
   "            exp_msg = exp_msg.replace(to_native(password), \"<redacted>\")\n",
   "            err_msg = \"Kerberos auth failure for principal %s with %s: %s\" \\\n",
   "                      % (principal, proc_mechanism, exp_msg)\n",
   "            raise AnsibleConnectionFailure(err_msg)\n",
   "        display.vvvvv(\"kinit succeeded for principal %s\" % principal)\n",
   "        display.vvv(\"ESTABLISH WINRM CONNECTION FOR USER: %s on PORT %s TO %s\" %\n",
   "        winrm_host = self._winrm_host\n",
   "        if HAS_IPADDRESS:\n",
   "            display.debug(\"checking if winrm_host %s is an IPv6 address\" % winrm_host)\n",
   "                ipaddress.IPv6Address(winrm_host)\n",
   "                winrm_host = \"[%s]\" % winrm_host\n",
   "        netloc = '%s:%d' % (winrm_host, self._winrm_port)\n",
   "        endpoint = urlunsplit((self._winrm_scheme, netloc, self._winrm_path, '', ''))\n",
   "        errors = []\n",
   "        for transport in self._winrm_transport:\n",
   "            if transport == 'kerberos':\n",
   "                if not HAVE_KERBEROS:\n",
   "                    errors.append('kerberos: the python kerberos library is not installed')\n",
   "            display.vvvvv('WINRM CONNECT: transport=%s endpoint=%s' % (transport, endpoint), host=self._winrm_host)\n",
   "                winrm_kwargs = self._winrm_kwargs.copy()\n",
   "                    winrm_kwargs['operation_timeout_sec'] = self._winrm_connection_timeout\n",
   "                    winrm_kwargs['read_timeout_sec'] = self._winrm_connection_timeout + 1\n",
   "                protocol = Protocol(endpoint, transport=transport, **winrm_kwargs)\n",
   "                    self.shell_id = protocol.open_shell(codepage=65001)  # UTF-8\n",
   "                    display.vvvvv('WINRM OPEN SHELL: %s' % self.shell_id, host=self._winrm_host)\n",
   "                return protocol\n",
   "                err_msg = to_text(e).strip()\n",
   "                if re.search(to_text(r'Operation\\s+?timed\\s+?out'), err_msg, re.I):\n",
   "                m = re.search(to_text(r'Code\\s+?(\\d{3})'), err_msg)\n",
   "                if m:\n",
   "                    code = int(m.groups()[0])\n",
   "                    if code == 401:\n",
   "                        err_msg = 'the specified credentials were rejected by the server'\n",
   "                    elif code == 411:\n",
   "                        return protocol\n",
   "                errors.append(u'%s: %s' % (transport, err_msg))\n",
   "                display.vvvvv(u'WINRM CONNECTION ERROR: %s\\n%s' % (err_msg, to_text(traceback.format_exc())), host=self._winrm_host)\n",
   "        if errors:\n",
   "            raise AnsibleConnectionFailure(', '.join(map(to_native, errors)))\n",
   "        rq = {'env:Envelope': protocol._get_soap_header(\n",
   "        stream = rq['env:Envelope'].setdefault('env:Body', {}).setdefault('rsp:Send', {})\\\n",
   "        stream['@Name'] = 'stdin'\n",
   "        stream['@CommandId'] = command_id\n",
   "        stream['#text'] = base64.b64encode(to_bytes(stdin))\n",
   "            stream['@End'] = 'true'\n",
   "        protocol.send_message(xmltodict.unparse(rq))\n",
   "            display.vvvvv(\"WINRM EXEC %r %r\" % (command, args), host=self._winrm_host)\n",
   "            display.vvvvvv(\"WINRM EXEC %r %r\" % (command, args), host=self._winrm_host)\n",
   "        command_id = None\n",
   "            stdin_push_failed = False\n",
   "            command_id = self.protocol.run_command(self.shell_id, to_bytes(command), map(to_bytes, args), console_mode_stdin=(stdin_iterator is None))\n",
   "                    for (data, is_last) in stdin_iterator:\n",
   "                        self._winrm_send_input(self.protocol, self.shell_id, command_id, data, eof=is_last)\n",
   "                display.warning(\"ERROR DURING WINRM SEND INPUT - attempting to recover: %s %s\"\n",
   "                display.debug(traceback.format_exc())\n",
   "                stdin_push_failed = True\n",
   "            resptuple = self.protocol.get_command_output(self.shell_id, command_id)\n",
   "            response = Response(tuple(to_text(v) if isinstance(v, binary_type) else v for v in resptuple))\n",
   "                display.vvvvv('WINRM RESULT %r' % to_text(response), host=self._winrm_host)\n",
   "                display.vvvvvv('WINRM RESULT %r' % to_text(response), host=self._winrm_host)\n",
   "            display.vvvvvv('WINRM STDOUT %s' % to_text(response.std_out), host=self._winrm_host)\n",
   "            display.vvvvvv('WINRM STDERR %s' % to_text(response.std_err), host=self._winrm_host)\n",
   "            if stdin_push_failed:\n",
   "                    filtered_output, dummy = _filter_non_json_lines(response.std_out)\n",
   "                    json.loads(filtered_output)\n",
   "                    stderr = to_bytes(response.std_err, encoding='utf-8')\n",
   "                    if stderr.startswith(b\"#< CLIXML\"):\n",
   "                        stderr = _parse_clixml(stderr)\n",
   "                                       % (to_native(response.std_out), to_native(stderr)))\n",
   "            return response\n",
   "            if command_id:\n",
   "                self.protocol.cleanup_command(self.shell_id, command_id)\n",
   "        if not HAS_WINRM:\n",
   "            raise AnsibleError(\"winrm or requests is not installed: %s\" % to_native(WINRM_IMPORT_ERR))\n",
   "        elif not HAS_XMLTODICT:\n",
   "            raise AnsibleError(\"xmltodict is not installed: %s\" % to_native(XMLTODICT_IMPORT_ERR))\n",
   "        payload_bytes = to_bytes(payload)\n",
   "        byte_count = len(payload_bytes)\n",
   "        for i in range(0, byte_count, buffer_size):\n",
   "            yield payload_bytes[i:i + buffer_size], i + buffer_size >= byte_count\n",
   "        cmd_parts = self._shell._encode_script(cmd, as_list=True, strict_mode=False, preserve_rc=False)\n",
   "        display.vvv(\"EXEC (via pipeline wrapper)\")\n",
   "        stdin_iterator = None\n",
   "            stdin_iterator = self._wrapper_payload_stream(in_data)\n",
   "        result = self._winrm_exec(cmd_parts[0], cmd_parts[1:], from_exec=True, stdin_iterator=stdin_iterator)\n",
   "        result.std_out = to_bytes(result.std_out)\n",
   "        result.std_err = to_bytes(result.std_err)\n",
   "        if result.std_err.startswith(b\"#< CLIXML\"):\n",
   "                result.std_err = _parse_clixml(result.std_err)\n",
   "        return (result.status_code, result.std_out, result.std_err)\n",
   "        in_size = os.path.getsize(to_bytes(in_path, errors='surrogate_or_strict'))\n",
   "        offset = 0\n",
   "        with open(to_bytes(in_path, errors='surrogate_or_strict'), 'rb') as in_file:\n",
   "            for out_data in iter((lambda: in_file.read(buffer_size)), b''):\n",
   "                offset += len(out_data)\n",
   "                self._display.vvvvv('WINRM PUT \"%s\" to \"%s\" (offset=%d size=%d)' % (in_path, out_path, offset, len(out_data)), host=self._winrm_host)\n",
   "                b64_data = base64.b64encode(out_data) + b'\\r\\n'\n",
   "                yield b64_data, (in_file.tell() == in_size)\n",
   "            if offset == 0:  # empty file, return an empty buffer + eof to close it\n",
   "        out_path = self._shell._unquote(out_path)\n",
   "        display.vvv('PUT \"%s\" TO \"%s\"' % (in_path, out_path), host=self._winrm_host)\n",
   "        script_template = u'''\n",
   "        script = script_template.format(self._shell._escape(out_path))\n",
   "        cmd_parts = self._shell._encode_script(script, as_list=True, strict_mode=False, preserve_rc=False)\n",
   "        result = self._winrm_exec(cmd_parts[0], cmd_parts[1:], stdin_iterator=self._put_file_stdin_iterator(in_path, out_path))\n",
   "        if result.status_code != 0:\n",
   "            raise AnsibleError(to_native(result.std_err))\n",
   "            put_output = json.loads(result.std_out)\n",
   "            stderr = to_bytes(result.std_err, encoding='utf-8')\n",
   "            if stderr.startswith(b\"#< CLIXML\"):\n",
   "                stderr = _parse_clixml(stderr)\n",
   "            raise AnsibleError('winrm put_file failed; \\nstdout: %s\\nstderr %s' % (to_native(result.std_out), to_native(stderr)))\n",
   "        remote_sha1 = put_output.get(\"sha1\")\n",
   "        if not remote_sha1:\n",
   "        local_sha1 = secure_hash(in_path)\n",
   "        if not remote_sha1 == local_sha1:\n",
   "            raise AnsibleError(\"Remote sha1 hash {0} does not match local hash {1}\".format(to_native(remote_sha1), to_native(local_sha1)))\n",
   "        super(Connection, self).fetch_file(in_path, out_path)\n",
   "        in_path = self._shell._unquote(in_path)\n",
   "        out_path = out_path.replace('\\\\', '/')\n",
   "        display.vvv('FETCH \"%s\" TO \"%s\"' % (in_path, out_path), host=self._winrm_host)\n",
   "        buffer_size = 2**19  # 0.5MB chunks\n",
   "        out_file = None\n",
   "            offset = 0\n",
   "                    script = '''\n",
   "                    ''' % dict(buffer_size=buffer_size, path=self._shell._escape(in_path), offset=offset)\n",
   "                    display.vvvvv('WINRM FETCH \"%s\" to \"%s\" (offset=%d)' % (in_path, out_path, offset), host=self._winrm_host)\n",
   "                    cmd_parts = self._shell._encode_script(script, as_list=True, preserve_rc=False)\n",
   "                    result = self._winrm_exec(cmd_parts[0], cmd_parts[1:])\n",
   "                    if result.status_code != 0:\n",
   "                        raise IOError(to_native(result.std_err))\n",
   "                    if result.std_out.strip() == '[DIR]':\n",
   "                        data = None\n",
   "                        data = base64.b64decode(result.std_out.strip())\n",
   "                    if data is None:\n",
   "                        if not out_file:\n",
   "                            if os.path.isdir(to_bytes(out_path, errors='surrogate_or_strict')):\n",
   "                            out_file = open(to_bytes(out_path, errors='surrogate_or_strict'), 'wb')\n",
   "                        out_file.write(data)\n",
   "                        if len(data) < buffer_size:\n",
   "                        offset += len(data)\n",
   "                    raise AnsibleError('failed to transfer file to \"%s\"' % to_native(out_path))\n",
   "            if out_file:\n",
   "                out_file.close()\n",
   "            display.vvvvv('WINRM CLOSE SHELL: %s' % self.shell_id, host=self._winrm_host)\n"
  ]
 },
 "138": {
  "name": "default_user",
  "type": "NoneType",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/plugins/connection/__init__.py",
  "lineno": "59",
  "column": "4",
  "context": "stence = False\n    force_persistence = False\n\n    default_user = None\n\n    def __init__(self, play_context, new_stdin, s",
  "context_lines": "    # the following control whether or not the connection supports the\n    # persistent connection framework or not\n    supports_persistence = False\n    force_persistence = False\n\n    default_user = None\n\n    def __init__(self, play_context, new_stdin, shell=None, *args, **kwargs):\n\n        super(ConnectionBase, self).__init__()\n\n        # All these hasattrs allow subclasses to override these parameters\n",
  "slicing": "    default_user = None\n"
 },
 "139": {
  "name": "transport",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/connection/paramiko_ssh.py",
  "lineno": "232",
  "column": "4",
  "context": " ''' SSH based connections with Paramiko '''\n\n    transport = 'paramiko'\n    _log_channel = None\n\n    def _cache_key(self):",
  "context_lines": "SSH_CONNECTION_CACHE = {}\nSFTP_CONNECTION_CACHE = {}\n\n\nclass Connection(ConnectionBase):\n    ''' SSH based connections with Paramiko '''\n\n    transport = 'paramiko'\n    _log_channel = None\n\n    def _cache_key(self):\n        return \"%s__%s__\" % (self._play_context.remote_addr, self._play_context.remote_user)\n\n    def _connect(self):\n",
  "slicing": "    transport = 'paramiko'\n"
 },
 "140": {
  "name": "_log_channel",
  "type": "NoneType",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/plugins/connection/paramiko_ssh.py",
  "lineno": "233",
  "column": "4",
  "context": "with Paramiko '''\n\n    transport = 'paramiko'\n    _log_channel = None\n\n    def _cache_key(self):\n        return \"%s__%s_",
  "context_lines": "SFTP_CONNECTION_CACHE = {}\n\n\nclass Connection(ConnectionBase):\n    ''' SSH based connections with Paramiko '''\n\n    transport = 'paramiko'\n    _log_channel = None\n\n    def _cache_key(self):\n        return \"%s__%s__\" % (self._play_context.remote_addr, self._play_context.remote_user)\n\n    def _connect(self):\n",
  "slicing": "    _log_channel = None\n"
 },
 "141": {
  "name": "transport",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/connection/ssh.py",
  "lineno": "465",
  "column": "4",
  "context": "tionBase):\n    ''' ssh based connections '''\n\n    transport = 'ssh'\n    has_pipelining = True\n\n    def __init__(self, ",
  "context_lines": "        return return_tuple\n    return wrapped\n\n\nclass Connection(ConnectionBase):\n    ''' ssh based connections '''\n\n    transport = 'ssh'\n    has_pipelining = True\n\n    def __init__(self, *args, **kwargs):\n        super(Connection, self).__init__(*args, **kwargs)\n\n        self.host = self._play_context.remote_addr\n",
  "slicing": "    transport = 'ssh'\n"
 },
 "142": {
  "name": "transport",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/connection/local.py",
  "lineno": "40",
  "column": "4",
  "context": "onBase):\n    ''' Local based connections '''\n\n    transport = 'local'\n    has_pipelining = True\n\n    def __init__(self, ",
  "context_lines": "from ansible.utils.path import unfrackpath\n\ndisplay = Display()\n\n\nclass Connection(ConnectionBase):\n    ''' Local based connections '''\n\n    transport = 'local'\n    has_pipelining = True\n\n    def __init__(self, *args, **kwargs):\n\n        super(Connection, self).__init__(*args, **kwargs)\n        self.cwd = None\n\n",
  "slicing": "    transport = 'local'\n"
 },
 "143": {
  "name": "NAME",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/inventory/constructed.py",
  "lineno": "85",
  "column": "4",
  "context": "d vars using Jinja2 template expressions \"\"\"\n\n    NAME = 'constructed'\n\n    def __init__(self):\n\n        super(InventoryM",
  "context_lines": "from ansible.utils.vars import combine_vars\nfrom ansible.vars.fact_cache import FactCache\n\n\nclass InventoryModule(BaseInventoryPlugin, Constructable):\n    \"\"\" constructs groups and vars using Jinja2 template expressions \"\"\"\n\n    NAME = 'constructed'\n\n    def __init__(self):\n\n        super(InventoryModule, self).__init__()\n\n        self._cache = FactCache()\n\n",
  "slicing": "    NAME = 'constructed'\n"
 },
 "144": {
  "name": "TYPE",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/inventory/__init__.py",
  "lineno": "152",
  "column": "4",
  "context": "ugin):\n    \"\"\" Parses an Inventory Source\"\"\"\n\n    TYPE = 'generator'\n    _sanitize_group_name = staticmethod(to_safe_gr",
  "context_lines": "                           'from ansible.constants.'.format(plugin_name))\n    return cache\n\n\nclass BaseInventoryPlugin(AnsiblePlugin):\n    \"\"\" Parses an Inventory Source\"\"\"\n\n    TYPE = 'generator'\n    _sanitize_group_name = staticmethod(to_safe_group_name)\n\n    def __init__(self):\n\n        super(BaseInventoryPlugin, self).__init__()\n\n        self._options = {}\n",
  "slicing": "    TYPE = 'generator'\n"
 },
 "145": {
  "name": "_sanitize_group_name",
  "type": "staticmethod",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/plugins/inventory/__init__.py",
  "lineno": "153",
  "column": "4",
  "context": "n Inventory Source\"\"\"\n\n    TYPE = 'generator'\n    _sanitize_group_name = staticmethod(to_safe_group_name)\n\n    def __init__(self):\n\n        super(BaseInvent",
  "context_lines": "    return cache\n\n\nclass BaseInventoryPlugin(AnsiblePlugin):\n    \"\"\" Parses an Inventory Source\"\"\"\n\n    TYPE = 'generator'\n    _sanitize_group_name = staticmethod(to_safe_group_name)\n\n    def __init__(self):\n\n        super(BaseInventoryPlugin, self).__init__()\n\n        self._options = {}\n",
  "slicing": [
   "def detect_range(line=None):\n",
   "    all_hosts = []\n",
   "        (head, nrange, tail) = line.replace('[', '|', 1).replace(']', '|', 1).split('|')\n",
   "        bounds = nrange.split(\":\")\n",
   "        if len(bounds) != 2 and len(bounds) != 3:\n",
   "        beg = bounds[0]\n",
   "        end = bounds[1]\n",
   "        if len(bounds) == 2:\n",
   "            step = 1\n",
   "            step = bounds[2]\n",
   "        if not beg:\n",
   "            beg = \"0\"\n",
   "        if not end:\n",
   "        if beg[0] == '0' and len(beg) > 1:\n",
   "            rlen = len(beg)  # range length formatting hint\n",
   "            if rlen != len(end):\n",
   "            def fill(x):\n",
   "                return str(x).zfill(rlen)  # range sequence\n",
   "            fill = str\n",
   "            i_beg = string.ascii_letters.index(beg)\n",
   "            i_end = string.ascii_letters.index(end)\n",
   "            if i_beg > i_end:\n",
   "            seq = list(string.ascii_letters[i_beg:i_end + 1:int(step)])\n",
   "            seq = range(int(beg), int(end) + 1, int(step))\n",
   "        for rseq in seq:\n",
   "            hname = ''.join((head, fill(rseq), tail))\n",
   "            if detect_range(hname):\n",
   "                all_hosts.extend(expand_hostname_range(hname))\n",
   "                all_hosts.append(hname)\n",
   "        return all_hosts\n",
   "        cache = CacheObject(plugin_name, **kwargs)\n",
   "    if plugin_name != 'memory' and kwargs and not getattr(cache._plugin, '_options', None):\n",
   "    return cache\n",
   "    _sanitize_group_name = staticmethod(to_safe_group_name)\n",
   "        if detect_range(pattern):\n"
  ]
 },
 "146": {
  "name": "TYPE",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/inventory/__init__.py",
  "lineno": "279",
  "column": "4",
  "context": " \"\"\" Parses a File based Inventory Source\"\"\"\n\n    TYPE = 'storage'\n\n    def __init__(self):\n\n        super(BaseFileIn",
  "context_lines": "            hostnames = [pattern]\n\n        return (hostnames, port)\n\n\nclass BaseFileInventoryPlugin(BaseInventoryPlugin):\n    \"\"\" Parses a File based Inventory Source\"\"\"\n\n    TYPE = 'storage'\n\n    def __init__(self):\n\n        super(BaseFileInventoryPlugin, self).__init__()\n\n\nclass DeprecatedCache(object):\n",
  "slicing": "    TYPE = 'storage'\n"
 },
 "147": {
  "name": "_cache",
  "type": "ansible.plugins.cache.CachePluginAdjudicator",
  "class": "imported",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/plugins/inventory/__init__.py",
  "lineno": "314",
  "column": "4",
  "context": "attribute__(name)\n\n\nclass Cacheable(object):\n\n    _cache = CacheObject()\n\n    @property\n    def cache(self):\n        return",
  "context_lines": "        display.deprecated('InventoryModule should utilize self._cache instead of self.cache',\n                           version='2.12', collection_name='ansible.builtin')\n        return self.real_cacheable._cache.__getattribute__(name)\n\n\nclass Cacheable(object):\n\n    _cache = CacheObject()\n\n    @property\n    def cache(self):\n        return DeprecatedCache(self)\n\n",
  "slicing": [
   "display = Display()\n",
   "def detect_range(line=None):\n",
   "def expand_hostname_range(line=None):\n",
   "    all_hosts = []\n",
   "        (head, nrange, tail) = line.replace('[', '|', 1).replace(']', '|', 1).split('|')\n",
   "        bounds = nrange.split(\":\")\n",
   "        if len(bounds) != 2 and len(bounds) != 3:\n",
   "        beg = bounds[0]\n",
   "        end = bounds[1]\n",
   "        if len(bounds) == 2:\n",
   "            step = 1\n",
   "            step = bounds[2]\n",
   "        if not beg:\n",
   "            beg = \"0\"\n",
   "        if not end:\n",
   "        if beg[0] == '0' and len(beg) > 1:\n",
   "            rlen = len(beg)  # range length formatting hint\n",
   "            if rlen != len(end):\n",
   "            def fill(x):\n",
   "                return str(x).zfill(rlen)  # range sequence\n",
   "            fill = str\n",
   "            i_beg = string.ascii_letters.index(beg)\n",
   "            i_end = string.ascii_letters.index(end)\n",
   "            if i_beg > i_end:\n",
   "            seq = list(string.ascii_letters[i_beg:i_end + 1:int(step)])\n",
   "            seq = range(int(beg), int(end) + 1, int(step))\n",
   "        for rseq in seq:\n",
   "            hname = ''.join((head, fill(rseq), tail))\n",
   "            if detect_range(hname):\n",
   "                all_hosts.extend(expand_hostname_range(hname))\n",
   "                all_hosts.append(hname)\n",
   "        return all_hosts\n",
   "def get_cache_plugin(plugin_name, **kwargs):\n",
   "        cache = CacheObject(plugin_name, **kwargs)\n",
   "    if plugin_name != 'memory' and kwargs and not getattr(cache._plugin, '_options', None):\n",
   "                           'from ansible.constants.'.format(plugin_name))\n",
   "    return cache\n",
   "        self.display = display\n",
   "        valid = False\n",
   "        b_path = to_bytes(path, errors='surrogate_or_strict')\n",
   "        if (os.path.exists(b_path) and os.access(b_path, os.R_OK)):\n",
   "            valid = True\n",
   "        return valid\n",
   "        for host in hosts:\n",
   "            self.inventory.add_host(host, group=group, port=port)\n",
   "            for k in variables:\n",
   "                self.inventory.set_variable(host, k, variables[k])\n",
   "        config = {}\n",
   "            config = self.loader.load_from_file(path, cache=False)\n",
   "        valid_names = getattr(self, '_redirected_names') or [self.NAME]\n",
   "        if not config:\n",
   "        elif config.get('plugin') not in valid_names:\n",
   "            raise AnsibleParserError(\"Incorrect plugin name in file: %s\" % config.get('plugin', 'none found'))\n",
   "        elif not isinstance(config, Mapping):\n",
   "            raise AnsibleParserError('inventory source has invalid structure, it should be a dictionary, got: %s' % type(config))\n",
   "        self.set_options(direct=config)\n",
   "            cache_option_keys = [('_uri', 'cache_connection'), ('_timeout', 'cache_timeout'), ('_prefix', 'cache_prefix')]\n",
   "            cache_options = dict((opt[0], self.get_option(opt[1])) for opt in cache_option_keys if self.get_option(opt[1]))\n",
   "            self._cache = get_cache_plugin(self.get_option('cache_plugin'), **cache_options)\n",
   "        return config\n",
   "        for k in self._options:\n",
   "            if k in data:\n",
   "                self._options[k] = data.pop(k)\n",
   "            (pattern, port) = parse_address(hostpattern, allow_ranges=True)\n",
   "            pattern = hostpattern\n",
   "            port = None\n",
   "        if detect_range(pattern):\n",
   "            hostnames = expand_hostname_range(pattern)\n",
   "            hostnames = [pattern]\n",
   "        return (hostnames, port)\n",
   "        display.deprecated('InventoryModule should utilize self._cache as a dict instead of self.cache. '\n",
   "        display.deprecated('InventoryModule should utilize self._cache as a dict instead of self.cache. '\n",
   "        display.deprecated('InventoryModule should utilize self._cache instead of self.cache',\n",
   "    _cache = CacheObject()\n",
   "        plugin_name = self.get_option('cache_plugin')\n",
   "        cache_option_keys = [('_uri', 'cache_connection'), ('_timeout', 'cache_timeout'), ('_prefix', 'cache_prefix')]\n",
   "        cache_options = dict((opt[0], self.get_option(opt[1])) for opt in cache_option_keys if self.get_option(opt[1]))\n",
   "        self._cache = get_cache_plugin(plugin_name, **cache_options)\n",
   "        m = hashlib.sha1()\n",
   "        m.update(to_bytes(self.NAME, errors='surrogate_or_strict'))\n",
   "        d1 = m.hexdigest()\n",
   "        n = hashlib.sha1()\n",
   "        n.update(to_bytes(path, errors='surrogate_or_strict'))\n",
   "        d2 = n.hexdigest()\n",
   "        return 's_'.join([d1[:5], d2[:5]])\n",
   "        t = self.templar\n",
   "        t.available_variables = variables\n",
   "        return t.template('%s%s%s' % (t.environment.variable_start_string, template, t.environment.variable_end_string), disable_lookups=True)\n",
   "            for varname in compose:\n",
   "                    composite = self._compose(compose[varname], variables)\n",
   "                        raise AnsibleError(\"Could not set %s for host %s: %s\" % (varname, host, to_native(e)))\n",
   "                self.inventory.set_variable(host, varname, composite)\n",
   "            variables = combine_vars(variables, self.inventory.get_host(host).get_vars())\n",
   "            self.templar.available_variables = variables\n",
   "            for group_name in groups:\n",
   "                conditional = \"{%% if %s %%} True {%% else %%} False {%% endif %%}\" % groups[group_name]\n",
   "                group_name = original_safe(group_name, force=True)\n",
   "                    result = boolean(self.templar.template(conditional))\n",
   "                        raise AnsibleParserError(\"Could not add host %s to group %s: %s\" % (host, group_name, to_native(e)))\n",
   "                if result:\n",
   "                    group_name = self.inventory.add_group(group_name)\n",
   "                    self.inventory.add_child(group_name, host)\n",
   "            for keyed in keys:\n",
   "                if keyed and isinstance(keyed, dict):\n",
   "                    variables = combine_vars(variables, self.inventory.get_host(host).get_vars())\n",
   "                        key = self._compose(keyed.get('key'), variables)\n",
   "                            raise AnsibleParserError(\"Could not generate group for host %s from %s entry: %s\" % (host, keyed.get('key'), to_native(e)))\n",
   "                    if key:\n",
   "                        prefix = keyed.get('prefix', '')\n",
   "                        sep = keyed.get('separator', '_')\n",
   "                        raw_parent_name = keyed.get('parent_group', None)\n",
   "                        if raw_parent_name:\n",
   "                                raw_parent_name = self.templar.template(raw_parent_name)\n",
   "                                    raise AnsibleParserError(\"Could not generate parent group %s for group %s: %s\" % (raw_parent_name, key, to_native(e)))\n",
   "                        new_raw_group_names = []\n",
   "                        if isinstance(key, string_types):\n",
   "                            new_raw_group_names.append(key)\n",
   "                        elif isinstance(key, list):\n",
   "                            for name in key:\n",
   "                                new_raw_group_names.append(name)\n",
   "                        elif isinstance(key, Mapping):\n",
   "                            for (gname, gval) in key.items():\n",
   "                                name = '%s%s%s' % (gname, sep, gval)\n",
   "                                new_raw_group_names.append(name)\n",
   "                            raise AnsibleParserError(\"Invalid group name format, expected a string or a list of them or dictionary, got: %s\" % type(key))\n",
   "                        for bare_name in new_raw_group_names:\n",
   "                            gname = self._sanitize_group_name('%s%s%s' % (prefix, sep, bare_name))\n",
   "                            result_gname = self.inventory.add_group(gname)\n",
   "                            self.inventory.add_host(host, result_gname)\n",
   "                            if raw_parent_name:\n",
   "                                parent_name = self._sanitize_group_name(raw_parent_name)\n",
   "                                self.inventory.add_group(parent_name)\n",
   "                                self.inventory.add_child(parent_name, result_gname)\n",
   "                        if strict and key not in ([], {}):\n",
   "                            raise AnsibleParserError(\"No key or key resulted empty for %s in host %s, invalid entry\" % (keyed.get('key'), host))\n",
   "                    raise AnsibleParserError(\"Invalid keyed group entry, it must be a dictionary: %s \" % keyed)\n"
  ]
 },
 "148": {
  "name": "COMPATIBLE_SHELLS",
  "type": "frozenset",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/plugins/shell/powershell.py",
  "lineno": "60",
  "column": "4",
  "context": "s selected when winrm is the\n    # connection\n    COMPATIBLE_SHELLS = frozenset()\n    # Family of shells this has.  Must match the f",
  "context_lines": "class ShellModule(ShellBase):\n\n    # Common shell filenames that this plugin handles\n    # Powershell is handled differently.  It's selected when winrm is the\n    # connection\n    COMPATIBLE_SHELLS = frozenset()\n    # Family of shells this has.  Must match the filename without extension\n    SHELL_FAMILY = 'powershell'\n\n    _SHELL_REDIRECT_ALLNULL = '> $null'\n    _SHELL_AND = ';'\n\n",
  "slicing": [
   "_common_args = ['PowerShell', '-NoProfile', '-NonInteractive', '-ExecutionPolicy', 'Unrestricted']\n",
   "_powershell_version = os.environ.get('POWERSHELL_VERSION', None)\n",
   "if _powershell_version:\n",
   "    _common_args = ['PowerShell', '-Version', _powershell_version] + _common_args[1:]\n",
   "    clixml = ET.fromstring(data.split(b\"\\r\\n\", 1)[-1])\n",
   "    namespace_match = re.match(r'{(.*)}', clixml.tag)\n",
   "    namespace = \"{%s}\" % namespace_match.group(1) if namespace_match else \"\"\n",
   "    strings = clixml.findall(\"./%sS\" % namespace)\n",
   "    lines = [e.text.replace('_x000D__x000A_', '') for e in strings if e.attrib.get('S') == stream]\n",
   "    return to_bytes('\\r\\n'.join(lines))\n",
   "    COMPATIBLE_SHELLS = frozenset()\n",
   "        parts = [ntpath.normpath(self._unquote(arg)) for arg in args]\n",
   "        return ntpath.join(parts[0], *[part.strip('\\\\') for part in parts[1:]])\n",
   "        base_name = os.path.basename(pathname.strip())\n",
   "        name, ext = os.path.splitext(base_name.strip())\n",
   "        if ext.lower() not in ['.ps1', '.exe']:\n",
   "            return name + '.ps1'\n",
   "        return base_name.strip()\n",
   "        path = self._unquote(path)\n",
   "        return path.endswith('/') or path.endswith('\\\\')\n",
   "        path = self._escape(self._unquote(path))\n",
   "            return self._encode_script('''Remove-Item \"%s\" -Force -Recurse;''' % path)\n",
   "            return self._encode_script('''Remove-Item \"%s\" -Force;''' % path)\n",
   "            basefile = self.__class__._generate_temp_dir_name()\n",
   "        basefile = self._escape(self._unquote(basefile))\n",
   "        basetmpdir = tmpdir if tmpdir else self.get_option('remote_tmp')\n",
   "        script = '''\n",
   "        ''' % (basetmpdir, basefile)\n",
   "        return self._encode_script(script.strip())\n",
   "        user_home_path = self._unquote(user_home_path)\n",
   "        if user_home_path == '~':\n",
   "            script = 'Write-Output (Get-Location).Path'\n",
   "        elif user_home_path.startswith('~\\\\'):\n",
   "            script = 'Write-Output ((Get-Location).Path + \"%s\")' % self._escape(user_home_path[1:])\n",
   "            script = 'Write-Output \"%s\"' % self._escape(user_home_path)\n",
   "        return self._encode_script(script)\n",
   "        path = self._escape(self._unquote(path))\n",
   "        script = '''\n",
   "         ''' % path\n",
   "        return self._encode_script(script)\n",
   "        path = self._escape(self._unquote(path))\n",
   "        script = '''\n",
   "        ''' % dict(path=path)\n",
   "        return self._encode_script(script)\n",
   "        bootstrap_wrapper = pkgutil.get_data(\"ansible.executor.powershell\", \"bootstrap_wrapper.ps1\")\n",
   "            return self._encode_script(script=bootstrap_wrapper, strict_mode=False, preserve_rc=False)\n",
   "        cmd_parts = shlex.split(cmd, posix=False)\n",
   "        cmd_parts = list(map(to_text, cmd_parts))\n",
   "            if not self._unquote(cmd_parts[0]).lower().endswith('.ps1'):\n",
   "                cmd_parts[0] = '\"%s.ps1\"' % self._unquote(cmd_parts[0])\n",
   "            wrapper_cmd = \"type \" + cmd_parts[0] + \" | \" + self._encode_script(script=bootstrap_wrapper, strict_mode=False, preserve_rc=False)\n",
   "            return wrapper_cmd\n",
   "            cmd_parts.insert(0, shebang[2:])\n",
   "            cmd_parts[0] = self._unquote(cmd_parts[0])\n",
   "            cmd_parts.append(arg_path)\n",
   "        script = '''\n",
   "        ''' % (env_string, ' '.join(cmd_parts))\n",
   "        return self._encode_script(script, preserve_rc=False)\n",
   "        value = to_text(value or '')\n",
   "        m = re.match(r'^\\s*?\\'(.*?)\\'\\s*?$', value)\n",
   "        if m:\n",
   "            return m.group(1)\n",
   "        m = re.match(r'^\\s*?\"(.*?)\"\\s*?$', value)\n",
   "        if m:\n",
   "            return m.group(1)\n",
   "        return value\n",
   "        subs = [('\\n', '`n'), ('\\r', '`r'), ('\\t', '`t'), ('\\a', '`a'),\n",
   "            subs.append(('$', '`$'))\n",
   "        pattern = '|'.join('(%s)' % re.escape(p) for p, s in subs)\n",
   "        substs = [s for p, s in subs]\n",
   "            return substs[m.lastindex - 1]\n",
   "        return re.sub(pattern, replace, value)\n",
   "        script = to_text(script)\n",
   "        if script == u'-':\n",
   "            cmd_parts = _common_args + ['-Command', '-']\n",
   "                script = u'Set-StrictMode -Version Latest\\r\\n%s' % script\n",
   "                script = u'%s\\r\\nIf (-not $?) { If (Get-Variable LASTEXITCODE -ErrorAction SilentlyContinue) { exit $LASTEXITCODE } Else { exit 1 } }\\r\\n'\\\n",
   "                    % script\n",
   "            script = '\\n'.join([x.strip() for x in script.splitlines() if x.strip()])\n",
   "            encoded_script = to_text(base64.b64encode(script.encode('utf-16-le')), 'utf-8')\n",
   "            cmd_parts = _common_args + ['-EncodedCommand', encoded_script]\n",
   "            return cmd_parts\n",
   "        return ' '.join(cmd_parts)\n"
  ]
 },
 "149": {
  "name": "SHELL_FAMILY",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/shell/powershell.py",
  "lineno": "62",
  "column": "4",
  "context": "s.  Must match the filename without extension\n    SHELL_FAMILY = 'powershell'\n\n    _SHELL_REDIRECT_ALLNULL = '> $null'\n    _SHEL",
  "context_lines": "    # Powershell is handled differently.  It's selected when winrm is the\n    # connection\n    COMPATIBLE_SHELLS = frozenset()\n    # Family of shells this has.  Must match the filename without extension\n    SHELL_FAMILY = 'powershell'\n\n    _SHELL_REDIRECT_ALLNULL = '> $null'\n    _SHELL_AND = ';'\n\n    # Used by various parts of Ansible to do Windows specific changes\n",
  "slicing": "    SHELL_FAMILY = 'powershell'\n"
 },
 "150": {
  "name": "_SHELL_REDIRECT_ALLNULL",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/shell/powershell.py",
  "lineno": "64",
  "column": "4",
  "context": "ut extension\n    SHELL_FAMILY = 'powershell'\n\n    _SHELL_REDIRECT_ALLNULL = '> $null'\n    _SHELL_AND = ';'\n\n    # Used by various parts ",
  "context_lines": "    # connection\n    COMPATIBLE_SHELLS = frozenset()\n    # Family of shells this has.  Must match the filename without extension\n    SHELL_FAMILY = 'powershell'\n\n    _SHELL_REDIRECT_ALLNULL = '> $null'\n    _SHELL_AND = ';'\n\n    # Used by various parts of Ansible to do Windows specific changes\n    _IS_WINDOWS = True\n\n    # TODO: add binary module support\n\n",
  "slicing": "    _SHELL_REDIRECT_ALLNULL = '> $null'\n"
 },
 "151": {
  "name": "_SHELL_AND",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/shell/powershell.py",
  "lineno": "65",
  "column": "4",
  "context": "ell'\n\n    _SHELL_REDIRECT_ALLNULL = '> $null'\n    _SHELL_AND = ';'\n\n    # Used by various parts of Ansible to do Wind",
  "context_lines": "    COMPATIBLE_SHELLS = frozenset()\n    # Family of shells this has.  Must match the filename without extension\n    SHELL_FAMILY = 'powershell'\n\n    _SHELL_REDIRECT_ALLNULL = '> $null'\n    _SHELL_AND = ';'\n\n    # Used by various parts of Ansible to do Windows specific changes\n    _IS_WINDOWS = True\n\n    # TODO: add binary module support\n\n",
  "slicing": "    _SHELL_AND = ';'\n"
 },
 "152": {
  "name": "COMPATIBLE_SHELLS",
  "type": "frozenset",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/plugins/shell/cmd.py",
  "lineno": "28",
  "column": "4",
  "context": "mmon shell filenames that this plugin handles\n    COMPATIBLE_SHELLS = frozenset()\n    # Family of shells this has.  Must match the f",
  "context_lines": "# these are the metachars that have a special meaning in cmd that we want to escape when quoting\n_find_unsafe = re.compile(r'[\\s\\(\\)\\%\\!^\\\"\\<\\>\\&\\|]').search\n\n\nclass ShellModule(PSShellModule):\n\n    # Common shell filenames that this plugin handles\n    COMPATIBLE_SHELLS = frozenset()\n    # Family of shells this has.  Must match the filename without extension\n    SHELL_FAMILY = 'cmd'\n\n    _SHELL_REDIRECT_ALLNULL = '>nul 2>&1'\n    _SHELL_AND = '&&'\n\n",
  "slicing": [
   "    COMPATIBLE_SHELLS = frozenset()\n"
  ]
 },
 "153": {
  "name": "SHELL_FAMILY",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/shell/cmd.py",
  "lineno": "30",
  "column": "4",
  "context": "s.  Must match the filename without extension\n    SHELL_FAMILY = 'cmd'\n\n    _SHELL_REDIRECT_ALLNULL = '>nul 2>&1'\n    _SH",
  "context_lines": "class ShellModule(PSShellModule):\n\n    # Common shell filenames that this plugin handles\n    COMPATIBLE_SHELLS = frozenset()\n    # Family of shells this has.  Must match the filename without extension\n    SHELL_FAMILY = 'cmd'\n\n    _SHELL_REDIRECT_ALLNULL = '>nul 2>&1'\n    _SHELL_AND = '&&'\n\n    # Used by various parts of Ansible to do Windows specific changes\n",
  "slicing": "    SHELL_FAMILY = 'cmd'\n"
 },
 "154": {
  "name": "_SHELL_REDIRECT_ALLNULL",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/shell/cmd.py",
  "lineno": "32",
  "column": "4",
  "context": "e without extension\n    SHELL_FAMILY = 'cmd'\n\n    _SHELL_REDIRECT_ALLNULL = '>nul 2>&1'\n    _SHELL_AND = '&&'\n\n    # Used by various parts",
  "context_lines": "    # Common shell filenames that this plugin handles\n    COMPATIBLE_SHELLS = frozenset()\n    # Family of shells this has.  Must match the filename without extension\n    SHELL_FAMILY = 'cmd'\n\n    _SHELL_REDIRECT_ALLNULL = '>nul 2>&1'\n    _SHELL_AND = '&&'\n\n    # Used by various parts of Ansible to do Windows specific changes\n    _IS_WINDOWS = True\n\n    def quote(self, s):\n",
  "slicing": "    _SHELL_REDIRECT_ALLNULL = '>nul 2>&1'\n"
 },
 "155": {
  "name": "_SHELL_AND",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/shell/cmd.py",
  "lineno": "33",
  "column": "4",
  "context": "d'\n\n    _SHELL_REDIRECT_ALLNULL = '>nul 2>&1'\n    _SHELL_AND = '&&'\n\n    # Used by various parts of Ansible to do Wind",
  "context_lines": "    COMPATIBLE_SHELLS = frozenset()\n    # Family of shells this has.  Must match the filename without extension\n    SHELL_FAMILY = 'cmd'\n\n    _SHELL_REDIRECT_ALLNULL = '>nul 2>&1'\n    _SHELL_AND = '&&'\n\n    # Used by various parts of Ansible to do Windows specific changes\n    _IS_WINDOWS = True\n\n    def quote(self, s):\n",
  "slicing": "    _SHELL_AND = '&&'\n"
 },
 "156": {
  "name": "DEFAULT_NEWLINE_SEQUENCE",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/action/template.py",
  "lineno": "26",
  "column": "4",
  "context": "dule(ActionBase):\n\n    TRANSFERS_FILES = True\n    DEFAULT_NEWLINE_SEQUENCE = \"\\n\"\n\n    def run(self, tmp=None, task_vars=None):\n    ",
  "context_lines": "from ansible.plugins.action import ActionBase\nfrom ansible.template import generate_ansible_template_vars\n\n\nclass ActionModule(ActionBase):\n\n    TRANSFERS_FILES = True\n    DEFAULT_NEWLINE_SEQUENCE = \"\\n\"\n\n    def run(self, tmp=None, task_vars=None):\n        ''' handler for template operations '''\n\n        if task_vars is None:\n",
  "slicing": "    DEFAULT_NEWLINE_SEQUENCE = \"\\n\"\n"
 },
 "157": {
  "name": "_VALID_ARGS",
  "type": "frozenset",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/plugins/action/__init__.py",
  "lineno": "49",
  "column": "4",
  "context": " use.\n    '''\n\n    # A set of valid arguments\n    _VALID_ARGS = frozenset([])\n\n    def __init__(self, task, connection, play_con",
  "context_lines": "    by putting/getting files and executing commands based on the current\n    action in use.\n    '''\n\n    # A set of valid arguments\n    _VALID_ARGS = frozenset([])\n\n    def __init__(self, task, connection, play_context, loader, templar, shared_loader_obj):\n        self._task = task\n        self._connection = connection\n",
  "slicing": [
   "    _VALID_ARGS = frozenset([])\n"
  ]
 },
 "158": {
  "name": "_VALID_ARGS",
  "type": "frozenset",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/plugins/action/reboot.py",
  "lineno": "29",
  "column": "4",
  "context": "dule(ActionBase):\n    TRANSFERS_FILES = False\n    _VALID_ARGS = frozenset((\n        'boot_time_command',\n        'connect_time",
  "context_lines": "class TimedOutException(Exception):\n    pass\n\n\nclass ActionModule(ActionBase):\n    TRANSFERS_FILES = False\n    _VALID_ARGS = frozenset((\n        'boot_time_command',\n        'connect_timeout',\n        'msg',\n        'post_reboot_delay',\n",
  "slicing": [
   "    _VALID_ARGS = frozenset((\n"
  ]
 },
 "159": {
  "name": "DEFAULT_REBOOT_TIMEOUT",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/action/reboot.py",
  "lineno": "40",
  "column": "4",
  "context": "boot_timeout',\n        'search_paths'\n    ))\n\n    DEFAULT_REBOOT_TIMEOUT = 600\n    DEFAULT_CONNECT_TIMEOUT = None\n    DEFAULT_PRE",
  "context_lines": "        'test_command',\n        'reboot_timeout',\n        'search_paths'\n    ))\n\n    DEFAULT_REBOOT_TIMEOUT = 600\n    DEFAULT_CONNECT_TIMEOUT = None\n    DEFAULT_PRE_REBOOT_DELAY = 0\n    DEFAULT_POST_REBOOT_DELAY = 0\n    DEFAULT_TEST_COMMAND = 'whoami'\n",
  "slicing": "    DEFAULT_REBOOT_TIMEOUT = 600\n"
 },
 "160": {
  "name": "DEFAULT_CONNECT_TIMEOUT",
  "type": "NoneType",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/plugins/action/reboot.py",
  "lineno": "41",
  "column": "4",
  "context": "ths'\n    ))\n\n    DEFAULT_REBOOT_TIMEOUT = 600\n    DEFAULT_CONNECT_TIMEOUT = None\n    DEFAULT_PRE_REBOOT_DELAY = 0\n    DEFAULT_POST_",
  "context_lines": "        'reboot_timeout',\n        'search_paths'\n    ))\n\n    DEFAULT_REBOOT_TIMEOUT = 600\n    DEFAULT_CONNECT_TIMEOUT = None\n    DEFAULT_PRE_REBOOT_DELAY = 0\n    DEFAULT_POST_REBOOT_DELAY = 0\n    DEFAULT_TEST_COMMAND = 'whoami'\n    DEFAULT_BOOT_TIME_COMMAND = 'cat /proc/sys/kernel/random/boot_id'\n",
  "slicing": "    DEFAULT_CONNECT_TIMEOUT = None\n"
 },
 "161": {
  "name": "DEFAULT_PRE_REBOOT_DELAY",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/action/reboot.py",
  "lineno": "42",
  "column": "4",
  "context": "EOUT = 600\n    DEFAULT_CONNECT_TIMEOUT = None\n    DEFAULT_PRE_REBOOT_DELAY = 0\n    DEFAULT_POST_REBOOT_DELAY = 0\n    DEFAULT_TEST",
  "context_lines": "        'search_paths'\n    ))\n\n    DEFAULT_REBOOT_TIMEOUT = 600\n    DEFAULT_CONNECT_TIMEOUT = None\n    DEFAULT_PRE_REBOOT_DELAY = 0\n    DEFAULT_POST_REBOOT_DELAY = 0\n    DEFAULT_TEST_COMMAND = 'whoami'\n    DEFAULT_BOOT_TIME_COMMAND = 'cat /proc/sys/kernel/random/boot_id'\n    DEFAULT_REBOOT_MESSAGE = 'Reboot initiated by Ansible'\n",
  "slicing": "    DEFAULT_PRE_REBOOT_DELAY = 0\n"
 },
 "162": {
  "name": "DEFAULT_POST_REBOOT_DELAY",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/action/reboot.py",
  "lineno": "43",
  "column": "4",
  "context": "MEOUT = None\n    DEFAULT_PRE_REBOOT_DELAY = 0\n    DEFAULT_POST_REBOOT_DELAY = 0\n    DEFAULT_TEST_COMMAND = 'whoami'\n    DEFAULT_BO",
  "context_lines": "    ))\n\n    DEFAULT_REBOOT_TIMEOUT = 600\n    DEFAULT_CONNECT_TIMEOUT = None\n    DEFAULT_PRE_REBOOT_DELAY = 0\n    DEFAULT_POST_REBOOT_DELAY = 0\n    DEFAULT_TEST_COMMAND = 'whoami'\n    DEFAULT_BOOT_TIME_COMMAND = 'cat /proc/sys/kernel/random/boot_id'\n    DEFAULT_REBOOT_MESSAGE = 'Reboot initiated by Ansible'\n    DEFAULT_SHUTDOWN_COMMAND = 'shutdown'\n",
  "slicing": "    DEFAULT_POST_REBOOT_DELAY = 0\n"
 },
 "163": {
  "name": "DEFAULT_TEST_COMMAND",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/action/reboot.py",
  "lineno": "44",
  "column": "4",
  "context": "T_DELAY = 0\n    DEFAULT_POST_REBOOT_DELAY = 0\n    DEFAULT_TEST_COMMAND = 'whoami'\n    DEFAULT_BOOT_TIME_COMMAND = 'cat /proc/sys/ker",
  "context_lines": "    DEFAULT_REBOOT_TIMEOUT = 600\n    DEFAULT_CONNECT_TIMEOUT = None\n    DEFAULT_PRE_REBOOT_DELAY = 0\n    DEFAULT_POST_REBOOT_DELAY = 0\n    DEFAULT_TEST_COMMAND = 'whoami'\n    DEFAULT_BOOT_TIME_COMMAND = 'cat /proc/sys/kernel/random/boot_id'\n    DEFAULT_REBOOT_MESSAGE = 'Reboot initiated by Ansible'\n    DEFAULT_SHUTDOWN_COMMAND = 'shutdown'\n    DEFAULT_SHUTDOWN_COMMAND_ARGS = '-r {delay_min} \"{message}\"'\n",
  "slicing": "    DEFAULT_TEST_COMMAND = 'whoami'\n"
 },
 "164": {
  "name": "DEFAULT_BOOT_TIME_COMMAND",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/action/reboot.py",
  "lineno": "45",
  "column": "4",
  "context": "DELAY = 0\n    DEFAULT_TEST_COMMAND = 'whoami'\n    DEFAULT_BOOT_TIME_COMMAND = 'cat /proc/sys/kernel/random/boot_id'\n    DEFAULT_REBOOT_MESSAGE = 'Reboot initiated by ",
  "context_lines": "    DEFAULT_CONNECT_TIMEOUT = None\n    DEFAULT_PRE_REBOOT_DELAY = 0\n    DEFAULT_POST_REBOOT_DELAY = 0\n    DEFAULT_TEST_COMMAND = 'whoami'\n    DEFAULT_BOOT_TIME_COMMAND = 'cat /proc/sys/kernel/random/boot_id'\n    DEFAULT_REBOOT_MESSAGE = 'Reboot initiated by Ansible'\n    DEFAULT_SHUTDOWN_COMMAND = 'shutdown'\n    DEFAULT_SHUTDOWN_COMMAND_ARGS = '-r {delay_min} \"{message}\"'\n    DEFAULT_SUDOABLE = True\n\n",
  "slicing": "    DEFAULT_BOOT_TIME_COMMAND = 'cat /proc/sys/kernel/random/boot_id'\n"
 },
 "165": {
  "name": "DEFAULT_REBOOT_MESSAGE",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/action/reboot.py",
  "lineno": "46",
  "column": "4",
  "context": "MMAND = 'cat /proc/sys/kernel/random/boot_id'\n    DEFAULT_REBOOT_MESSAGE = 'Reboot initiated by Ansible'\n    DEFAULT_SHUTDOWN_COMMAND = 'shutdown'\n    DEFA",
  "context_lines": "    DEFAULT_PRE_REBOOT_DELAY = 0\n    DEFAULT_POST_REBOOT_DELAY = 0\n    DEFAULT_TEST_COMMAND = 'whoami'\n    DEFAULT_BOOT_TIME_COMMAND = 'cat /proc/sys/kernel/random/boot_id'\n    DEFAULT_REBOOT_MESSAGE = 'Reboot initiated by Ansible'\n    DEFAULT_SHUTDOWN_COMMAND = 'shutdown'\n    DEFAULT_SHUTDOWN_COMMAND_ARGS = '-r {delay_min} \"{message}\"'\n    DEFAULT_SUDOABLE = True\n\n    DEPRECATED_ARGS = {}\n\n",
  "slicing": "    DEFAULT_REBOOT_MESSAGE = 'Reboot initiated by Ansible'\n"
 },
 "166": {
  "name": "DEFAULT_SHUTDOWN_COMMAND",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/action/reboot.py",
  "lineno": "47",
  "column": "4",
  "context": "EBOOT_MESSAGE = 'Reboot initiated by Ansible'\n    DEFAULT_SHUTDOWN_COMMAND = 'shutdown'\n    DEFAULT_SHUTDOWN_COMMAND_ARGS = '-r {delay_min",
  "context_lines": "    DEFAULT_POST_REBOOT_DELAY = 0\n    DEFAULT_TEST_COMMAND = 'whoami'\n    DEFAULT_BOOT_TIME_COMMAND = 'cat /proc/sys/kernel/random/boot_id'\n    DEFAULT_REBOOT_MESSAGE = 'Reboot initiated by Ansible'\n    DEFAULT_SHUTDOWN_COMMAND = 'shutdown'\n    DEFAULT_SHUTDOWN_COMMAND_ARGS = '-r {delay_min} \"{message}\"'\n    DEFAULT_SUDOABLE = True\n\n    DEPRECATED_ARGS = {}\n\n    BOOT_TIME_COMMANDS = {\n",
  "slicing": "    DEFAULT_SHUTDOWN_COMMAND = 'shutdown'\n"
 },
 "167": {
  "name": "DEFAULT_SHUTDOWN_COMMAND_ARGS",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/plugins/action/reboot.py",
  "lineno": "48",
  "column": "4",
  "context": "le'\n    DEFAULT_SHUTDOWN_COMMAND = 'shutdown'\n    DEFAULT_SHUTDOWN_COMMAND_ARGS = '-r {delay_min} \"{message}\"'\n    DEFAULT_SUDOABLE = True\n\n    DEPRECATED_ARGS =",
  "context_lines": "    DEFAULT_TEST_COMMAND = 'whoami'\n    DEFAULT_BOOT_TIME_COMMAND = 'cat /proc/sys/kernel/random/boot_id'\n    DEFAULT_REBOOT_MESSAGE = 'Reboot initiated by Ansible'\n    DEFAULT_SHUTDOWN_COMMAND = 'shutdown'\n    DEFAULT_SHUTDOWN_COMMAND_ARGS = '-r {delay_min} \"{message}\"'\n    DEFAULT_SUDOABLE = True\n\n    DEPRECATED_ARGS = {}\n\n    BOOT_TIME_COMMANDS = {\n        'freebsd': '/sbin/sysctl kern.boottime',\n",
  "slicing": [
   "    DEFAULT_SHUTDOWN_COMMAND_ARGS = '-r {delay_min} \"{message}\"'\n",
   "        'linux': DEFAULT_SHUTDOWN_COMMAND_ARGS,\n"
  ]
 },
 "168": {
  "name": "buffer",
  "type": "bytes",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/module_utils/basic.py",
  "lineno": "595",
  "column": "8",
  "context": "LE_ARGS\n    if _ANSIBLE_ARGS is not None:\n        buffer = _ANSIBLE_ARGS\n    else:\n        # debug overrides to read args f",
  "context_lines": "    inside it as a copy in your own code.\n    '''\n    global _ANSIBLE_ARGS\n    if _ANSIBLE_ARGS is not None:\n        buffer = _ANSIBLE_ARGS\n    else:\n        # debug overrides to read args from file or cmdline\n\n        # Avoid tracebacks when locale is non-utf8\n        # We control the args and we pass them as utf8\n",
  "slicing": [
   "    HAS_SYSLOG = True\n",
   "    HAS_SYSLOG = False\n",
   "    has_journal = hasattr(journal, 'sendv')\n",
   "    has_journal = False\n",
   "HAVE_SELINUX = False\n",
   "    HAVE_SELINUX = True\n",
   "NoneType = type(None)\n",
   "AVAILABLE_HASH_ALGORITHMS = dict()\n",
   "    for attribute in ('available_algorithms', 'algorithms'):\n",
   "        algorithms = getattr(hashlib, attribute, None)\n",
   "        if algorithms:\n",
   "    if algorithms is None:\n",
   "        algorithms = ('md5', 'sha1', 'sha224', 'sha256', 'sha384', 'sha512')\n",
   "    for algorithm in algorithms:\n",
   "        AVAILABLE_HASH_ALGORITHMS[algorithm] = getattr(hashlib, algorithm)\n",
   "        AVAILABLE_HASH_ALGORITHMS.pop('md5', None)\n",
   "    AVAILABLE_HASH_ALGORITHMS = {'sha1': sha.sha}\n",
   "        AVAILABLE_HASH_ALGORITHMS['md5'] = md5.md5\n",
   "SEQUENCETYPE = frozenset, KeysView, Sequence\n",
   "PASSWORD_MATCH = re.compile(r'^(?:.+[-_\\s])?pass(?:[-_\\s]?(?:word|phrase|wrd|wd)?)(?:[-_\\s].+)?$', re.I)\n",
   "    unicode = text_type\n",
   "    basestring = string_types\n",
   "_ANSIBLE_ARGS = None\n",
   "FILE_COMMON_ARGUMENTS = dict(\n",
   "PASSWD_ARG_RE = re.compile(r'^[-]{0,2}pass[-]?(word|wd)?')\n",
   "MODE_OPERATOR_RE = re.compile(r'[+=-]')\n",
   "USERS_RE = re.compile(r'[^ugo]')\n",
   "PERMS_RE = re.compile(r'[^rwxXstugo]')\n",
   "_PY3_MIN = sys.version_info[:2] >= (3, 5)\n",
   "_PY2_MIN = (2, 6) <= sys.version_info[:2] < (3,)\n",
   "_PY_MIN = _PY3_MIN or _PY2_MIN\n",
   "if not _PY_MIN:\n",
   "    platform_cls = get_platform_subclass(cls)\n",
   "    return super(cls, platform_cls).__new__(platform_cls)\n",
   "def _remove_values_conditions(value, no_log_strings, deferred_removals):\n",
   "        native_str_value = value\n",
   "            value_is_text = True\n",
   "                native_str_value = to_bytes(value, errors='surrogate_or_strict')\n",
   "            value_is_text = False\n",
   "                native_str_value = to_text(value, errors='surrogate_or_strict')\n",
   "        if native_str_value in no_log_strings:\n",
   "        for omit_me in no_log_strings:\n",
   "            native_str_value = native_str_value.replace(omit_me, '*' * 8)\n",
   "        if value_is_text and isinstance(native_str_value, binary_type):\n",
   "            value = to_text(native_str_value, encoding='utf-8', errors='surrogate_then_replace')\n",
   "        elif not value_is_text and isinstance(native_str_value, text_type):\n",
   "            value = to_bytes(native_str_value, encoding='utf-8', errors='surrogate_then_replace')\n",
   "            value = native_str_value\n",
   "    elif isinstance(value, Sequence):\n",
   "        if isinstance(value, MutableSequence):\n",
   "            new_value = type(value)()\n",
   "            new_value = []  # Need a mutable value\n",
   "        deferred_removals.append((value, new_value))\n",
   "        value = new_value\n",
   "    elif isinstance(value, Set):\n",
   "        if isinstance(value, MutableSet):\n",
   "            new_value = type(value)()\n",
   "            new_value = set()  # Need a mutable value\n",
   "        deferred_removals.append((value, new_value))\n",
   "        value = new_value\n",
   "    elif isinstance(value, Mapping):\n",
   "        if isinstance(value, MutableMapping):\n",
   "            new_value = type(value)()\n",
   "            new_value = {}  # Need a mutable value\n",
   "        deferred_removals.append((value, new_value))\n",
   "        value = new_value\n",
   "    elif isinstance(value, tuple(chain(integer_types, (float, bool, NoneType)))):\n",
   "        stringy_value = to_native(value, encoding='utf-8', errors='surrogate_or_strict')\n",
   "        if stringy_value in no_log_strings:\n",
   "        for omit_me in no_log_strings:\n",
   "            if omit_me in stringy_value:\n",
   "    elif isinstance(value, (datetime.datetime, datetime.date)):\n",
   "        value = value.isoformat()\n",
   "        raise TypeError('Value of unknown type: %s, %s' % (type(value), value))\n",
   "    return value\n",
   "def remove_values(value, no_log_strings):\n",
   "    deferred_removals = deque()\n",
   "    no_log_strings = [to_native(s, errors='surrogate_or_strict') for s in no_log_strings]\n",
   "    new_value = _remove_values_conditions(value, no_log_strings, deferred_removals)\n",
   "    while deferred_removals:\n",
   "        old_data, new_data = deferred_removals.popleft()\n",
   "        if isinstance(new_data, Mapping):\n",
   "            for old_key, old_elem in old_data.items():\n",
   "                new_elem = _remove_values_conditions(old_elem, no_log_strings, deferred_removals)\n",
   "                new_data[old_key] = new_elem\n",
   "            for elem in old_data:\n",
   "                new_elem = _remove_values_conditions(elem, no_log_strings, deferred_removals)\n",
   "                if isinstance(new_data, MutableSequence):\n",
   "                    new_data.append(new_elem)\n",
   "                elif isinstance(new_data, MutableSet):\n",
   "                    new_data.add(new_elem)\n",
   "    return new_value\n",
   "def _sanitize_keys_conditions(value, no_log_strings, ignore_keys, deferred_removals):\n",
   "    if isinstance(value, (text_type, binary_type)):\n",
   "        return value\n",
   "    if isinstance(value, Sequence):\n",
   "        if isinstance(value, MutableSequence):\n",
   "            new_value = type(value)()\n",
   "            new_value = []  # Need a mutable value\n",
   "        deferred_removals.append((value, new_value))\n",
   "        return new_value\n",
   "    if isinstance(value, Set):\n",
   "        if isinstance(value, MutableSet):\n",
   "            new_value = type(value)()\n",
   "            new_value = set()  # Need a mutable value\n",
   "        deferred_removals.append((value, new_value))\n",
   "        return new_value\n",
   "    if isinstance(value, Mapping):\n",
   "        if isinstance(value, MutableMapping):\n",
   "            new_value = type(value)()\n",
   "            new_value = {}  # Need a mutable value\n",
   "        deferred_removals.append((value, new_value))\n",
   "        return new_value\n",
   "    if isinstance(value, tuple(chain(integer_types, (float, bool, NoneType)))):\n",
   "        return value\n",
   "    if isinstance(value, (datetime.datetime, datetime.date)):\n",
   "        return value\n",
   "    raise TypeError('Value of unknown type: %s, %s' % (type(value), value))\n",
   "    deferred_removals = deque()\n",
   "    no_log_strings = [to_native(s, errors='surrogate_or_strict') for s in no_log_strings]\n",
   "    new_value = _sanitize_keys_conditions(obj, no_log_strings, ignore_keys, deferred_removals)\n",
   "    while deferred_removals:\n",
   "        old_data, new_data = deferred_removals.popleft()\n",
   "        if isinstance(new_data, Mapping):\n",
   "            for old_key, old_elem in old_data.items():\n",
   "                if old_key in ignore_keys or old_key.startswith('_ansible'):\n",
   "                    new_data[old_key] = _sanitize_keys_conditions(old_elem, no_log_strings, ignore_keys, deferred_removals)\n",
   "                    new_key = _remove_values_conditions(old_key, no_log_strings, None)\n",
   "                    new_data[new_key] = _sanitize_keys_conditions(old_elem, no_log_strings, ignore_keys, deferred_removals)\n",
   "            for elem in old_data:\n",
   "                new_elem = _sanitize_keys_conditions(elem, no_log_strings, ignore_keys, deferred_removals)\n",
   "                if isinstance(new_data, MutableSequence):\n",
   "                    new_data.append(new_elem)\n",
   "                elif isinstance(new_data, MutableSet):\n",
   "                    new_data.add(new_elem)\n",
   "    return new_value\n",
   "def heuristic_log_sanitize(data, no_log_values=None):\n",
   "    data = to_native(data)\n",
   "    output = []\n",
   "    begin = len(data)\n",
   "    prev_begin = begin\n",
   "    sep = 1\n",
   "    while sep:\n",
   "            end = data.rindex('@', 0, begin)\n",
   "            output.insert(0, data[0:begin])\n",
   "        sep = None\n",
   "        sep_search_end = end\n",
   "        while not sep:\n",
   "                begin = data.rindex('://', 0, sep_search_end)\n",
   "                begin = 0\n",
   "                sep = data.index(':', begin + 3, end)\n",
   "                if begin == 0:\n",
   "                    output.insert(0, data[0:begin])\n",
   "                sep_search_end = begin\n",
   "        if sep:\n",
   "            output.insert(0, data[end:prev_begin])\n",
   "            output.insert(0, '********')\n",
   "            output.insert(0, data[begin:sep + 1])\n",
   "            prev_begin = begin\n",
   "    output = ''.join(output)\n",
   "        output = remove_values(output, no_log_values)\n",
   "    return output\n",
   "    if _ANSIBLE_ARGS is not None:\n",
   "        buffer = _ANSIBLE_ARGS\n",
   "                fd = open(sys.argv[1], 'rb')\n",
   "                buffer = fd.read()\n",
   "                fd.close()\n",
   "                buffer = sys.argv[1]\n",
   "                    buffer = buffer.encode('utf-8', errors='surrogateescape')\n",
   "                buffer = sys.stdin.read()\n",
   "                buffer = sys.stdin.buffer.read()\n",
   "        _ANSIBLE_ARGS = buffer\n",
   "        params = json.loads(buffer.decode('utf-8'))\n",
   "        params = json_dict_unicode_to_bytes(params)\n",
   "        return params['ANSIBLE_MODULE_ARGS']\n",
   "    for arg in args:\n",
   "        if arg in os.environ:\n",
   "            return os.environ[arg]\n",
   "    hostname = platform.node()\n",
   "    msg = \"Failed to import the required Python library (%s) on %s's Python %s.\" % (library, hostname, sys.executable)\n",
   "        msg += \" This is required %s.\" % reason\n",
   "        msg += \" See %s for more info.\" % url\n",
   "    msg += (\" Please read the module documentation and install it in the appropriate location.\"\n",
   "    return msg\n",
   "            for k, v in FILE_COMMON_ARGUMENTS.items():\n",
   "                if k not in self.argument_spec:\n",
   "                    self.argument_spec[k] = v\n",
   "            basedir = None\n",
   "                basedir = os.path.expanduser(os.path.expandvars(self._remote_tmp))\n",
   "            if basedir is not None and not os.path.exists(basedir):\n",
   "                    os.makedirs(basedir, mode=0o700)\n",
   "                              \"failing back to system: %s\" % (basedir, to_native(e)))\n",
   "                    basedir = None\n",
   "                              \"the correct permissions manually\" % basedir)\n",
   "            basefile = \"ansible-moduletmp-%s-\" % time.time()\n",
   "                tmpdir = tempfile.mkdtemp(prefix=basefile, dir=basedir)\n",
   "                        \"with prefix %s: %s\" % (basedir, basefile, to_native(e))\n",
   "                atexit.register(shutil.rmtree, tmpdir)\n",
   "            self._tmpdir = tmpdir\n",
   "    def warn(self, warning):\n",
   "    def deprecate(self, msg, version=None, date=None, collection_name=None):\n",
   "        deprecate(msg, version=version, date=date, collection_name=collection_name)\n",
   "            self.log('[DEPRECATION WARNING] %s %s' % (msg, date))\n",
   "            self.log('[DEPRECATION WARNING] %s %s' % (msg, version))\n",
   "            path = params.get('path', params.get('dest', None))\n",
   "        if path is None:\n",
   "            path = os.path.expanduser(os.path.expandvars(path))\n",
   "        b_path = to_bytes(path, errors='surrogate_or_strict')\n",
   "        if params.get('follow', False) and os.path.islink(b_path):\n",
   "            b_path = os.path.realpath(b_path)\n",
   "            path = to_native(b_path)\n",
   "        mode = params.get('mode', None)\n",
   "        owner = params.get('owner', None)\n",
   "        group = params.get('group', None)\n",
   "        seuser = params.get('seuser', None)\n",
   "        serole = params.get('serole', None)\n",
   "        setype = params.get('setype', None)\n",
   "        selevel = params.get('selevel', None)\n",
   "        secontext = [seuser, serole, setype]\n",
   "            secontext.append(selevel)\n",
   "        default_secontext = self.selinux_default_context(path)\n",
   "        for i in range(len(default_secontext)):\n",
   "            if i is not None and secontext[i] == '_default':\n",
   "                secontext[i] = default_secontext[i]\n",
   "        attributes = params.get('attributes', None)\n",
   "            path=path, mode=mode, owner=owner, group=group,\n",
   "            seuser=seuser, serole=serole, setype=setype,\n",
   "            selevel=selevel, secontext=secontext, attributes=attributes,\n",
   "        if not HAVE_SELINUX:\n",
   "        if not HAVE_SELINUX:\n",
   "            seenabled = self.get_bin_path('selinuxenabled')\n",
   "            if seenabled is not None:\n",
   "                (rc, out, err) = self.run_command(seenabled)\n",
   "                if rc == 0:\n",
   "        context = [None, None, None]\n",
   "            context.append(None)\n",
   "        return context\n",
   "        context = self.selinux_initial_context()\n",
   "        if not HAVE_SELINUX or not self.selinux_enabled():\n",
   "            return context\n",
   "            ret = selinux.matchpathcon(to_native(path, errors='surrogate_or_strict'), mode)\n",
   "            return context\n",
   "        if ret[0] == -1:\n",
   "            return context\n",
   "        context = ret[1].split(':', 3)\n",
   "        return context\n",
   "        context = self.selinux_initial_context()\n",
   "        if not HAVE_SELINUX or not self.selinux_enabled():\n",
   "            return context\n",
   "            ret = selinux.lgetfilecon_raw(to_native(path, errors='surrogate_or_strict'))\n",
   "                self.fail_json(path=path, msg='path %s does not exist' % path)\n",
   "                self.fail_json(path=path, msg='failed to retrieve selinux context')\n",
   "        if ret[0] == -1:\n",
   "            return context\n",
   "        context = ret[1].split(':', 3)\n",
   "        return context\n",
   "        b_path = to_bytes(path, errors='surrogate_or_strict')\n",
   "            b_path = os.path.expanduser(os.path.expandvars(b_path))\n",
   "        st = os.lstat(b_path)\n",
   "        uid = st.st_uid\n",
   "        gid = st.st_gid\n",
   "        return (uid, gid)\n",
   "        path_is_bytes = False\n",
   "        if isinstance(path, binary_type):\n",
   "            path_is_bytes = True\n",
   "        b_path = os.path.realpath(to_bytes(os.path.expanduser(os.path.expandvars(path)), errors='surrogate_or_strict'))\n",
   "        while not os.path.ismount(b_path):\n",
   "            b_path = os.path.dirname(b_path)\n",
   "        if path_is_bytes:\n",
   "            return b_path\n",
   "        return to_text(b_path, errors='surrogate_or_strict')\n",
   "            f = open('/proc/mounts', 'r')\n",
   "            mount_data = f.readlines()\n",
   "            f.close()\n",
   "        path_mount_point = self.find_mount_point(path)\n",
   "        for line in mount_data:\n",
   "            (device, mount_point, fstype, options, rest) = line.split(' ', 4)\n",
   "            if to_bytes(path_mount_point) == to_bytes(mount_point):\n",
   "                for fs in self._selinux_special_fs:\n",
   "                    if fs in fstype:\n",
   "                        special_context = self.selinux_context(path_mount_point)\n",
   "                        return (True, special_context)\n",
   "        if not HAVE_SELINUX or not self.selinux_enabled():\n",
   "        context = self.selinux_default_context(path)\n",
   "        return self.set_context_if_different(path, context, False)\n",
   "        if not HAVE_SELINUX or not self.selinux_enabled():\n",
   "        if self.check_file_absent_if_check_mode(path):\n",
   "        cur_context = self.selinux_context(path)\n",
   "        new_context = list(cur_context)\n",
   "        (is_special_se, sp_context) = self.is_special_selinux_path(path)\n",
   "        if is_special_se:\n",
   "            new_context = sp_context\n",
   "            for i in range(len(cur_context)):\n",
   "                if len(context) > i:\n",
   "                    if context[i] is not None and context[i] != cur_context[i]:\n",
   "                        new_context[i] = context[i]\n",
   "                    elif context[i] is None:\n",
   "                        new_context[i] = cur_context[i]\n",
   "        if cur_context != new_context:\n",
   "                diff['before']['secontext'] = cur_context\n",
   "                diff['after']['secontext'] = new_context\n",
   "                rc = selinux.lsetfilecon(to_native(path), ':'.join(new_context))\n",
   "                self.fail_json(path=path, msg='invalid selinux context: %s' % to_native(e),\n",
   "                               new_context=new_context, cur_context=cur_context, input_was=context)\n",
   "            if rc != 0:\n",
   "                self.fail_json(path=path, msg='set selinux context failed')\n",
   "            changed = True\n",
   "        return changed\n",
   "        if owner is None:\n",
   "            return changed\n",
   "        b_path = to_bytes(path, errors='surrogate_or_strict')\n",
   "            b_path = os.path.expanduser(os.path.expandvars(b_path))\n",
   "        if self.check_file_absent_if_check_mode(b_path):\n",
   "        orig_uid, orig_gid = self.user_and_group(b_path, expand)\n",
   "            uid = int(owner)\n",
   "                uid = pwd.getpwnam(owner).pw_uid\n",
   "                path = to_text(b_path)\n",
   "                self.fail_json(path=path, msg='chown failed: failed to look up user %s' % owner)\n",
   "        if orig_uid != uid:\n",
   "                diff['before']['owner'] = orig_uid\n",
   "                diff['after']['owner'] = uid\n",
   "                os.lchown(b_path, uid, -1)\n",
   "                path = to_text(b_path)\n",
   "                self.fail_json(path=path, msg='chown failed: %s' % (to_text(e)))\n",
   "            changed = True\n",
   "        return changed\n",
   "        if group is None:\n",
   "            return changed\n",
   "        b_path = to_bytes(path, errors='surrogate_or_strict')\n",
   "            b_path = os.path.expanduser(os.path.expandvars(b_path))\n",
   "        if self.check_file_absent_if_check_mode(b_path):\n",
   "        orig_uid, orig_gid = self.user_and_group(b_path, expand)\n",
   "            gid = int(group)\n",
   "                gid = grp.getgrnam(group).gr_gid\n",
   "                path = to_text(b_path)\n",
   "                self.fail_json(path=path, msg='chgrp failed: failed to look up group %s' % group)\n",
   "        if orig_gid != gid:\n",
   "                diff['before']['group'] = orig_gid\n",
   "                diff['after']['group'] = gid\n",
   "                os.lchown(b_path, -1, gid)\n",
   "                path = to_text(b_path)\n",
   "                self.fail_json(path=path, msg='chgrp failed')\n",
   "            changed = True\n",
   "        return changed\n",
   "        if mode is None:\n",
   "            return changed\n",
   "            self._created_files.remove(path)\n",
   "        b_path = to_bytes(path, errors='surrogate_or_strict')\n",
   "            b_path = os.path.expanduser(os.path.expandvars(b_path))\n",
   "        path_stat = os.lstat(b_path)\n",
   "        if self.check_file_absent_if_check_mode(b_path):\n",
   "        if not isinstance(mode, int):\n",
   "                mode = int(mode, 8)\n",
   "                    mode = self._symbolic_mode_to_octal(path_stat, mode)\n",
   "                    path = to_text(b_path)\n",
   "                    self.fail_json(path=path,\n",
   "                if mode != stat.S_IMODE(mode):\n",
   "                    path = to_text(b_path)\n",
   "                    self.fail_json(path=path, msg=\"Invalid mode supplied, only permission info is allowed\", details=mode)\n",
   "        prev_mode = stat.S_IMODE(path_stat.st_mode)\n",
   "        if prev_mode != mode:\n",
   "                diff['before']['mode'] = '0%03o' % prev_mode\n",
   "                diff['after']['mode'] = '0%03o' % mode\n",
   "                    os.lchmod(b_path, mode)\n",
   "                    if not os.path.islink(b_path):\n",
   "                        os.chmod(b_path, mode)\n",
   "                        underlying_stat = os.stat(b_path)\n",
   "                        os.chmod(b_path, mode)\n",
   "                        new_underlying_stat = os.stat(b_path)\n",
   "                        if underlying_stat.st_mode != new_underlying_stat.st_mode:\n",
   "                            os.chmod(b_path, stat.S_IMODE(underlying_stat.st_mode))\n",
   "                if os.path.islink(b_path) and e.errno in (errno.EPERM, errno.EROFS):  # Can't set mode on symbolic links\n",
   "                path = to_text(b_path)\n",
   "                self.fail_json(path=path, msg='chmod failed', details=to_native(e),\n",
   "            path_stat = os.lstat(b_path)\n",
   "            new_mode = stat.S_IMODE(path_stat.st_mode)\n",
   "            if new_mode != prev_mode:\n",
   "                changed = True\n",
   "        return changed\n",
   "        if attributes is None:\n",
   "            return changed\n",
   "        b_path = to_bytes(path, errors='surrogate_or_strict')\n",
   "            b_path = os.path.expanduser(os.path.expandvars(b_path))\n",
   "        if self.check_file_absent_if_check_mode(b_path):\n",
   "        existing = self.get_file_attributes(b_path)\n",
   "        attr_mod = '='\n",
   "        if attributes.startswith(('-', '+')):\n",
   "            attr_mod = attributes[0]\n",
   "            attributes = attributes[1:]\n",
   "        if existing.get('attr_flags', '') != attributes or attr_mod == '-':\n",
   "            attrcmd = self.get_bin_path('chattr')\n",
   "            if attrcmd:\n",
   "                attrcmd = [attrcmd, '%s%s' % (attr_mod, attributes), b_path]\n",
   "                changed = True\n",
   "                    diff['before']['attributes'] = existing.get('attr_flags')\n",
   "                    diff['after']['attributes'] = '%s%s' % (attr_mod, attributes)\n",
   "                        rc, out, err = self.run_command(attrcmd)\n",
   "                        if rc != 0 or err:\n",
   "                            raise Exception(\"Error while setting attributes: %s\" % (out + err))\n",
   "                        self.fail_json(path=to_text(b_path), msg='chattr failed',\n",
   "        return changed\n",
   "        output = {}\n",
   "        attrcmd = self.get_bin_path('lsattr', False)\n",
   "        if attrcmd:\n",
   "            attrcmd = [attrcmd, '-vd', path]\n",
   "                rc, out, err = self.run_command(attrcmd)\n",
   "                if rc == 0:\n",
   "                    res = out.split()\n",
   "                    output['attr_flags'] = res[1].replace('-', '').strip()\n",
   "                    output['version'] = res[0].strip()\n",
   "                    output['attributes'] = format_attributes(output['attr_flags'])\n",
   "        return output\n",
   "        new_mode = stat.S_IMODE(path_stat.st_mode)\n",
   "        for mode in symbolic_mode.split(','):\n",
   "            permlist = MODE_OPERATOR_RE.split(mode)\n",
   "            opers = MODE_OPERATOR_RE.findall(mode)\n",
   "            users = permlist.pop(0)\n",
   "            use_umask = (users == '')\n",
   "            if users == 'a' or users == '':\n",
   "                users = 'ugo'\n",
   "            if USERS_RE.match(users):\n",
   "                raise ValueError(\"bad symbolic permission for mode: %s\" % mode)\n",
   "            for idx, perms in enumerate(permlist):\n",
   "                if PERMS_RE.match(perms):\n",
   "                    raise ValueError(\"bad symbolic permission for mode: %s\" % mode)\n",
   "                for user in users:\n",
   "                    mode_to_apply = cls._get_octal_mode_from_symbolic_perms(path_stat, user, perms, use_umask)\n",
   "                    new_mode = cls._apply_operation_to_mode(user, opers[idx], mode_to_apply, new_mode)\n",
   "        return new_mode\n",
   "            if user == 'u':\n",
   "                mask = stat.S_IRWXU | stat.S_ISUID\n",
   "            elif user == 'g':\n",
   "                mask = stat.S_IRWXG | stat.S_ISGID\n",
   "            elif user == 'o':\n",
   "                mask = stat.S_IRWXO | stat.S_ISVTX\n",
   "            inverse_mask = mask ^ PERM_BITS\n",
   "            new_mode = (current_mode & inverse_mask) | mode_to_apply\n",
   "            new_mode = current_mode | mode_to_apply\n",
   "            new_mode = current_mode - (current_mode & mode_to_apply)\n",
   "        return new_mode\n",
   "        prev_mode = stat.S_IMODE(path_stat.st_mode)\n",
   "        is_directory = stat.S_ISDIR(path_stat.st_mode)\n",
   "        has_x_permissions = (prev_mode & EXEC_PERM_BITS) > 0\n",
   "        apply_X_permission = is_directory or has_x_permissions\n",
   "        umask = os.umask(0)\n",
   "        os.umask(umask)\n",
   "        rev_umask = umask ^ PERM_BITS\n",
   "        if apply_X_permission:\n",
   "            X_perms = {\n",
   "            X_perms = {\n",
   "        user_perms_to_modes = {\n",
   "                'r': rev_umask & stat.S_IRUSR if use_umask else stat.S_IRUSR,\n",
   "                'w': rev_umask & stat.S_IWUSR if use_umask else stat.S_IWUSR,\n",
   "                'x': rev_umask & stat.S_IXUSR if use_umask else stat.S_IXUSR,\n",
   "                'u': prev_mode & stat.S_IRWXU,\n",
   "                'g': (prev_mode & stat.S_IRWXG) << 3,\n",
   "                'o': (prev_mode & stat.S_IRWXO) << 6},\n",
   "                'r': rev_umask & stat.S_IRGRP if use_umask else stat.S_IRGRP,\n",
   "                'w': rev_umask & stat.S_IWGRP if use_umask else stat.S_IWGRP,\n",
   "                'x': rev_umask & stat.S_IXGRP if use_umask else stat.S_IXGRP,\n",
   "                'u': (prev_mode & stat.S_IRWXU) >> 3,\n",
   "                'g': prev_mode & stat.S_IRWXG,\n",
   "                'o': (prev_mode & stat.S_IRWXO) << 3},\n",
   "                'r': rev_umask & stat.S_IROTH if use_umask else stat.S_IROTH,\n",
   "                'w': rev_umask & stat.S_IWOTH if use_umask else stat.S_IWOTH,\n",
   "                'x': rev_umask & stat.S_IXOTH if use_umask else stat.S_IXOTH,\n",
   "                'u': (prev_mode & stat.S_IRWXU) >> 6,\n",
   "                'g': (prev_mode & stat.S_IRWXG) >> 3,\n",
   "                'o': prev_mode & stat.S_IRWXO},\n",
   "        for key, value in X_perms.items():\n",
   "            user_perms_to_modes[key].update(value)\n",
   "            return mode | user_perms_to_modes[user][perm]\n",
   "        return reduce(or_reduce, perms, 0)\n",
   "        changed = self.set_context_if_different(\n",
   "            file_args['path'], file_args['secontext'], changed, diff\n",
   "        changed = self.set_owner_if_different(\n",
   "            file_args['path'], file_args['owner'], changed, diff, expand\n",
   "        changed = self.set_group_if_different(\n",
   "            file_args['path'], file_args['group'], changed, diff, expand\n",
   "        changed = self.set_mode_if_different(\n",
   "            file_args['path'], file_args['mode'], changed, diff, expand\n",
   "        changed = self.set_attributes_if_different(\n",
   "            file_args['path'], file_args['attributes'], changed, diff, expand\n",
   "        return changed\n",
   "        return self.set_fs_attributes_if_different(file_args, changed, diff, expand)\n",
   "        return self.set_fs_attributes_if_different(file_args, changed, diff, expand)\n",
   "        for path in sorted(self._created_files):\n",
   "                      \"Specify 'mode' to avoid this warning.\".format(to_native(path), DEFAULT_PERM))\n",
   "        path = kwargs.get('path', kwargs.get('dest', None))\n",
   "        if path is None:\n",
   "        b_path = to_bytes(path, errors='surrogate_or_strict')\n",
   "        if os.path.exists(b_path):\n",
   "            (uid, gid) = self.user_and_group(path)\n",
   "            kwargs['uid'] = uid\n",
   "            kwargs['gid'] = gid\n",
   "                user = pwd.getpwuid(uid)[0]\n",
   "                user = str(uid)\n",
   "                group = grp.getgrgid(gid)[0]\n",
   "                group = str(gid)\n",
   "            kwargs['owner'] = user\n",
   "            kwargs['group'] = group\n",
   "            st = os.lstat(b_path)\n",
   "            kwargs['mode'] = '0%03o' % stat.S_IMODE(st[stat.ST_MODE])\n",
   "            if os.path.islink(b_path):\n",
   "            elif os.path.isdir(b_path):\n",
   "            elif os.stat(b_path).st_nlink > 1:\n",
   "            if HAVE_SELINUX and self.selinux_enabled():\n",
   "                kwargs['secontext'] = ':'.join(self.selinux_context(path))\n",
   "            kwargs['size'] = st[stat.ST_SIZE]\n",
   "            spec = self.argument_spec\n",
   "            param = self.params\n",
   "        alias_warnings = []\n",
   "        alias_results, self._legal_inputs = handle_aliases(spec, param, alias_warnings=alias_warnings)\n",
   "        for option, alias in alias_warnings:\n",
   "            warn('Both option %s and its alias %s are set.' % (option_prefix + option, option_prefix + alias))\n",
   "        deprecated_aliases = []\n",
   "        for i in spec.keys():\n",
   "            if 'deprecated_aliases' in spec[i].keys():\n",
   "                for alias in spec[i]['deprecated_aliases']:\n",
   "                    deprecated_aliases.append(alias)\n",
   "        for deprecation in deprecated_aliases:\n",
   "            if deprecation['name'] in param.keys():\n",
   "                deprecate(\"Alias '%s' is deprecated. See the module docs for more information\" % deprecation['name'],\n",
   "                          version=deprecation.get('version'), date=deprecation.get('date'),\n",
   "                          collection_name=deprecation.get('collection_name'))\n",
   "        return alias_results\n",
   "        if spec is None:\n",
   "            spec = self.argument_spec\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            self.no_log_values.update(list_no_log_values(spec, param))\n",
   "            self.fail_json(msg=\"Failure when processing no_log parameters. Module invocation will be hidden. \"\n",
   "        for message in list_deprecations(spec, param):\n",
   "            deprecate(message['msg'], version=message.get('version'), date=message.get('date'),\n",
   "                      collection_name=message.get('collection_name'))\n",
   "        self._syslog_facility = 'LOG_USER'\n",
   "        unsupported_parameters = set()\n",
   "        if spec is None:\n",
   "            spec = self.argument_spec\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            legal_inputs = self._legal_inputs\n",
   "        for k in list(param.keys()):\n",
   "            if k not in legal_inputs:\n",
   "                unsupported_parameters.add(k)\n",
   "        for k in PASS_VARS:\n",
   "            param_key = '_ansible_%s' % k\n",
   "            if param_key in param:\n",
   "                if k in PASS_BOOLS:\n",
   "                    setattr(self, PASS_VARS[k][0], self.boolean(param[param_key]))\n",
   "                    setattr(self, PASS_VARS[k][0], param[param_key])\n",
   "                if param_key in self.params:\n",
   "                    del self.params[param_key]\n",
   "                if not hasattr(self, PASS_VARS[k][0]):\n",
   "                    setattr(self, PASS_VARS[k][0], PASS_VARS[k][1])\n",
   "        if unsupported_parameters:\n",
   "            msg = \"Unsupported parameters for (%s) module: %s\" % (self._name, ', '.join(sorted(list(unsupported_parameters))))\n",
   "            if self._options_context:\n",
   "                msg += \" found in %s.\" % \" -> \".join(self._options_context)\n",
   "            supported_parameters = list()\n",
   "            for key in sorted(spec.keys()):\n",
   "                if 'aliases' in spec[key] and spec[key]['aliases']:\n",
   "                    supported_parameters.append(\"%s (%s)\" % (key, ', '.join(sorted(spec[key]['aliases']))))\n",
   "                    supported_parameters.append(key)\n",
   "            msg += \" Supported parameters include: %s\" % (', '.join(supported_parameters))\n",
   "            self.fail_json(msg=msg)\n",
   "        if self.check_mode and not self.supports_check_mode:\n",
   "            self.exit_json(skipped=True, msg=\"remote module (%s) does not support check mode\" % self._name)\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "        return count_terms(check, param)\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            check_mutually_exclusive(spec, param)\n",
   "            msg = to_native(e)\n",
   "            if self._options_context:\n",
   "                msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "            self.fail_json(msg=msg)\n",
   "        if spec is None:\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            check_required_one_of(spec, param)\n",
   "            msg = to_native(e)\n",
   "            if self._options_context:\n",
   "                msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "            self.fail_json(msg=msg)\n",
   "        if spec is None:\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            check_required_together(spec, param)\n",
   "            msg = to_native(e)\n",
   "            if self._options_context:\n",
   "                msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "            self.fail_json(msg=msg)\n",
   "        if spec is None:\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            check_required_by(spec, param)\n",
   "            self.fail_json(msg=to_native(e))\n",
   "        if spec is None:\n",
   "            spec = self.argument_spec\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            check_required_arguments(spec, param)\n",
   "            msg = to_native(e)\n",
   "            if self._options_context:\n",
   "                msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "            self.fail_json(msg=msg)\n",
   "        if spec is None:\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            check_required_if(spec, param)\n",
   "            msg = to_native(e)\n",
   "            if self._options_context:\n",
   "                msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "            self.fail_json(msg=msg)\n",
   "        if spec is None:\n",
   "            spec = self.argument_spec\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "        for (k, v) in spec.items():\n",
   "            choices = v.get('choices', None)\n",
   "            if choices is None:\n",
   "            if isinstance(choices, SEQUENCETYPE) and not isinstance(choices, (binary_type, text_type)):\n",
   "                if k in param:\n",
   "                    if isinstance(param[k], list):\n",
   "                        diff_list = \", \".join([item for item in param[k] if item not in choices])\n",
   "                        if diff_list:\n",
   "                            choices_str = \", \".join([to_native(c) for c in choices])\n",
   "                            msg = \"value of %s must be one or more of: %s. Got no match for: %s\" % (k, choices_str, diff_list)\n",
   "                            if self._options_context:\n",
   "                                msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "                            self.fail_json(msg=msg)\n",
   "                    elif param[k] not in choices:\n",
   "                        lowered_choices = None\n",
   "                        if param[k] == 'False':\n",
   "                            lowered_choices = lenient_lowercase(choices)\n",
   "                            overlap = BOOLEANS_FALSE.intersection(choices)\n",
   "                            if len(overlap) == 1:\n",
   "                                (param[k],) = overlap\n",
   "                        if param[k] == 'True':\n",
   "                            if lowered_choices is None:\n",
   "                                lowered_choices = lenient_lowercase(choices)\n",
   "                            overlap = BOOLEANS_TRUE.intersection(choices)\n",
   "                            if len(overlap) == 1:\n",
   "                                (param[k],) = overlap\n",
   "                        if param[k] not in choices:\n",
   "                            choices_str = \", \".join([to_native(c) for c in choices])\n",
   "                            msg = \"value of %s must be one of: %s, got: %s\" % (k, choices_str, param[k])\n",
   "                            if self._options_context:\n",
   "                                msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "                            self.fail_json(msg=msg)\n",
   "                msg = \"internal error: choices for argument %s are not iterable: %s\" % (k, choices)\n",
   "                if self._options_context:\n",
   "                    msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "                self.fail_json(msg=msg)\n",
   "        return safe_eval(value, locals, include_exceptions)\n",
   "        opts = {\n",
   "        allow_conversion = opts.get(self._string_conversion_action, True)\n",
   "            return check_type_str(value, allow_conversion)\n",
   "            common_msg = 'quote the entire value to ensure it does not change.'\n",
   "            from_msg = '{0!r}'.format(value)\n",
   "            to_msg = '{0!r}'.format(to_text(value))\n",
   "            if param is not None:\n",
   "                    param = '{0}{1}'.format(prefix, param)\n",
   "                from_msg = '{0}: {1!r}'.format(param, value)\n",
   "                to_msg = '{0}: {1!r}'.format(param, to_text(value))\n",
   "            if self._string_conversion_action == 'error':\n",
   "                msg = common_msg.capitalize()\n",
   "                raise TypeError(to_native(msg))\n",
   "            elif self._string_conversion_action == 'warn':\n",
   "                msg = ('The value \"{0}\" (type {1.__class__.__name__}) was converted to \"{2}\" (type string). '\n",
   "                       'If this does not look like what you expect, {3}').format(from_msg, value, to_msg, common_msg)\n",
   "                self.warn(to_native(msg))\n",
   "                return to_native(value, errors='surrogate_or_strict')\n",
   "        return check_type_list(value)\n",
   "        return check_type_dict(value)\n",
   "        return check_type_bool(value)\n",
   "        return check_type_int(value)\n",
   "        return check_type_float(value)\n",
   "        return check_type_path(value)\n",
   "        return check_type_jsonarg(value)\n",
   "        return check_type_raw(value)\n",
   "        return check_type_bytes(value)\n",
   "        return check_type_bits(value)\n",
   "            argument_spec = self.argument_spec\n",
   "        if params is None:\n",
   "            params = self.params\n",
   "        for (k, v) in argument_spec.items():\n",
   "            wanted = v.get('type', None)\n",
   "            if wanted == 'dict' or (wanted == 'list' and v.get('elements', '') == 'dict'):\n",
   "                spec = v.get('options', None)\n",
   "                if v.get('apply_defaults', False):\n",
   "                    if spec is not None:\n",
   "                        if params.get(k) is None:\n",
   "                            params[k] = {}\n",
   "                elif spec is None or k not in params or params[k] is None:\n",
   "                self._options_context.append(k)\n",
   "                if isinstance(params[k], dict):\n",
   "                    elements = [params[k]]\n",
   "                    elements = params[k]\n",
   "                for idx, param in enumerate(elements):\n",
   "                    if not isinstance(param, dict):\n",
   "                        self.fail_json(msg=\"value of %s must be of type dict or list of dict\" % k)\n",
   "                    new_prefix = prefix + k\n",
   "                    if wanted == 'list':\n",
   "                        new_prefix += '[%d]' % idx\n",
   "                    new_prefix += '.'\n",
   "                    self._set_fallbacks(spec, param)\n",
   "                    options_aliases = self._handle_aliases(spec, param, option_prefix=new_prefix)\n",
   "                    options_legal_inputs = list(spec.keys()) + list(options_aliases.keys())\n",
   "                    self._check_arguments(spec, param, options_legal_inputs)\n",
   "                    if not self.bypass_checks:\n",
   "                        self._check_mutually_exclusive(v.get('mutually_exclusive', None), param)\n",
   "                    self._set_defaults(pre=True, spec=spec, param=param)\n",
   "                    if not self.bypass_checks:\n",
   "                        self._check_required_arguments(spec, param)\n",
   "                        self._check_argument_types(spec, param, new_prefix)\n",
   "                        self._check_argument_values(spec, param)\n",
   "                        self._check_required_together(v.get('required_together', None), param)\n",
   "                        self._check_required_one_of(v.get('required_one_of', None), param)\n",
   "                        self._check_required_if(v.get('required_if', None), param)\n",
   "                        self._check_required_by(v.get('required_by', None), param)\n",
   "                    self._set_defaults(pre=False, spec=spec, param=param)\n",
   "                    self._handle_options(spec, param, new_prefix)\n",
   "                self._options_context.pop()\n",
   "        if not callable(wanted):\n",
   "            if wanted is None:\n",
   "                wanted = 'str'\n",
   "                type_checker = self._CHECK_ARGUMENT_TYPES_DISPATCHER[wanted]\n",
   "                self.fail_json(msg=\"implementation error: unknown type %s requested for %s\" % (wanted, k))\n",
   "            type_checker = wanted\n",
   "            wanted = getattr(wanted, '__name__', to_native(type(wanted)))\n",
   "        return type_checker, wanted\n",
   "        type_checker, wanted_name = self._get_wanted_type(wanted, param)\n",
   "        validated_params = []\n",
   "        kwargs = {}\n",
   "        if wanted_name == 'str' and isinstance(wanted, string_types):\n",
   "            if isinstance(param, string_types):\n",
   "                kwargs['param'] = param\n",
   "            elif isinstance(param, dict):\n",
   "                kwargs['param'] = list(param.keys())[0]\n",
   "        for value in values:\n",
   "                validated_params.append(type_checker(value, **kwargs))\n",
   "                msg = \"Elements value for option %s\" % param\n",
   "                if self._options_context:\n",
   "                    msg += \" found in '%s'\" % \" -> \".join(self._options_context)\n",
   "                msg += \" is of type %s and we were unable to convert to %s: %s\" % (type(value), wanted_name, to_native(e))\n",
   "                self.fail_json(msg=msg)\n",
   "        return validated_params\n",
   "        if spec is None:\n",
   "            spec = self.argument_spec\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "        for (k, v) in spec.items():\n",
   "            wanted = v.get('type', None)\n",
   "            if k not in param:\n",
   "            value = param[k]\n",
   "            if value is None:\n",
   "            type_checker, wanted_name = self._get_wanted_type(wanted, k)\n",
   "            kwargs = {}\n",
   "            if wanted_name == 'str' and isinstance(type_checker, string_types):\n",
   "                kwargs['param'] = list(param.keys())[0]\n",
   "                    kwargs['prefix'] = prefix\n",
   "                param[k] = type_checker(value, **kwargs)\n",
   "                wanted_elements = v.get('elements', None)\n",
   "                if wanted_elements:\n",
   "                    if wanted != 'list' or not isinstance(param[k], list):\n",
   "                        msg = \"Invalid type %s for option '%s'\" % (wanted_name, param)\n",
   "                        if self._options_context:\n",
   "                            msg += \" found in '%s'.\" % \" -> \".join(self._options_context)\n",
   "                        msg += \", elements value check is supported only with 'list' type\"\n",
   "                        self.fail_json(msg=msg)\n",
   "                    param[k] = self._handle_elements(wanted_elements, k, param[k])\n",
   "                msg = \"argument %s is of type %s\" % (k, type(value))\n",
   "                if self._options_context:\n",
   "                    msg += \" found in '%s'.\" % \" -> \".join(self._options_context)\n",
   "                msg += \" and we were unable to convert to %s: %s\" % (wanted_name, to_native(e))\n",
   "                self.fail_json(msg=msg)\n",
   "        if spec is None:\n",
   "            spec = self.argument_spec\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "        for (k, v) in spec.items():\n",
   "            default = v.get('default', None)\n",
   "                if default is not None and k not in param:\n",
   "                    param[k] = default\n",
   "                if k not in param:\n",
   "                    param[k] = default\n",
   "        if spec is None:\n",
   "            spec = self.argument_spec\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "        for (k, v) in spec.items():\n",
   "            fallback = v.get('fallback', (None,))\n",
   "            fallback_strategy = fallback[0]\n",
   "            fallback_args = []\n",
   "            fallback_kwargs = {}\n",
   "            if k not in param and fallback_strategy is not None:\n",
   "                for item in fallback[1:]:\n",
   "                    if isinstance(item, dict):\n",
   "                        fallback_kwargs = item\n",
   "                        fallback_args = item\n",
   "                    param[k] = fallback_strategy(*fallback_args, **fallback_kwargs)\n",
   "        self.params = _load_params()\n",
   "        if HAS_SYSLOG:\n",
   "                module = 'ansible-%s' % self._name\n",
   "                facility = getattr(syslog, self._syslog_facility, syslog.LOG_USER)\n",
   "                syslog.openlog(str(module), 0, facility)\n",
   "                syslog.syslog(syslog.LOG_INFO, msg)\n",
   "                self.fail_json(\n",
   "                    msg_to_log=msg,\n",
   "        if self._debug:\n",
   "            self.log('[debug] %s' % msg)\n",
   "        if not self.no_log:\n",
   "                log_args = dict()\n",
   "            module = 'ansible-%s' % self._name\n",
   "            if isinstance(module, binary_type):\n",
   "                module = module.decode('utf-8', 'replace')\n",
   "            if not isinstance(msg, (binary_type, text_type)):\n",
   "                raise TypeError(\"msg should be a string (got %s)\" % type(msg))\n",
   "            if isinstance(msg, binary_type):\n",
   "                journal_msg = remove_values(msg.decode('utf-8', 'replace'), self.no_log_values)\n",
   "                journal_msg = remove_values(msg, self.no_log_values)\n",
   "                syslog_msg = journal_msg\n",
   "                syslog_msg = journal_msg.encode('utf-8', 'replace')\n",
   "            if has_journal:\n",
   "                journal_args = [(\"MODULE\", os.path.basename(__file__))]\n",
   "                for arg in log_args:\n",
   "                    journal_args.append((arg.upper(), str(log_args[arg])))\n",
   "                    if HAS_SYSLOG:\n",
   "                        facility = getattr(syslog,\n",
   "                                           self._syslog_facility,\n",
   "                        journal.send(MESSAGE=u\"%s %s\" % (module, journal_msg),\n",
   "                                     SYSLOG_FACILITY=facility,\n",
   "                                     **dict(journal_args))\n",
   "                        journal.send(MESSAGE=u\"%s %s\" % (module, journal_msg),\n",
   "                                     **dict(journal_args))\n",
   "                    self._log_to_syslog(syslog_msg)\n",
   "                self._log_to_syslog(syslog_msg)\n",
   "        log_args = dict()\n",
   "        for param in self.params:\n",
   "            canon = self.aliases.get(param, param)\n",
   "            arg_opts = self.argument_spec.get(canon, {})\n",
   "            no_log = arg_opts.get('no_log', None)\n",
   "            if no_log is None and PASSWORD_MATCH.search(param):\n",
   "                log_args[param] = 'NOT_LOGGING_PASSWORD'\n",
   "                self.warn('Module did not set no_log for %s' % param)\n",
   "            elif self.boolean(no_log):\n",
   "                log_args[param] = 'NOT_LOGGING_PARAMETER'\n",
   "                param_val = self.params[param]\n",
   "                if not isinstance(param_val, (text_type, binary_type)):\n",
   "                    param_val = str(param_val)\n",
   "                elif isinstance(param_val, text_type):\n",
   "                    param_val = param_val.encode('utf-8')\n",
   "                log_args[param] = heuristic_log_sanitize(param_val, self.no_log_values)\n",
   "        msg = ['%s=%s' % (to_native(arg), to_native(val)) for arg, val in log_args.items()]\n",
   "        if msg:\n",
   "            msg = 'Invoked with %s' % ' '.join(msg)\n",
   "            msg = 'Invoked'\n",
   "        self.log(msg, log_args=log_args)\n",
   "            cwd = os.getcwd()\n",
   "            if not os.access(cwd, os.F_OK | os.R_OK):\n",
   "            return cwd\n",
   "            for cwd in [self.tmpdir, os.path.expandvars('$HOME'), tempfile.gettempdir()]:\n",
   "                    if os.access(cwd, os.F_OK | os.R_OK):\n",
   "                        os.chdir(cwd)\n",
   "                        return cwd\n",
   "        bin_path = None\n",
   "            bin_path = get_bin_path(arg=arg, opt_dirs=opt_dirs)\n",
   "                self.fail_json(msg=to_text(e))\n",
   "                return bin_path\n",
   "        return bin_path\n",
   "        if arg is None:\n",
   "            return arg\n",
   "            return boolean(arg)\n",
   "            self.fail_json(msg=to_native(e))\n",
   "            return jsonify(data)\n",
   "            self.fail_json(msg=to_text(e))\n",
   "        return json.loads(data)\n",
   "        if path not in self.cleanup_files:\n",
   "            self.cleanup_files.append(path)\n",
   "        for path in self.cleanup_files:\n",
   "            self.cleanup(path)\n",
   "        self.add_atomic_move_warnings()\n",
   "        self.add_path_info(kwargs)\n",
   "        if 'invocation' not in kwargs:\n",
   "            kwargs['invocation'] = {'module_args': self.params}\n",
   "        if 'warnings' in kwargs:\n",
   "            if isinstance(kwargs['warnings'], list):\n",
   "                for w in kwargs['warnings']:\n",
   "                    self.warn(w)\n",
   "                self.warn(kwargs['warnings'])\n",
   "        warnings = get_warning_messages()\n",
   "        if warnings:\n",
   "            kwargs['warnings'] = warnings\n",
   "        if 'deprecations' in kwargs:\n",
   "            if isinstance(kwargs['deprecations'], list):\n",
   "                for d in kwargs['deprecations']:\n",
   "                    if isinstance(d, SEQUENCETYPE) and len(d) == 2:\n",
   "                        self.deprecate(d[0], version=d[1])\n",
   "                    elif isinstance(d, Mapping):\n",
   "                        self.deprecate(d['msg'], version=d.get('version'), date=d.get('date'),\n",
   "                                       collection_name=d.get('collection_name'))\n",
   "                        self.deprecate(d)  # pylint: disable=ansible-deprecated-no-version\n",
   "                self.deprecate(kwargs['deprecations'])  # pylint: disable=ansible-deprecated-no-version\n",
   "        deprecations = get_deprecation_messages()\n",
   "        if deprecations:\n",
   "            kwargs['deprecations'] = deprecations\n",
   "        kwargs = remove_values(kwargs, self.no_log_values)\n",
   "        print('\\n%s' % self.jsonify(kwargs))\n",
   "        self.do_cleanup_files()\n",
   "        self._return_formatted(kwargs)\n",
   "        kwargs['failed'] = True\n",
   "        kwargs['msg'] = msg\n",
   "        if 'exception' not in kwargs and sys.exc_info()[2] and (self._debug or self._verbosity >= 3):\n",
   "                kwargs['exception'] = 'WARNING: The below traceback may *not* be related to the actual failure.\\n' +\\\n",
   "                kwargs['exception'] = ''.join(traceback.format_tb(sys.exc_info()[2]))\n",
   "        self.do_cleanup_files()\n",
   "        self._return_formatted(kwargs)\n",
   "            check_missing_parameters(self.params, required_params)\n",
   "            self.fail_json(msg=to_native(e))\n",
   "        b_filename = to_bytes(filename, errors='surrogate_or_strict')\n",
   "        if not os.path.exists(b_filename):\n",
   "        if os.path.isdir(b_filename):\n",
   "            self.fail_json(msg=\"attempted to take checksum of directory: %s\" % filename)\n",
   "        if hasattr(algorithm, 'hexdigest'):\n",
   "            digest_method = algorithm\n",
   "                digest_method = AVAILABLE_HASH_ALGORITHMS[algorithm]()\n",
   "                self.fail_json(msg=\"Could not hash file '%s' with algorithm '%s'. Available algorithms: %s\" %\n",
   "                                   (filename, algorithm, ', '.join(AVAILABLE_HASH_ALGORITHMS)))\n",
   "        blocksize = 64 * 1024\n",
   "        infile = open(os.path.realpath(b_filename), 'rb')\n",
   "        block = infile.read(blocksize)\n",
   "        while block:\n",
   "            digest_method.update(block)\n",
   "            block = infile.read(blocksize)\n",
   "        infile.close()\n",
   "        return digest_method.hexdigest()\n",
   "        if 'md5' not in AVAILABLE_HASH_ALGORITHMS:\n",
   "        return self.digest_from_file(filename, 'md5')\n",
   "        return self.digest_from_file(filename, 'sha1')\n",
   "        return self.digest_from_file(filename, 'sha256')\n",
   "        backupdest = ''\n",
   "            ext = time.strftime(\"%Y-%m-%d@%H:%M:%S~\", time.localtime(time.time()))\n",
   "            backupdest = '%s.%s.%s' % (fn, os.getpid(), ext)\n",
   "                self.preserved_copy(fn, backupdest)\n",
   "                self.fail_json(msg='Could not make backup of %s to %s: %s' % (fn, backupdest, to_native(e)))\n",
   "        return backupdest\n",
   "        if self.selinux_enabled():\n",
   "            context = self.selinux_context(src)\n",
   "            self.set_context_if_different(dest, context, False)\n",
   "            dest_stat = os.stat(src)\n",
   "            tmp_stat = os.stat(dest)\n",
   "            if dest_stat and (tmp_stat.st_uid != dest_stat.st_uid or tmp_stat.st_gid != dest_stat.st_gid):\n",
   "                os.chown(dest, dest_stat.st_uid, dest_stat.st_gid)\n",
   "        current_attribs = self.get_file_attributes(src)\n",
   "        current_attribs = current_attribs.get('attr_flags', '')\n",
   "        self.set_attributes_if_different(dest, current_attribs, True)\n",
   "        context = None\n",
   "        dest_stat = None\n",
   "        b_src = to_bytes(src, errors='surrogate_or_strict')\n",
   "        b_dest = to_bytes(dest, errors='surrogate_or_strict')\n",
   "        if os.path.exists(b_dest):\n",
   "                dest_stat = os.stat(b_dest)\n",
   "                os.chmod(b_src, dest_stat.st_mode & PERM_BITS)\n",
   "                os.chown(b_src, dest_stat.st_uid, dest_stat.st_gid)\n",
   "                if hasattr(os, 'chflags') and hasattr(dest_stat, 'st_flags'):\n",
   "                        os.chflags(b_src, dest_stat.st_flags)\n",
   "                        for err in 'EOPNOTSUPP', 'ENOTSUP':\n",
   "                            if hasattr(errno, err) and e.errno == getattr(errno, err):\n",
   "            if self.selinux_enabled():\n",
   "                context = self.selinux_context(dest)\n",
   "            if self.selinux_enabled():\n",
   "                context = self.selinux_default_context(dest)\n",
   "        creating = not os.path.exists(b_dest)\n",
   "            os.rename(b_src, b_dest)\n",
   "                self.fail_json(msg='Could not replace file: %s to %s: %s' % (src, dest, to_native(e)),\n",
   "                b_dest_dir = os.path.dirname(b_dest)\n",
   "                b_suffix = os.path.basename(b_dest)\n",
   "                error_msg = None\n",
   "                tmp_dest_name = None\n",
   "                    tmp_dest_fd, tmp_dest_name = tempfile.mkstemp(prefix=b'.ansible_tmp',\n",
   "                                                                  dir=b_dest_dir, suffix=b_suffix)\n",
   "                    error_msg = 'The destination directory (%s) is not writable by the current user. Error was: %s' % (os.path.dirname(dest), to_native(e))\n",
   "                    error_msg = ('Failed creating tmp file for atomic move.  This usually happens when using Python3 less than Python3.5. '\n",
   "                    if error_msg:\n",
   "                            self._unsafe_writes(b_src, b_dest)\n",
   "                            self.fail_json(msg=error_msg, exception=traceback.format_exc())\n",
   "                if tmp_dest_name:\n",
   "                    b_tmp_dest_name = to_bytes(tmp_dest_name, errors='surrogate_or_strict')\n",
   "                            os.close(tmp_dest_fd)\n",
   "                                shutil.move(b_src, b_tmp_dest_name)\n",
   "                                shutil.copy2(b_src, b_tmp_dest_name)\n",
   "                            if self.selinux_enabled():\n",
   "                                self.set_context_if_different(\n",
   "                                    b_tmp_dest_name, context, False)\n",
   "                                tmp_stat = os.stat(b_tmp_dest_name)\n",
   "                                if dest_stat and (tmp_stat.st_uid != dest_stat.st_uid or tmp_stat.st_gid != dest_stat.st_gid):\n",
   "                                    os.chown(b_tmp_dest_name, dest_stat.st_uid, dest_stat.st_gid)\n",
   "                                os.rename(b_tmp_dest_name, b_dest)\n",
   "                                    self._unsafe_writes(b_tmp_dest_name, b_dest)\n",
   "                                    self.fail_json(msg='Unable to make %s into to %s, failed final rename from %s: %s' %\n",
   "                                                       (src, dest, b_tmp_dest_name, to_native(e)),\n",
   "                            self.fail_json(msg='Failed to replace file: %s to %s: %s' % (src, dest, to_native(e)),\n",
   "                        self.cleanup(b_tmp_dest_name)\n",
   "        if creating:\n",
   "            if self.argument_spec.get('mode') and self.params.get('mode') is None:\n",
   "                self._created_files.add(dest)\n",
   "            umask = os.umask(0)\n",
   "            os.umask(umask)\n",
   "            os.chmod(b_dest, DEFAULT_PERM & ~umask)\n",
   "                os.chown(b_dest, os.geteuid(), os.getegid())\n",
   "        if self.selinux_enabled():\n",
   "            self.set_context_if_different(dest, context, False)\n",
   "            out_dest = in_src = None\n",
   "                out_dest = open(dest, 'wb')\n",
   "                in_src = open(src, 'rb')\n",
   "                shutil.copyfileobj(in_src, out_dest)\n",
   "                if out_dest:\n",
   "                    out_dest.close()\n",
   "                if in_src:\n",
   "                    in_src.close()\n",
   "            self.fail_json(msg='Could not write data to file (%s) from (%s): %s' % (dest, src, to_native(e)),\n",
   "        if not self._clean:\n",
   "            to_clean_args = args\n",
   "                    to_clean_args = to_bytes(args)\n",
   "                    to_clean_args = to_text(args)\n",
   "                to_clean_args = shlex.split(to_clean_args)\n",
   "            clean_args = []\n",
   "            is_passwd = False\n",
   "            for arg in (to_native(a) for a in to_clean_args):\n",
   "                if is_passwd:\n",
   "                    is_passwd = False\n",
   "                    clean_args.append('********')\n",
   "                if PASSWD_ARG_RE.match(arg):\n",
   "                    sep_idx = arg.find('=')\n",
   "                    if sep_idx > -1:\n",
   "                        clean_args.append('%s=********' % arg[:sep_idx])\n",
   "                        is_passwd = True\n",
   "                arg = heuristic_log_sanitize(arg, self.no_log_values)\n",
   "                clean_args.append(arg)\n",
   "            self._clean = ' '.join(shlex_quote(arg) for arg in clean_args)\n",
   "        return self._clean\n",
   "        self._clean = None\n",
   "            msg = \"Argument 'args' to run_command must be list or string\"\n",
   "            self.fail_json(rc=257, cmd=args, msg=msg)\n",
   "        shell = False\n",
   "                args = b\" \".join([to_bytes(shlex_quote(x), errors='surrogate_or_strict') for x in args])\n",
   "                args = to_bytes(args, errors='surrogate_or_strict')\n",
   "                executable = to_bytes(executable, errors='surrogate_or_strict')\n",
   "                args = [executable, b'-c', args]\n",
   "            elif self._shell not in (None, '/bin/sh'):\n",
   "                args = [to_bytes(self._shell, errors='surrogate_or_strict'), b'-c', args]\n",
   "                shell = True\n",
   "            if isinstance(args, (binary_type, text_type)):\n",
   "                    args = to_bytes(args, errors='surrogate_or_strict')\n",
   "                    args = to_text(args, errors='surrogateescape')\n",
   "                args = shlex.split(args)\n",
   "                args = [to_bytes(os.path.expanduser(os.path.expandvars(x)), errors='surrogate_or_strict') for x in args if x is not None]\n",
   "                args = [to_bytes(x, errors='surrogate_or_strict') for x in args if x is not None]\n",
   "        prompt_re = None\n",
   "                    prompt_regex = to_bytes(prompt_regex, errors='surrogateescape')\n",
   "                    prompt_regex = to_bytes(prompt_regex, errors='surrogate_or_strict')\n",
   "                prompt_re = re.compile(prompt_regex, re.MULTILINE)\n",
   "                self.fail_json(msg=\"invalid prompt regular expression given to run_command\")\n",
   "        rc = 0\n",
   "        msg = None\n",
   "        st_in = None\n",
   "        old_env_vals = {}\n",
   "        for key, val in self.run_command_environ_update.items():\n",
   "            old_env_vals[key] = os.environ.get(key, None)\n",
   "            os.environ[key] = val\n",
   "            for key, val in environ_update.items():\n",
   "                old_env_vals[key] = os.environ.get(key, None)\n",
   "                os.environ[key] = val\n",
   "            old_env_vals['PATH'] = os.environ['PATH']\n",
   "            pypaths = os.environ['PYTHONPATH'].split(':')\n",
   "            pypaths = [x for x in pypaths\n",
   "                       if not x.endswith('/ansible_modlib.zip') and\n",
   "                       not x.endswith('/debug_dir')]\n",
   "            os.environ['PYTHONPATH'] = ':'.join(pypaths)\n",
   "        if data:\n",
   "            st_in = subprocess.PIPE\n",
   "        kwargs = dict(\n",
   "            executable=executable,\n",
   "            shell=shell,\n",
   "            stdin=st_in,\n",
   "            preexec_fn=self._restore_signal_handlers,\n",
   "            kwargs[\"pass_fds\"] = pass_fds\n",
   "            kwargs['close_fds'] = False\n",
   "        prev_dir = os.getcwd()\n",
   "        if cwd and os.path.isdir(cwd):\n",
   "            cwd = to_bytes(os.path.abspath(os.path.expanduser(cwd)), errors='surrogate_or_strict')\n",
   "            kwargs['cwd'] = cwd\n",
   "                os.chdir(cwd)\n",
   "                self.fail_json(rc=e.errno, msg=\"Could not open %s, %s\" % (cwd, to_native(e)),\n",
   "        old_umask = None\n",
   "        if umask:\n",
   "            old_umask = os.umask(umask)\n",
   "            if self._debug:\n",
   "                self.log('Executing: ' + self._clean_args(args))\n",
   "            cmd = subprocess.Popen(args, **kwargs)\n",
   "                before_communicate_callback(cmd)\n",
   "            stdout = b''\n",
   "            stderr = b''\n",
   "                selector = selectors.DefaultSelector()\n",
   "                selector = selectors.PollSelector()\n",
   "            selector.register(cmd.stdout, selectors.EVENT_READ)\n",
   "            selector.register(cmd.stderr, selectors.EVENT_READ)\n",
   "                fcntl.fcntl(cmd.stdout.fileno(), fcntl.F_SETFL, fcntl.fcntl(cmd.stdout.fileno(), fcntl.F_GETFL) | os.O_NONBLOCK)\n",
   "                fcntl.fcntl(cmd.stderr.fileno(), fcntl.F_SETFL, fcntl.fcntl(cmd.stderr.fileno(), fcntl.F_GETFL) | os.O_NONBLOCK)\n",
   "            if data:\n",
   "                    data += '\\n'\n",
   "                if isinstance(data, text_type):\n",
   "                    data = to_bytes(data)\n",
   "                cmd.stdin.write(data)\n",
   "                cmd.stdin.close()\n",
   "                events = selector.select(1)\n",
   "                for key, event in events:\n",
   "                    b_chunk = key.fileobj.read()\n",
   "                    if b_chunk == b(''):\n",
   "                        selector.unregister(key.fileobj)\n",
   "                    if key.fileobj == cmd.stdout:\n",
   "                        stdout += b_chunk\n",
   "                    elif key.fileobj == cmd.stderr:\n",
   "                        stderr += b_chunk\n",
   "                if prompt_re:\n",
   "                    if prompt_re.search(stdout) and not data:\n",
   "                            stdout = to_native(stdout, encoding=encoding, errors=errors)\n",
   "                        return (257, stdout, \"A prompt was encountered while running a command, but no input data was specified\")\n",
   "                if (not events or not selector.get_map()) and cmd.poll() is not None:\n",
   "                elif not selector.get_map() and cmd.poll() is None:\n",
   "                    cmd.wait()\n",
   "            cmd.stdout.close()\n",
   "            cmd.stderr.close()\n",
   "            selector.close()\n",
   "            rc = cmd.returncode\n",
   "            self.log(\"Error Executing CMD:%s Exception:%s\" % (self._clean_args(args), to_native(e)))\n",
   "            self.fail_json(rc=e.errno, msg=to_native(e), cmd=self._clean_args(args))\n",
   "            self.log(\"Error Executing CMD:%s Exception:%s\" % (self._clean_args(args), to_native(traceback.format_exc())))\n",
   "            self.fail_json(rc=257, msg=to_native(e), exception=traceback.format_exc(), cmd=self._clean_args(args))\n",
   "        for key, val in old_env_vals.items():\n",
   "            if val is None:\n",
   "                del os.environ[key]\n",
   "                os.environ[key] = val\n",
   "        if old_umask:\n",
   "            os.umask(old_umask)\n",
   "        if rc != 0 and check_rc:\n",
   "            msg = heuristic_log_sanitize(stderr.rstrip(), self.no_log_values)\n",
   "            self.fail_json(cmd=self._clean_args(args), rc=rc, stdout=stdout, stderr=stderr, msg=msg)\n",
   "        os.chdir(prev_dir)\n",
   "            return (rc, to_native(stdout, encoding=encoding, errors=errors),\n",
   "                    to_native(stderr, encoding=encoding, errors=errors))\n",
   "        return (rc, stdout, stderr)\n",
   "        filename = os.path.expandvars(os.path.expanduser(filename))\n",
   "        fh = open(filename, 'a')\n",
   "        fh.write(str)\n",
   "        fh.close()\n",
   "    is_executable = is_executable\n",
   "            buffer_size = fcntl.fcntl(fd, 1032)\n",
   "        return buffer_size\n"
  ]
 },
 "169": {
  "name": "buffer",
  "type": "bytes",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/basic.py",
  "lineno": "609",
  "column": "20",
  "context": "gv[1]\n                if PY3:\n                    buffer = buffer.encode('utf-8', errors='surrogateescape')\n        # default case, read from stdin\n        el",
  "context_lines": "                fd.close()\n            else:\n                buffer = sys.argv[1]\n                if PY3:\n                    buffer = buffer.encode('utf-8', errors='surrogateescape')\n        # default case, read from stdin\n        else:\n            if PY2:\n                buffer = sys.stdin.read()\n",
  "slicing": [
   "    HAS_SYSLOG = True\n",
   "    HAS_SYSLOG = False\n",
   "    has_journal = hasattr(journal, 'sendv')\n",
   "    has_journal = False\n",
   "HAVE_SELINUX = False\n",
   "    HAVE_SELINUX = True\n",
   "NoneType = type(None)\n",
   "AVAILABLE_HASH_ALGORITHMS = dict()\n",
   "    for attribute in ('available_algorithms', 'algorithms'):\n",
   "        algorithms = getattr(hashlib, attribute, None)\n",
   "        if algorithms:\n",
   "    if algorithms is None:\n",
   "        algorithms = ('md5', 'sha1', 'sha224', 'sha256', 'sha384', 'sha512')\n",
   "    for algorithm in algorithms:\n",
   "        AVAILABLE_HASH_ALGORITHMS[algorithm] = getattr(hashlib, algorithm)\n",
   "        AVAILABLE_HASH_ALGORITHMS.pop('md5', None)\n",
   "    AVAILABLE_HASH_ALGORITHMS = {'sha1': sha.sha}\n",
   "        AVAILABLE_HASH_ALGORITHMS['md5'] = md5.md5\n",
   "SEQUENCETYPE = frozenset, KeysView, Sequence\n",
   "PASSWORD_MATCH = re.compile(r'^(?:.+[-_\\s])?pass(?:[-_\\s]?(?:word|phrase|wrd|wd)?)(?:[-_\\s].+)?$', re.I)\n",
   "    unicode = text_type\n",
   "    basestring = string_types\n",
   "_ANSIBLE_ARGS = None\n",
   "FILE_COMMON_ARGUMENTS = dict(\n",
   "PASSWD_ARG_RE = re.compile(r'^[-]{0,2}pass[-]?(word|wd)?')\n",
   "MODE_OPERATOR_RE = re.compile(r'[+=-]')\n",
   "USERS_RE = re.compile(r'[^ugo]')\n",
   "PERMS_RE = re.compile(r'[^rwxXstugo]')\n",
   "_PY3_MIN = sys.version_info[:2] >= (3, 5)\n",
   "_PY2_MIN = (2, 6) <= sys.version_info[:2] < (3,)\n",
   "_PY_MIN = _PY3_MIN or _PY2_MIN\n",
   "if not _PY_MIN:\n",
   "    platform_cls = get_platform_subclass(cls)\n",
   "    return super(cls, platform_cls).__new__(platform_cls)\n",
   "def _remove_values_conditions(value, no_log_strings, deferred_removals):\n",
   "        native_str_value = value\n",
   "            value_is_text = True\n",
   "                native_str_value = to_bytes(value, errors='surrogate_or_strict')\n",
   "            value_is_text = False\n",
   "                native_str_value = to_text(value, errors='surrogate_or_strict')\n",
   "        if native_str_value in no_log_strings:\n",
   "        for omit_me in no_log_strings:\n",
   "            native_str_value = native_str_value.replace(omit_me, '*' * 8)\n",
   "        if value_is_text and isinstance(native_str_value, binary_type):\n",
   "            value = to_text(native_str_value, encoding='utf-8', errors='surrogate_then_replace')\n",
   "        elif not value_is_text and isinstance(native_str_value, text_type):\n",
   "            value = to_bytes(native_str_value, encoding='utf-8', errors='surrogate_then_replace')\n",
   "            value = native_str_value\n",
   "    elif isinstance(value, Sequence):\n",
   "        if isinstance(value, MutableSequence):\n",
   "            new_value = type(value)()\n",
   "            new_value = []  # Need a mutable value\n",
   "        deferred_removals.append((value, new_value))\n",
   "        value = new_value\n",
   "    elif isinstance(value, Set):\n",
   "        if isinstance(value, MutableSet):\n",
   "            new_value = type(value)()\n",
   "            new_value = set()  # Need a mutable value\n",
   "        deferred_removals.append((value, new_value))\n",
   "        value = new_value\n",
   "    elif isinstance(value, Mapping):\n",
   "        if isinstance(value, MutableMapping):\n",
   "            new_value = type(value)()\n",
   "            new_value = {}  # Need a mutable value\n",
   "        deferred_removals.append((value, new_value))\n",
   "        value = new_value\n",
   "    elif isinstance(value, tuple(chain(integer_types, (float, bool, NoneType)))):\n",
   "        stringy_value = to_native(value, encoding='utf-8', errors='surrogate_or_strict')\n",
   "        if stringy_value in no_log_strings:\n",
   "        for omit_me in no_log_strings:\n",
   "            if omit_me in stringy_value:\n",
   "    elif isinstance(value, (datetime.datetime, datetime.date)):\n",
   "        value = value.isoformat()\n",
   "        raise TypeError('Value of unknown type: %s, %s' % (type(value), value))\n",
   "    return value\n",
   "def remove_values(value, no_log_strings):\n",
   "    deferred_removals = deque()\n",
   "    no_log_strings = [to_native(s, errors='surrogate_or_strict') for s in no_log_strings]\n",
   "    new_value = _remove_values_conditions(value, no_log_strings, deferred_removals)\n",
   "    while deferred_removals:\n",
   "        old_data, new_data = deferred_removals.popleft()\n",
   "        if isinstance(new_data, Mapping):\n",
   "            for old_key, old_elem in old_data.items():\n",
   "                new_elem = _remove_values_conditions(old_elem, no_log_strings, deferred_removals)\n",
   "                new_data[old_key] = new_elem\n",
   "            for elem in old_data:\n",
   "                new_elem = _remove_values_conditions(elem, no_log_strings, deferred_removals)\n",
   "                if isinstance(new_data, MutableSequence):\n",
   "                    new_data.append(new_elem)\n",
   "                elif isinstance(new_data, MutableSet):\n",
   "                    new_data.add(new_elem)\n",
   "    return new_value\n",
   "def _sanitize_keys_conditions(value, no_log_strings, ignore_keys, deferred_removals):\n",
   "    if isinstance(value, (text_type, binary_type)):\n",
   "        return value\n",
   "    if isinstance(value, Sequence):\n",
   "        if isinstance(value, MutableSequence):\n",
   "            new_value = type(value)()\n",
   "            new_value = []  # Need a mutable value\n",
   "        deferred_removals.append((value, new_value))\n",
   "        return new_value\n",
   "    if isinstance(value, Set):\n",
   "        if isinstance(value, MutableSet):\n",
   "            new_value = type(value)()\n",
   "            new_value = set()  # Need a mutable value\n",
   "        deferred_removals.append((value, new_value))\n",
   "        return new_value\n",
   "    if isinstance(value, Mapping):\n",
   "        if isinstance(value, MutableMapping):\n",
   "            new_value = type(value)()\n",
   "            new_value = {}  # Need a mutable value\n",
   "        deferred_removals.append((value, new_value))\n",
   "        return new_value\n",
   "    if isinstance(value, tuple(chain(integer_types, (float, bool, NoneType)))):\n",
   "        return value\n",
   "    if isinstance(value, (datetime.datetime, datetime.date)):\n",
   "        return value\n",
   "    raise TypeError('Value of unknown type: %s, %s' % (type(value), value))\n",
   "    deferred_removals = deque()\n",
   "    no_log_strings = [to_native(s, errors='surrogate_or_strict') for s in no_log_strings]\n",
   "    new_value = _sanitize_keys_conditions(obj, no_log_strings, ignore_keys, deferred_removals)\n",
   "    while deferred_removals:\n",
   "        old_data, new_data = deferred_removals.popleft()\n",
   "        if isinstance(new_data, Mapping):\n",
   "            for old_key, old_elem in old_data.items():\n",
   "                if old_key in ignore_keys or old_key.startswith('_ansible'):\n",
   "                    new_data[old_key] = _sanitize_keys_conditions(old_elem, no_log_strings, ignore_keys, deferred_removals)\n",
   "                    new_key = _remove_values_conditions(old_key, no_log_strings, None)\n",
   "                    new_data[new_key] = _sanitize_keys_conditions(old_elem, no_log_strings, ignore_keys, deferred_removals)\n",
   "            for elem in old_data:\n",
   "                new_elem = _sanitize_keys_conditions(elem, no_log_strings, ignore_keys, deferred_removals)\n",
   "                if isinstance(new_data, MutableSequence):\n",
   "                    new_data.append(new_elem)\n",
   "                elif isinstance(new_data, MutableSet):\n",
   "                    new_data.add(new_elem)\n",
   "    return new_value\n",
   "def heuristic_log_sanitize(data, no_log_values=None):\n",
   "    data = to_native(data)\n",
   "    output = []\n",
   "    begin = len(data)\n",
   "    prev_begin = begin\n",
   "    sep = 1\n",
   "    while sep:\n",
   "            end = data.rindex('@', 0, begin)\n",
   "            output.insert(0, data[0:begin])\n",
   "        sep = None\n",
   "        sep_search_end = end\n",
   "        while not sep:\n",
   "                begin = data.rindex('://', 0, sep_search_end)\n",
   "                begin = 0\n",
   "                sep = data.index(':', begin + 3, end)\n",
   "                if begin == 0:\n",
   "                    output.insert(0, data[0:begin])\n",
   "                sep_search_end = begin\n",
   "        if sep:\n",
   "            output.insert(0, data[end:prev_begin])\n",
   "            output.insert(0, '********')\n",
   "            output.insert(0, data[begin:sep + 1])\n",
   "            prev_begin = begin\n",
   "    output = ''.join(output)\n",
   "        output = remove_values(output, no_log_values)\n",
   "    return output\n",
   "    if _ANSIBLE_ARGS is not None:\n",
   "        buffer = _ANSIBLE_ARGS\n",
   "                fd = open(sys.argv[1], 'rb')\n",
   "                buffer = fd.read()\n",
   "                fd.close()\n",
   "                buffer = sys.argv[1]\n",
   "                    buffer = buffer.encode('utf-8', errors='surrogateescape')\n",
   "                buffer = sys.stdin.read()\n",
   "                buffer = sys.stdin.buffer.read()\n",
   "        _ANSIBLE_ARGS = buffer\n",
   "        params = json.loads(buffer.decode('utf-8'))\n",
   "        params = json_dict_unicode_to_bytes(params)\n",
   "        return params['ANSIBLE_MODULE_ARGS']\n",
   "    for arg in args:\n",
   "        if arg in os.environ:\n",
   "            return os.environ[arg]\n",
   "    hostname = platform.node()\n",
   "    msg = \"Failed to import the required Python library (%s) on %s's Python %s.\" % (library, hostname, sys.executable)\n",
   "        msg += \" This is required %s.\" % reason\n",
   "        msg += \" See %s for more info.\" % url\n",
   "    msg += (\" Please read the module documentation and install it in the appropriate location.\"\n",
   "    return msg\n",
   "            for k, v in FILE_COMMON_ARGUMENTS.items():\n",
   "                if k not in self.argument_spec:\n",
   "                    self.argument_spec[k] = v\n",
   "            basedir = None\n",
   "                basedir = os.path.expanduser(os.path.expandvars(self._remote_tmp))\n",
   "            if basedir is not None and not os.path.exists(basedir):\n",
   "                    os.makedirs(basedir, mode=0o700)\n",
   "                              \"failing back to system: %s\" % (basedir, to_native(e)))\n",
   "                    basedir = None\n",
   "                              \"the correct permissions manually\" % basedir)\n",
   "            basefile = \"ansible-moduletmp-%s-\" % time.time()\n",
   "                tmpdir = tempfile.mkdtemp(prefix=basefile, dir=basedir)\n",
   "                        \"with prefix %s: %s\" % (basedir, basefile, to_native(e))\n",
   "                atexit.register(shutil.rmtree, tmpdir)\n",
   "            self._tmpdir = tmpdir\n",
   "    def warn(self, warning):\n",
   "    def deprecate(self, msg, version=None, date=None, collection_name=None):\n",
   "        deprecate(msg, version=version, date=date, collection_name=collection_name)\n",
   "            self.log('[DEPRECATION WARNING] %s %s' % (msg, date))\n",
   "            self.log('[DEPRECATION WARNING] %s %s' % (msg, version))\n",
   "            path = params.get('path', params.get('dest', None))\n",
   "        if path is None:\n",
   "            path = os.path.expanduser(os.path.expandvars(path))\n",
   "        b_path = to_bytes(path, errors='surrogate_or_strict')\n",
   "        if params.get('follow', False) and os.path.islink(b_path):\n",
   "            b_path = os.path.realpath(b_path)\n",
   "            path = to_native(b_path)\n",
   "        mode = params.get('mode', None)\n",
   "        owner = params.get('owner', None)\n",
   "        group = params.get('group', None)\n",
   "        seuser = params.get('seuser', None)\n",
   "        serole = params.get('serole', None)\n",
   "        setype = params.get('setype', None)\n",
   "        selevel = params.get('selevel', None)\n",
   "        secontext = [seuser, serole, setype]\n",
   "            secontext.append(selevel)\n",
   "        default_secontext = self.selinux_default_context(path)\n",
   "        for i in range(len(default_secontext)):\n",
   "            if i is not None and secontext[i] == '_default':\n",
   "                secontext[i] = default_secontext[i]\n",
   "        attributes = params.get('attributes', None)\n",
   "            path=path, mode=mode, owner=owner, group=group,\n",
   "            seuser=seuser, serole=serole, setype=setype,\n",
   "            selevel=selevel, secontext=secontext, attributes=attributes,\n",
   "        if not HAVE_SELINUX:\n",
   "        if not HAVE_SELINUX:\n",
   "            seenabled = self.get_bin_path('selinuxenabled')\n",
   "            if seenabled is not None:\n",
   "                (rc, out, err) = self.run_command(seenabled)\n",
   "                if rc == 0:\n",
   "        context = [None, None, None]\n",
   "            context.append(None)\n",
   "        return context\n",
   "        context = self.selinux_initial_context()\n",
   "        if not HAVE_SELINUX or not self.selinux_enabled():\n",
   "            return context\n",
   "            ret = selinux.matchpathcon(to_native(path, errors='surrogate_or_strict'), mode)\n",
   "            return context\n",
   "        if ret[0] == -1:\n",
   "            return context\n",
   "        context = ret[1].split(':', 3)\n",
   "        return context\n",
   "        context = self.selinux_initial_context()\n",
   "        if not HAVE_SELINUX or not self.selinux_enabled():\n",
   "            return context\n",
   "            ret = selinux.lgetfilecon_raw(to_native(path, errors='surrogate_or_strict'))\n",
   "                self.fail_json(path=path, msg='path %s does not exist' % path)\n",
   "                self.fail_json(path=path, msg='failed to retrieve selinux context')\n",
   "        if ret[0] == -1:\n",
   "            return context\n",
   "        context = ret[1].split(':', 3)\n",
   "        return context\n",
   "        b_path = to_bytes(path, errors='surrogate_or_strict')\n",
   "            b_path = os.path.expanduser(os.path.expandvars(b_path))\n",
   "        st = os.lstat(b_path)\n",
   "        uid = st.st_uid\n",
   "        gid = st.st_gid\n",
   "        return (uid, gid)\n",
   "        path_is_bytes = False\n",
   "        if isinstance(path, binary_type):\n",
   "            path_is_bytes = True\n",
   "        b_path = os.path.realpath(to_bytes(os.path.expanduser(os.path.expandvars(path)), errors='surrogate_or_strict'))\n",
   "        while not os.path.ismount(b_path):\n",
   "            b_path = os.path.dirname(b_path)\n",
   "        if path_is_bytes:\n",
   "            return b_path\n",
   "        return to_text(b_path, errors='surrogate_or_strict')\n",
   "            f = open('/proc/mounts', 'r')\n",
   "            mount_data = f.readlines()\n",
   "            f.close()\n",
   "        path_mount_point = self.find_mount_point(path)\n",
   "        for line in mount_data:\n",
   "            (device, mount_point, fstype, options, rest) = line.split(' ', 4)\n",
   "            if to_bytes(path_mount_point) == to_bytes(mount_point):\n",
   "                for fs in self._selinux_special_fs:\n",
   "                    if fs in fstype:\n",
   "                        special_context = self.selinux_context(path_mount_point)\n",
   "                        return (True, special_context)\n",
   "        if not HAVE_SELINUX or not self.selinux_enabled():\n",
   "        context = self.selinux_default_context(path)\n",
   "        return self.set_context_if_different(path, context, False)\n",
   "        if not HAVE_SELINUX or not self.selinux_enabled():\n",
   "        if self.check_file_absent_if_check_mode(path):\n",
   "        cur_context = self.selinux_context(path)\n",
   "        new_context = list(cur_context)\n",
   "        (is_special_se, sp_context) = self.is_special_selinux_path(path)\n",
   "        if is_special_se:\n",
   "            new_context = sp_context\n",
   "            for i in range(len(cur_context)):\n",
   "                if len(context) > i:\n",
   "                    if context[i] is not None and context[i] != cur_context[i]:\n",
   "                        new_context[i] = context[i]\n",
   "                    elif context[i] is None:\n",
   "                        new_context[i] = cur_context[i]\n",
   "        if cur_context != new_context:\n",
   "                diff['before']['secontext'] = cur_context\n",
   "                diff['after']['secontext'] = new_context\n",
   "                rc = selinux.lsetfilecon(to_native(path), ':'.join(new_context))\n",
   "                self.fail_json(path=path, msg='invalid selinux context: %s' % to_native(e),\n",
   "                               new_context=new_context, cur_context=cur_context, input_was=context)\n",
   "            if rc != 0:\n",
   "                self.fail_json(path=path, msg='set selinux context failed')\n",
   "            changed = True\n",
   "        return changed\n",
   "        if owner is None:\n",
   "            return changed\n",
   "        b_path = to_bytes(path, errors='surrogate_or_strict')\n",
   "            b_path = os.path.expanduser(os.path.expandvars(b_path))\n",
   "        if self.check_file_absent_if_check_mode(b_path):\n",
   "        orig_uid, orig_gid = self.user_and_group(b_path, expand)\n",
   "            uid = int(owner)\n",
   "                uid = pwd.getpwnam(owner).pw_uid\n",
   "                path = to_text(b_path)\n",
   "                self.fail_json(path=path, msg='chown failed: failed to look up user %s' % owner)\n",
   "        if orig_uid != uid:\n",
   "                diff['before']['owner'] = orig_uid\n",
   "                diff['after']['owner'] = uid\n",
   "                os.lchown(b_path, uid, -1)\n",
   "                path = to_text(b_path)\n",
   "                self.fail_json(path=path, msg='chown failed: %s' % (to_text(e)))\n",
   "            changed = True\n",
   "        return changed\n",
   "        if group is None:\n",
   "            return changed\n",
   "        b_path = to_bytes(path, errors='surrogate_or_strict')\n",
   "            b_path = os.path.expanduser(os.path.expandvars(b_path))\n",
   "        if self.check_file_absent_if_check_mode(b_path):\n",
   "        orig_uid, orig_gid = self.user_and_group(b_path, expand)\n",
   "            gid = int(group)\n",
   "                gid = grp.getgrnam(group).gr_gid\n",
   "                path = to_text(b_path)\n",
   "                self.fail_json(path=path, msg='chgrp failed: failed to look up group %s' % group)\n",
   "        if orig_gid != gid:\n",
   "                diff['before']['group'] = orig_gid\n",
   "                diff['after']['group'] = gid\n",
   "                os.lchown(b_path, -1, gid)\n",
   "                path = to_text(b_path)\n",
   "                self.fail_json(path=path, msg='chgrp failed')\n",
   "            changed = True\n",
   "        return changed\n",
   "        if mode is None:\n",
   "            return changed\n",
   "            self._created_files.remove(path)\n",
   "        b_path = to_bytes(path, errors='surrogate_or_strict')\n",
   "            b_path = os.path.expanduser(os.path.expandvars(b_path))\n",
   "        path_stat = os.lstat(b_path)\n",
   "        if self.check_file_absent_if_check_mode(b_path):\n",
   "        if not isinstance(mode, int):\n",
   "                mode = int(mode, 8)\n",
   "                    mode = self._symbolic_mode_to_octal(path_stat, mode)\n",
   "                    path = to_text(b_path)\n",
   "                    self.fail_json(path=path,\n",
   "                if mode != stat.S_IMODE(mode):\n",
   "                    path = to_text(b_path)\n",
   "                    self.fail_json(path=path, msg=\"Invalid mode supplied, only permission info is allowed\", details=mode)\n",
   "        prev_mode = stat.S_IMODE(path_stat.st_mode)\n",
   "        if prev_mode != mode:\n",
   "                diff['before']['mode'] = '0%03o' % prev_mode\n",
   "                diff['after']['mode'] = '0%03o' % mode\n",
   "                    os.lchmod(b_path, mode)\n",
   "                    if not os.path.islink(b_path):\n",
   "                        os.chmod(b_path, mode)\n",
   "                        underlying_stat = os.stat(b_path)\n",
   "                        os.chmod(b_path, mode)\n",
   "                        new_underlying_stat = os.stat(b_path)\n",
   "                        if underlying_stat.st_mode != new_underlying_stat.st_mode:\n",
   "                            os.chmod(b_path, stat.S_IMODE(underlying_stat.st_mode))\n",
   "                if os.path.islink(b_path) and e.errno in (errno.EPERM, errno.EROFS):  # Can't set mode on symbolic links\n",
   "                path = to_text(b_path)\n",
   "                self.fail_json(path=path, msg='chmod failed', details=to_native(e),\n",
   "            path_stat = os.lstat(b_path)\n",
   "            new_mode = stat.S_IMODE(path_stat.st_mode)\n",
   "            if new_mode != prev_mode:\n",
   "                changed = True\n",
   "        return changed\n",
   "        if attributes is None:\n",
   "            return changed\n",
   "        b_path = to_bytes(path, errors='surrogate_or_strict')\n",
   "            b_path = os.path.expanduser(os.path.expandvars(b_path))\n",
   "        if self.check_file_absent_if_check_mode(b_path):\n",
   "        existing = self.get_file_attributes(b_path)\n",
   "        attr_mod = '='\n",
   "        if attributes.startswith(('-', '+')):\n",
   "            attr_mod = attributes[0]\n",
   "            attributes = attributes[1:]\n",
   "        if existing.get('attr_flags', '') != attributes or attr_mod == '-':\n",
   "            attrcmd = self.get_bin_path('chattr')\n",
   "            if attrcmd:\n",
   "                attrcmd = [attrcmd, '%s%s' % (attr_mod, attributes), b_path]\n",
   "                changed = True\n",
   "                    diff['before']['attributes'] = existing.get('attr_flags')\n",
   "                    diff['after']['attributes'] = '%s%s' % (attr_mod, attributes)\n",
   "                        rc, out, err = self.run_command(attrcmd)\n",
   "                        if rc != 0 or err:\n",
   "                            raise Exception(\"Error while setting attributes: %s\" % (out + err))\n",
   "                        self.fail_json(path=to_text(b_path), msg='chattr failed',\n",
   "        return changed\n",
   "        output = {}\n",
   "        attrcmd = self.get_bin_path('lsattr', False)\n",
   "        if attrcmd:\n",
   "            attrcmd = [attrcmd, '-vd', path]\n",
   "                rc, out, err = self.run_command(attrcmd)\n",
   "                if rc == 0:\n",
   "                    res = out.split()\n",
   "                    output['attr_flags'] = res[1].replace('-', '').strip()\n",
   "                    output['version'] = res[0].strip()\n",
   "                    output['attributes'] = format_attributes(output['attr_flags'])\n",
   "        return output\n",
   "        new_mode = stat.S_IMODE(path_stat.st_mode)\n",
   "        for mode in symbolic_mode.split(','):\n",
   "            permlist = MODE_OPERATOR_RE.split(mode)\n",
   "            opers = MODE_OPERATOR_RE.findall(mode)\n",
   "            users = permlist.pop(0)\n",
   "            use_umask = (users == '')\n",
   "            if users == 'a' or users == '':\n",
   "                users = 'ugo'\n",
   "            if USERS_RE.match(users):\n",
   "                raise ValueError(\"bad symbolic permission for mode: %s\" % mode)\n",
   "            for idx, perms in enumerate(permlist):\n",
   "                if PERMS_RE.match(perms):\n",
   "                    raise ValueError(\"bad symbolic permission for mode: %s\" % mode)\n",
   "                for user in users:\n",
   "                    mode_to_apply = cls._get_octal_mode_from_symbolic_perms(path_stat, user, perms, use_umask)\n",
   "                    new_mode = cls._apply_operation_to_mode(user, opers[idx], mode_to_apply, new_mode)\n",
   "        return new_mode\n",
   "            if user == 'u':\n",
   "                mask = stat.S_IRWXU | stat.S_ISUID\n",
   "            elif user == 'g':\n",
   "                mask = stat.S_IRWXG | stat.S_ISGID\n",
   "            elif user == 'o':\n",
   "                mask = stat.S_IRWXO | stat.S_ISVTX\n",
   "            inverse_mask = mask ^ PERM_BITS\n",
   "            new_mode = (current_mode & inverse_mask) | mode_to_apply\n",
   "            new_mode = current_mode | mode_to_apply\n",
   "            new_mode = current_mode - (current_mode & mode_to_apply)\n",
   "        return new_mode\n",
   "        prev_mode = stat.S_IMODE(path_stat.st_mode)\n",
   "        is_directory = stat.S_ISDIR(path_stat.st_mode)\n",
   "        has_x_permissions = (prev_mode & EXEC_PERM_BITS) > 0\n",
   "        apply_X_permission = is_directory or has_x_permissions\n",
   "        umask = os.umask(0)\n",
   "        os.umask(umask)\n",
   "        rev_umask = umask ^ PERM_BITS\n",
   "        if apply_X_permission:\n",
   "            X_perms = {\n",
   "            X_perms = {\n",
   "        user_perms_to_modes = {\n",
   "                'r': rev_umask & stat.S_IRUSR if use_umask else stat.S_IRUSR,\n",
   "                'w': rev_umask & stat.S_IWUSR if use_umask else stat.S_IWUSR,\n",
   "                'x': rev_umask & stat.S_IXUSR if use_umask else stat.S_IXUSR,\n",
   "                'u': prev_mode & stat.S_IRWXU,\n",
   "                'g': (prev_mode & stat.S_IRWXG) << 3,\n",
   "                'o': (prev_mode & stat.S_IRWXO) << 6},\n",
   "                'r': rev_umask & stat.S_IRGRP if use_umask else stat.S_IRGRP,\n",
   "                'w': rev_umask & stat.S_IWGRP if use_umask else stat.S_IWGRP,\n",
   "                'x': rev_umask & stat.S_IXGRP if use_umask else stat.S_IXGRP,\n",
   "                'u': (prev_mode & stat.S_IRWXU) >> 3,\n",
   "                'g': prev_mode & stat.S_IRWXG,\n",
   "                'o': (prev_mode & stat.S_IRWXO) << 3},\n",
   "                'r': rev_umask & stat.S_IROTH if use_umask else stat.S_IROTH,\n",
   "                'w': rev_umask & stat.S_IWOTH if use_umask else stat.S_IWOTH,\n",
   "                'x': rev_umask & stat.S_IXOTH if use_umask else stat.S_IXOTH,\n",
   "                'u': (prev_mode & stat.S_IRWXU) >> 6,\n",
   "                'g': (prev_mode & stat.S_IRWXG) >> 3,\n",
   "                'o': prev_mode & stat.S_IRWXO},\n",
   "        for key, value in X_perms.items():\n",
   "            user_perms_to_modes[key].update(value)\n",
   "            return mode | user_perms_to_modes[user][perm]\n",
   "        return reduce(or_reduce, perms, 0)\n",
   "        changed = self.set_context_if_different(\n",
   "            file_args['path'], file_args['secontext'], changed, diff\n",
   "        changed = self.set_owner_if_different(\n",
   "            file_args['path'], file_args['owner'], changed, diff, expand\n",
   "        changed = self.set_group_if_different(\n",
   "            file_args['path'], file_args['group'], changed, diff, expand\n",
   "        changed = self.set_mode_if_different(\n",
   "            file_args['path'], file_args['mode'], changed, diff, expand\n",
   "        changed = self.set_attributes_if_different(\n",
   "            file_args['path'], file_args['attributes'], changed, diff, expand\n",
   "        return changed\n",
   "        return self.set_fs_attributes_if_different(file_args, changed, diff, expand)\n",
   "        return self.set_fs_attributes_if_different(file_args, changed, diff, expand)\n",
   "        for path in sorted(self._created_files):\n",
   "                      \"Specify 'mode' to avoid this warning.\".format(to_native(path), DEFAULT_PERM))\n",
   "        path = kwargs.get('path', kwargs.get('dest', None))\n",
   "        if path is None:\n",
   "        b_path = to_bytes(path, errors='surrogate_or_strict')\n",
   "        if os.path.exists(b_path):\n",
   "            (uid, gid) = self.user_and_group(path)\n",
   "            kwargs['uid'] = uid\n",
   "            kwargs['gid'] = gid\n",
   "                user = pwd.getpwuid(uid)[0]\n",
   "                user = str(uid)\n",
   "                group = grp.getgrgid(gid)[0]\n",
   "                group = str(gid)\n",
   "            kwargs['owner'] = user\n",
   "            kwargs['group'] = group\n",
   "            st = os.lstat(b_path)\n",
   "            kwargs['mode'] = '0%03o' % stat.S_IMODE(st[stat.ST_MODE])\n",
   "            if os.path.islink(b_path):\n",
   "            elif os.path.isdir(b_path):\n",
   "            elif os.stat(b_path).st_nlink > 1:\n",
   "            if HAVE_SELINUX and self.selinux_enabled():\n",
   "                kwargs['secontext'] = ':'.join(self.selinux_context(path))\n",
   "            kwargs['size'] = st[stat.ST_SIZE]\n",
   "            spec = self.argument_spec\n",
   "            param = self.params\n",
   "        alias_warnings = []\n",
   "        alias_results, self._legal_inputs = handle_aliases(spec, param, alias_warnings=alias_warnings)\n",
   "        for option, alias in alias_warnings:\n",
   "            warn('Both option %s and its alias %s are set.' % (option_prefix + option, option_prefix + alias))\n",
   "        deprecated_aliases = []\n",
   "        for i in spec.keys():\n",
   "            if 'deprecated_aliases' in spec[i].keys():\n",
   "                for alias in spec[i]['deprecated_aliases']:\n",
   "                    deprecated_aliases.append(alias)\n",
   "        for deprecation in deprecated_aliases:\n",
   "            if deprecation['name'] in param.keys():\n",
   "                deprecate(\"Alias '%s' is deprecated. See the module docs for more information\" % deprecation['name'],\n",
   "                          version=deprecation.get('version'), date=deprecation.get('date'),\n",
   "                          collection_name=deprecation.get('collection_name'))\n",
   "        return alias_results\n",
   "        if spec is None:\n",
   "            spec = self.argument_spec\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            self.no_log_values.update(list_no_log_values(spec, param))\n",
   "            self.fail_json(msg=\"Failure when processing no_log parameters. Module invocation will be hidden. \"\n",
   "        for message in list_deprecations(spec, param):\n",
   "            deprecate(message['msg'], version=message.get('version'), date=message.get('date'),\n",
   "                      collection_name=message.get('collection_name'))\n",
   "        self._syslog_facility = 'LOG_USER'\n",
   "        unsupported_parameters = set()\n",
   "        if spec is None:\n",
   "            spec = self.argument_spec\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            legal_inputs = self._legal_inputs\n",
   "        for k in list(param.keys()):\n",
   "            if k not in legal_inputs:\n",
   "                unsupported_parameters.add(k)\n",
   "        for k in PASS_VARS:\n",
   "            param_key = '_ansible_%s' % k\n",
   "            if param_key in param:\n",
   "                if k in PASS_BOOLS:\n",
   "                    setattr(self, PASS_VARS[k][0], self.boolean(param[param_key]))\n",
   "                    setattr(self, PASS_VARS[k][0], param[param_key])\n",
   "                if param_key in self.params:\n",
   "                    del self.params[param_key]\n",
   "                if not hasattr(self, PASS_VARS[k][0]):\n",
   "                    setattr(self, PASS_VARS[k][0], PASS_VARS[k][1])\n",
   "        if unsupported_parameters:\n",
   "            msg = \"Unsupported parameters for (%s) module: %s\" % (self._name, ', '.join(sorted(list(unsupported_parameters))))\n",
   "            if self._options_context:\n",
   "                msg += \" found in %s.\" % \" -> \".join(self._options_context)\n",
   "            supported_parameters = list()\n",
   "            for key in sorted(spec.keys()):\n",
   "                if 'aliases' in spec[key] and spec[key]['aliases']:\n",
   "                    supported_parameters.append(\"%s (%s)\" % (key, ', '.join(sorted(spec[key]['aliases']))))\n",
   "                    supported_parameters.append(key)\n",
   "            msg += \" Supported parameters include: %s\" % (', '.join(supported_parameters))\n",
   "            self.fail_json(msg=msg)\n",
   "        if self.check_mode and not self.supports_check_mode:\n",
   "            self.exit_json(skipped=True, msg=\"remote module (%s) does not support check mode\" % self._name)\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "        return count_terms(check, param)\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            check_mutually_exclusive(spec, param)\n",
   "            msg = to_native(e)\n",
   "            if self._options_context:\n",
   "                msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "            self.fail_json(msg=msg)\n",
   "        if spec is None:\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            check_required_one_of(spec, param)\n",
   "            msg = to_native(e)\n",
   "            if self._options_context:\n",
   "                msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "            self.fail_json(msg=msg)\n",
   "        if spec is None:\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            check_required_together(spec, param)\n",
   "            msg = to_native(e)\n",
   "            if self._options_context:\n",
   "                msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "            self.fail_json(msg=msg)\n",
   "        if spec is None:\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            check_required_by(spec, param)\n",
   "            self.fail_json(msg=to_native(e))\n",
   "        if spec is None:\n",
   "            spec = self.argument_spec\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            check_required_arguments(spec, param)\n",
   "            msg = to_native(e)\n",
   "            if self._options_context:\n",
   "                msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "            self.fail_json(msg=msg)\n",
   "        if spec is None:\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            check_required_if(spec, param)\n",
   "            msg = to_native(e)\n",
   "            if self._options_context:\n",
   "                msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "            self.fail_json(msg=msg)\n",
   "        if spec is None:\n",
   "            spec = self.argument_spec\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "        for (k, v) in spec.items():\n",
   "            choices = v.get('choices', None)\n",
   "            if choices is None:\n",
   "            if isinstance(choices, SEQUENCETYPE) and not isinstance(choices, (binary_type, text_type)):\n",
   "                if k in param:\n",
   "                    if isinstance(param[k], list):\n",
   "                        diff_list = \", \".join([item for item in param[k] if item not in choices])\n",
   "                        if diff_list:\n",
   "                            choices_str = \", \".join([to_native(c) for c in choices])\n",
   "                            msg = \"value of %s must be one or more of: %s. Got no match for: %s\" % (k, choices_str, diff_list)\n",
   "                            if self._options_context:\n",
   "                                msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "                            self.fail_json(msg=msg)\n",
   "                    elif param[k] not in choices:\n",
   "                        lowered_choices = None\n",
   "                        if param[k] == 'False':\n",
   "                            lowered_choices = lenient_lowercase(choices)\n",
   "                            overlap = BOOLEANS_FALSE.intersection(choices)\n",
   "                            if len(overlap) == 1:\n",
   "                                (param[k],) = overlap\n",
   "                        if param[k] == 'True':\n",
   "                            if lowered_choices is None:\n",
   "                                lowered_choices = lenient_lowercase(choices)\n",
   "                            overlap = BOOLEANS_TRUE.intersection(choices)\n",
   "                            if len(overlap) == 1:\n",
   "                                (param[k],) = overlap\n",
   "                        if param[k] not in choices:\n",
   "                            choices_str = \", \".join([to_native(c) for c in choices])\n",
   "                            msg = \"value of %s must be one of: %s, got: %s\" % (k, choices_str, param[k])\n",
   "                            if self._options_context:\n",
   "                                msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "                            self.fail_json(msg=msg)\n",
   "                msg = \"internal error: choices for argument %s are not iterable: %s\" % (k, choices)\n",
   "                if self._options_context:\n",
   "                    msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "                self.fail_json(msg=msg)\n",
   "        return safe_eval(value, locals, include_exceptions)\n",
   "        opts = {\n",
   "        allow_conversion = opts.get(self._string_conversion_action, True)\n",
   "            return check_type_str(value, allow_conversion)\n",
   "            common_msg = 'quote the entire value to ensure it does not change.'\n",
   "            from_msg = '{0!r}'.format(value)\n",
   "            to_msg = '{0!r}'.format(to_text(value))\n",
   "            if param is not None:\n",
   "                    param = '{0}{1}'.format(prefix, param)\n",
   "                from_msg = '{0}: {1!r}'.format(param, value)\n",
   "                to_msg = '{0}: {1!r}'.format(param, to_text(value))\n",
   "            if self._string_conversion_action == 'error':\n",
   "                msg = common_msg.capitalize()\n",
   "                raise TypeError(to_native(msg))\n",
   "            elif self._string_conversion_action == 'warn':\n",
   "                msg = ('The value \"{0}\" (type {1.__class__.__name__}) was converted to \"{2}\" (type string). '\n",
   "                       'If this does not look like what you expect, {3}').format(from_msg, value, to_msg, common_msg)\n",
   "                self.warn(to_native(msg))\n",
   "                return to_native(value, errors='surrogate_or_strict')\n",
   "        return check_type_list(value)\n",
   "        return check_type_dict(value)\n",
   "        return check_type_bool(value)\n",
   "        return check_type_int(value)\n",
   "        return check_type_float(value)\n",
   "        return check_type_path(value)\n",
   "        return check_type_jsonarg(value)\n",
   "        return check_type_raw(value)\n",
   "        return check_type_bytes(value)\n",
   "        return check_type_bits(value)\n",
   "            argument_spec = self.argument_spec\n",
   "        if params is None:\n",
   "            params = self.params\n",
   "        for (k, v) in argument_spec.items():\n",
   "            wanted = v.get('type', None)\n",
   "            if wanted == 'dict' or (wanted == 'list' and v.get('elements', '') == 'dict'):\n",
   "                spec = v.get('options', None)\n",
   "                if v.get('apply_defaults', False):\n",
   "                    if spec is not None:\n",
   "                        if params.get(k) is None:\n",
   "                            params[k] = {}\n",
   "                elif spec is None or k not in params or params[k] is None:\n",
   "                self._options_context.append(k)\n",
   "                if isinstance(params[k], dict):\n",
   "                    elements = [params[k]]\n",
   "                    elements = params[k]\n",
   "                for idx, param in enumerate(elements):\n",
   "                    if not isinstance(param, dict):\n",
   "                        self.fail_json(msg=\"value of %s must be of type dict or list of dict\" % k)\n",
   "                    new_prefix = prefix + k\n",
   "                    if wanted == 'list':\n",
   "                        new_prefix += '[%d]' % idx\n",
   "                    new_prefix += '.'\n",
   "                    self._set_fallbacks(spec, param)\n",
   "                    options_aliases = self._handle_aliases(spec, param, option_prefix=new_prefix)\n",
   "                    options_legal_inputs = list(spec.keys()) + list(options_aliases.keys())\n",
   "                    self._check_arguments(spec, param, options_legal_inputs)\n",
   "                    if not self.bypass_checks:\n",
   "                        self._check_mutually_exclusive(v.get('mutually_exclusive', None), param)\n",
   "                    self._set_defaults(pre=True, spec=spec, param=param)\n",
   "                    if not self.bypass_checks:\n",
   "                        self._check_required_arguments(spec, param)\n",
   "                        self._check_argument_types(spec, param, new_prefix)\n",
   "                        self._check_argument_values(spec, param)\n",
   "                        self._check_required_together(v.get('required_together', None), param)\n",
   "                        self._check_required_one_of(v.get('required_one_of', None), param)\n",
   "                        self._check_required_if(v.get('required_if', None), param)\n",
   "                        self._check_required_by(v.get('required_by', None), param)\n",
   "                    self._set_defaults(pre=False, spec=spec, param=param)\n",
   "                    self._handle_options(spec, param, new_prefix)\n",
   "                self._options_context.pop()\n",
   "        if not callable(wanted):\n",
   "            if wanted is None:\n",
   "                wanted = 'str'\n",
   "                type_checker = self._CHECK_ARGUMENT_TYPES_DISPATCHER[wanted]\n",
   "                self.fail_json(msg=\"implementation error: unknown type %s requested for %s\" % (wanted, k))\n",
   "            type_checker = wanted\n",
   "            wanted = getattr(wanted, '__name__', to_native(type(wanted)))\n",
   "        return type_checker, wanted\n",
   "        type_checker, wanted_name = self._get_wanted_type(wanted, param)\n",
   "        validated_params = []\n",
   "        kwargs = {}\n",
   "        if wanted_name == 'str' and isinstance(wanted, string_types):\n",
   "            if isinstance(param, string_types):\n",
   "                kwargs['param'] = param\n",
   "            elif isinstance(param, dict):\n",
   "                kwargs['param'] = list(param.keys())[0]\n",
   "        for value in values:\n",
   "                validated_params.append(type_checker(value, **kwargs))\n",
   "                msg = \"Elements value for option %s\" % param\n",
   "                if self._options_context:\n",
   "                    msg += \" found in '%s'\" % \" -> \".join(self._options_context)\n",
   "                msg += \" is of type %s and we were unable to convert to %s: %s\" % (type(value), wanted_name, to_native(e))\n",
   "                self.fail_json(msg=msg)\n",
   "        return validated_params\n",
   "        if spec is None:\n",
   "            spec = self.argument_spec\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "        for (k, v) in spec.items():\n",
   "            wanted = v.get('type', None)\n",
   "            if k not in param:\n",
   "            value = param[k]\n",
   "            if value is None:\n",
   "            type_checker, wanted_name = self._get_wanted_type(wanted, k)\n",
   "            kwargs = {}\n",
   "            if wanted_name == 'str' and isinstance(type_checker, string_types):\n",
   "                kwargs['param'] = list(param.keys())[0]\n",
   "                    kwargs['prefix'] = prefix\n",
   "                param[k] = type_checker(value, **kwargs)\n",
   "                wanted_elements = v.get('elements', None)\n",
   "                if wanted_elements:\n",
   "                    if wanted != 'list' or not isinstance(param[k], list):\n",
   "                        msg = \"Invalid type %s for option '%s'\" % (wanted_name, param)\n",
   "                        if self._options_context:\n",
   "                            msg += \" found in '%s'.\" % \" -> \".join(self._options_context)\n",
   "                        msg += \", elements value check is supported only with 'list' type\"\n",
   "                        self.fail_json(msg=msg)\n",
   "                    param[k] = self._handle_elements(wanted_elements, k, param[k])\n",
   "                msg = \"argument %s is of type %s\" % (k, type(value))\n",
   "                if self._options_context:\n",
   "                    msg += \" found in '%s'.\" % \" -> \".join(self._options_context)\n",
   "                msg += \" and we were unable to convert to %s: %s\" % (wanted_name, to_native(e))\n",
   "                self.fail_json(msg=msg)\n",
   "        if spec is None:\n",
   "            spec = self.argument_spec\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "        for (k, v) in spec.items():\n",
   "            default = v.get('default', None)\n",
   "                if default is not None and k not in param:\n",
   "                    param[k] = default\n",
   "                if k not in param:\n",
   "                    param[k] = default\n",
   "        if spec is None:\n",
   "            spec = self.argument_spec\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "        for (k, v) in spec.items():\n",
   "            fallback = v.get('fallback', (None,))\n",
   "            fallback_strategy = fallback[0]\n",
   "            fallback_args = []\n",
   "            fallback_kwargs = {}\n",
   "            if k not in param and fallback_strategy is not None:\n",
   "                for item in fallback[1:]:\n",
   "                    if isinstance(item, dict):\n",
   "                        fallback_kwargs = item\n",
   "                        fallback_args = item\n",
   "                    param[k] = fallback_strategy(*fallback_args, **fallback_kwargs)\n",
   "        self.params = _load_params()\n",
   "        if HAS_SYSLOG:\n",
   "                module = 'ansible-%s' % self._name\n",
   "                facility = getattr(syslog, self._syslog_facility, syslog.LOG_USER)\n",
   "                syslog.openlog(str(module), 0, facility)\n",
   "                syslog.syslog(syslog.LOG_INFO, msg)\n",
   "                self.fail_json(\n",
   "                    msg_to_log=msg,\n",
   "        if self._debug:\n",
   "            self.log('[debug] %s' % msg)\n",
   "        if not self.no_log:\n",
   "                log_args = dict()\n",
   "            module = 'ansible-%s' % self._name\n",
   "            if isinstance(module, binary_type):\n",
   "                module = module.decode('utf-8', 'replace')\n",
   "            if not isinstance(msg, (binary_type, text_type)):\n",
   "                raise TypeError(\"msg should be a string (got %s)\" % type(msg))\n",
   "            if isinstance(msg, binary_type):\n",
   "                journal_msg = remove_values(msg.decode('utf-8', 'replace'), self.no_log_values)\n",
   "                journal_msg = remove_values(msg, self.no_log_values)\n",
   "                syslog_msg = journal_msg\n",
   "                syslog_msg = journal_msg.encode('utf-8', 'replace')\n",
   "            if has_journal:\n",
   "                journal_args = [(\"MODULE\", os.path.basename(__file__))]\n",
   "                for arg in log_args:\n",
   "                    journal_args.append((arg.upper(), str(log_args[arg])))\n",
   "                    if HAS_SYSLOG:\n",
   "                        facility = getattr(syslog,\n",
   "                                           self._syslog_facility,\n",
   "                        journal.send(MESSAGE=u\"%s %s\" % (module, journal_msg),\n",
   "                                     SYSLOG_FACILITY=facility,\n",
   "                                     **dict(journal_args))\n",
   "                        journal.send(MESSAGE=u\"%s %s\" % (module, journal_msg),\n",
   "                                     **dict(journal_args))\n",
   "                    self._log_to_syslog(syslog_msg)\n",
   "                self._log_to_syslog(syslog_msg)\n",
   "        log_args = dict()\n",
   "        for param in self.params:\n",
   "            canon = self.aliases.get(param, param)\n",
   "            arg_opts = self.argument_spec.get(canon, {})\n",
   "            no_log = arg_opts.get('no_log', None)\n",
   "            if no_log is None and PASSWORD_MATCH.search(param):\n",
   "                log_args[param] = 'NOT_LOGGING_PASSWORD'\n",
   "                self.warn('Module did not set no_log for %s' % param)\n",
   "            elif self.boolean(no_log):\n",
   "                log_args[param] = 'NOT_LOGGING_PARAMETER'\n",
   "                param_val = self.params[param]\n",
   "                if not isinstance(param_val, (text_type, binary_type)):\n",
   "                    param_val = str(param_val)\n",
   "                elif isinstance(param_val, text_type):\n",
   "                    param_val = param_val.encode('utf-8')\n",
   "                log_args[param] = heuristic_log_sanitize(param_val, self.no_log_values)\n",
   "        msg = ['%s=%s' % (to_native(arg), to_native(val)) for arg, val in log_args.items()]\n",
   "        if msg:\n",
   "            msg = 'Invoked with %s' % ' '.join(msg)\n",
   "            msg = 'Invoked'\n",
   "        self.log(msg, log_args=log_args)\n",
   "            cwd = os.getcwd()\n",
   "            if not os.access(cwd, os.F_OK | os.R_OK):\n",
   "            return cwd\n",
   "            for cwd in [self.tmpdir, os.path.expandvars('$HOME'), tempfile.gettempdir()]:\n",
   "                    if os.access(cwd, os.F_OK | os.R_OK):\n",
   "                        os.chdir(cwd)\n",
   "                        return cwd\n",
   "        bin_path = None\n",
   "            bin_path = get_bin_path(arg=arg, opt_dirs=opt_dirs)\n",
   "                self.fail_json(msg=to_text(e))\n",
   "                return bin_path\n",
   "        return bin_path\n",
   "        if arg is None:\n",
   "            return arg\n",
   "            return boolean(arg)\n",
   "            self.fail_json(msg=to_native(e))\n",
   "            return jsonify(data)\n",
   "            self.fail_json(msg=to_text(e))\n",
   "        return json.loads(data)\n",
   "        if path not in self.cleanup_files:\n",
   "            self.cleanup_files.append(path)\n",
   "        for path in self.cleanup_files:\n",
   "            self.cleanup(path)\n",
   "        self.add_atomic_move_warnings()\n",
   "        self.add_path_info(kwargs)\n",
   "        if 'invocation' not in kwargs:\n",
   "            kwargs['invocation'] = {'module_args': self.params}\n",
   "        if 'warnings' in kwargs:\n",
   "            if isinstance(kwargs['warnings'], list):\n",
   "                for w in kwargs['warnings']:\n",
   "                    self.warn(w)\n",
   "                self.warn(kwargs['warnings'])\n",
   "        warnings = get_warning_messages()\n",
   "        if warnings:\n",
   "            kwargs['warnings'] = warnings\n",
   "        if 'deprecations' in kwargs:\n",
   "            if isinstance(kwargs['deprecations'], list):\n",
   "                for d in kwargs['deprecations']:\n",
   "                    if isinstance(d, SEQUENCETYPE) and len(d) == 2:\n",
   "                        self.deprecate(d[0], version=d[1])\n",
   "                    elif isinstance(d, Mapping):\n",
   "                        self.deprecate(d['msg'], version=d.get('version'), date=d.get('date'),\n",
   "                                       collection_name=d.get('collection_name'))\n",
   "                        self.deprecate(d)  # pylint: disable=ansible-deprecated-no-version\n",
   "                self.deprecate(kwargs['deprecations'])  # pylint: disable=ansible-deprecated-no-version\n",
   "        deprecations = get_deprecation_messages()\n",
   "        if deprecations:\n",
   "            kwargs['deprecations'] = deprecations\n",
   "        kwargs = remove_values(kwargs, self.no_log_values)\n",
   "        print('\\n%s' % self.jsonify(kwargs))\n",
   "        self.do_cleanup_files()\n",
   "        self._return_formatted(kwargs)\n",
   "        kwargs['failed'] = True\n",
   "        kwargs['msg'] = msg\n",
   "        if 'exception' not in kwargs and sys.exc_info()[2] and (self._debug or self._verbosity >= 3):\n",
   "                kwargs['exception'] = 'WARNING: The below traceback may *not* be related to the actual failure.\\n' +\\\n",
   "                kwargs['exception'] = ''.join(traceback.format_tb(sys.exc_info()[2]))\n",
   "        self.do_cleanup_files()\n",
   "        self._return_formatted(kwargs)\n",
   "            check_missing_parameters(self.params, required_params)\n",
   "            self.fail_json(msg=to_native(e))\n",
   "        b_filename = to_bytes(filename, errors='surrogate_or_strict')\n",
   "        if not os.path.exists(b_filename):\n",
   "        if os.path.isdir(b_filename):\n",
   "            self.fail_json(msg=\"attempted to take checksum of directory: %s\" % filename)\n",
   "        if hasattr(algorithm, 'hexdigest'):\n",
   "            digest_method = algorithm\n",
   "                digest_method = AVAILABLE_HASH_ALGORITHMS[algorithm]()\n",
   "                self.fail_json(msg=\"Could not hash file '%s' with algorithm '%s'. Available algorithms: %s\" %\n",
   "                                   (filename, algorithm, ', '.join(AVAILABLE_HASH_ALGORITHMS)))\n",
   "        blocksize = 64 * 1024\n",
   "        infile = open(os.path.realpath(b_filename), 'rb')\n",
   "        block = infile.read(blocksize)\n",
   "        while block:\n",
   "            digest_method.update(block)\n",
   "            block = infile.read(blocksize)\n",
   "        infile.close()\n",
   "        return digest_method.hexdigest()\n",
   "        if 'md5' not in AVAILABLE_HASH_ALGORITHMS:\n",
   "        return self.digest_from_file(filename, 'md5')\n",
   "        return self.digest_from_file(filename, 'sha1')\n",
   "        return self.digest_from_file(filename, 'sha256')\n",
   "        backupdest = ''\n",
   "            ext = time.strftime(\"%Y-%m-%d@%H:%M:%S~\", time.localtime(time.time()))\n",
   "            backupdest = '%s.%s.%s' % (fn, os.getpid(), ext)\n",
   "                self.preserved_copy(fn, backupdest)\n",
   "                self.fail_json(msg='Could not make backup of %s to %s: %s' % (fn, backupdest, to_native(e)))\n",
   "        return backupdest\n",
   "        if self.selinux_enabled():\n",
   "            context = self.selinux_context(src)\n",
   "            self.set_context_if_different(dest, context, False)\n",
   "            dest_stat = os.stat(src)\n",
   "            tmp_stat = os.stat(dest)\n",
   "            if dest_stat and (tmp_stat.st_uid != dest_stat.st_uid or tmp_stat.st_gid != dest_stat.st_gid):\n",
   "                os.chown(dest, dest_stat.st_uid, dest_stat.st_gid)\n",
   "        current_attribs = self.get_file_attributes(src)\n",
   "        current_attribs = current_attribs.get('attr_flags', '')\n",
   "        self.set_attributes_if_different(dest, current_attribs, True)\n",
   "        context = None\n",
   "        dest_stat = None\n",
   "        b_src = to_bytes(src, errors='surrogate_or_strict')\n",
   "        b_dest = to_bytes(dest, errors='surrogate_or_strict')\n",
   "        if os.path.exists(b_dest):\n",
   "                dest_stat = os.stat(b_dest)\n",
   "                os.chmod(b_src, dest_stat.st_mode & PERM_BITS)\n",
   "                os.chown(b_src, dest_stat.st_uid, dest_stat.st_gid)\n",
   "                if hasattr(os, 'chflags') and hasattr(dest_stat, 'st_flags'):\n",
   "                        os.chflags(b_src, dest_stat.st_flags)\n",
   "                        for err in 'EOPNOTSUPP', 'ENOTSUP':\n",
   "                            if hasattr(errno, err) and e.errno == getattr(errno, err):\n",
   "            if self.selinux_enabled():\n",
   "                context = self.selinux_context(dest)\n",
   "            if self.selinux_enabled():\n",
   "                context = self.selinux_default_context(dest)\n",
   "        creating = not os.path.exists(b_dest)\n",
   "            os.rename(b_src, b_dest)\n",
   "                self.fail_json(msg='Could not replace file: %s to %s: %s' % (src, dest, to_native(e)),\n",
   "                b_dest_dir = os.path.dirname(b_dest)\n",
   "                b_suffix = os.path.basename(b_dest)\n",
   "                error_msg = None\n",
   "                tmp_dest_name = None\n",
   "                    tmp_dest_fd, tmp_dest_name = tempfile.mkstemp(prefix=b'.ansible_tmp',\n",
   "                                                                  dir=b_dest_dir, suffix=b_suffix)\n",
   "                    error_msg = 'The destination directory (%s) is not writable by the current user. Error was: %s' % (os.path.dirname(dest), to_native(e))\n",
   "                    error_msg = ('Failed creating tmp file for atomic move.  This usually happens when using Python3 less than Python3.5. '\n",
   "                    if error_msg:\n",
   "                            self._unsafe_writes(b_src, b_dest)\n",
   "                            self.fail_json(msg=error_msg, exception=traceback.format_exc())\n",
   "                if tmp_dest_name:\n",
   "                    b_tmp_dest_name = to_bytes(tmp_dest_name, errors='surrogate_or_strict')\n",
   "                            os.close(tmp_dest_fd)\n",
   "                                shutil.move(b_src, b_tmp_dest_name)\n",
   "                                shutil.copy2(b_src, b_tmp_dest_name)\n",
   "                            if self.selinux_enabled():\n",
   "                                self.set_context_if_different(\n",
   "                                    b_tmp_dest_name, context, False)\n",
   "                                tmp_stat = os.stat(b_tmp_dest_name)\n",
   "                                if dest_stat and (tmp_stat.st_uid != dest_stat.st_uid or tmp_stat.st_gid != dest_stat.st_gid):\n",
   "                                    os.chown(b_tmp_dest_name, dest_stat.st_uid, dest_stat.st_gid)\n",
   "                                os.rename(b_tmp_dest_name, b_dest)\n",
   "                                    self._unsafe_writes(b_tmp_dest_name, b_dest)\n",
   "                                    self.fail_json(msg='Unable to make %s into to %s, failed final rename from %s: %s' %\n",
   "                                                       (src, dest, b_tmp_dest_name, to_native(e)),\n",
   "                            self.fail_json(msg='Failed to replace file: %s to %s: %s' % (src, dest, to_native(e)),\n",
   "                        self.cleanup(b_tmp_dest_name)\n",
   "        if creating:\n",
   "            if self.argument_spec.get('mode') and self.params.get('mode') is None:\n",
   "                self._created_files.add(dest)\n",
   "            umask = os.umask(0)\n",
   "            os.umask(umask)\n",
   "            os.chmod(b_dest, DEFAULT_PERM & ~umask)\n",
   "                os.chown(b_dest, os.geteuid(), os.getegid())\n",
   "        if self.selinux_enabled():\n",
   "            self.set_context_if_different(dest, context, False)\n",
   "            out_dest = in_src = None\n",
   "                out_dest = open(dest, 'wb')\n",
   "                in_src = open(src, 'rb')\n",
   "                shutil.copyfileobj(in_src, out_dest)\n",
   "                if out_dest:\n",
   "                    out_dest.close()\n",
   "                if in_src:\n",
   "                    in_src.close()\n",
   "            self.fail_json(msg='Could not write data to file (%s) from (%s): %s' % (dest, src, to_native(e)),\n",
   "        if not self._clean:\n",
   "            to_clean_args = args\n",
   "                    to_clean_args = to_bytes(args)\n",
   "                    to_clean_args = to_text(args)\n",
   "                to_clean_args = shlex.split(to_clean_args)\n",
   "            clean_args = []\n",
   "            is_passwd = False\n",
   "            for arg in (to_native(a) for a in to_clean_args):\n",
   "                if is_passwd:\n",
   "                    is_passwd = False\n",
   "                    clean_args.append('********')\n",
   "                if PASSWD_ARG_RE.match(arg):\n",
   "                    sep_idx = arg.find('=')\n",
   "                    if sep_idx > -1:\n",
   "                        clean_args.append('%s=********' % arg[:sep_idx])\n",
   "                        is_passwd = True\n",
   "                arg = heuristic_log_sanitize(arg, self.no_log_values)\n",
   "                clean_args.append(arg)\n",
   "            self._clean = ' '.join(shlex_quote(arg) for arg in clean_args)\n",
   "        return self._clean\n",
   "        self._clean = None\n",
   "            msg = \"Argument 'args' to run_command must be list or string\"\n",
   "            self.fail_json(rc=257, cmd=args, msg=msg)\n",
   "        shell = False\n",
   "                args = b\" \".join([to_bytes(shlex_quote(x), errors='surrogate_or_strict') for x in args])\n",
   "                args = to_bytes(args, errors='surrogate_or_strict')\n",
   "                executable = to_bytes(executable, errors='surrogate_or_strict')\n",
   "                args = [executable, b'-c', args]\n",
   "            elif self._shell not in (None, '/bin/sh'):\n",
   "                args = [to_bytes(self._shell, errors='surrogate_or_strict'), b'-c', args]\n",
   "                shell = True\n",
   "            if isinstance(args, (binary_type, text_type)):\n",
   "                    args = to_bytes(args, errors='surrogate_or_strict')\n",
   "                    args = to_text(args, errors='surrogateescape')\n",
   "                args = shlex.split(args)\n",
   "                args = [to_bytes(os.path.expanduser(os.path.expandvars(x)), errors='surrogate_or_strict') for x in args if x is not None]\n",
   "                args = [to_bytes(x, errors='surrogate_or_strict') for x in args if x is not None]\n",
   "        prompt_re = None\n",
   "                    prompt_regex = to_bytes(prompt_regex, errors='surrogateescape')\n",
   "                    prompt_regex = to_bytes(prompt_regex, errors='surrogate_or_strict')\n",
   "                prompt_re = re.compile(prompt_regex, re.MULTILINE)\n",
   "                self.fail_json(msg=\"invalid prompt regular expression given to run_command\")\n",
   "        rc = 0\n",
   "        msg = None\n",
   "        st_in = None\n",
   "        old_env_vals = {}\n",
   "        for key, val in self.run_command_environ_update.items():\n",
   "            old_env_vals[key] = os.environ.get(key, None)\n",
   "            os.environ[key] = val\n",
   "            for key, val in environ_update.items():\n",
   "                old_env_vals[key] = os.environ.get(key, None)\n",
   "                os.environ[key] = val\n",
   "            old_env_vals['PATH'] = os.environ['PATH']\n",
   "            pypaths = os.environ['PYTHONPATH'].split(':')\n",
   "            pypaths = [x for x in pypaths\n",
   "                       if not x.endswith('/ansible_modlib.zip') and\n",
   "                       not x.endswith('/debug_dir')]\n",
   "            os.environ['PYTHONPATH'] = ':'.join(pypaths)\n",
   "        if data:\n",
   "            st_in = subprocess.PIPE\n",
   "        kwargs = dict(\n",
   "            executable=executable,\n",
   "            shell=shell,\n",
   "            stdin=st_in,\n",
   "            preexec_fn=self._restore_signal_handlers,\n",
   "            kwargs[\"pass_fds\"] = pass_fds\n",
   "            kwargs['close_fds'] = False\n",
   "        prev_dir = os.getcwd()\n",
   "        if cwd and os.path.isdir(cwd):\n",
   "            cwd = to_bytes(os.path.abspath(os.path.expanduser(cwd)), errors='surrogate_or_strict')\n",
   "            kwargs['cwd'] = cwd\n",
   "                os.chdir(cwd)\n",
   "                self.fail_json(rc=e.errno, msg=\"Could not open %s, %s\" % (cwd, to_native(e)),\n",
   "        old_umask = None\n",
   "        if umask:\n",
   "            old_umask = os.umask(umask)\n",
   "            if self._debug:\n",
   "                self.log('Executing: ' + self._clean_args(args))\n",
   "            cmd = subprocess.Popen(args, **kwargs)\n",
   "                before_communicate_callback(cmd)\n",
   "            stdout = b''\n",
   "            stderr = b''\n",
   "                selector = selectors.DefaultSelector()\n",
   "                selector = selectors.PollSelector()\n",
   "            selector.register(cmd.stdout, selectors.EVENT_READ)\n",
   "            selector.register(cmd.stderr, selectors.EVENT_READ)\n",
   "                fcntl.fcntl(cmd.stdout.fileno(), fcntl.F_SETFL, fcntl.fcntl(cmd.stdout.fileno(), fcntl.F_GETFL) | os.O_NONBLOCK)\n",
   "                fcntl.fcntl(cmd.stderr.fileno(), fcntl.F_SETFL, fcntl.fcntl(cmd.stderr.fileno(), fcntl.F_GETFL) | os.O_NONBLOCK)\n",
   "            if data:\n",
   "                    data += '\\n'\n",
   "                if isinstance(data, text_type):\n",
   "                    data = to_bytes(data)\n",
   "                cmd.stdin.write(data)\n",
   "                cmd.stdin.close()\n",
   "                events = selector.select(1)\n",
   "                for key, event in events:\n",
   "                    b_chunk = key.fileobj.read()\n",
   "                    if b_chunk == b(''):\n",
   "                        selector.unregister(key.fileobj)\n",
   "                    if key.fileobj == cmd.stdout:\n",
   "                        stdout += b_chunk\n",
   "                    elif key.fileobj == cmd.stderr:\n",
   "                        stderr += b_chunk\n",
   "                if prompt_re:\n",
   "                    if prompt_re.search(stdout) and not data:\n",
   "                            stdout = to_native(stdout, encoding=encoding, errors=errors)\n",
   "                        return (257, stdout, \"A prompt was encountered while running a command, but no input data was specified\")\n",
   "                if (not events or not selector.get_map()) and cmd.poll() is not None:\n",
   "                elif not selector.get_map() and cmd.poll() is None:\n",
   "                    cmd.wait()\n",
   "            cmd.stdout.close()\n",
   "            cmd.stderr.close()\n",
   "            selector.close()\n",
   "            rc = cmd.returncode\n",
   "            self.log(\"Error Executing CMD:%s Exception:%s\" % (self._clean_args(args), to_native(e)))\n",
   "            self.fail_json(rc=e.errno, msg=to_native(e), cmd=self._clean_args(args))\n",
   "            self.log(\"Error Executing CMD:%s Exception:%s\" % (self._clean_args(args), to_native(traceback.format_exc())))\n",
   "            self.fail_json(rc=257, msg=to_native(e), exception=traceback.format_exc(), cmd=self._clean_args(args))\n",
   "        for key, val in old_env_vals.items():\n",
   "            if val is None:\n",
   "                del os.environ[key]\n",
   "                os.environ[key] = val\n",
   "        if old_umask:\n",
   "            os.umask(old_umask)\n",
   "        if rc != 0 and check_rc:\n",
   "            msg = heuristic_log_sanitize(stderr.rstrip(), self.no_log_values)\n",
   "            self.fail_json(cmd=self._clean_args(args), rc=rc, stdout=stdout, stderr=stderr, msg=msg)\n",
   "        os.chdir(prev_dir)\n",
   "            return (rc, to_native(stdout, encoding=encoding, errors=errors),\n",
   "                    to_native(stderr, encoding=encoding, errors=errors))\n",
   "        return (rc, stdout, stderr)\n",
   "        filename = os.path.expandvars(os.path.expanduser(filename))\n",
   "        fh = open(filename, 'a')\n",
   "        fh.write(str)\n",
   "        fh.close()\n",
   "    is_executable = is_executable\n",
   "            buffer_size = fcntl.fcntl(fd, 1032)\n",
   "        return buffer_size\n"
  ]
 },
 "170": {
  "name": "_ANSIBLE_ARGS",
  "type": "bytes",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/module_utils/basic.py",
  "lineno": "616",
  "column": "8",
  "context": "         buffer = sys.stdin.buffer.read()\n        _ANSIBLE_ARGS = buffer\n\n    try:\n        params = json.loads(buffer.decod",
  "context_lines": "            if PY2:\n                buffer = sys.stdin.read()\n            else:\n                buffer = sys.stdin.buffer.read()\n        _ANSIBLE_ARGS = buffer\n\n    try:\n        params = json.loads(buffer.decode('utf-8'))\n    except ValueError:\n",
  "slicing": [
   "    HAS_SYSLOG = True\n",
   "    HAS_SYSLOG = False\n",
   "    has_journal = hasattr(journal, 'sendv')\n",
   "    has_journal = False\n",
   "HAVE_SELINUX = False\n",
   "    HAVE_SELINUX = True\n",
   "NoneType = type(None)\n",
   "AVAILABLE_HASH_ALGORITHMS = dict()\n",
   "    for attribute in ('available_algorithms', 'algorithms'):\n",
   "        algorithms = getattr(hashlib, attribute, None)\n",
   "        if algorithms:\n",
   "    if algorithms is None:\n",
   "        algorithms = ('md5', 'sha1', 'sha224', 'sha256', 'sha384', 'sha512')\n",
   "    for algorithm in algorithms:\n",
   "        AVAILABLE_HASH_ALGORITHMS[algorithm] = getattr(hashlib, algorithm)\n",
   "        AVAILABLE_HASH_ALGORITHMS.pop('md5', None)\n",
   "    AVAILABLE_HASH_ALGORITHMS = {'sha1': sha.sha}\n",
   "        AVAILABLE_HASH_ALGORITHMS['md5'] = md5.md5\n",
   "SEQUENCETYPE = frozenset, KeysView, Sequence\n",
   "PASSWORD_MATCH = re.compile(r'^(?:.+[-_\\s])?pass(?:[-_\\s]?(?:word|phrase|wrd|wd)?)(?:[-_\\s].+)?$', re.I)\n",
   "    unicode = text_type\n",
   "    basestring = string_types\n",
   "_ANSIBLE_ARGS = None\n",
   "FILE_COMMON_ARGUMENTS = dict(\n",
   "PASSWD_ARG_RE = re.compile(r'^[-]{0,2}pass[-]?(word|wd)?')\n",
   "MODE_OPERATOR_RE = re.compile(r'[+=-]')\n",
   "USERS_RE = re.compile(r'[^ugo]')\n",
   "PERMS_RE = re.compile(r'[^rwxXstugo]')\n",
   "_PY3_MIN = sys.version_info[:2] >= (3, 5)\n",
   "_PY2_MIN = (2, 6) <= sys.version_info[:2] < (3,)\n",
   "_PY_MIN = _PY3_MIN or _PY2_MIN\n",
   "if not _PY_MIN:\n",
   "    platform_cls = get_platform_subclass(cls)\n",
   "    return super(cls, platform_cls).__new__(platform_cls)\n",
   "def _remove_values_conditions(value, no_log_strings, deferred_removals):\n",
   "        native_str_value = value\n",
   "            value_is_text = True\n",
   "                native_str_value = to_bytes(value, errors='surrogate_or_strict')\n",
   "            value_is_text = False\n",
   "                native_str_value = to_text(value, errors='surrogate_or_strict')\n",
   "        if native_str_value in no_log_strings:\n",
   "        for omit_me in no_log_strings:\n",
   "            native_str_value = native_str_value.replace(omit_me, '*' * 8)\n",
   "        if value_is_text and isinstance(native_str_value, binary_type):\n",
   "            value = to_text(native_str_value, encoding='utf-8', errors='surrogate_then_replace')\n",
   "        elif not value_is_text and isinstance(native_str_value, text_type):\n",
   "            value = to_bytes(native_str_value, encoding='utf-8', errors='surrogate_then_replace')\n",
   "            value = native_str_value\n",
   "    elif isinstance(value, Sequence):\n",
   "        if isinstance(value, MutableSequence):\n",
   "            new_value = type(value)()\n",
   "            new_value = []  # Need a mutable value\n",
   "        deferred_removals.append((value, new_value))\n",
   "        value = new_value\n",
   "    elif isinstance(value, Set):\n",
   "        if isinstance(value, MutableSet):\n",
   "            new_value = type(value)()\n",
   "            new_value = set()  # Need a mutable value\n",
   "        deferred_removals.append((value, new_value))\n",
   "        value = new_value\n",
   "    elif isinstance(value, Mapping):\n",
   "        if isinstance(value, MutableMapping):\n",
   "            new_value = type(value)()\n",
   "            new_value = {}  # Need a mutable value\n",
   "        deferred_removals.append((value, new_value))\n",
   "        value = new_value\n",
   "    elif isinstance(value, tuple(chain(integer_types, (float, bool, NoneType)))):\n",
   "        stringy_value = to_native(value, encoding='utf-8', errors='surrogate_or_strict')\n",
   "        if stringy_value in no_log_strings:\n",
   "        for omit_me in no_log_strings:\n",
   "            if omit_me in stringy_value:\n",
   "    elif isinstance(value, (datetime.datetime, datetime.date)):\n",
   "        value = value.isoformat()\n",
   "        raise TypeError('Value of unknown type: %s, %s' % (type(value), value))\n",
   "    return value\n",
   "def remove_values(value, no_log_strings):\n",
   "    deferred_removals = deque()\n",
   "    no_log_strings = [to_native(s, errors='surrogate_or_strict') for s in no_log_strings]\n",
   "    new_value = _remove_values_conditions(value, no_log_strings, deferred_removals)\n",
   "    while deferred_removals:\n",
   "        old_data, new_data = deferred_removals.popleft()\n",
   "        if isinstance(new_data, Mapping):\n",
   "            for old_key, old_elem in old_data.items():\n",
   "                new_elem = _remove_values_conditions(old_elem, no_log_strings, deferred_removals)\n",
   "                new_data[old_key] = new_elem\n",
   "            for elem in old_data:\n",
   "                new_elem = _remove_values_conditions(elem, no_log_strings, deferred_removals)\n",
   "                if isinstance(new_data, MutableSequence):\n",
   "                    new_data.append(new_elem)\n",
   "                elif isinstance(new_data, MutableSet):\n",
   "                    new_data.add(new_elem)\n",
   "    return new_value\n",
   "def _sanitize_keys_conditions(value, no_log_strings, ignore_keys, deferred_removals):\n",
   "    if isinstance(value, (text_type, binary_type)):\n",
   "        return value\n",
   "    if isinstance(value, Sequence):\n",
   "        if isinstance(value, MutableSequence):\n",
   "            new_value = type(value)()\n",
   "            new_value = []  # Need a mutable value\n",
   "        deferred_removals.append((value, new_value))\n",
   "        return new_value\n",
   "    if isinstance(value, Set):\n",
   "        if isinstance(value, MutableSet):\n",
   "            new_value = type(value)()\n",
   "            new_value = set()  # Need a mutable value\n",
   "        deferred_removals.append((value, new_value))\n",
   "        return new_value\n",
   "    if isinstance(value, Mapping):\n",
   "        if isinstance(value, MutableMapping):\n",
   "            new_value = type(value)()\n",
   "            new_value = {}  # Need a mutable value\n",
   "        deferred_removals.append((value, new_value))\n",
   "        return new_value\n",
   "    if isinstance(value, tuple(chain(integer_types, (float, bool, NoneType)))):\n",
   "        return value\n",
   "    if isinstance(value, (datetime.datetime, datetime.date)):\n",
   "        return value\n",
   "    raise TypeError('Value of unknown type: %s, %s' % (type(value), value))\n",
   "    deferred_removals = deque()\n",
   "    no_log_strings = [to_native(s, errors='surrogate_or_strict') for s in no_log_strings]\n",
   "    new_value = _sanitize_keys_conditions(obj, no_log_strings, ignore_keys, deferred_removals)\n",
   "    while deferred_removals:\n",
   "        old_data, new_data = deferred_removals.popleft()\n",
   "        if isinstance(new_data, Mapping):\n",
   "            for old_key, old_elem in old_data.items():\n",
   "                if old_key in ignore_keys or old_key.startswith('_ansible'):\n",
   "                    new_data[old_key] = _sanitize_keys_conditions(old_elem, no_log_strings, ignore_keys, deferred_removals)\n",
   "                    new_key = _remove_values_conditions(old_key, no_log_strings, None)\n",
   "                    new_data[new_key] = _sanitize_keys_conditions(old_elem, no_log_strings, ignore_keys, deferred_removals)\n",
   "            for elem in old_data:\n",
   "                new_elem = _sanitize_keys_conditions(elem, no_log_strings, ignore_keys, deferred_removals)\n",
   "                if isinstance(new_data, MutableSequence):\n",
   "                    new_data.append(new_elem)\n",
   "                elif isinstance(new_data, MutableSet):\n",
   "                    new_data.add(new_elem)\n",
   "    return new_value\n",
   "def heuristic_log_sanitize(data, no_log_values=None):\n",
   "    data = to_native(data)\n",
   "    output = []\n",
   "    begin = len(data)\n",
   "    prev_begin = begin\n",
   "    sep = 1\n",
   "    while sep:\n",
   "            end = data.rindex('@', 0, begin)\n",
   "            output.insert(0, data[0:begin])\n",
   "        sep = None\n",
   "        sep_search_end = end\n",
   "        while not sep:\n",
   "                begin = data.rindex('://', 0, sep_search_end)\n",
   "                begin = 0\n",
   "                sep = data.index(':', begin + 3, end)\n",
   "                if begin == 0:\n",
   "                    output.insert(0, data[0:begin])\n",
   "                sep_search_end = begin\n",
   "        if sep:\n",
   "            output.insert(0, data[end:prev_begin])\n",
   "            output.insert(0, '********')\n",
   "            output.insert(0, data[begin:sep + 1])\n",
   "            prev_begin = begin\n",
   "    output = ''.join(output)\n",
   "        output = remove_values(output, no_log_values)\n",
   "    return output\n",
   "    if _ANSIBLE_ARGS is not None:\n",
   "        buffer = _ANSIBLE_ARGS\n",
   "                fd = open(sys.argv[1], 'rb')\n",
   "                buffer = fd.read()\n",
   "                fd.close()\n",
   "                buffer = sys.argv[1]\n",
   "                    buffer = buffer.encode('utf-8', errors='surrogateescape')\n",
   "                buffer = sys.stdin.read()\n",
   "                buffer = sys.stdin.buffer.read()\n",
   "        _ANSIBLE_ARGS = buffer\n",
   "        params = json.loads(buffer.decode('utf-8'))\n",
   "        params = json_dict_unicode_to_bytes(params)\n",
   "        return params['ANSIBLE_MODULE_ARGS']\n",
   "    for arg in args:\n",
   "        if arg in os.environ:\n",
   "            return os.environ[arg]\n",
   "    hostname = platform.node()\n",
   "    msg = \"Failed to import the required Python library (%s) on %s's Python %s.\" % (library, hostname, sys.executable)\n",
   "        msg += \" This is required %s.\" % reason\n",
   "        msg += \" See %s for more info.\" % url\n",
   "    msg += (\" Please read the module documentation and install it in the appropriate location.\"\n",
   "    return msg\n",
   "            for k, v in FILE_COMMON_ARGUMENTS.items():\n",
   "                if k not in self.argument_spec:\n",
   "                    self.argument_spec[k] = v\n",
   "            basedir = None\n",
   "                basedir = os.path.expanduser(os.path.expandvars(self._remote_tmp))\n",
   "            if basedir is not None and not os.path.exists(basedir):\n",
   "                    os.makedirs(basedir, mode=0o700)\n",
   "                              \"failing back to system: %s\" % (basedir, to_native(e)))\n",
   "                    basedir = None\n",
   "                              \"the correct permissions manually\" % basedir)\n",
   "            basefile = \"ansible-moduletmp-%s-\" % time.time()\n",
   "                tmpdir = tempfile.mkdtemp(prefix=basefile, dir=basedir)\n",
   "                        \"with prefix %s: %s\" % (basedir, basefile, to_native(e))\n",
   "                atexit.register(shutil.rmtree, tmpdir)\n",
   "            self._tmpdir = tmpdir\n",
   "    def warn(self, warning):\n",
   "    def deprecate(self, msg, version=None, date=None, collection_name=None):\n",
   "        deprecate(msg, version=version, date=date, collection_name=collection_name)\n",
   "            self.log('[DEPRECATION WARNING] %s %s' % (msg, date))\n",
   "            self.log('[DEPRECATION WARNING] %s %s' % (msg, version))\n",
   "            path = params.get('path', params.get('dest', None))\n",
   "        if path is None:\n",
   "            path = os.path.expanduser(os.path.expandvars(path))\n",
   "        b_path = to_bytes(path, errors='surrogate_or_strict')\n",
   "        if params.get('follow', False) and os.path.islink(b_path):\n",
   "            b_path = os.path.realpath(b_path)\n",
   "            path = to_native(b_path)\n",
   "        mode = params.get('mode', None)\n",
   "        owner = params.get('owner', None)\n",
   "        group = params.get('group', None)\n",
   "        seuser = params.get('seuser', None)\n",
   "        serole = params.get('serole', None)\n",
   "        setype = params.get('setype', None)\n",
   "        selevel = params.get('selevel', None)\n",
   "        secontext = [seuser, serole, setype]\n",
   "            secontext.append(selevel)\n",
   "        default_secontext = self.selinux_default_context(path)\n",
   "        for i in range(len(default_secontext)):\n",
   "            if i is not None and secontext[i] == '_default':\n",
   "                secontext[i] = default_secontext[i]\n",
   "        attributes = params.get('attributes', None)\n",
   "            path=path, mode=mode, owner=owner, group=group,\n",
   "            seuser=seuser, serole=serole, setype=setype,\n",
   "            selevel=selevel, secontext=secontext, attributes=attributes,\n",
   "        if not HAVE_SELINUX:\n",
   "        if not HAVE_SELINUX:\n",
   "            seenabled = self.get_bin_path('selinuxenabled')\n",
   "            if seenabled is not None:\n",
   "                (rc, out, err) = self.run_command(seenabled)\n",
   "                if rc == 0:\n",
   "        context = [None, None, None]\n",
   "            context.append(None)\n",
   "        return context\n",
   "        context = self.selinux_initial_context()\n",
   "        if not HAVE_SELINUX or not self.selinux_enabled():\n",
   "            return context\n",
   "            ret = selinux.matchpathcon(to_native(path, errors='surrogate_or_strict'), mode)\n",
   "            return context\n",
   "        if ret[0] == -1:\n",
   "            return context\n",
   "        context = ret[1].split(':', 3)\n",
   "        return context\n",
   "        context = self.selinux_initial_context()\n",
   "        if not HAVE_SELINUX or not self.selinux_enabled():\n",
   "            return context\n",
   "            ret = selinux.lgetfilecon_raw(to_native(path, errors='surrogate_or_strict'))\n",
   "                self.fail_json(path=path, msg='path %s does not exist' % path)\n",
   "                self.fail_json(path=path, msg='failed to retrieve selinux context')\n",
   "        if ret[0] == -1:\n",
   "            return context\n",
   "        context = ret[1].split(':', 3)\n",
   "        return context\n",
   "        b_path = to_bytes(path, errors='surrogate_or_strict')\n",
   "            b_path = os.path.expanduser(os.path.expandvars(b_path))\n",
   "        st = os.lstat(b_path)\n",
   "        uid = st.st_uid\n",
   "        gid = st.st_gid\n",
   "        return (uid, gid)\n",
   "        path_is_bytes = False\n",
   "        if isinstance(path, binary_type):\n",
   "            path_is_bytes = True\n",
   "        b_path = os.path.realpath(to_bytes(os.path.expanduser(os.path.expandvars(path)), errors='surrogate_or_strict'))\n",
   "        while not os.path.ismount(b_path):\n",
   "            b_path = os.path.dirname(b_path)\n",
   "        if path_is_bytes:\n",
   "            return b_path\n",
   "        return to_text(b_path, errors='surrogate_or_strict')\n",
   "            f = open('/proc/mounts', 'r')\n",
   "            mount_data = f.readlines()\n",
   "            f.close()\n",
   "        path_mount_point = self.find_mount_point(path)\n",
   "        for line in mount_data:\n",
   "            (device, mount_point, fstype, options, rest) = line.split(' ', 4)\n",
   "            if to_bytes(path_mount_point) == to_bytes(mount_point):\n",
   "                for fs in self._selinux_special_fs:\n",
   "                    if fs in fstype:\n",
   "                        special_context = self.selinux_context(path_mount_point)\n",
   "                        return (True, special_context)\n",
   "        if not HAVE_SELINUX or not self.selinux_enabled():\n",
   "        context = self.selinux_default_context(path)\n",
   "        return self.set_context_if_different(path, context, False)\n",
   "        if not HAVE_SELINUX or not self.selinux_enabled():\n",
   "        if self.check_file_absent_if_check_mode(path):\n",
   "        cur_context = self.selinux_context(path)\n",
   "        new_context = list(cur_context)\n",
   "        (is_special_se, sp_context) = self.is_special_selinux_path(path)\n",
   "        if is_special_se:\n",
   "            new_context = sp_context\n",
   "            for i in range(len(cur_context)):\n",
   "                if len(context) > i:\n",
   "                    if context[i] is not None and context[i] != cur_context[i]:\n",
   "                        new_context[i] = context[i]\n",
   "                    elif context[i] is None:\n",
   "                        new_context[i] = cur_context[i]\n",
   "        if cur_context != new_context:\n",
   "                diff['before']['secontext'] = cur_context\n",
   "                diff['after']['secontext'] = new_context\n",
   "                rc = selinux.lsetfilecon(to_native(path), ':'.join(new_context))\n",
   "                self.fail_json(path=path, msg='invalid selinux context: %s' % to_native(e),\n",
   "                               new_context=new_context, cur_context=cur_context, input_was=context)\n",
   "            if rc != 0:\n",
   "                self.fail_json(path=path, msg='set selinux context failed')\n",
   "            changed = True\n",
   "        return changed\n",
   "        if owner is None:\n",
   "            return changed\n",
   "        b_path = to_bytes(path, errors='surrogate_or_strict')\n",
   "            b_path = os.path.expanduser(os.path.expandvars(b_path))\n",
   "        if self.check_file_absent_if_check_mode(b_path):\n",
   "        orig_uid, orig_gid = self.user_and_group(b_path, expand)\n",
   "            uid = int(owner)\n",
   "                uid = pwd.getpwnam(owner).pw_uid\n",
   "                path = to_text(b_path)\n",
   "                self.fail_json(path=path, msg='chown failed: failed to look up user %s' % owner)\n",
   "        if orig_uid != uid:\n",
   "                diff['before']['owner'] = orig_uid\n",
   "                diff['after']['owner'] = uid\n",
   "                os.lchown(b_path, uid, -1)\n",
   "                path = to_text(b_path)\n",
   "                self.fail_json(path=path, msg='chown failed: %s' % (to_text(e)))\n",
   "            changed = True\n",
   "        return changed\n",
   "        if group is None:\n",
   "            return changed\n",
   "        b_path = to_bytes(path, errors='surrogate_or_strict')\n",
   "            b_path = os.path.expanduser(os.path.expandvars(b_path))\n",
   "        if self.check_file_absent_if_check_mode(b_path):\n",
   "        orig_uid, orig_gid = self.user_and_group(b_path, expand)\n",
   "            gid = int(group)\n",
   "                gid = grp.getgrnam(group).gr_gid\n",
   "                path = to_text(b_path)\n",
   "                self.fail_json(path=path, msg='chgrp failed: failed to look up group %s' % group)\n",
   "        if orig_gid != gid:\n",
   "                diff['before']['group'] = orig_gid\n",
   "                diff['after']['group'] = gid\n",
   "                os.lchown(b_path, -1, gid)\n",
   "                path = to_text(b_path)\n",
   "                self.fail_json(path=path, msg='chgrp failed')\n",
   "            changed = True\n",
   "        return changed\n",
   "        if mode is None:\n",
   "            return changed\n",
   "            self._created_files.remove(path)\n",
   "        b_path = to_bytes(path, errors='surrogate_or_strict')\n",
   "            b_path = os.path.expanduser(os.path.expandvars(b_path))\n",
   "        path_stat = os.lstat(b_path)\n",
   "        if self.check_file_absent_if_check_mode(b_path):\n",
   "        if not isinstance(mode, int):\n",
   "                mode = int(mode, 8)\n",
   "                    mode = self._symbolic_mode_to_octal(path_stat, mode)\n",
   "                    path = to_text(b_path)\n",
   "                    self.fail_json(path=path,\n",
   "                if mode != stat.S_IMODE(mode):\n",
   "                    path = to_text(b_path)\n",
   "                    self.fail_json(path=path, msg=\"Invalid mode supplied, only permission info is allowed\", details=mode)\n",
   "        prev_mode = stat.S_IMODE(path_stat.st_mode)\n",
   "        if prev_mode != mode:\n",
   "                diff['before']['mode'] = '0%03o' % prev_mode\n",
   "                diff['after']['mode'] = '0%03o' % mode\n",
   "                    os.lchmod(b_path, mode)\n",
   "                    if not os.path.islink(b_path):\n",
   "                        os.chmod(b_path, mode)\n",
   "                        underlying_stat = os.stat(b_path)\n",
   "                        os.chmod(b_path, mode)\n",
   "                        new_underlying_stat = os.stat(b_path)\n",
   "                        if underlying_stat.st_mode != new_underlying_stat.st_mode:\n",
   "                            os.chmod(b_path, stat.S_IMODE(underlying_stat.st_mode))\n",
   "                if os.path.islink(b_path) and e.errno in (errno.EPERM, errno.EROFS):  # Can't set mode on symbolic links\n",
   "                path = to_text(b_path)\n",
   "                self.fail_json(path=path, msg='chmod failed', details=to_native(e),\n",
   "            path_stat = os.lstat(b_path)\n",
   "            new_mode = stat.S_IMODE(path_stat.st_mode)\n",
   "            if new_mode != prev_mode:\n",
   "                changed = True\n",
   "        return changed\n",
   "        if attributes is None:\n",
   "            return changed\n",
   "        b_path = to_bytes(path, errors='surrogate_or_strict')\n",
   "            b_path = os.path.expanduser(os.path.expandvars(b_path))\n",
   "        if self.check_file_absent_if_check_mode(b_path):\n",
   "        existing = self.get_file_attributes(b_path)\n",
   "        attr_mod = '='\n",
   "        if attributes.startswith(('-', '+')):\n",
   "            attr_mod = attributes[0]\n",
   "            attributes = attributes[1:]\n",
   "        if existing.get('attr_flags', '') != attributes or attr_mod == '-':\n",
   "            attrcmd = self.get_bin_path('chattr')\n",
   "            if attrcmd:\n",
   "                attrcmd = [attrcmd, '%s%s' % (attr_mod, attributes), b_path]\n",
   "                changed = True\n",
   "                    diff['before']['attributes'] = existing.get('attr_flags')\n",
   "                    diff['after']['attributes'] = '%s%s' % (attr_mod, attributes)\n",
   "                        rc, out, err = self.run_command(attrcmd)\n",
   "                        if rc != 0 or err:\n",
   "                            raise Exception(\"Error while setting attributes: %s\" % (out + err))\n",
   "                        self.fail_json(path=to_text(b_path), msg='chattr failed',\n",
   "        return changed\n",
   "        output = {}\n",
   "        attrcmd = self.get_bin_path('lsattr', False)\n",
   "        if attrcmd:\n",
   "            attrcmd = [attrcmd, '-vd', path]\n",
   "                rc, out, err = self.run_command(attrcmd)\n",
   "                if rc == 0:\n",
   "                    res = out.split()\n",
   "                    output['attr_flags'] = res[1].replace('-', '').strip()\n",
   "                    output['version'] = res[0].strip()\n",
   "                    output['attributes'] = format_attributes(output['attr_flags'])\n",
   "        return output\n",
   "        new_mode = stat.S_IMODE(path_stat.st_mode)\n",
   "        for mode in symbolic_mode.split(','):\n",
   "            permlist = MODE_OPERATOR_RE.split(mode)\n",
   "            opers = MODE_OPERATOR_RE.findall(mode)\n",
   "            users = permlist.pop(0)\n",
   "            use_umask = (users == '')\n",
   "            if users == 'a' or users == '':\n",
   "                users = 'ugo'\n",
   "            if USERS_RE.match(users):\n",
   "                raise ValueError(\"bad symbolic permission for mode: %s\" % mode)\n",
   "            for idx, perms in enumerate(permlist):\n",
   "                if PERMS_RE.match(perms):\n",
   "                    raise ValueError(\"bad symbolic permission for mode: %s\" % mode)\n",
   "                for user in users:\n",
   "                    mode_to_apply = cls._get_octal_mode_from_symbolic_perms(path_stat, user, perms, use_umask)\n",
   "                    new_mode = cls._apply_operation_to_mode(user, opers[idx], mode_to_apply, new_mode)\n",
   "        return new_mode\n",
   "            if user == 'u':\n",
   "                mask = stat.S_IRWXU | stat.S_ISUID\n",
   "            elif user == 'g':\n",
   "                mask = stat.S_IRWXG | stat.S_ISGID\n",
   "            elif user == 'o':\n",
   "                mask = stat.S_IRWXO | stat.S_ISVTX\n",
   "            inverse_mask = mask ^ PERM_BITS\n",
   "            new_mode = (current_mode & inverse_mask) | mode_to_apply\n",
   "            new_mode = current_mode | mode_to_apply\n",
   "            new_mode = current_mode - (current_mode & mode_to_apply)\n",
   "        return new_mode\n",
   "        prev_mode = stat.S_IMODE(path_stat.st_mode)\n",
   "        is_directory = stat.S_ISDIR(path_stat.st_mode)\n",
   "        has_x_permissions = (prev_mode & EXEC_PERM_BITS) > 0\n",
   "        apply_X_permission = is_directory or has_x_permissions\n",
   "        umask = os.umask(0)\n",
   "        os.umask(umask)\n",
   "        rev_umask = umask ^ PERM_BITS\n",
   "        if apply_X_permission:\n",
   "            X_perms = {\n",
   "            X_perms = {\n",
   "        user_perms_to_modes = {\n",
   "                'r': rev_umask & stat.S_IRUSR if use_umask else stat.S_IRUSR,\n",
   "                'w': rev_umask & stat.S_IWUSR if use_umask else stat.S_IWUSR,\n",
   "                'x': rev_umask & stat.S_IXUSR if use_umask else stat.S_IXUSR,\n",
   "                'u': prev_mode & stat.S_IRWXU,\n",
   "                'g': (prev_mode & stat.S_IRWXG) << 3,\n",
   "                'o': (prev_mode & stat.S_IRWXO) << 6},\n",
   "                'r': rev_umask & stat.S_IRGRP if use_umask else stat.S_IRGRP,\n",
   "                'w': rev_umask & stat.S_IWGRP if use_umask else stat.S_IWGRP,\n",
   "                'x': rev_umask & stat.S_IXGRP if use_umask else stat.S_IXGRP,\n",
   "                'u': (prev_mode & stat.S_IRWXU) >> 3,\n",
   "                'g': prev_mode & stat.S_IRWXG,\n",
   "                'o': (prev_mode & stat.S_IRWXO) << 3},\n",
   "                'r': rev_umask & stat.S_IROTH if use_umask else stat.S_IROTH,\n",
   "                'w': rev_umask & stat.S_IWOTH if use_umask else stat.S_IWOTH,\n",
   "                'x': rev_umask & stat.S_IXOTH if use_umask else stat.S_IXOTH,\n",
   "                'u': (prev_mode & stat.S_IRWXU) >> 6,\n",
   "                'g': (prev_mode & stat.S_IRWXG) >> 3,\n",
   "                'o': prev_mode & stat.S_IRWXO},\n",
   "        for key, value in X_perms.items():\n",
   "            user_perms_to_modes[key].update(value)\n",
   "            return mode | user_perms_to_modes[user][perm]\n",
   "        return reduce(or_reduce, perms, 0)\n",
   "        changed = self.set_context_if_different(\n",
   "            file_args['path'], file_args['secontext'], changed, diff\n",
   "        changed = self.set_owner_if_different(\n",
   "            file_args['path'], file_args['owner'], changed, diff, expand\n",
   "        changed = self.set_group_if_different(\n",
   "            file_args['path'], file_args['group'], changed, diff, expand\n",
   "        changed = self.set_mode_if_different(\n",
   "            file_args['path'], file_args['mode'], changed, diff, expand\n",
   "        changed = self.set_attributes_if_different(\n",
   "            file_args['path'], file_args['attributes'], changed, diff, expand\n",
   "        return changed\n",
   "        return self.set_fs_attributes_if_different(file_args, changed, diff, expand)\n",
   "        return self.set_fs_attributes_if_different(file_args, changed, diff, expand)\n",
   "        for path in sorted(self._created_files):\n",
   "                      \"Specify 'mode' to avoid this warning.\".format(to_native(path), DEFAULT_PERM))\n",
   "        path = kwargs.get('path', kwargs.get('dest', None))\n",
   "        if path is None:\n",
   "        b_path = to_bytes(path, errors='surrogate_or_strict')\n",
   "        if os.path.exists(b_path):\n",
   "            (uid, gid) = self.user_and_group(path)\n",
   "            kwargs['uid'] = uid\n",
   "            kwargs['gid'] = gid\n",
   "                user = pwd.getpwuid(uid)[0]\n",
   "                user = str(uid)\n",
   "                group = grp.getgrgid(gid)[0]\n",
   "                group = str(gid)\n",
   "            kwargs['owner'] = user\n",
   "            kwargs['group'] = group\n",
   "            st = os.lstat(b_path)\n",
   "            kwargs['mode'] = '0%03o' % stat.S_IMODE(st[stat.ST_MODE])\n",
   "            if os.path.islink(b_path):\n",
   "            elif os.path.isdir(b_path):\n",
   "            elif os.stat(b_path).st_nlink > 1:\n",
   "            if HAVE_SELINUX and self.selinux_enabled():\n",
   "                kwargs['secontext'] = ':'.join(self.selinux_context(path))\n",
   "            kwargs['size'] = st[stat.ST_SIZE]\n",
   "            spec = self.argument_spec\n",
   "            param = self.params\n",
   "        alias_warnings = []\n",
   "        alias_results, self._legal_inputs = handle_aliases(spec, param, alias_warnings=alias_warnings)\n",
   "        for option, alias in alias_warnings:\n",
   "            warn('Both option %s and its alias %s are set.' % (option_prefix + option, option_prefix + alias))\n",
   "        deprecated_aliases = []\n",
   "        for i in spec.keys():\n",
   "            if 'deprecated_aliases' in spec[i].keys():\n",
   "                for alias in spec[i]['deprecated_aliases']:\n",
   "                    deprecated_aliases.append(alias)\n",
   "        for deprecation in deprecated_aliases:\n",
   "            if deprecation['name'] in param.keys():\n",
   "                deprecate(\"Alias '%s' is deprecated. See the module docs for more information\" % deprecation['name'],\n",
   "                          version=deprecation.get('version'), date=deprecation.get('date'),\n",
   "                          collection_name=deprecation.get('collection_name'))\n",
   "        return alias_results\n",
   "        if spec is None:\n",
   "            spec = self.argument_spec\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            self.no_log_values.update(list_no_log_values(spec, param))\n",
   "            self.fail_json(msg=\"Failure when processing no_log parameters. Module invocation will be hidden. \"\n",
   "        for message in list_deprecations(spec, param):\n",
   "            deprecate(message['msg'], version=message.get('version'), date=message.get('date'),\n",
   "                      collection_name=message.get('collection_name'))\n",
   "        self._syslog_facility = 'LOG_USER'\n",
   "        unsupported_parameters = set()\n",
   "        if spec is None:\n",
   "            spec = self.argument_spec\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            legal_inputs = self._legal_inputs\n",
   "        for k in list(param.keys()):\n",
   "            if k not in legal_inputs:\n",
   "                unsupported_parameters.add(k)\n",
   "        for k in PASS_VARS:\n",
   "            param_key = '_ansible_%s' % k\n",
   "            if param_key in param:\n",
   "                if k in PASS_BOOLS:\n",
   "                    setattr(self, PASS_VARS[k][0], self.boolean(param[param_key]))\n",
   "                    setattr(self, PASS_VARS[k][0], param[param_key])\n",
   "                if param_key in self.params:\n",
   "                    del self.params[param_key]\n",
   "                if not hasattr(self, PASS_VARS[k][0]):\n",
   "                    setattr(self, PASS_VARS[k][0], PASS_VARS[k][1])\n",
   "        if unsupported_parameters:\n",
   "            msg = \"Unsupported parameters for (%s) module: %s\" % (self._name, ', '.join(sorted(list(unsupported_parameters))))\n",
   "            if self._options_context:\n",
   "                msg += \" found in %s.\" % \" -> \".join(self._options_context)\n",
   "            supported_parameters = list()\n",
   "            for key in sorted(spec.keys()):\n",
   "                if 'aliases' in spec[key] and spec[key]['aliases']:\n",
   "                    supported_parameters.append(\"%s (%s)\" % (key, ', '.join(sorted(spec[key]['aliases']))))\n",
   "                    supported_parameters.append(key)\n",
   "            msg += \" Supported parameters include: %s\" % (', '.join(supported_parameters))\n",
   "            self.fail_json(msg=msg)\n",
   "        if self.check_mode and not self.supports_check_mode:\n",
   "            self.exit_json(skipped=True, msg=\"remote module (%s) does not support check mode\" % self._name)\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "        return count_terms(check, param)\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            check_mutually_exclusive(spec, param)\n",
   "            msg = to_native(e)\n",
   "            if self._options_context:\n",
   "                msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "            self.fail_json(msg=msg)\n",
   "        if spec is None:\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            check_required_one_of(spec, param)\n",
   "            msg = to_native(e)\n",
   "            if self._options_context:\n",
   "                msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "            self.fail_json(msg=msg)\n",
   "        if spec is None:\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            check_required_together(spec, param)\n",
   "            msg = to_native(e)\n",
   "            if self._options_context:\n",
   "                msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "            self.fail_json(msg=msg)\n",
   "        if spec is None:\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            check_required_by(spec, param)\n",
   "            self.fail_json(msg=to_native(e))\n",
   "        if spec is None:\n",
   "            spec = self.argument_spec\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            check_required_arguments(spec, param)\n",
   "            msg = to_native(e)\n",
   "            if self._options_context:\n",
   "                msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "            self.fail_json(msg=msg)\n",
   "        if spec is None:\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "            check_required_if(spec, param)\n",
   "            msg = to_native(e)\n",
   "            if self._options_context:\n",
   "                msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "            self.fail_json(msg=msg)\n",
   "        if spec is None:\n",
   "            spec = self.argument_spec\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "        for (k, v) in spec.items():\n",
   "            choices = v.get('choices', None)\n",
   "            if choices is None:\n",
   "            if isinstance(choices, SEQUENCETYPE) and not isinstance(choices, (binary_type, text_type)):\n",
   "                if k in param:\n",
   "                    if isinstance(param[k], list):\n",
   "                        diff_list = \", \".join([item for item in param[k] if item not in choices])\n",
   "                        if diff_list:\n",
   "                            choices_str = \", \".join([to_native(c) for c in choices])\n",
   "                            msg = \"value of %s must be one or more of: %s. Got no match for: %s\" % (k, choices_str, diff_list)\n",
   "                            if self._options_context:\n",
   "                                msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "                            self.fail_json(msg=msg)\n",
   "                    elif param[k] not in choices:\n",
   "                        lowered_choices = None\n",
   "                        if param[k] == 'False':\n",
   "                            lowered_choices = lenient_lowercase(choices)\n",
   "                            overlap = BOOLEANS_FALSE.intersection(choices)\n",
   "                            if len(overlap) == 1:\n",
   "                                (param[k],) = overlap\n",
   "                        if param[k] == 'True':\n",
   "                            if lowered_choices is None:\n",
   "                                lowered_choices = lenient_lowercase(choices)\n",
   "                            overlap = BOOLEANS_TRUE.intersection(choices)\n",
   "                            if len(overlap) == 1:\n",
   "                                (param[k],) = overlap\n",
   "                        if param[k] not in choices:\n",
   "                            choices_str = \", \".join([to_native(c) for c in choices])\n",
   "                            msg = \"value of %s must be one of: %s, got: %s\" % (k, choices_str, param[k])\n",
   "                            if self._options_context:\n",
   "                                msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "                            self.fail_json(msg=msg)\n",
   "                msg = \"internal error: choices for argument %s are not iterable: %s\" % (k, choices)\n",
   "                if self._options_context:\n",
   "                    msg += \" found in %s\" % \" -> \".join(self._options_context)\n",
   "                self.fail_json(msg=msg)\n",
   "        return safe_eval(value, locals, include_exceptions)\n",
   "        opts = {\n",
   "        allow_conversion = opts.get(self._string_conversion_action, True)\n",
   "            return check_type_str(value, allow_conversion)\n",
   "            common_msg = 'quote the entire value to ensure it does not change.'\n",
   "            from_msg = '{0!r}'.format(value)\n",
   "            to_msg = '{0!r}'.format(to_text(value))\n",
   "            if param is not None:\n",
   "                    param = '{0}{1}'.format(prefix, param)\n",
   "                from_msg = '{0}: {1!r}'.format(param, value)\n",
   "                to_msg = '{0}: {1!r}'.format(param, to_text(value))\n",
   "            if self._string_conversion_action == 'error':\n",
   "                msg = common_msg.capitalize()\n",
   "                raise TypeError(to_native(msg))\n",
   "            elif self._string_conversion_action == 'warn':\n",
   "                msg = ('The value \"{0}\" (type {1.__class__.__name__}) was converted to \"{2}\" (type string). '\n",
   "                       'If this does not look like what you expect, {3}').format(from_msg, value, to_msg, common_msg)\n",
   "                self.warn(to_native(msg))\n",
   "                return to_native(value, errors='surrogate_or_strict')\n",
   "        return check_type_list(value)\n",
   "        return check_type_dict(value)\n",
   "        return check_type_bool(value)\n",
   "        return check_type_int(value)\n",
   "        return check_type_float(value)\n",
   "        return check_type_path(value)\n",
   "        return check_type_jsonarg(value)\n",
   "        return check_type_raw(value)\n",
   "        return check_type_bytes(value)\n",
   "        return check_type_bits(value)\n",
   "            argument_spec = self.argument_spec\n",
   "        if params is None:\n",
   "            params = self.params\n",
   "        for (k, v) in argument_spec.items():\n",
   "            wanted = v.get('type', None)\n",
   "            if wanted == 'dict' or (wanted == 'list' and v.get('elements', '') == 'dict'):\n",
   "                spec = v.get('options', None)\n",
   "                if v.get('apply_defaults', False):\n",
   "                    if spec is not None:\n",
   "                        if params.get(k) is None:\n",
   "                            params[k] = {}\n",
   "                elif spec is None or k not in params or params[k] is None:\n",
   "                self._options_context.append(k)\n",
   "                if isinstance(params[k], dict):\n",
   "                    elements = [params[k]]\n",
   "                    elements = params[k]\n",
   "                for idx, param in enumerate(elements):\n",
   "                    if not isinstance(param, dict):\n",
   "                        self.fail_json(msg=\"value of %s must be of type dict or list of dict\" % k)\n",
   "                    new_prefix = prefix + k\n",
   "                    if wanted == 'list':\n",
   "                        new_prefix += '[%d]' % idx\n",
   "                    new_prefix += '.'\n",
   "                    self._set_fallbacks(spec, param)\n",
   "                    options_aliases = self._handle_aliases(spec, param, option_prefix=new_prefix)\n",
   "                    options_legal_inputs = list(spec.keys()) + list(options_aliases.keys())\n",
   "                    self._check_arguments(spec, param, options_legal_inputs)\n",
   "                    if not self.bypass_checks:\n",
   "                        self._check_mutually_exclusive(v.get('mutually_exclusive', None), param)\n",
   "                    self._set_defaults(pre=True, spec=spec, param=param)\n",
   "                    if not self.bypass_checks:\n",
   "                        self._check_required_arguments(spec, param)\n",
   "                        self._check_argument_types(spec, param, new_prefix)\n",
   "                        self._check_argument_values(spec, param)\n",
   "                        self._check_required_together(v.get('required_together', None), param)\n",
   "                        self._check_required_one_of(v.get('required_one_of', None), param)\n",
   "                        self._check_required_if(v.get('required_if', None), param)\n",
   "                        self._check_required_by(v.get('required_by', None), param)\n",
   "                    self._set_defaults(pre=False, spec=spec, param=param)\n",
   "                    self._handle_options(spec, param, new_prefix)\n",
   "                self._options_context.pop()\n",
   "        if not callable(wanted):\n",
   "            if wanted is None:\n",
   "                wanted = 'str'\n",
   "                type_checker = self._CHECK_ARGUMENT_TYPES_DISPATCHER[wanted]\n",
   "                self.fail_json(msg=\"implementation error: unknown type %s requested for %s\" % (wanted, k))\n",
   "            type_checker = wanted\n",
   "            wanted = getattr(wanted, '__name__', to_native(type(wanted)))\n",
   "        return type_checker, wanted\n",
   "        type_checker, wanted_name = self._get_wanted_type(wanted, param)\n",
   "        validated_params = []\n",
   "        kwargs = {}\n",
   "        if wanted_name == 'str' and isinstance(wanted, string_types):\n",
   "            if isinstance(param, string_types):\n",
   "                kwargs['param'] = param\n",
   "            elif isinstance(param, dict):\n",
   "                kwargs['param'] = list(param.keys())[0]\n",
   "        for value in values:\n",
   "                validated_params.append(type_checker(value, **kwargs))\n",
   "                msg = \"Elements value for option %s\" % param\n",
   "                if self._options_context:\n",
   "                    msg += \" found in '%s'\" % \" -> \".join(self._options_context)\n",
   "                msg += \" is of type %s and we were unable to convert to %s: %s\" % (type(value), wanted_name, to_native(e))\n",
   "                self.fail_json(msg=msg)\n",
   "        return validated_params\n",
   "        if spec is None:\n",
   "            spec = self.argument_spec\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "        for (k, v) in spec.items():\n",
   "            wanted = v.get('type', None)\n",
   "            if k not in param:\n",
   "            value = param[k]\n",
   "            if value is None:\n",
   "            type_checker, wanted_name = self._get_wanted_type(wanted, k)\n",
   "            kwargs = {}\n",
   "            if wanted_name == 'str' and isinstance(type_checker, string_types):\n",
   "                kwargs['param'] = list(param.keys())[0]\n",
   "                    kwargs['prefix'] = prefix\n",
   "                param[k] = type_checker(value, **kwargs)\n",
   "                wanted_elements = v.get('elements', None)\n",
   "                if wanted_elements:\n",
   "                    if wanted != 'list' or not isinstance(param[k], list):\n",
   "                        msg = \"Invalid type %s for option '%s'\" % (wanted_name, param)\n",
   "                        if self._options_context:\n",
   "                            msg += \" found in '%s'.\" % \" -> \".join(self._options_context)\n",
   "                        msg += \", elements value check is supported only with 'list' type\"\n",
   "                        self.fail_json(msg=msg)\n",
   "                    param[k] = self._handle_elements(wanted_elements, k, param[k])\n",
   "                msg = \"argument %s is of type %s\" % (k, type(value))\n",
   "                if self._options_context:\n",
   "                    msg += \" found in '%s'.\" % \" -> \".join(self._options_context)\n",
   "                msg += \" and we were unable to convert to %s: %s\" % (wanted_name, to_native(e))\n",
   "                self.fail_json(msg=msg)\n",
   "        if spec is None:\n",
   "            spec = self.argument_spec\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "        for (k, v) in spec.items():\n",
   "            default = v.get('default', None)\n",
   "                if default is not None and k not in param:\n",
   "                    param[k] = default\n",
   "                if k not in param:\n",
   "                    param[k] = default\n",
   "        if spec is None:\n",
   "            spec = self.argument_spec\n",
   "        if param is None:\n",
   "            param = self.params\n",
   "        for (k, v) in spec.items():\n",
   "            fallback = v.get('fallback', (None,))\n",
   "            fallback_strategy = fallback[0]\n",
   "            fallback_args = []\n",
   "            fallback_kwargs = {}\n",
   "            if k not in param and fallback_strategy is not None:\n",
   "                for item in fallback[1:]:\n",
   "                    if isinstance(item, dict):\n",
   "                        fallback_kwargs = item\n",
   "                        fallback_args = item\n",
   "                    param[k] = fallback_strategy(*fallback_args, **fallback_kwargs)\n",
   "        self.params = _load_params()\n",
   "        if HAS_SYSLOG:\n",
   "                module = 'ansible-%s' % self._name\n",
   "                facility = getattr(syslog, self._syslog_facility, syslog.LOG_USER)\n",
   "                syslog.openlog(str(module), 0, facility)\n",
   "                syslog.syslog(syslog.LOG_INFO, msg)\n",
   "                self.fail_json(\n",
   "                    msg_to_log=msg,\n",
   "        if self._debug:\n",
   "            self.log('[debug] %s' % msg)\n",
   "        if not self.no_log:\n",
   "                log_args = dict()\n",
   "            module = 'ansible-%s' % self._name\n",
   "            if isinstance(module, binary_type):\n",
   "                module = module.decode('utf-8', 'replace')\n",
   "            if not isinstance(msg, (binary_type, text_type)):\n",
   "                raise TypeError(\"msg should be a string (got %s)\" % type(msg))\n",
   "            if isinstance(msg, binary_type):\n",
   "                journal_msg = remove_values(msg.decode('utf-8', 'replace'), self.no_log_values)\n",
   "                journal_msg = remove_values(msg, self.no_log_values)\n",
   "                syslog_msg = journal_msg\n",
   "                syslog_msg = journal_msg.encode('utf-8', 'replace')\n",
   "            if has_journal:\n",
   "                journal_args = [(\"MODULE\", os.path.basename(__file__))]\n",
   "                for arg in log_args:\n",
   "                    journal_args.append((arg.upper(), str(log_args[arg])))\n",
   "                    if HAS_SYSLOG:\n",
   "                        facility = getattr(syslog,\n",
   "                                           self._syslog_facility,\n",
   "                        journal.send(MESSAGE=u\"%s %s\" % (module, journal_msg),\n",
   "                                     SYSLOG_FACILITY=facility,\n",
   "                                     **dict(journal_args))\n",
   "                        journal.send(MESSAGE=u\"%s %s\" % (module, journal_msg),\n",
   "                                     **dict(journal_args))\n",
   "                    self._log_to_syslog(syslog_msg)\n",
   "                self._log_to_syslog(syslog_msg)\n",
   "        log_args = dict()\n",
   "        for param in self.params:\n",
   "            canon = self.aliases.get(param, param)\n",
   "            arg_opts = self.argument_spec.get(canon, {})\n",
   "            no_log = arg_opts.get('no_log', None)\n",
   "            if no_log is None and PASSWORD_MATCH.search(param):\n",
   "                log_args[param] = 'NOT_LOGGING_PASSWORD'\n",
   "                self.warn('Module did not set no_log for %s' % param)\n",
   "            elif self.boolean(no_log):\n",
   "                log_args[param] = 'NOT_LOGGING_PARAMETER'\n",
   "                param_val = self.params[param]\n",
   "                if not isinstance(param_val, (text_type, binary_type)):\n",
   "                    param_val = str(param_val)\n",
   "                elif isinstance(param_val, text_type):\n",
   "                    param_val = param_val.encode('utf-8')\n",
   "                log_args[param] = heuristic_log_sanitize(param_val, self.no_log_values)\n",
   "        msg = ['%s=%s' % (to_native(arg), to_native(val)) for arg, val in log_args.items()]\n",
   "        if msg:\n",
   "            msg = 'Invoked with %s' % ' '.join(msg)\n",
   "            msg = 'Invoked'\n",
   "        self.log(msg, log_args=log_args)\n",
   "            cwd = os.getcwd()\n",
   "            if not os.access(cwd, os.F_OK | os.R_OK):\n",
   "            return cwd\n",
   "            for cwd in [self.tmpdir, os.path.expandvars('$HOME'), tempfile.gettempdir()]:\n",
   "                    if os.access(cwd, os.F_OK | os.R_OK):\n",
   "                        os.chdir(cwd)\n",
   "                        return cwd\n",
   "        bin_path = None\n",
   "            bin_path = get_bin_path(arg=arg, opt_dirs=opt_dirs)\n",
   "                self.fail_json(msg=to_text(e))\n",
   "                return bin_path\n",
   "        return bin_path\n",
   "        if arg is None:\n",
   "            return arg\n",
   "            return boolean(arg)\n",
   "            self.fail_json(msg=to_native(e))\n",
   "            return jsonify(data)\n",
   "            self.fail_json(msg=to_text(e))\n",
   "        return json.loads(data)\n",
   "        if path not in self.cleanup_files:\n",
   "            self.cleanup_files.append(path)\n",
   "        for path in self.cleanup_files:\n",
   "            self.cleanup(path)\n",
   "        self.add_atomic_move_warnings()\n",
   "        self.add_path_info(kwargs)\n",
   "        if 'invocation' not in kwargs:\n",
   "            kwargs['invocation'] = {'module_args': self.params}\n",
   "        if 'warnings' in kwargs:\n",
   "            if isinstance(kwargs['warnings'], list):\n",
   "                for w in kwargs['warnings']:\n",
   "                    self.warn(w)\n",
   "                self.warn(kwargs['warnings'])\n",
   "        warnings = get_warning_messages()\n",
   "        if warnings:\n",
   "            kwargs['warnings'] = warnings\n",
   "        if 'deprecations' in kwargs:\n",
   "            if isinstance(kwargs['deprecations'], list):\n",
   "                for d in kwargs['deprecations']:\n",
   "                    if isinstance(d, SEQUENCETYPE) and len(d) == 2:\n",
   "                        self.deprecate(d[0], version=d[1])\n",
   "                    elif isinstance(d, Mapping):\n",
   "                        self.deprecate(d['msg'], version=d.get('version'), date=d.get('date'),\n",
   "                                       collection_name=d.get('collection_name'))\n",
   "                        self.deprecate(d)  # pylint: disable=ansible-deprecated-no-version\n",
   "                self.deprecate(kwargs['deprecations'])  # pylint: disable=ansible-deprecated-no-version\n",
   "        deprecations = get_deprecation_messages()\n",
   "        if deprecations:\n",
   "            kwargs['deprecations'] = deprecations\n",
   "        kwargs = remove_values(kwargs, self.no_log_values)\n",
   "        print('\\n%s' % self.jsonify(kwargs))\n",
   "        self.do_cleanup_files()\n",
   "        self._return_formatted(kwargs)\n",
   "        kwargs['failed'] = True\n",
   "        kwargs['msg'] = msg\n",
   "        if 'exception' not in kwargs and sys.exc_info()[2] and (self._debug or self._verbosity >= 3):\n",
   "                kwargs['exception'] = 'WARNING: The below traceback may *not* be related to the actual failure.\\n' +\\\n",
   "                kwargs['exception'] = ''.join(traceback.format_tb(sys.exc_info()[2]))\n",
   "        self.do_cleanup_files()\n",
   "        self._return_formatted(kwargs)\n",
   "            check_missing_parameters(self.params, required_params)\n",
   "            self.fail_json(msg=to_native(e))\n",
   "        b_filename = to_bytes(filename, errors='surrogate_or_strict')\n",
   "        if not os.path.exists(b_filename):\n",
   "        if os.path.isdir(b_filename):\n",
   "            self.fail_json(msg=\"attempted to take checksum of directory: %s\" % filename)\n",
   "        if hasattr(algorithm, 'hexdigest'):\n",
   "            digest_method = algorithm\n",
   "                digest_method = AVAILABLE_HASH_ALGORITHMS[algorithm]()\n",
   "                self.fail_json(msg=\"Could not hash file '%s' with algorithm '%s'. Available algorithms: %s\" %\n",
   "                                   (filename, algorithm, ', '.join(AVAILABLE_HASH_ALGORITHMS)))\n",
   "        blocksize = 64 * 1024\n",
   "        infile = open(os.path.realpath(b_filename), 'rb')\n",
   "        block = infile.read(blocksize)\n",
   "        while block:\n",
   "            digest_method.update(block)\n",
   "            block = infile.read(blocksize)\n",
   "        infile.close()\n",
   "        return digest_method.hexdigest()\n",
   "        if 'md5' not in AVAILABLE_HASH_ALGORITHMS:\n",
   "        return self.digest_from_file(filename, 'md5')\n",
   "        return self.digest_from_file(filename, 'sha1')\n",
   "        return self.digest_from_file(filename, 'sha256')\n",
   "        backupdest = ''\n",
   "            ext = time.strftime(\"%Y-%m-%d@%H:%M:%S~\", time.localtime(time.time()))\n",
   "            backupdest = '%s.%s.%s' % (fn, os.getpid(), ext)\n",
   "                self.preserved_copy(fn, backupdest)\n",
   "                self.fail_json(msg='Could not make backup of %s to %s: %s' % (fn, backupdest, to_native(e)))\n",
   "        return backupdest\n",
   "        if self.selinux_enabled():\n",
   "            context = self.selinux_context(src)\n",
   "            self.set_context_if_different(dest, context, False)\n",
   "            dest_stat = os.stat(src)\n",
   "            tmp_stat = os.stat(dest)\n",
   "            if dest_stat and (tmp_stat.st_uid != dest_stat.st_uid or tmp_stat.st_gid != dest_stat.st_gid):\n",
   "                os.chown(dest, dest_stat.st_uid, dest_stat.st_gid)\n",
   "        current_attribs = self.get_file_attributes(src)\n",
   "        current_attribs = current_attribs.get('attr_flags', '')\n",
   "        self.set_attributes_if_different(dest, current_attribs, True)\n",
   "        context = None\n",
   "        dest_stat = None\n",
   "        b_src = to_bytes(src, errors='surrogate_or_strict')\n",
   "        b_dest = to_bytes(dest, errors='surrogate_or_strict')\n",
   "        if os.path.exists(b_dest):\n",
   "                dest_stat = os.stat(b_dest)\n",
   "                os.chmod(b_src, dest_stat.st_mode & PERM_BITS)\n",
   "                os.chown(b_src, dest_stat.st_uid, dest_stat.st_gid)\n",
   "                if hasattr(os, 'chflags') and hasattr(dest_stat, 'st_flags'):\n",
   "                        os.chflags(b_src, dest_stat.st_flags)\n",
   "                        for err in 'EOPNOTSUPP', 'ENOTSUP':\n",
   "                            if hasattr(errno, err) and e.errno == getattr(errno, err):\n",
   "            if self.selinux_enabled():\n",
   "                context = self.selinux_context(dest)\n",
   "            if self.selinux_enabled():\n",
   "                context = self.selinux_default_context(dest)\n",
   "        creating = not os.path.exists(b_dest)\n",
   "            os.rename(b_src, b_dest)\n",
   "                self.fail_json(msg='Could not replace file: %s to %s: %s' % (src, dest, to_native(e)),\n",
   "                b_dest_dir = os.path.dirname(b_dest)\n",
   "                b_suffix = os.path.basename(b_dest)\n",
   "                error_msg = None\n",
   "                tmp_dest_name = None\n",
   "                    tmp_dest_fd, tmp_dest_name = tempfile.mkstemp(prefix=b'.ansible_tmp',\n",
   "                                                                  dir=b_dest_dir, suffix=b_suffix)\n",
   "                    error_msg = 'The destination directory (%s) is not writable by the current user. Error was: %s' % (os.path.dirname(dest), to_native(e))\n",
   "                    error_msg = ('Failed creating tmp file for atomic move.  This usually happens when using Python3 less than Python3.5. '\n",
   "                    if error_msg:\n",
   "                            self._unsafe_writes(b_src, b_dest)\n",
   "                            self.fail_json(msg=error_msg, exception=traceback.format_exc())\n",
   "                if tmp_dest_name:\n",
   "                    b_tmp_dest_name = to_bytes(tmp_dest_name, errors='surrogate_or_strict')\n",
   "                            os.close(tmp_dest_fd)\n",
   "                                shutil.move(b_src, b_tmp_dest_name)\n",
   "                                shutil.copy2(b_src, b_tmp_dest_name)\n",
   "                            if self.selinux_enabled():\n",
   "                                self.set_context_if_different(\n",
   "                                    b_tmp_dest_name, context, False)\n",
   "                                tmp_stat = os.stat(b_tmp_dest_name)\n",
   "                                if dest_stat and (tmp_stat.st_uid != dest_stat.st_uid or tmp_stat.st_gid != dest_stat.st_gid):\n",
   "                                    os.chown(b_tmp_dest_name, dest_stat.st_uid, dest_stat.st_gid)\n",
   "                                os.rename(b_tmp_dest_name, b_dest)\n",
   "                                    self._unsafe_writes(b_tmp_dest_name, b_dest)\n",
   "                                    self.fail_json(msg='Unable to make %s into to %s, failed final rename from %s: %s' %\n",
   "                                                       (src, dest, b_tmp_dest_name, to_native(e)),\n",
   "                            self.fail_json(msg='Failed to replace file: %s to %s: %s' % (src, dest, to_native(e)),\n",
   "                        self.cleanup(b_tmp_dest_name)\n",
   "        if creating:\n",
   "            if self.argument_spec.get('mode') and self.params.get('mode') is None:\n",
   "                self._created_files.add(dest)\n",
   "            umask = os.umask(0)\n",
   "            os.umask(umask)\n",
   "            os.chmod(b_dest, DEFAULT_PERM & ~umask)\n",
   "                os.chown(b_dest, os.geteuid(), os.getegid())\n",
   "        if self.selinux_enabled():\n",
   "            self.set_context_if_different(dest, context, False)\n",
   "            out_dest = in_src = None\n",
   "                out_dest = open(dest, 'wb')\n",
   "                in_src = open(src, 'rb')\n",
   "                shutil.copyfileobj(in_src, out_dest)\n",
   "                if out_dest:\n",
   "                    out_dest.close()\n",
   "                if in_src:\n",
   "                    in_src.close()\n",
   "            self.fail_json(msg='Could not write data to file (%s) from (%s): %s' % (dest, src, to_native(e)),\n",
   "        if not self._clean:\n",
   "            to_clean_args = args\n",
   "                    to_clean_args = to_bytes(args)\n",
   "                    to_clean_args = to_text(args)\n",
   "                to_clean_args = shlex.split(to_clean_args)\n",
   "            clean_args = []\n",
   "            is_passwd = False\n",
   "            for arg in (to_native(a) for a in to_clean_args):\n",
   "                if is_passwd:\n",
   "                    is_passwd = False\n",
   "                    clean_args.append('********')\n",
   "                if PASSWD_ARG_RE.match(arg):\n",
   "                    sep_idx = arg.find('=')\n",
   "                    if sep_idx > -1:\n",
   "                        clean_args.append('%s=********' % arg[:sep_idx])\n",
   "                        is_passwd = True\n",
   "                arg = heuristic_log_sanitize(arg, self.no_log_values)\n",
   "                clean_args.append(arg)\n",
   "            self._clean = ' '.join(shlex_quote(arg) for arg in clean_args)\n",
   "        return self._clean\n",
   "        self._clean = None\n",
   "            msg = \"Argument 'args' to run_command must be list or string\"\n",
   "            self.fail_json(rc=257, cmd=args, msg=msg)\n",
   "        shell = False\n",
   "                args = b\" \".join([to_bytes(shlex_quote(x), errors='surrogate_or_strict') for x in args])\n",
   "                args = to_bytes(args, errors='surrogate_or_strict')\n",
   "                executable = to_bytes(executable, errors='surrogate_or_strict')\n",
   "                args = [executable, b'-c', args]\n",
   "            elif self._shell not in (None, '/bin/sh'):\n",
   "                args = [to_bytes(self._shell, errors='surrogate_or_strict'), b'-c', args]\n",
   "                shell = True\n",
   "            if isinstance(args, (binary_type, text_type)):\n",
   "                    args = to_bytes(args, errors='surrogate_or_strict')\n",
   "                    args = to_text(args, errors='surrogateescape')\n",
   "                args = shlex.split(args)\n",
   "                args = [to_bytes(os.path.expanduser(os.path.expandvars(x)), errors='surrogate_or_strict') for x in args if x is not None]\n",
   "                args = [to_bytes(x, errors='surrogate_or_strict') for x in args if x is not None]\n",
   "        prompt_re = None\n",
   "                    prompt_regex = to_bytes(prompt_regex, errors='surrogateescape')\n",
   "                    prompt_regex = to_bytes(prompt_regex, errors='surrogate_or_strict')\n",
   "                prompt_re = re.compile(prompt_regex, re.MULTILINE)\n",
   "                self.fail_json(msg=\"invalid prompt regular expression given to run_command\")\n",
   "        rc = 0\n",
   "        msg = None\n",
   "        st_in = None\n",
   "        old_env_vals = {}\n",
   "        for key, val in self.run_command_environ_update.items():\n",
   "            old_env_vals[key] = os.environ.get(key, None)\n",
   "            os.environ[key] = val\n",
   "            for key, val in environ_update.items():\n",
   "                old_env_vals[key] = os.environ.get(key, None)\n",
   "                os.environ[key] = val\n",
   "            old_env_vals['PATH'] = os.environ['PATH']\n",
   "            pypaths = os.environ['PYTHONPATH'].split(':')\n",
   "            pypaths = [x for x in pypaths\n",
   "                       if not x.endswith('/ansible_modlib.zip') and\n",
   "                       not x.endswith('/debug_dir')]\n",
   "            os.environ['PYTHONPATH'] = ':'.join(pypaths)\n",
   "        if data:\n",
   "            st_in = subprocess.PIPE\n",
   "        kwargs = dict(\n",
   "            executable=executable,\n",
   "            shell=shell,\n",
   "            stdin=st_in,\n",
   "            preexec_fn=self._restore_signal_handlers,\n",
   "            kwargs[\"pass_fds\"] = pass_fds\n",
   "            kwargs['close_fds'] = False\n",
   "        prev_dir = os.getcwd()\n",
   "        if cwd and os.path.isdir(cwd):\n",
   "            cwd = to_bytes(os.path.abspath(os.path.expanduser(cwd)), errors='surrogate_or_strict')\n",
   "            kwargs['cwd'] = cwd\n",
   "                os.chdir(cwd)\n",
   "                self.fail_json(rc=e.errno, msg=\"Could not open %s, %s\" % (cwd, to_native(e)),\n",
   "        old_umask = None\n",
   "        if umask:\n",
   "            old_umask = os.umask(umask)\n",
   "            if self._debug:\n",
   "                self.log('Executing: ' + self._clean_args(args))\n",
   "            cmd = subprocess.Popen(args, **kwargs)\n",
   "                before_communicate_callback(cmd)\n",
   "            stdout = b''\n",
   "            stderr = b''\n",
   "                selector = selectors.DefaultSelector()\n",
   "                selector = selectors.PollSelector()\n",
   "            selector.register(cmd.stdout, selectors.EVENT_READ)\n",
   "            selector.register(cmd.stderr, selectors.EVENT_READ)\n",
   "                fcntl.fcntl(cmd.stdout.fileno(), fcntl.F_SETFL, fcntl.fcntl(cmd.stdout.fileno(), fcntl.F_GETFL) | os.O_NONBLOCK)\n",
   "                fcntl.fcntl(cmd.stderr.fileno(), fcntl.F_SETFL, fcntl.fcntl(cmd.stderr.fileno(), fcntl.F_GETFL) | os.O_NONBLOCK)\n",
   "            if data:\n",
   "                    data += '\\n'\n",
   "                if isinstance(data, text_type):\n",
   "                    data = to_bytes(data)\n",
   "                cmd.stdin.write(data)\n",
   "                cmd.stdin.close()\n",
   "                events = selector.select(1)\n",
   "                for key, event in events:\n",
   "                    b_chunk = key.fileobj.read()\n",
   "                    if b_chunk == b(''):\n",
   "                        selector.unregister(key.fileobj)\n",
   "                    if key.fileobj == cmd.stdout:\n",
   "                        stdout += b_chunk\n",
   "                    elif key.fileobj == cmd.stderr:\n",
   "                        stderr += b_chunk\n",
   "                if prompt_re:\n",
   "                    if prompt_re.search(stdout) and not data:\n",
   "                            stdout = to_native(stdout, encoding=encoding, errors=errors)\n",
   "                        return (257, stdout, \"A prompt was encountered while running a command, but no input data was specified\")\n",
   "                if (not events or not selector.get_map()) and cmd.poll() is not None:\n",
   "                elif not selector.get_map() and cmd.poll() is None:\n",
   "                    cmd.wait()\n",
   "            cmd.stdout.close()\n",
   "            cmd.stderr.close()\n",
   "            selector.close()\n",
   "            rc = cmd.returncode\n",
   "            self.log(\"Error Executing CMD:%s Exception:%s\" % (self._clean_args(args), to_native(e)))\n",
   "            self.fail_json(rc=e.errno, msg=to_native(e), cmd=self._clean_args(args))\n",
   "            self.log(\"Error Executing CMD:%s Exception:%s\" % (self._clean_args(args), to_native(traceback.format_exc())))\n",
   "            self.fail_json(rc=257, msg=to_native(e), exception=traceback.format_exc(), cmd=self._clean_args(args))\n",
   "        for key, val in old_env_vals.items():\n",
   "            if val is None:\n",
   "                del os.environ[key]\n",
   "                os.environ[key] = val\n",
   "        if old_umask:\n",
   "            os.umask(old_umask)\n",
   "        if rc != 0 and check_rc:\n",
   "            msg = heuristic_log_sanitize(stderr.rstrip(), self.no_log_values)\n",
   "            self.fail_json(cmd=self._clean_args(args), rc=rc, stdout=stdout, stderr=stderr, msg=msg)\n",
   "        os.chdir(prev_dir)\n",
   "            return (rc, to_native(stdout, encoding=encoding, errors=errors),\n",
   "                    to_native(stderr, encoding=encoding, errors=errors))\n",
   "        return (rc, stdout, stderr)\n",
   "        filename = os.path.expandvars(os.path.expanduser(filename))\n",
   "        fh = open(filename, 'a')\n",
   "        fh.write(str)\n",
   "        fh.close()\n",
   "    is_executable = is_executable\n",
   "            buffer_size = fcntl.fcntl(fd, 1032)\n",
   "        return buffer_size\n"
  ]
 },
 "171": {
  "name": "https_request",
  "type": "function",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/urls.py",
  "lineno": "467",
  "column": "8",
  "context": "    ),\n                req\n            )\n\n        https_request = AbstractHTTPHandler.do_request_\n\n    class HTTPSClientAuthHandler(urllib_request.H",
  "context_lines": "                    **kwargs\n                ),\n                req\n            )\n\n        https_request = AbstractHTTPHandler.do_request_\n\n    class HTTPSClientAuthHandler(urllib_request.HTTPSHandler):\n        '''Handles client authentication via cert/key\n\n        This is a fairly lightweight extension on HTTPSHandler, and can be used\n",
  "slicing": [
   "    HAS_URLPARSE = True\n",
   "    HAS_URLPARSE = False\n",
   "    HAS_SSL = True\n",
   "    HAS_SSL = False\n",
   "    HAS_SSLCONTEXT = True\n",
   "    HAS_SSLCONTEXT = False\n",
   "    HAS_URLLIB3_SSL_WRAP_SOCKET = False\n",
   "    HAS_URLLIB3_PYOPENSSLCONTEXT = True\n",
   "    HAS_URLLIB3_PYOPENSSLCONTEXT = False\n",
   "        HAS_URLLIB3_SSL_WRAP_SOCKET = True\n",
   "if HAS_SSL:\n",
   "    PROTOCOL = ssl.PROTOCOL_TLSv1\n",
   "if not HAS_SSLCONTEXT and HAS_SSL:\n",
   "        libssl_name = ctypes.util.find_library('ssl')\n",
   "        libssl = ctypes.CDLL(libssl_name)\n",
   "        for method in ('TLSv1_1_method', 'TLSv1_2_method'):\n",
   "                libssl[method]\n",
   "                PROTOCOL = ssl.PROTOCOL_SSLv23\n",
   "LOADED_VERIFY_LOCATIONS = set()\n",
   "HAS_MATCH_HOSTNAME = True\n",
   "        HAS_MATCH_HOSTNAME = False\n",
   "    HAS_GSSAPI = True\n",
   "    HAS_GSSAPI = False\n",
   "if not HAS_MATCH_HOSTNAME:\n",
   "        CertificateError = SSLCertVerificationError\n",
   "    def _dnsname_match(dn, hostname):\n",
   "        wildcards = dn.count('*')\n",
   "        if not wildcards:\n",
   "        if wildcards > 1:\n",
   "            raise CertificateError(\n",
   "        dn_leftmost, sep, dn_remainder = dn.partition('.')\n",
   "        if '*' in dn_remainder:\n",
   "            raise CertificateError(\n",
   "        if not sep:\n",
   "            raise CertificateError(\n",
   "        if dn_leftmost != '*':\n",
   "            raise CertificateError(\n",
   "                \"%s.\" % repr(dn))\n",
   "        hostname_leftmost, sep, hostname_remainder = hostname.partition('.')\n",
   "        if not hostname_leftmost or not sep:\n",
   "        return dn_remainder.lower() == hostname_remainder.lower()\n",
   "    def _inet_paton(ipname):\n",
   "            b_ipname = to_bytes(ipname, errors='strict')\n",
   "            n_ipname = b_ipname\n",
   "            n_ipname = ipname\n",
   "        if n_ipname.count('.') == 3:\n",
   "                return socket.inet_aton(n_ipname)\n",
   "            return socket.inet_pton(socket.AF_INET6, n_ipname)\n",
   "    def _ipaddress_match(ipname, host_ip):\n",
   "        ip = _inet_paton(ipname.rstrip())\n",
   "        return ip == host_ip\n",
   "    def match_hostname(cert, hostname):\n",
   "            host_ip = _inet_paton(to_text(hostname, errors='strict'))\n",
   "            host_ip = None\n",
   "            host_ip = None\n",
   "        dnsnames = []\n",
   "        san = cert.get('subjectAltName', ())\n",
   "        for key, value in san:\n",
   "            if key == 'DNS':\n",
   "                if host_ip is None and _dnsname_match(value, hostname):\n",
   "                dnsnames.append(value)\n",
   "            elif key == 'IP Address':\n",
   "                if host_ip is not None and _ipaddress_match(value, host_ip):\n",
   "                dnsnames.append(value)\n",
   "        if not dnsnames:\n",
   "            for sub in cert.get('subject', ()):\n",
   "                for key, value in sub:\n",
   "                    if key == 'commonName':\n",
   "                        if _dnsname_match(value, hostname):\n",
   "                        dnsnames.append(value)\n",
   "        if len(dnsnames) > 1:\n",
   "            raise CertificateError(\"hostname %r doesn't match either of %s\" % (hostname, ', '.join(map(repr, dnsnames))))\n",
   "        elif len(dnsnames) == 1:\n",
   "            raise CertificateError(\"hostname %r doesn't match %r\" % (hostname, dnsnames[0]))\n",
   "            raise CertificateError(\"no appropriate commonName or subjectAltName fields were found\")\n",
   "    HAS_MATCH_HOSTNAME = True\n",
   "b_DUMMY_CA_CERT = b\"\"\"-----BEGIN CERTIFICATE-----\n",
   "CustomHTTPSConnection = None\n",
   "CustomHTTPSHandler = None\n",
   "HTTPSClientAuthHandler = None\n",
   "UnixHTTPSConnection = None\n",
   "            if HAS_SSLCONTEXT:\n",
   "            elif HAS_URLLIB3_PYOPENSSLCONTEXT:\n",
   "                self.context = self._context = PyOpenSSLContext(PROTOCOL)\n",
   "                sock = socket.create_connection((self.host, self.port), self.timeout, self.source_address)\n",
   "                sock = socket.create_connection((self.host, self.port), self.timeout)\n",
   "            server_hostname = self.host\n",
   "                self.sock = sock\n",
   "                server_hostname = self._tunnel_host\n",
   "            if HAS_SSLCONTEXT or HAS_URLLIB3_PYOPENSSLCONTEXT:\n",
   "                self.sock = self.context.wrap_socket(sock, server_hostname=server_hostname)\n",
   "            elif HAS_URLLIB3_SSL_WRAP_SOCKET:\n",
   "                self.sock = ssl_wrap_socket(sock, keyfile=self.key_file, cert_reqs=ssl.CERT_NONE, certfile=self.cert_file, ssl_version=PROTOCOL,\n",
   "                                            server_hostname=server_hostname)\n",
   "                self.sock = ssl.wrap_socket(sock, keyfile=self.key_file, certfile=self.cert_file, ssl_version=PROTOCOL)\n",
   "            kwargs = {}\n",
   "            if HAS_SSLCONTEXT:\n",
   "                kwargs['context'] = self._context\n",
   "                    CustomHTTPSConnection,\n",
   "                    **kwargs\n",
   "        https_request = AbstractHTTPHandler.do_request_\n",
   "            urllib_request.HTTPSHandler.__init__(self, **kwargs)\n",
   "            kwargs.update({\n",
   "                kwargs['context'] = self._context\n",
   "                return UnixHTTPSConnection(self._unix_socket)(host, **kwargs)\n",
   "            return httplib.HTTPSConnection(host, **kwargs)\n",
   "        _connect = httplib.HTTPConnection.connect\n",
   "        httplib.HTTPConnection.connect = _connect\n",
   "                super(UnixHTTPSConnection, self).connect()\n",
   "            httplib.HTTPSConnection.__init__(self, *args, **kwargs)\n",
   "        httplib.HTTPConnection.__init__(self, *args, **kwargs)\n",
   "        urllib_request.HTTPHandler.__init__(self, **kwargs)\n",
   "        super(ParseResultDottedDict, self).__init__(*args, **kwargs)\n",
   "        return [self.get(k, None) for k in ('scheme', 'netloc', 'path', 'params', 'query', 'fragment')]\n",
   "def generic_urlparse(parts):\n",
   "    generic_parts = ParseResultDottedDict()\n",
   "        generic_parts['scheme'] = parts.scheme\n",
   "        generic_parts['netloc'] = parts.netloc\n",
   "        generic_parts['path'] = parts.path\n",
   "        generic_parts['params'] = parts.params\n",
   "        generic_parts['query'] = parts.query\n",
   "        generic_parts['fragment'] = parts.fragment\n",
   "        generic_parts['username'] = parts.username\n",
   "        generic_parts['password'] = parts.password\n",
   "        hostname = parts.hostname\n",
   "        if hostname and hostname[0] == '[' and '[' in parts.netloc and ']' in parts.netloc:\n",
   "            hostname = parts.netloc.split(']')[0][1:].lower()\n",
   "        generic_parts['hostname'] = hostname\n",
   "            port = parts.port\n",
   "            netloc = parts.netloc.split('@')[-1].split(']')[-1]\n",
   "            if ':' in netloc:\n",
   "                port = netloc.split(':')[1]\n",
   "                if port:\n",
   "                    port = int(port)\n",
   "                port = None\n",
   "        generic_parts['port'] = port\n",
   "        generic_parts['scheme'] = parts[0]\n",
   "        generic_parts['netloc'] = parts[1]\n",
   "        generic_parts['path'] = parts[2]\n",
   "        generic_parts['params'] = parts[3]\n",
   "        generic_parts['query'] = parts[4]\n",
   "        generic_parts['fragment'] = parts[5]\n",
   "            netloc_re = re.compile(r'^((?:\\w)+(?::(?:\\w)+)?@)?([A-Za-z0-9.-]+)(:\\d+)?$')\n",
   "            match = netloc_re.match(parts[1])\n",
   "            auth = match.group(1)\n",
   "            hostname = match.group(2)\n",
   "            port = match.group(3)\n",
   "            if port:\n",
   "                port = int(port[1:])\n",
   "            if auth:\n",
   "                auth = auth[:-1]\n",
   "                username, password = auth.split(':', 1)\n",
   "                username = password = None\n",
   "            generic_parts['username'] = username\n",
   "            generic_parts['password'] = password\n",
   "            generic_parts['hostname'] = hostname\n",
   "            generic_parts['port'] = port\n",
   "            generic_parts['username'] = None\n",
   "            generic_parts['password'] = None\n",
   "            generic_parts['hostname'] = parts[1]\n",
   "            generic_parts['port'] = None\n",
   "    return generic_parts\n",
   "            headers = {}\n",
   "        self._method = method.upper()\n",
   "        urllib_request.Request.__init__(self, url, data, headers, origin_req_host, unverifiable)\n",
   "def RedirectHandlerFactory(follow_redirects=None, validate_certs=True, ca_path=None):\n",
   "            if not HAS_SSLCONTEXT:\n",
   "                handler = maybe_add_ssl_handler(newurl, validate_certs, ca_path=ca_path)\n",
   "                if handler:\n",
   "                    urllib_request._opener.add_handler(handler)\n",
   "            method = req.get_method()\n",
   "                if code < 300 or code >= 400 or method not in ('GET', 'HEAD'):\n",
   "                data = req.get_data()\n",
   "                origin_req_host = req.get_origin_req_host()\n",
   "                data = req.data\n",
   "                origin_req_host = req.origin_req_host\n",
   "            newurl = newurl.replace(' ', '%20')\n",
   "                headers = req.headers\n",
   "                data = None\n",
   "                headers = dict((k, v) for k, v in req.headers.items()\n",
   "                               if k.lower() not in (\"content-length\", \"content-type\", \"transfer-encoding\"))\n",
   "                if code == 303 and method != 'HEAD':\n",
   "                    method = 'GET'\n",
   "                if code == 302 and method != 'HEAD':\n",
   "                    method = 'GET'\n",
   "                if code == 301 and method == 'POST':\n",
   "                    method = 'GET'\n",
   "            return RequestWithMethod(newurl,\n",
   "                                     method=method,\n",
   "                                     headers=headers,\n",
   "                                     data=data,\n",
   "                                     origin_req_host=origin_req_host,\n",
   "def build_ssl_validation_error(hostname, port, paths, exc=None):\n",
   "    msg = [\n",
   "    if not HAS_SSLCONTEXT:\n",
   "        msg.append('If the website serving the url uses SNI you need'\n",
   "        msg.append(' (the python executable used (%s) is version: %s)' %\n",
   "        if not HAS_URLLIB3_PYOPENSSLCONTEXT and not HAS_URLLIB3_SSL_WRAP_SOCKET:\n",
   "            msg.append('or you can install the `urllib3`, `pyOpenSSL`,'\n",
   "        msg.append('to perform SNI verification in python >= 2.6.')\n",
   "    msg.append('You can use validate_certs=False if you do'\n",
   "        msg.append('The exception msg was: %s.' % to_native(exc))\n",
   "    raise SSLValidationError(' '.join(msg) % (hostname, port, \", \".join(paths)))\n",
   "        self.hostname = hostname\n",
   "        self.port = port\n",
   "        ca_certs = []\n",
   "        cadata = bytearray()\n",
   "        paths_checked = []\n",
   "            paths_checked = [self.ca_path]\n",
   "            with open(to_bytes(self.ca_path, errors='surrogate_or_strict'), 'rb') as f:\n",
   "                if HAS_SSLCONTEXT:\n",
   "                    cadata.extend(\n",
   "                            to_native(f.read(), errors='surrogate_or_strict')\n",
   "                    ca_certs.append(f.read())\n",
   "            return ca_certs, cadata, paths_checked\n",
   "        if not HAS_SSLCONTEXT:\n",
   "            paths_checked.append('/etc/ssl/certs')\n",
   "        system = to_text(platform.system(), errors='surrogate_or_strict')\n",
   "        if system == u'Linux':\n",
   "            paths_checked.append('/etc/pki/ca-trust/extracted/pem')\n",
   "            paths_checked.append('/etc/pki/tls/certs')\n",
   "            paths_checked.append('/usr/share/ca-certificates/cacert.org')\n",
   "        elif system == u'FreeBSD':\n",
   "            paths_checked.append('/usr/local/share/certs')\n",
   "        elif system == u'OpenBSD':\n",
   "            paths_checked.append('/etc/ssl')\n",
   "        elif system == u'NetBSD':\n",
   "            ca_certs.append('/etc/openssl/certs')\n",
   "        elif system == u'SunOS':\n",
   "            paths_checked.append('/opt/local/etc/openssl/certs')\n",
   "        paths_checked.append('/etc/ansible')\n",
   "        tmp_path = None\n",
   "        if not HAS_SSLCONTEXT:\n",
   "            tmp_fd, tmp_path = tempfile.mkstemp()\n",
   "            atexit.register(atexit_remove_file, tmp_path)\n",
   "        if system == u'Darwin':\n",
   "            if HAS_SSLCONTEXT:\n",
   "                cadata.extend(\n",
   "                        to_native(b_DUMMY_CA_CERT, errors='surrogate_or_strict')\n",
   "                os.write(tmp_fd, b_DUMMY_CA_CERT)\n",
   "            paths_checked.append('/usr/local/etc/openssl')\n",
   "        for path in paths_checked:\n",
   "            if os.path.exists(path) and os.path.isdir(path):\n",
   "                dir_contents = os.listdir(path)\n",
   "                for f in dir_contents:\n",
   "                    full_path = os.path.join(path, f)\n",
   "                    if os.path.isfile(full_path) and os.path.splitext(f)[1] in ('.crt', '.pem'):\n",
   "                            if full_path not in LOADED_VERIFY_LOCATIONS:\n",
   "                                with open(full_path, 'rb') as cert_file:\n",
   "                                    b_cert = cert_file.read()\n",
   "                                if HAS_SSLCONTEXT:\n",
   "                                        cadata.extend(\n",
   "                                                to_native(b_cert, errors='surrogate_or_strict')\n",
   "                                    os.write(tmp_fd, b_cert)\n",
   "                                    os.write(tmp_fd, b'\\n')\n",
   "        if HAS_SSLCONTEXT:\n",
   "            default_verify_paths = ssl.get_default_verify_paths()\n",
   "            paths_checked[:0] = [default_verify_paths.capath]\n",
   "        return (tmp_path, cadata, paths_checked)\n",
   "        valid_codes = [200] if valid_codes is None else valid_codes\n",
   "            (http_version, resp_code, msg) = re.match(br'(HTTP/\\d\\.\\d) (\\d\\d\\d) (.*)', response).groups()\n",
   "            if int(resp_code) not in valid_codes:\n",
   "        env_no_proxy = os.environ.get('no_proxy')\n",
   "        if env_no_proxy:\n",
   "            env_no_proxy = env_no_proxy.split(',')\n",
   "            netloc = urlparse(url).netloc\n",
   "            for host in env_no_proxy:\n",
   "                if netloc.endswith(host) or netloc.split(':')[0].endswith(host):\n",
   "        cafile = self.ca_path or cafile\n",
   "            cadata = None\n",
   "            cadata = cadata or None\n",
   "        if HAS_SSLCONTEXT:\n",
   "            context = create_default_context(cafile=cafile)\n",
   "        elif HAS_URLLIB3_PYOPENSSLCONTEXT:\n",
   "            context = PyOpenSSLContext(PROTOCOL)\n",
   "        if cafile or cadata:\n",
   "            context.load_verify_locations(cafile=cafile, cadata=cadata)\n",
   "        return context\n",
   "        tmp_ca_cert_path, cadata, paths_checked = self.get_ca_certs()\n",
   "        use_proxy = self.detect_no_proxy(req.get_full_url())\n",
   "        https_proxy = os.environ.get('https_proxy')\n",
   "        context = None\n",
   "            context = self.make_context(tmp_ca_cert_path, cadata)\n",
   "            if use_proxy and https_proxy:\n",
   "                proxy_parts = generic_urlparse(urlparse(https_proxy))\n",
   "                port = proxy_parts.get('port') or 443\n",
   "                proxy_hostname = proxy_parts.get('hostname', None)\n",
   "                if proxy_hostname is None or proxy_parts.get('scheme') == '':\n",
   "                s = socket.create_connection((proxy_hostname, port))\n",
   "                if proxy_parts.get('scheme') == 'http':\n",
   "                    s.sendall(to_bytes(self.CONNECT_COMMAND % (self.hostname, self.port), errors='surrogate_or_strict'))\n",
   "                    if proxy_parts.get('username'):\n",
   "                        credentials = \"%s:%s\" % (proxy_parts.get('username', ''), proxy_parts.get('password', ''))\n",
   "                        s.sendall(b'Proxy-Authorization: Basic %s\\r\\n' % base64.b64encode(to_bytes(credentials, errors='surrogate_or_strict')).strip())\n",
   "                    s.sendall(b'\\r\\n')\n",
   "                    connect_result = b\"\"\n",
   "                    while connect_result.find(b\"\\r\\n\\r\\n\") <= 0:\n",
   "                        connect_result += s.recv(4096)\n",
   "                        if len(connect_result) > 131072:\n",
   "                    self.validate_proxy_response(connect_result)\n",
   "                    if context:\n",
   "                        ssl_s = context.wrap_socket(s, server_hostname=self.hostname)\n",
   "                    elif HAS_URLLIB3_SSL_WRAP_SOCKET:\n",
   "                        ssl_s = ssl_wrap_socket(s, ca_certs=tmp_ca_cert_path, cert_reqs=ssl.CERT_REQUIRED, ssl_version=PROTOCOL, server_hostname=self.hostname)\n",
   "                        ssl_s = ssl.wrap_socket(s, ca_certs=tmp_ca_cert_path, cert_reqs=ssl.CERT_REQUIRED, ssl_version=PROTOCOL)\n",
   "                        match_hostname(ssl_s.getpeercert(), self.hostname)\n",
   "                    raise ProxyError('Unsupported proxy scheme: %s. Currently ansible only supports HTTP proxies.' % proxy_parts.get('scheme'))\n",
   "                s = socket.create_connection((self.hostname, self.port))\n",
   "                if context:\n",
   "                    ssl_s = context.wrap_socket(s, server_hostname=self.hostname)\n",
   "                elif HAS_URLLIB3_SSL_WRAP_SOCKET:\n",
   "                    ssl_s = ssl_wrap_socket(s, ca_certs=tmp_ca_cert_path, cert_reqs=ssl.CERT_REQUIRED, ssl_version=PROTOCOL, server_hostname=self.hostname)\n",
   "                    ssl_s = ssl.wrap_socket(s, ca_certs=tmp_ca_cert_path, cert_reqs=ssl.CERT_REQUIRED, ssl_version=PROTOCOL)\n",
   "                    match_hostname(ssl_s.getpeercert(), self.hostname)\n",
   "            s.close()\n",
   "        except (ssl.SSLError, CertificateError) as e:\n",
   "            build_ssl_validation_error(self.hostname, self.port, paths_checked, e)\n",
   "def maybe_add_ssl_handler(url, validate_certs, ca_path=None):\n",
   "    parsed = generic_urlparse(urlparse(url))\n",
   "    if parsed.scheme == 'https' and validate_certs:\n",
   "        if not HAS_SSL:\n",
   "        return SSLValidationHandler(parsed.hostname, parsed.port or 443, ca_path=ca_path)\n",
   "def rfc2822_date_string(timetuple, zone='-0000'):\n",
   "        timetuple[0], timetuple[3], timetuple[4], timetuple[5],\n",
   "        zone)\n",
   "        self.headers = headers or {}\n",
   "        self.use_proxy = use_proxy\n",
   "        if value is None:\n",
   "        return value\n",
   "    def open(self, method, url, data=None, headers=None, use_proxy=None,\n",
   "        method = method.upper()\n",
   "        if headers is None:\n",
   "            headers = {}\n",
   "        elif not isinstance(headers, dict):\n",
   "        headers = dict(self.headers, **headers)\n",
   "        use_proxy = self._fallback(use_proxy, self.use_proxy)\n",
   "        force = self._fallback(force, self.force)\n",
   "        timeout = self._fallback(timeout, self.timeout)\n",
   "        validate_certs = self._fallback(validate_certs, self.validate_certs)\n",
   "        url_username = self._fallback(url_username, self.url_username)\n",
   "        url_password = self._fallback(url_password, self.url_password)\n",
   "        http_agent = self._fallback(http_agent, self.http_agent)\n",
   "        force_basic_auth = self._fallback(force_basic_auth, self.force_basic_auth)\n",
   "        follow_redirects = self._fallback(follow_redirects, self.follow_redirects)\n",
   "        client_cert = self._fallback(client_cert, self.client_cert)\n",
   "        client_key = self._fallback(client_key, self.client_key)\n",
   "        cookies = self._fallback(cookies, self.cookies)\n",
   "        unix_socket = self._fallback(unix_socket, self.unix_socket)\n",
   "        ca_path = self._fallback(ca_path, self.ca_path)\n",
   "        handlers = []\n",
   "        if unix_socket:\n",
   "            handlers.append(UnixHTTPHandler(unix_socket))\n",
   "        ssl_handler = maybe_add_ssl_handler(url, validate_certs, ca_path=ca_path)\n",
   "        if ssl_handler and not HAS_SSLCONTEXT:\n",
   "            handlers.append(ssl_handler)\n",
   "        if HAS_GSSAPI and use_gssapi:\n",
   "            handlers.append(urllib_gssapi.HTTPSPNEGOAuthHandler())\n",
   "        parsed = generic_urlparse(urlparse(url))\n",
   "        if parsed.scheme != 'ftp':\n",
   "            username = url_username\n",
   "            if username:\n",
   "                password = url_password\n",
   "                netloc = parsed.netloc\n",
   "            elif '@' in parsed.netloc:\n",
   "                credentials, netloc = parsed.netloc.split('@', 1)\n",
   "                if ':' in credentials:\n",
   "                    username, password = credentials.split(':', 1)\n",
   "                    username = credentials\n",
   "                    password = ''\n",
   "                parsed_list = parsed.as_list()\n",
   "                parsed_list[1] = netloc\n",
   "                url = urlunparse(parsed_list)\n",
   "            if username and not force_basic_auth:\n",
   "                passman = urllib_request.HTTPPasswordMgrWithDefaultRealm()\n",
   "                passman.add_password(None, netloc, username, password)\n",
   "                authhandler = urllib_request.HTTPBasicAuthHandler(passman)\n",
   "                digest_authhandler = urllib_request.HTTPDigestAuthHandler(passman)\n",
   "                handlers.append(authhandler)\n",
   "                handlers.append(digest_authhandler)\n",
   "            elif username and force_basic_auth:\n",
   "                headers[\"Authorization\"] = basic_auth_header(username, password)\n",
   "                    rc = netrc.netrc(os.environ.get('NETRC'))\n",
   "                    login = rc.authenticators(parsed.hostname)\n",
   "                    login = None\n",
   "                if login:\n",
   "                    username, _, password = login\n",
   "                    if username and password:\n",
   "                        headers[\"Authorization\"] = basic_auth_header(username, password)\n",
   "        if not use_proxy:\n",
   "            proxyhandler = urllib_request.ProxyHandler({})\n",
   "            handlers.append(proxyhandler)\n",
   "        context = None\n",
   "        if HAS_SSLCONTEXT and not validate_certs:\n",
   "            context = SSLContext(ssl.PROTOCOL_SSLv23)\n",
   "                context.options |= ssl.OP_NO_SSLv2\n",
   "            context.options |= ssl.OP_NO_SSLv3\n",
   "            context.verify_mode = ssl.CERT_NONE\n",
   "            context.check_hostname = False\n",
   "            handlers.append(HTTPSClientAuthHandler(client_cert=client_cert,\n",
   "                                                   client_key=client_key,\n",
   "                                                   context=context,\n",
   "                                                   unix_socket=unix_socket))\n",
   "        elif client_cert or unix_socket:\n",
   "            handlers.append(HTTPSClientAuthHandler(client_cert=client_cert,\n",
   "                                                   client_key=client_key,\n",
   "                                                   unix_socket=unix_socket))\n",
   "        if ssl_handler and HAS_SSLCONTEXT and validate_certs:\n",
   "            tmp_ca_path, cadata, paths_checked = ssl_handler.get_ca_certs()\n",
   "                context = ssl_handler.make_context(tmp_ca_path, cadata)\n",
   "        if hasattr(socket, 'create_connection') and CustomHTTPSHandler:\n",
   "            kwargs = {}\n",
   "            if HAS_SSLCONTEXT:\n",
   "                kwargs['context'] = context\n",
   "            handlers.append(CustomHTTPSHandler(**kwargs))\n",
   "        handlers.append(RedirectHandlerFactory(follow_redirects, validate_certs, ca_path=ca_path))\n",
   "        if cookies is not None:\n",
   "            handlers.append(urllib_request.HTTPCookieProcessor(cookies))\n",
   "        opener = urllib_request.build_opener(*handlers)\n",
   "        urllib_request.install_opener(opener)\n",
   "        data = to_bytes(data, nonstring='passthru')\n",
   "        request = RequestWithMethod(url, method, data)\n",
   "        if http_agent:\n",
   "            request.add_header('User-agent', http_agent)\n",
   "        if force:\n",
   "            request.add_header('cache-control', 'no-cache')\n",
   "            tstamp = rfc2822_date_string(last_mod_time.timetuple(), 'GMT')\n",
   "            request.add_header('If-Modified-Since', tstamp)\n",
   "        unredirected_headers = unredirected_headers or []\n",
   "        for header in headers:\n",
   "            if header in unredirected_headers:\n",
   "                request.add_unredirected_header(header, headers[header])\n",
   "                request.add_header(header, headers[header])\n",
   "        return urllib_request.urlopen(request, None, timeout)\n",
   "        return self.open('GET', url, **kwargs)\n",
   "        return self.open('OPTIONS', url, **kwargs)\n",
   "        return self.open('HEAD', url, **kwargs)\n",
   "        return self.open('POST', url, data=data, **kwargs)\n",
   "        return self.open('PUT', url, data=data, **kwargs)\n",
   "        return self.open('PATCH', url, data=data, **kwargs)\n",
   "        return self.open('DELETE', url, **kwargs)\n",
   "def open_url(url, data=None, headers=None, method=None, use_proxy=True,\n",
   "    method = method or ('POST' if data else 'GET')\n",
   "    return Request().open(method, url, data=data, headers=headers, use_proxy=use_proxy,\n",
   "                          force=force, last_mod_time=last_mod_time, timeout=timeout, validate_certs=validate_certs,\n",
   "                          url_username=url_username, url_password=url_password, http_agent=http_agent,\n",
   "                          force_basic_auth=force_basic_auth, follow_redirects=follow_redirects,\n",
   "                          client_cert=client_cert, client_key=client_key, cookies=cookies,\n",
   "                          use_gssapi=use_gssapi, unix_socket=unix_socket, ca_path=ca_path,\n",
   "                          unredirected_headers=unredirected_headers)\n",
   "    m = email.mime.multipart.MIMEMultipart('form-data')\n",
   "    for field, value in sorted(fields.items()):\n",
   "        if isinstance(value, string_types):\n",
   "            main_type = 'text'\n",
   "            sub_type = 'plain'\n",
   "            content = value\n",
   "            filename = None\n",
   "        elif isinstance(value, Mapping):\n",
   "            filename = value.get('filename')\n",
   "            content = value.get('content')\n",
   "            if not any((filename, content)):\n",
   "            mime = value.get('mime_type')\n",
   "            if not mime:\n",
   "                    mime = mimetypes.guess_type(filename or '', strict=False)[0] or 'application/octet-stream'\n",
   "                    mime = 'application/octet-stream'\n",
   "            main_type, sep, sub_type = mime.partition('/')\n",
   "                'value must be a string, or mapping, cannot be type %s' % value.__class__.__name__\n",
   "        if not content and filename:\n",
   "            with open(to_bytes(filename, errors='surrogate_or_strict'), 'rb') as f:\n",
   "                part = email.mime.application.MIMEApplication(f.read())\n",
   "                del part['Content-Type']\n",
   "                part.add_header('Content-Type', '%s/%s' % (main_type, sub_type))\n",
   "            part = email.mime.nonmultipart.MIMENonMultipart(main_type, sub_type)\n",
   "            part.set_payload(to_bytes(content))\n",
   "        part.add_header('Content-Disposition', 'form-data')\n",
   "        del part['MIME-Version']\n",
   "        part.set_param(\n",
   "            field,\n",
   "        if filename:\n",
   "            part.set_param(\n",
   "                to_native(os.path.basename(filename)),\n",
   "        m.attach(part)\n",
   "        b_data = m.as_bytes(policy=email.policy.HTTP)\n",
   "        fp = cStringIO()  # cStringIO seems to be required here\n",
   "        g = email.generator.Generator(fp, maxheaderlen=0)\n",
   "        g.flatten(m)\n",
   "        b_data = email.utils.fix_eols(fp.getvalue())\n",
   "    headers, sep, b_content = b_data.partition(b'\\r\\n\\r\\n')\n",
   "        parser = email.parser.BytesHeaderParser().parsebytes\n",
   "        parser = email.parser.HeaderParser().parsestr\n",
   "        parser(headers)['content-type'],  # Message converts to native strings\n",
   "        b_content\n",
   "    return b\"Basic %s\" % base64.b64encode(to_bytes(\"%s:%s\" % (username, password), errors='surrogate_or_strict'))\n",
   "def fetch_url(module, url, data=None, headers=None, method=None,\n",
   "              use_proxy=True, force=False, last_mod_time=None, timeout=10,\n",
   "    if not HAS_URLPARSE:\n",
   "    old_tempdir = tempfile.tempdir\n",
   "    validate_certs = module.params.get('validate_certs', True)\n",
   "    username = module.params.get('url_username', '')\n",
   "    password = module.params.get('url_password', '')\n",
   "    http_agent = module.params.get('http_agent', 'ansible-httpget')\n",
   "    force_basic_auth = module.params.get('force_basic_auth', '')\n",
   "    follow_redirects = module.params.get('follow_redirects', 'urllib2')\n",
   "    client_cert = module.params.get('client_cert')\n",
   "    client_key = module.params.get('client_key')\n",
   "    if not isinstance(cookies, cookiejar.CookieJar):\n",
   "        cookies = cookiejar.LWPCookieJar()\n",
   "    r = None\n",
   "    info = dict(url=url, status=-1)\n",
   "        r = open_url(url, data=data, headers=headers, method=method,\n",
   "                     use_proxy=use_proxy, force=force, last_mod_time=last_mod_time, timeout=timeout,\n",
   "                     validate_certs=validate_certs, url_username=username,\n",
   "                     url_password=password, http_agent=http_agent, force_basic_auth=force_basic_auth,\n",
   "                     follow_redirects=follow_redirects, client_cert=client_cert,\n",
   "                     client_key=client_key, cookies=cookies, use_gssapi=use_gssapi,\n",
   "                     unix_socket=unix_socket, ca_path=ca_path)\n",
   "        info.update(dict((k.lower(), v) for k, v in r.info().items()))\n",
   "            temp_headers = {}\n",
   "            for name, value in r.headers.items():\n",
   "                name = name.lower()\n",
   "                if name in temp_headers:\n",
   "                    temp_headers[name] = ', '.join((temp_headers[name], value))\n",
   "                    temp_headers[name] = value\n",
   "            info.update(temp_headers)\n",
   "        cookie_list = []\n",
   "        cookie_dict = dict()\n",
   "        for cookie in cookies:\n",
   "            cookie_dict[cookie.name] = cookie.value\n",
   "            cookie_list.append((cookie.name, cookie.value))\n",
   "        info['cookies_string'] = '; '.join('%s=%s' % c for c in cookie_list)\n",
   "        info['cookies'] = cookie_dict\n",
   "        info.update(dict(msg=\"OK (%s bytes)\" % r.headers.get('Content-Length', 'unknown'), url=r.geturl(), status=r.code))\n",
   "        distribution = get_distribution()\n",
   "        if distribution is not None and distribution.lower() == 'redhat':\n",
   "            module.fail_json(msg='%s. You can also install python-ssl from EPEL' % to_native(e), **info)\n",
   "            module.fail_json(msg='%s' % to_native(e), **info)\n",
   "        module.fail_json(msg=to_native(e), **info)\n",
   "            body = e.read()\n",
   "            body = ''\n",
   "            info.update(dict((k.lower(), v) for k, v in e.info().items()))\n",
   "        info.update({'msg': to_native(e), 'body': body, 'status': e.code})\n",
   "        code = int(getattr(e, 'code', -1))\n",
   "        info.update(dict(msg=\"Request failed: %s\" % to_native(e), status=code))\n",
   "        info.update(dict(msg=\"Connection failure: %s\" % to_native(e), status=-1))\n",
   "        info.update(dict(msg=\"Connection failure: connection was closed before a valid response was received: %s\" % to_native(e.line), status=-1))\n",
   "        info.update(dict(msg=\"An unknown error occurred: %s\" % to_native(e), status=-1),\n",
   "        tempfile.tempdir = old_tempdir\n",
   "    return r, info\n",
   "    bufsize = 65536\n",
   "    file_name, file_ext = os.path.splitext(str(url.rsplit('/', 1)[1]))\n",
   "    fetch_temp_file = tempfile.NamedTemporaryFile(dir=module.tmpdir, prefix=file_name, suffix=file_ext, delete=False)\n",
   "    module.add_cleanup_file(fetch_temp_file.name)\n",
   "        rsp, info = fetch_url(module, url, data, headers, method, use_proxy, force, last_mod_time, timeout)\n",
   "        if not rsp:\n",
   "            module.fail_json(msg=\"Failure downloading %s, %s\" % (url, info['msg']))\n",
   "        data = rsp.read(bufsize)\n",
   "        while data:\n",
   "            fetch_temp_file.write(data)\n",
   "            data = rsp.read(bufsize)\n",
   "        fetch_temp_file.close()\n",
   "        module.fail_json(msg=\"Failure downloading %s, %s\" % (url, to_native(e)))\n",
   "    return fetch_temp_file.name\n"
  ]
 },
 "172": {
  "name": "CONNECT_COMMAND",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/urls.py",
  "lineno": "803",
  "column": "4",
  "context": "echknack.net/python-urllib2-handlers/\n    '''\n    CONNECT_COMMAND = \"CONNECT %s:%s HTTP/1.0\\r\\n\"\n\n    def __init__(self, hostname, port, ca_path=No",
  "context_lines": "    Based on:\n    http://stackoverflow.com/questions/1087227/validate-ssl-certificates-with-python\n    http://techknack.net/python-urllib2-handlers/\n    '''\n    CONNECT_COMMAND = \"CONNECT %s:%s HTTP/1.0\\r\\n\"\n\n    def __init__(self, hostname, port, ca_path=None):\n        self.hostname = hostname\n        self.port = port\n",
  "slicing": "    CONNECT_COMMAND = \"CONNECT %s:%s HTTP/1.0\\r\\n\"\n"
 },
 "173": {
  "name": "https_request",
  "type": "function",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/module_utils/urls.py",
  "lineno": "1018",
  "column": "4",
  "context": "elf.port, to_native(e)))\n\n        return req\n\n    https_request = http_request\n\n\ndef maybe_add_ssl_handler(url, validate_certs, c",
  "context_lines": "            build_ssl_validation_error(self.hostname, self.port, paths_checked, e)\n        except socket.error as e:\n            raise ConnectionError('Failed to connect to %s at port %s: %s' % (self.hostname, self.port, to_native(e)))\n\n        return req\n\n    https_request = http_request\n\n\ndef maybe_add_ssl_handler(url, validate_certs, ca_path=None):\n    parsed = generic_urlparse(urlparse(url))\n    if parsed.scheme == 'https' and validate_certs:\n",
  "slicing": [
   "    HAS_URLPARSE = True\n",
   "    HAS_URLPARSE = False\n",
   "    HAS_SSL = True\n",
   "    HAS_SSL = False\n",
   "    HAS_SSLCONTEXT = True\n",
   "    HAS_SSLCONTEXT = False\n",
   "    HAS_URLLIB3_SSL_WRAP_SOCKET = False\n",
   "    HAS_URLLIB3_PYOPENSSLCONTEXT = True\n",
   "    HAS_URLLIB3_PYOPENSSLCONTEXT = False\n",
   "        HAS_URLLIB3_SSL_WRAP_SOCKET = True\n",
   "if HAS_SSL:\n",
   "    PROTOCOL = ssl.PROTOCOL_TLSv1\n",
   "if not HAS_SSLCONTEXT and HAS_SSL:\n",
   "        libssl_name = ctypes.util.find_library('ssl')\n",
   "        libssl = ctypes.CDLL(libssl_name)\n",
   "        for method in ('TLSv1_1_method', 'TLSv1_2_method'):\n",
   "                libssl[method]\n",
   "                PROTOCOL = ssl.PROTOCOL_SSLv23\n",
   "LOADED_VERIFY_LOCATIONS = set()\n",
   "HAS_MATCH_HOSTNAME = True\n",
   "        HAS_MATCH_HOSTNAME = False\n",
   "    HAS_GSSAPI = True\n",
   "    HAS_GSSAPI = False\n",
   "if not HAS_MATCH_HOSTNAME:\n",
   "        CertificateError = SSLCertVerificationError\n",
   "    def _dnsname_match(dn, hostname):\n",
   "        wildcards = dn.count('*')\n",
   "        if not wildcards:\n",
   "        if wildcards > 1:\n",
   "            raise CertificateError(\n",
   "        dn_leftmost, sep, dn_remainder = dn.partition('.')\n",
   "        if '*' in dn_remainder:\n",
   "            raise CertificateError(\n",
   "        if not sep:\n",
   "            raise CertificateError(\n",
   "        if dn_leftmost != '*':\n",
   "            raise CertificateError(\n",
   "                \"%s.\" % repr(dn))\n",
   "        hostname_leftmost, sep, hostname_remainder = hostname.partition('.')\n",
   "        if not hostname_leftmost or not sep:\n",
   "        return dn_remainder.lower() == hostname_remainder.lower()\n",
   "    def _inet_paton(ipname):\n",
   "            b_ipname = to_bytes(ipname, errors='strict')\n",
   "            n_ipname = b_ipname\n",
   "            n_ipname = ipname\n",
   "        if n_ipname.count('.') == 3:\n",
   "                return socket.inet_aton(n_ipname)\n",
   "            return socket.inet_pton(socket.AF_INET6, n_ipname)\n",
   "    def _ipaddress_match(ipname, host_ip):\n",
   "        ip = _inet_paton(ipname.rstrip())\n",
   "        return ip == host_ip\n",
   "    def match_hostname(cert, hostname):\n",
   "            host_ip = _inet_paton(to_text(hostname, errors='strict'))\n",
   "            host_ip = None\n",
   "            host_ip = None\n",
   "        dnsnames = []\n",
   "        san = cert.get('subjectAltName', ())\n",
   "        for key, value in san:\n",
   "            if key == 'DNS':\n",
   "                if host_ip is None and _dnsname_match(value, hostname):\n",
   "                dnsnames.append(value)\n",
   "            elif key == 'IP Address':\n",
   "                if host_ip is not None and _ipaddress_match(value, host_ip):\n",
   "                dnsnames.append(value)\n",
   "        if not dnsnames:\n",
   "            for sub in cert.get('subject', ()):\n",
   "                for key, value in sub:\n",
   "                    if key == 'commonName':\n",
   "                        if _dnsname_match(value, hostname):\n",
   "                        dnsnames.append(value)\n",
   "        if len(dnsnames) > 1:\n",
   "            raise CertificateError(\"hostname %r doesn't match either of %s\" % (hostname, ', '.join(map(repr, dnsnames))))\n",
   "        elif len(dnsnames) == 1:\n",
   "            raise CertificateError(\"hostname %r doesn't match %r\" % (hostname, dnsnames[0]))\n",
   "            raise CertificateError(\"no appropriate commonName or subjectAltName fields were found\")\n",
   "    HAS_MATCH_HOSTNAME = True\n",
   "b_DUMMY_CA_CERT = b\"\"\"-----BEGIN CERTIFICATE-----\n",
   "CustomHTTPSConnection = None\n",
   "CustomHTTPSHandler = None\n",
   "HTTPSClientAuthHandler = None\n",
   "UnixHTTPSConnection = None\n",
   "            if HAS_SSLCONTEXT:\n",
   "            elif HAS_URLLIB3_PYOPENSSLCONTEXT:\n",
   "                self.context = self._context = PyOpenSSLContext(PROTOCOL)\n",
   "                sock = socket.create_connection((self.host, self.port), self.timeout, self.source_address)\n",
   "                sock = socket.create_connection((self.host, self.port), self.timeout)\n",
   "            server_hostname = self.host\n",
   "                self.sock = sock\n",
   "                server_hostname = self._tunnel_host\n",
   "            if HAS_SSLCONTEXT or HAS_URLLIB3_PYOPENSSLCONTEXT:\n",
   "                self.sock = self.context.wrap_socket(sock, server_hostname=server_hostname)\n",
   "            elif HAS_URLLIB3_SSL_WRAP_SOCKET:\n",
   "                self.sock = ssl_wrap_socket(sock, keyfile=self.key_file, cert_reqs=ssl.CERT_NONE, certfile=self.cert_file, ssl_version=PROTOCOL,\n",
   "                                            server_hostname=server_hostname)\n",
   "                self.sock = ssl.wrap_socket(sock, keyfile=self.key_file, certfile=self.cert_file, ssl_version=PROTOCOL)\n",
   "            kwargs = {}\n",
   "            if HAS_SSLCONTEXT:\n",
   "                kwargs['context'] = self._context\n",
   "                    CustomHTTPSConnection,\n",
   "                    **kwargs\n",
   "            urllib_request.HTTPSHandler.__init__(self, **kwargs)\n",
   "            kwargs.update({\n",
   "                kwargs['context'] = self._context\n",
   "                return UnixHTTPSConnection(self._unix_socket)(host, **kwargs)\n",
   "            return httplib.HTTPSConnection(host, **kwargs)\n",
   "        _connect = httplib.HTTPConnection.connect\n",
   "        httplib.HTTPConnection.connect = _connect\n",
   "                super(UnixHTTPSConnection, self).connect()\n",
   "            httplib.HTTPSConnection.__init__(self, *args, **kwargs)\n",
   "        httplib.HTTPConnection.__init__(self, *args, **kwargs)\n",
   "        urllib_request.HTTPHandler.__init__(self, **kwargs)\n",
   "        super(ParseResultDottedDict, self).__init__(*args, **kwargs)\n",
   "        return [self.get(k, None) for k in ('scheme', 'netloc', 'path', 'params', 'query', 'fragment')]\n",
   "def generic_urlparse(parts):\n",
   "    generic_parts = ParseResultDottedDict()\n",
   "        generic_parts['scheme'] = parts.scheme\n",
   "        generic_parts['netloc'] = parts.netloc\n",
   "        generic_parts['path'] = parts.path\n",
   "        generic_parts['params'] = parts.params\n",
   "        generic_parts['query'] = parts.query\n",
   "        generic_parts['fragment'] = parts.fragment\n",
   "        generic_parts['username'] = parts.username\n",
   "        generic_parts['password'] = parts.password\n",
   "        hostname = parts.hostname\n",
   "        if hostname and hostname[0] == '[' and '[' in parts.netloc and ']' in parts.netloc:\n",
   "            hostname = parts.netloc.split(']')[0][1:].lower()\n",
   "        generic_parts['hostname'] = hostname\n",
   "            port = parts.port\n",
   "            netloc = parts.netloc.split('@')[-1].split(']')[-1]\n",
   "            if ':' in netloc:\n",
   "                port = netloc.split(':')[1]\n",
   "                if port:\n",
   "                    port = int(port)\n",
   "                port = None\n",
   "        generic_parts['port'] = port\n",
   "        generic_parts['scheme'] = parts[0]\n",
   "        generic_parts['netloc'] = parts[1]\n",
   "        generic_parts['path'] = parts[2]\n",
   "        generic_parts['params'] = parts[3]\n",
   "        generic_parts['query'] = parts[4]\n",
   "        generic_parts['fragment'] = parts[5]\n",
   "            netloc_re = re.compile(r'^((?:\\w)+(?::(?:\\w)+)?@)?([A-Za-z0-9.-]+)(:\\d+)?$')\n",
   "            match = netloc_re.match(parts[1])\n",
   "            auth = match.group(1)\n",
   "            hostname = match.group(2)\n",
   "            port = match.group(3)\n",
   "            if port:\n",
   "                port = int(port[1:])\n",
   "            if auth:\n",
   "                auth = auth[:-1]\n",
   "                username, password = auth.split(':', 1)\n",
   "                username = password = None\n",
   "            generic_parts['username'] = username\n",
   "            generic_parts['password'] = password\n",
   "            generic_parts['hostname'] = hostname\n",
   "            generic_parts['port'] = port\n",
   "            generic_parts['username'] = None\n",
   "            generic_parts['password'] = None\n",
   "            generic_parts['hostname'] = parts[1]\n",
   "            generic_parts['port'] = None\n",
   "    return generic_parts\n",
   "            headers = {}\n",
   "        self._method = method.upper()\n",
   "        urllib_request.Request.__init__(self, url, data, headers, origin_req_host, unverifiable)\n",
   "def RedirectHandlerFactory(follow_redirects=None, validate_certs=True, ca_path=None):\n",
   "            if not HAS_SSLCONTEXT:\n",
   "                handler = maybe_add_ssl_handler(newurl, validate_certs, ca_path=ca_path)\n",
   "                if handler:\n",
   "                    urllib_request._opener.add_handler(handler)\n",
   "            method = req.get_method()\n",
   "                if code < 300 or code >= 400 or method not in ('GET', 'HEAD'):\n",
   "                data = req.get_data()\n",
   "                origin_req_host = req.get_origin_req_host()\n",
   "                data = req.data\n",
   "                origin_req_host = req.origin_req_host\n",
   "            newurl = newurl.replace(' ', '%20')\n",
   "                headers = req.headers\n",
   "                data = None\n",
   "                headers = dict((k, v) for k, v in req.headers.items()\n",
   "                               if k.lower() not in (\"content-length\", \"content-type\", \"transfer-encoding\"))\n",
   "                if code == 303 and method != 'HEAD':\n",
   "                    method = 'GET'\n",
   "                if code == 302 and method != 'HEAD':\n",
   "                    method = 'GET'\n",
   "                if code == 301 and method == 'POST':\n",
   "                    method = 'GET'\n",
   "            return RequestWithMethod(newurl,\n",
   "                                     method=method,\n",
   "                                     headers=headers,\n",
   "                                     data=data,\n",
   "                                     origin_req_host=origin_req_host,\n",
   "def build_ssl_validation_error(hostname, port, paths, exc=None):\n",
   "    msg = [\n",
   "    if not HAS_SSLCONTEXT:\n",
   "        msg.append('If the website serving the url uses SNI you need'\n",
   "        msg.append(' (the python executable used (%s) is version: %s)' %\n",
   "        if not HAS_URLLIB3_PYOPENSSLCONTEXT and not HAS_URLLIB3_SSL_WRAP_SOCKET:\n",
   "            msg.append('or you can install the `urllib3`, `pyOpenSSL`,'\n",
   "        msg.append('to perform SNI verification in python >= 2.6.')\n",
   "    msg.append('You can use validate_certs=False if you do'\n",
   "        msg.append('The exception msg was: %s.' % to_native(exc))\n",
   "    raise SSLValidationError(' '.join(msg) % (hostname, port, \", \".join(paths)))\n",
   "        self.hostname = hostname\n",
   "        self.port = port\n",
   "        ca_certs = []\n",
   "        cadata = bytearray()\n",
   "        paths_checked = []\n",
   "            paths_checked = [self.ca_path]\n",
   "            with open(to_bytes(self.ca_path, errors='surrogate_or_strict'), 'rb') as f:\n",
   "                if HAS_SSLCONTEXT:\n",
   "                    cadata.extend(\n",
   "                            to_native(f.read(), errors='surrogate_or_strict')\n",
   "                    ca_certs.append(f.read())\n",
   "            return ca_certs, cadata, paths_checked\n",
   "        if not HAS_SSLCONTEXT:\n",
   "            paths_checked.append('/etc/ssl/certs')\n",
   "        system = to_text(platform.system(), errors='surrogate_or_strict')\n",
   "        if system == u'Linux':\n",
   "            paths_checked.append('/etc/pki/ca-trust/extracted/pem')\n",
   "            paths_checked.append('/etc/pki/tls/certs')\n",
   "            paths_checked.append('/usr/share/ca-certificates/cacert.org')\n",
   "        elif system == u'FreeBSD':\n",
   "            paths_checked.append('/usr/local/share/certs')\n",
   "        elif system == u'OpenBSD':\n",
   "            paths_checked.append('/etc/ssl')\n",
   "        elif system == u'NetBSD':\n",
   "            ca_certs.append('/etc/openssl/certs')\n",
   "        elif system == u'SunOS':\n",
   "            paths_checked.append('/opt/local/etc/openssl/certs')\n",
   "        paths_checked.append('/etc/ansible')\n",
   "        tmp_path = None\n",
   "        if not HAS_SSLCONTEXT:\n",
   "            tmp_fd, tmp_path = tempfile.mkstemp()\n",
   "            atexit.register(atexit_remove_file, tmp_path)\n",
   "        if system == u'Darwin':\n",
   "            if HAS_SSLCONTEXT:\n",
   "                cadata.extend(\n",
   "                        to_native(b_DUMMY_CA_CERT, errors='surrogate_or_strict')\n",
   "                os.write(tmp_fd, b_DUMMY_CA_CERT)\n",
   "            paths_checked.append('/usr/local/etc/openssl')\n",
   "        for path in paths_checked:\n",
   "            if os.path.exists(path) and os.path.isdir(path):\n",
   "                dir_contents = os.listdir(path)\n",
   "                for f in dir_contents:\n",
   "                    full_path = os.path.join(path, f)\n",
   "                    if os.path.isfile(full_path) and os.path.splitext(f)[1] in ('.crt', '.pem'):\n",
   "                            if full_path not in LOADED_VERIFY_LOCATIONS:\n",
   "                                with open(full_path, 'rb') as cert_file:\n",
   "                                    b_cert = cert_file.read()\n",
   "                                if HAS_SSLCONTEXT:\n",
   "                                        cadata.extend(\n",
   "                                                to_native(b_cert, errors='surrogate_or_strict')\n",
   "                                    os.write(tmp_fd, b_cert)\n",
   "                                    os.write(tmp_fd, b'\\n')\n",
   "        if HAS_SSLCONTEXT:\n",
   "            default_verify_paths = ssl.get_default_verify_paths()\n",
   "            paths_checked[:0] = [default_verify_paths.capath]\n",
   "        return (tmp_path, cadata, paths_checked)\n",
   "        valid_codes = [200] if valid_codes is None else valid_codes\n",
   "            (http_version, resp_code, msg) = re.match(br'(HTTP/\\d\\.\\d) (\\d\\d\\d) (.*)', response).groups()\n",
   "            if int(resp_code) not in valid_codes:\n",
   "        env_no_proxy = os.environ.get('no_proxy')\n",
   "        if env_no_proxy:\n",
   "            env_no_proxy = env_no_proxy.split(',')\n",
   "            netloc = urlparse(url).netloc\n",
   "            for host in env_no_proxy:\n",
   "                if netloc.endswith(host) or netloc.split(':')[0].endswith(host):\n",
   "        cafile = self.ca_path or cafile\n",
   "            cadata = None\n",
   "            cadata = cadata or None\n",
   "        if HAS_SSLCONTEXT:\n",
   "            context = create_default_context(cafile=cafile)\n",
   "        elif HAS_URLLIB3_PYOPENSSLCONTEXT:\n",
   "            context = PyOpenSSLContext(PROTOCOL)\n",
   "        if cafile or cadata:\n",
   "            context.load_verify_locations(cafile=cafile, cadata=cadata)\n",
   "        return context\n",
   "        tmp_ca_cert_path, cadata, paths_checked = self.get_ca_certs()\n",
   "        use_proxy = self.detect_no_proxy(req.get_full_url())\n",
   "        https_proxy = os.environ.get('https_proxy')\n",
   "        context = None\n",
   "            context = self.make_context(tmp_ca_cert_path, cadata)\n",
   "            if use_proxy and https_proxy:\n",
   "                proxy_parts = generic_urlparse(urlparse(https_proxy))\n",
   "                port = proxy_parts.get('port') or 443\n",
   "                proxy_hostname = proxy_parts.get('hostname', None)\n",
   "                if proxy_hostname is None or proxy_parts.get('scheme') == '':\n",
   "                s = socket.create_connection((proxy_hostname, port))\n",
   "                if proxy_parts.get('scheme') == 'http':\n",
   "                    s.sendall(to_bytes(self.CONNECT_COMMAND % (self.hostname, self.port), errors='surrogate_or_strict'))\n",
   "                    if proxy_parts.get('username'):\n",
   "                        credentials = \"%s:%s\" % (proxy_parts.get('username', ''), proxy_parts.get('password', ''))\n",
   "                        s.sendall(b'Proxy-Authorization: Basic %s\\r\\n' % base64.b64encode(to_bytes(credentials, errors='surrogate_or_strict')).strip())\n",
   "                    s.sendall(b'\\r\\n')\n",
   "                    connect_result = b\"\"\n",
   "                    while connect_result.find(b\"\\r\\n\\r\\n\") <= 0:\n",
   "                        connect_result += s.recv(4096)\n",
   "                        if len(connect_result) > 131072:\n",
   "                    self.validate_proxy_response(connect_result)\n",
   "                    if context:\n",
   "                        ssl_s = context.wrap_socket(s, server_hostname=self.hostname)\n",
   "                    elif HAS_URLLIB3_SSL_WRAP_SOCKET:\n",
   "                        ssl_s = ssl_wrap_socket(s, ca_certs=tmp_ca_cert_path, cert_reqs=ssl.CERT_REQUIRED, ssl_version=PROTOCOL, server_hostname=self.hostname)\n",
   "                        ssl_s = ssl.wrap_socket(s, ca_certs=tmp_ca_cert_path, cert_reqs=ssl.CERT_REQUIRED, ssl_version=PROTOCOL)\n",
   "                        match_hostname(ssl_s.getpeercert(), self.hostname)\n",
   "                    raise ProxyError('Unsupported proxy scheme: %s. Currently ansible only supports HTTP proxies.' % proxy_parts.get('scheme'))\n",
   "                s = socket.create_connection((self.hostname, self.port))\n",
   "                if context:\n",
   "                    ssl_s = context.wrap_socket(s, server_hostname=self.hostname)\n",
   "                elif HAS_URLLIB3_SSL_WRAP_SOCKET:\n",
   "                    ssl_s = ssl_wrap_socket(s, ca_certs=tmp_ca_cert_path, cert_reqs=ssl.CERT_REQUIRED, ssl_version=PROTOCOL, server_hostname=self.hostname)\n",
   "                    ssl_s = ssl.wrap_socket(s, ca_certs=tmp_ca_cert_path, cert_reqs=ssl.CERT_REQUIRED, ssl_version=PROTOCOL)\n",
   "                    match_hostname(ssl_s.getpeercert(), self.hostname)\n",
   "            s.close()\n",
   "        except (ssl.SSLError, CertificateError) as e:\n",
   "            build_ssl_validation_error(self.hostname, self.port, paths_checked, e)\n",
   "    https_request = http_request\n",
   "def maybe_add_ssl_handler(url, validate_certs, ca_path=None):\n",
   "    parsed = generic_urlparse(urlparse(url))\n",
   "    if parsed.scheme == 'https' and validate_certs:\n",
   "        if not HAS_SSL:\n",
   "        return SSLValidationHandler(parsed.hostname, parsed.port or 443, ca_path=ca_path)\n",
   "def rfc2822_date_string(timetuple, zone='-0000'):\n",
   "        timetuple[0], timetuple[3], timetuple[4], timetuple[5],\n",
   "        zone)\n",
   "        self.headers = headers or {}\n",
   "        self.use_proxy = use_proxy\n",
   "        if value is None:\n",
   "        return value\n",
   "    def open(self, method, url, data=None, headers=None, use_proxy=None,\n",
   "        method = method.upper()\n",
   "        if headers is None:\n",
   "            headers = {}\n",
   "        elif not isinstance(headers, dict):\n",
   "        headers = dict(self.headers, **headers)\n",
   "        use_proxy = self._fallback(use_proxy, self.use_proxy)\n",
   "        force = self._fallback(force, self.force)\n",
   "        timeout = self._fallback(timeout, self.timeout)\n",
   "        validate_certs = self._fallback(validate_certs, self.validate_certs)\n",
   "        url_username = self._fallback(url_username, self.url_username)\n",
   "        url_password = self._fallback(url_password, self.url_password)\n",
   "        http_agent = self._fallback(http_agent, self.http_agent)\n",
   "        force_basic_auth = self._fallback(force_basic_auth, self.force_basic_auth)\n",
   "        follow_redirects = self._fallback(follow_redirects, self.follow_redirects)\n",
   "        client_cert = self._fallback(client_cert, self.client_cert)\n",
   "        client_key = self._fallback(client_key, self.client_key)\n",
   "        cookies = self._fallback(cookies, self.cookies)\n",
   "        unix_socket = self._fallback(unix_socket, self.unix_socket)\n",
   "        ca_path = self._fallback(ca_path, self.ca_path)\n",
   "        handlers = []\n",
   "        if unix_socket:\n",
   "            handlers.append(UnixHTTPHandler(unix_socket))\n",
   "        ssl_handler = maybe_add_ssl_handler(url, validate_certs, ca_path=ca_path)\n",
   "        if ssl_handler and not HAS_SSLCONTEXT:\n",
   "            handlers.append(ssl_handler)\n",
   "        if HAS_GSSAPI and use_gssapi:\n",
   "            handlers.append(urllib_gssapi.HTTPSPNEGOAuthHandler())\n",
   "        parsed = generic_urlparse(urlparse(url))\n",
   "        if parsed.scheme != 'ftp':\n",
   "            username = url_username\n",
   "            if username:\n",
   "                password = url_password\n",
   "                netloc = parsed.netloc\n",
   "            elif '@' in parsed.netloc:\n",
   "                credentials, netloc = parsed.netloc.split('@', 1)\n",
   "                if ':' in credentials:\n",
   "                    username, password = credentials.split(':', 1)\n",
   "                    username = credentials\n",
   "                    password = ''\n",
   "                parsed_list = parsed.as_list()\n",
   "                parsed_list[1] = netloc\n",
   "                url = urlunparse(parsed_list)\n",
   "            if username and not force_basic_auth:\n",
   "                passman = urllib_request.HTTPPasswordMgrWithDefaultRealm()\n",
   "                passman.add_password(None, netloc, username, password)\n",
   "                authhandler = urllib_request.HTTPBasicAuthHandler(passman)\n",
   "                digest_authhandler = urllib_request.HTTPDigestAuthHandler(passman)\n",
   "                handlers.append(authhandler)\n",
   "                handlers.append(digest_authhandler)\n",
   "            elif username and force_basic_auth:\n",
   "                headers[\"Authorization\"] = basic_auth_header(username, password)\n",
   "                    rc = netrc.netrc(os.environ.get('NETRC'))\n",
   "                    login = rc.authenticators(parsed.hostname)\n",
   "                    login = None\n",
   "                if login:\n",
   "                    username, _, password = login\n",
   "                    if username and password:\n",
   "                        headers[\"Authorization\"] = basic_auth_header(username, password)\n",
   "        if not use_proxy:\n",
   "            proxyhandler = urllib_request.ProxyHandler({})\n",
   "            handlers.append(proxyhandler)\n",
   "        context = None\n",
   "        if HAS_SSLCONTEXT and not validate_certs:\n",
   "            context = SSLContext(ssl.PROTOCOL_SSLv23)\n",
   "                context.options |= ssl.OP_NO_SSLv2\n",
   "            context.options |= ssl.OP_NO_SSLv3\n",
   "            context.verify_mode = ssl.CERT_NONE\n",
   "            context.check_hostname = False\n",
   "            handlers.append(HTTPSClientAuthHandler(client_cert=client_cert,\n",
   "                                                   client_key=client_key,\n",
   "                                                   context=context,\n",
   "                                                   unix_socket=unix_socket))\n",
   "        elif client_cert or unix_socket:\n",
   "            handlers.append(HTTPSClientAuthHandler(client_cert=client_cert,\n",
   "                                                   client_key=client_key,\n",
   "                                                   unix_socket=unix_socket))\n",
   "        if ssl_handler and HAS_SSLCONTEXT and validate_certs:\n",
   "            tmp_ca_path, cadata, paths_checked = ssl_handler.get_ca_certs()\n",
   "                context = ssl_handler.make_context(tmp_ca_path, cadata)\n",
   "        if hasattr(socket, 'create_connection') and CustomHTTPSHandler:\n",
   "            kwargs = {}\n",
   "            if HAS_SSLCONTEXT:\n",
   "                kwargs['context'] = context\n",
   "            handlers.append(CustomHTTPSHandler(**kwargs))\n",
   "        handlers.append(RedirectHandlerFactory(follow_redirects, validate_certs, ca_path=ca_path))\n",
   "        if cookies is not None:\n",
   "            handlers.append(urllib_request.HTTPCookieProcessor(cookies))\n",
   "        opener = urllib_request.build_opener(*handlers)\n",
   "        urllib_request.install_opener(opener)\n",
   "        data = to_bytes(data, nonstring='passthru')\n",
   "        request = RequestWithMethod(url, method, data)\n",
   "        if http_agent:\n",
   "            request.add_header('User-agent', http_agent)\n",
   "        if force:\n",
   "            request.add_header('cache-control', 'no-cache')\n",
   "            tstamp = rfc2822_date_string(last_mod_time.timetuple(), 'GMT')\n",
   "            request.add_header('If-Modified-Since', tstamp)\n",
   "        unredirected_headers = unredirected_headers or []\n",
   "        for header in headers:\n",
   "            if header in unredirected_headers:\n",
   "                request.add_unredirected_header(header, headers[header])\n",
   "                request.add_header(header, headers[header])\n",
   "        return urllib_request.urlopen(request, None, timeout)\n",
   "        return self.open('GET', url, **kwargs)\n",
   "        return self.open('OPTIONS', url, **kwargs)\n",
   "        return self.open('HEAD', url, **kwargs)\n",
   "        return self.open('POST', url, data=data, **kwargs)\n",
   "        return self.open('PUT', url, data=data, **kwargs)\n",
   "        return self.open('PATCH', url, data=data, **kwargs)\n",
   "        return self.open('DELETE', url, **kwargs)\n",
   "def open_url(url, data=None, headers=None, method=None, use_proxy=True,\n",
   "    method = method or ('POST' if data else 'GET')\n",
   "    return Request().open(method, url, data=data, headers=headers, use_proxy=use_proxy,\n",
   "                          force=force, last_mod_time=last_mod_time, timeout=timeout, validate_certs=validate_certs,\n",
   "                          url_username=url_username, url_password=url_password, http_agent=http_agent,\n",
   "                          force_basic_auth=force_basic_auth, follow_redirects=follow_redirects,\n",
   "                          client_cert=client_cert, client_key=client_key, cookies=cookies,\n",
   "                          use_gssapi=use_gssapi, unix_socket=unix_socket, ca_path=ca_path,\n",
   "                          unredirected_headers=unredirected_headers)\n",
   "    m = email.mime.multipart.MIMEMultipart('form-data')\n",
   "    for field, value in sorted(fields.items()):\n",
   "        if isinstance(value, string_types):\n",
   "            main_type = 'text'\n",
   "            sub_type = 'plain'\n",
   "            content = value\n",
   "            filename = None\n",
   "        elif isinstance(value, Mapping):\n",
   "            filename = value.get('filename')\n",
   "            content = value.get('content')\n",
   "            if not any((filename, content)):\n",
   "            mime = value.get('mime_type')\n",
   "            if not mime:\n",
   "                    mime = mimetypes.guess_type(filename or '', strict=False)[0] or 'application/octet-stream'\n",
   "                    mime = 'application/octet-stream'\n",
   "            main_type, sep, sub_type = mime.partition('/')\n",
   "                'value must be a string, or mapping, cannot be type %s' % value.__class__.__name__\n",
   "        if not content and filename:\n",
   "            with open(to_bytes(filename, errors='surrogate_or_strict'), 'rb') as f:\n",
   "                part = email.mime.application.MIMEApplication(f.read())\n",
   "                del part['Content-Type']\n",
   "                part.add_header('Content-Type', '%s/%s' % (main_type, sub_type))\n",
   "            part = email.mime.nonmultipart.MIMENonMultipart(main_type, sub_type)\n",
   "            part.set_payload(to_bytes(content))\n",
   "        part.add_header('Content-Disposition', 'form-data')\n",
   "        del part['MIME-Version']\n",
   "        part.set_param(\n",
   "            field,\n",
   "        if filename:\n",
   "            part.set_param(\n",
   "                to_native(os.path.basename(filename)),\n",
   "        m.attach(part)\n",
   "        b_data = m.as_bytes(policy=email.policy.HTTP)\n",
   "        fp = cStringIO()  # cStringIO seems to be required here\n",
   "        g = email.generator.Generator(fp, maxheaderlen=0)\n",
   "        g.flatten(m)\n",
   "        b_data = email.utils.fix_eols(fp.getvalue())\n",
   "    headers, sep, b_content = b_data.partition(b'\\r\\n\\r\\n')\n",
   "        parser = email.parser.BytesHeaderParser().parsebytes\n",
   "        parser = email.parser.HeaderParser().parsestr\n",
   "        parser(headers)['content-type'],  # Message converts to native strings\n",
   "        b_content\n",
   "    return b\"Basic %s\" % base64.b64encode(to_bytes(\"%s:%s\" % (username, password), errors='surrogate_or_strict'))\n",
   "def fetch_url(module, url, data=None, headers=None, method=None,\n",
   "              use_proxy=True, force=False, last_mod_time=None, timeout=10,\n",
   "    if not HAS_URLPARSE:\n",
   "    old_tempdir = tempfile.tempdir\n",
   "    validate_certs = module.params.get('validate_certs', True)\n",
   "    username = module.params.get('url_username', '')\n",
   "    password = module.params.get('url_password', '')\n",
   "    http_agent = module.params.get('http_agent', 'ansible-httpget')\n",
   "    force_basic_auth = module.params.get('force_basic_auth', '')\n",
   "    follow_redirects = module.params.get('follow_redirects', 'urllib2')\n",
   "    client_cert = module.params.get('client_cert')\n",
   "    client_key = module.params.get('client_key')\n",
   "    if not isinstance(cookies, cookiejar.CookieJar):\n",
   "        cookies = cookiejar.LWPCookieJar()\n",
   "    r = None\n",
   "    info = dict(url=url, status=-1)\n",
   "        r = open_url(url, data=data, headers=headers, method=method,\n",
   "                     use_proxy=use_proxy, force=force, last_mod_time=last_mod_time, timeout=timeout,\n",
   "                     validate_certs=validate_certs, url_username=username,\n",
   "                     url_password=password, http_agent=http_agent, force_basic_auth=force_basic_auth,\n",
   "                     follow_redirects=follow_redirects, client_cert=client_cert,\n",
   "                     client_key=client_key, cookies=cookies, use_gssapi=use_gssapi,\n",
   "                     unix_socket=unix_socket, ca_path=ca_path)\n",
   "        info.update(dict((k.lower(), v) for k, v in r.info().items()))\n",
   "            temp_headers = {}\n",
   "            for name, value in r.headers.items():\n",
   "                name = name.lower()\n",
   "                if name in temp_headers:\n",
   "                    temp_headers[name] = ', '.join((temp_headers[name], value))\n",
   "                    temp_headers[name] = value\n",
   "            info.update(temp_headers)\n",
   "        cookie_list = []\n",
   "        cookie_dict = dict()\n",
   "        for cookie in cookies:\n",
   "            cookie_dict[cookie.name] = cookie.value\n",
   "            cookie_list.append((cookie.name, cookie.value))\n",
   "        info['cookies_string'] = '; '.join('%s=%s' % c for c in cookie_list)\n",
   "        info['cookies'] = cookie_dict\n",
   "        info.update(dict(msg=\"OK (%s bytes)\" % r.headers.get('Content-Length', 'unknown'), url=r.geturl(), status=r.code))\n",
   "        distribution = get_distribution()\n",
   "        if distribution is not None and distribution.lower() == 'redhat':\n",
   "            module.fail_json(msg='%s. You can also install python-ssl from EPEL' % to_native(e), **info)\n",
   "            module.fail_json(msg='%s' % to_native(e), **info)\n",
   "        module.fail_json(msg=to_native(e), **info)\n",
   "            body = e.read()\n",
   "            body = ''\n",
   "            info.update(dict((k.lower(), v) for k, v in e.info().items()))\n",
   "        info.update({'msg': to_native(e), 'body': body, 'status': e.code})\n",
   "        code = int(getattr(e, 'code', -1))\n",
   "        info.update(dict(msg=\"Request failed: %s\" % to_native(e), status=code))\n",
   "        info.update(dict(msg=\"Connection failure: %s\" % to_native(e), status=-1))\n",
   "        info.update(dict(msg=\"Connection failure: connection was closed before a valid response was received: %s\" % to_native(e.line), status=-1))\n",
   "        info.update(dict(msg=\"An unknown error occurred: %s\" % to_native(e), status=-1),\n",
   "        tempfile.tempdir = old_tempdir\n",
   "    return r, info\n",
   "    bufsize = 65536\n",
   "    file_name, file_ext = os.path.splitext(str(url.rsplit('/', 1)[1]))\n",
   "    fetch_temp_file = tempfile.NamedTemporaryFile(dir=module.tmpdir, prefix=file_name, suffix=file_ext, delete=False)\n",
   "    module.add_cleanup_file(fetch_temp_file.name)\n",
   "        rsp, info = fetch_url(module, url, data, headers, method, use_proxy, force, last_mod_time, timeout)\n",
   "        if not rsp:\n",
   "            module.fail_json(msg=\"Failure downloading %s, %s\" % (url, info['msg']))\n",
   "        data = rsp.read(bufsize)\n",
   "        while data:\n",
   "            fetch_temp_file.write(data)\n",
   "            data = rsp.read(bufsize)\n",
   "        fetch_temp_file.close()\n",
   "        module.fail_json(msg=\"Failure downloading %s, %s\" % (url, to_native(e)))\n",
   "    return fetch_temp_file.name\n"
  ]
 },
 "174": {
  "name": "result",
  "type": "builtin_function_or_method|module|type|function",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/six/__init__.py",
  "lineno": "102",
  "column": "8",
  "context": "e = name\n\n    def __get__(self, obj, tp):\n        result = self._resolve()\n        setattr(obj, self.name, result)  # Invokes",
  "context_lines": "class _LazyDescr(object):\n\n    def __init__(self, name):\n        self.name = name\n\n    def __get__(self, obj, tp):\n        result = self._resolve()\n        setattr(obj, self.name, result)  # Invokes __set__.\n        try:\n            # This is a bit ugly, but it avoids running this again by\n            # removing this descriptor.\n",
  "slicing": [
   "PY3 = sys.version_info[0] == 3\n",
   "if PY3:\n",
   "        result = self._resolve()\n",
   "        setattr(obj, self.name, result)  # Invokes __set__.\n",
   "        return result\n",
   "        if PY3:\n",
   "        if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "    elif PY3 and isinstance(s, binary_type):\n"
  ]
 },
 "175": {
  "name": "new",
  "type": "str",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/module_utils/six/__init__.py",
  "lineno": "119",
  "column": "16",
  "context": " PY3:\n            if new is None:\n                new = name\n            self.mod = new\n        else:\n         ",
  "context_lines": "    def __init__(self, name, old, new=None):\n        super(MovedModule, self).__init__(name)\n        if PY3:\n            if new is None:\n                new = name\n            self.mod = new\n        else:\n            self.mod = old\n\n    def _resolve(self):\n",
  "slicing": [
   "PY3 = sys.version_info[0] == 3\n",
   "if PY3:\n",
   "        result = self._resolve()\n",
   "        setattr(obj, self.name, result)  # Invokes __set__.\n",
   "        return result\n",
   "        if PY3:\n",
   "                new = name\n",
   "            self.mod = new\n",
   "        if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "    elif PY3 and isinstance(s, binary_type):\n"
  ]
 },
 "176": {
  "name": "_module",
  "type": "module",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/six/__init__.py",
  "lineno": "128",
  "column": "8",
  "context": "lf.mod)\n\n    def __getattr__(self, attr):\n        _module = self._resolve()\n        value = getattr(_module, attr)\n        set",
  "context_lines": "            self.mod = old\n\n    def _resolve(self):\n        return _import_module(self.mod)\n\n    def __getattr__(self, attr):\n        _module = self._resolve()\n        value = getattr(_module, attr)\n        setattr(self, attr, value)\n        return value\n\n\nclass _LazyModule(types.ModuleType):\n\n",
  "slicing": [
   "PY3 = sys.version_info[0] == 3\n",
   "if PY3:\n",
   "def _import_module(name):\n",
   "        result = self._resolve()\n",
   "        setattr(obj, self.name, result)  # Invokes __set__.\n",
   "        return result\n",
   "        if PY3:\n",
   "                new = name\n",
   "            self.mod = new\n",
   "        _module = self._resolve()\n",
   "        value = getattr(_module, attr)\n",
   "        setattr(self, attr, value)\n",
   "        return value\n",
   "        super(_LazyModule, self).__init__(name)\n",
   "        attrs = [\"__doc__\", \"__name__\"]\n",
   "        attrs += [attr.name for attr in self._moved_attributes]\n",
   "        return attrs\n",
   "        super(MovedAttribute, self).__init__(name)\n",
   "        if PY3:\n",
   "                new_mod = name\n",
   "            self.mod = new_mod\n",
   "                    new_attr = name\n",
   "                    new_attr = old_attr\n",
   "            self.attr = new_attr\n",
   "                old_attr = name\n",
   "            self.attr = old_attr\n",
   "    setattr(_MovedItems, attr.name, attr)\n",
   "    if isinstance(attr, MovedModule):\n",
   "        _importer._add_module(attr, \"moves.\" + attr.name)\n",
   "    setattr(Module_six_moves_urllib_parse, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_error, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_request, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_response, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\n",
   "        delattr(_MovedItems, name)\n",
   "            del moves.__dict__[name]\n",
   "            raise AttributeError(\"no such move, %r\" % (name,))\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "            if value is None:\n",
   "            if value.__traceback__ is not tb:\n",
   "                raise value.with_traceback(tb)\n",
   "            raise value\n",
   "        raise value\n",
   "            return meta(name, resolved_bases, d)\n",
   "            return meta.__prepare__(name, bases)\n",
   "    elif PY3 and isinstance(s, binary_type):\n"
  ]
 },
 "177": {
  "name": "new_attr",
  "type": "str",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/module_utils/six/__init__.py",
  "lineno": "159",
  "column": "20",
  "context": "         if old_attr is None:\n                    new_attr = name\n                else:\n                    new_attr",
  "context_lines": "                new_mod = name\n            self.mod = new_mod\n            if new_attr is None:\n                if old_attr is None:\n                    new_attr = name\n                else:\n                    new_attr = old_attr\n            self.attr = new_attr\n        else:\n",
  "slicing": [
   "PY3 = sys.version_info[0] == 3\n",
   "if PY3:\n",
   "def _import_module(name):\n",
   "        result = self._resolve()\n",
   "        setattr(obj, self.name, result)  # Invokes __set__.\n",
   "        return result\n",
   "        if PY3:\n",
   "                new = name\n",
   "            self.mod = new\n",
   "        _module = self._resolve()\n",
   "        value = getattr(_module, attr)\n",
   "        setattr(self, attr, value)\n",
   "        return value\n",
   "        super(_LazyModule, self).__init__(name)\n",
   "        attrs = [\"__doc__\", \"__name__\"]\n",
   "        attrs += [attr.name for attr in self._moved_attributes]\n",
   "        return attrs\n",
   "        super(MovedAttribute, self).__init__(name)\n",
   "        if PY3:\n",
   "                new_mod = name\n",
   "            self.mod = new_mod\n",
   "                    new_attr = name\n",
   "                    new_attr = old_attr\n",
   "            self.attr = new_attr\n",
   "                old_attr = name\n",
   "            self.attr = old_attr\n",
   "    setattr(_MovedItems, attr.name, attr)\n",
   "    if isinstance(attr, MovedModule):\n",
   "        _importer._add_module(attr, \"moves.\" + attr.name)\n",
   "    setattr(Module_six_moves_urllib_parse, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_error, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_request, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_response, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\n",
   "        delattr(_MovedItems, name)\n",
   "            del moves.__dict__[name]\n",
   "            raise AttributeError(\"no such move, %r\" % (name,))\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "            if value is None:\n",
   "            if value.__traceback__ is not tb:\n",
   "                raise value.with_traceback(tb)\n",
   "            raise value\n",
   "        raise value\n",
   "            return meta(name, resolved_bases, d)\n",
   "            return meta.__prepare__(name, bases)\n",
   "    elif PY3 and isinstance(s, binary_type):\n"
  ]
 },
 "178": {
  "name": "new_attr",
  "type": "str",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/module_utils/six/__init__.py",
  "lineno": "161",
  "column": "20",
  "context": " = name\n                else:\n                    new_attr = old_attr\n            self.attr = new_attr\n        else:\n   ",
  "context_lines": "            if new_attr is None:\n                if old_attr is None:\n                    new_attr = name\n                else:\n                    new_attr = old_attr\n            self.attr = new_attr\n        else:\n            self.mod = old_mod\n            if old_attr is None:\n",
  "slicing": [
   "PY3 = sys.version_info[0] == 3\n",
   "if PY3:\n",
   "def _import_module(name):\n",
   "        result = self._resolve()\n",
   "        setattr(obj, self.name, result)  # Invokes __set__.\n",
   "        return result\n",
   "        if PY3:\n",
   "                new = name\n",
   "            self.mod = new\n",
   "        _module = self._resolve()\n",
   "        value = getattr(_module, attr)\n",
   "        setattr(self, attr, value)\n",
   "        return value\n",
   "        super(_LazyModule, self).__init__(name)\n",
   "        attrs = [\"__doc__\", \"__name__\"]\n",
   "        attrs += [attr.name for attr in self._moved_attributes]\n",
   "        return attrs\n",
   "        super(MovedAttribute, self).__init__(name)\n",
   "        if PY3:\n",
   "                new_mod = name\n",
   "            self.mod = new_mod\n",
   "                    new_attr = name\n",
   "                    new_attr = old_attr\n",
   "            self.attr = new_attr\n",
   "                old_attr = name\n",
   "            self.attr = old_attr\n",
   "    setattr(_MovedItems, attr.name, attr)\n",
   "    if isinstance(attr, MovedModule):\n",
   "        _importer._add_module(attr, \"moves.\" + attr.name)\n",
   "    setattr(Module_six_moves_urllib_parse, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_error, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_request, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_response, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\n",
   "        delattr(_MovedItems, name)\n",
   "            del moves.__dict__[name]\n",
   "            raise AttributeError(\"no such move, %r\" % (name,))\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "            if value is None:\n",
   "            if value.__traceback__ is not tb:\n",
   "                raise value.with_traceback(tb)\n",
   "            raise value\n",
   "        raise value\n",
   "            return meta(name, resolved_bases, d)\n",
   "            return meta.__prepare__(name, bases)\n",
   "    elif PY3 and isinstance(s, binary_type):\n"
  ]
 },
 "179": {
  "name": "module",
  "type": "module",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/module_utils/six/__init__.py",
  "lineno": "170",
  "column": "8",
  "context": ".attr = old_attr\n\n    def _resolve(self):\n        module = _import_module(self.mod)\n        return getattr(module, self.attr)\n\n\nclass ",
  "context_lines": "            if old_attr is None:\n                old_attr = name\n            self.attr = old_attr\n\n    def _resolve(self):\n        module = _import_module(self.mod)\n        return getattr(module, self.attr)\n\n\nclass _SixMetaPathImporter(object):\n\n    \"\"\"\n    A meta path importer to import six.moves and its submodules.\n\n",
  "slicing": [
   "PY3 = sys.version_info[0] == 3\n",
   "if PY3:\n",
   "def _import_module(name):\n",
   "        result = self._resolve()\n",
   "        setattr(obj, self.name, result)  # Invokes __set__.\n",
   "        return result\n",
   "        if PY3:\n",
   "                new = name\n",
   "            self.mod = new\n",
   "        _module = self._resolve()\n",
   "        value = getattr(_module, attr)\n",
   "        setattr(self, attr, value)\n",
   "        return value\n",
   "        super(_LazyModule, self).__init__(name)\n",
   "        attrs = [\"__doc__\", \"__name__\"]\n",
   "        attrs += [attr.name for attr in self._moved_attributes]\n",
   "        return attrs\n",
   "        super(MovedAttribute, self).__init__(name)\n",
   "        if PY3:\n",
   "                new_mod = name\n",
   "            self.mod = new_mod\n",
   "                    new_attr = name\n",
   "                    new_attr = old_attr\n",
   "            self.attr = new_attr\n",
   "                old_attr = name\n",
   "            self.attr = old_attr\n",
   "        module = _import_module(self.mod)\n",
   "        return getattr(module, self.attr)\n",
   "    setattr(_MovedItems, attr.name, attr)\n",
   "    if isinstance(attr, MovedModule):\n",
   "        _importer._add_module(attr, \"moves.\" + attr.name)\n",
   "    setattr(Module_six_moves_urllib_parse, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_error, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_request, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_response, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\n",
   "        delattr(_MovedItems, name)\n",
   "            del moves.__dict__[name]\n",
   "            raise AttributeError(\"no such move, %r\" % (name,))\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "            if value is None:\n",
   "            if value.__traceback__ is not tb:\n",
   "                raise value.with_traceback(tb)\n",
   "            raise value\n",
   "        raise value\n",
   "            return meta(name, resolved_bases, d)\n",
   "            return meta.__prepare__(name, bases)\n",
   "    elif PY3 and isinstance(s, binary_type):\n"
  ]
 },
 "180": {
  "name": "mod",
  "type": "Module_six_moves_urllib_error",
  "class": "customized",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/six/__init__.py",
  "lineno": "211",
  "column": "8",
  "context": "        except KeyError:\n            pass\n        mod = self.__get_module(fullname)\n        if isinstance(mod, MovedModule):\n         ",
  "context_lines": "            # in case of a reload\n            return sys.modules[fullname]\n        except KeyError:\n            pass\n        mod = self.__get_module(fullname)\n        if isinstance(mod, MovedModule):\n            mod = mod._resolve()\n        else:\n            mod.__loader__ = self\n",
  "slicing": [
   "PY3 = sys.version_info[0] == 3\n",
   "if PY3:\n",
   "def _import_module(name):\n",
   "        result = self._resolve()\n",
   "        setattr(obj, self.name, result)  # Invokes __set__.\n",
   "        return result\n",
   "        if PY3:\n",
   "                new = name\n",
   "            self.mod = new\n",
   "        _module = self._resolve()\n",
   "        value = getattr(_module, attr)\n",
   "        setattr(self, attr, value)\n",
   "        return value\n",
   "        super(_LazyModule, self).__init__(name)\n",
   "        attrs = [\"__doc__\", \"__name__\"]\n",
   "        attrs += [attr.name for attr in self._moved_attributes]\n",
   "        return attrs\n",
   "        super(MovedAttribute, self).__init__(name)\n",
   "        if PY3:\n",
   "                new_mod = name\n",
   "            self.mod = new_mod\n",
   "                    new_attr = name\n",
   "                    new_attr = old_attr\n",
   "            self.attr = new_attr\n",
   "                old_attr = name\n",
   "            self.attr = old_attr\n",
   "        module = _import_module(self.mod)\n",
   "        return getattr(module, self.attr)\n",
   "        for fullname in fullnames:\n",
   "            self.known_modules[self.name + \".\" + fullname] = mod\n",
   "        return self.known_modules[self.name + \".\" + fullname]\n",
   "        if fullname in self.known_modules:\n",
   "            return self.known_modules[fullname]\n",
   "            raise ImportError(\"This loader does not know module \" + fullname)\n",
   "            return sys.modules[fullname]\n",
   "        mod = self.__get_module(fullname)\n",
   "        if isinstance(mod, MovedModule):\n",
   "            mod = mod._resolve()\n",
   "            mod.__loader__ = self\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "        return hasattr(self.__get_module(fullname), \"__path__\")\n",
   "        self.__get_module(fullname)  # eventually raises ImportError\n",
   "    setattr(_MovedItems, attr.name, attr)\n",
   "    if isinstance(attr, MovedModule):\n",
   "        _importer._add_module(attr, \"moves.\" + attr.name)\n",
   "    setattr(Module_six_moves_urllib_parse, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_error, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_request, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_response, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\n",
   "        delattr(_MovedItems, name)\n",
   "            del moves.__dict__[name]\n",
   "            raise AttributeError(\"no such move, %r\" % (name,))\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "            if value is None:\n",
   "            if value.__traceback__ is not tb:\n",
   "                raise value.with_traceback(tb)\n",
   "            raise value\n",
   "        raise value\n",
   "            return meta(name, resolved_bases, d)\n",
   "            return meta.__prepare__(name, bases)\n",
   "    elif PY3 and isinstance(s, binary_type):\n"
  ]
 },
 "181": {
  "name": "mod",
  "type": "module",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/six/__init__.py",
  "lineno": "213",
  "column": "12",
  "context": "     if isinstance(mod, MovedModule):\n            mod = mod._resolve()\n        else:\n            mod.__loader__ = self\n  ",
  "context_lines": "        except KeyError:\n            pass\n        mod = self.__get_module(fullname)\n        if isinstance(mod, MovedModule):\n            mod = mod._resolve()\n        else:\n            mod.__loader__ = self\n        sys.modules[fullname] = mod\n        return mod\n\n",
  "slicing": [
   "PY3 = sys.version_info[0] == 3\n",
   "if PY3:\n",
   "def _import_module(name):\n",
   "        result = self._resolve()\n",
   "        setattr(obj, self.name, result)  # Invokes __set__.\n",
   "        return result\n",
   "        if PY3:\n",
   "                new = name\n",
   "            self.mod = new\n",
   "        _module = self._resolve()\n",
   "        value = getattr(_module, attr)\n",
   "        setattr(self, attr, value)\n",
   "        return value\n",
   "        super(_LazyModule, self).__init__(name)\n",
   "        attrs = [\"__doc__\", \"__name__\"]\n",
   "        attrs += [attr.name for attr in self._moved_attributes]\n",
   "        return attrs\n",
   "        super(MovedAttribute, self).__init__(name)\n",
   "        if PY3:\n",
   "                new_mod = name\n",
   "            self.mod = new_mod\n",
   "                    new_attr = name\n",
   "                    new_attr = old_attr\n",
   "            self.attr = new_attr\n",
   "                old_attr = name\n",
   "            self.attr = old_attr\n",
   "        module = _import_module(self.mod)\n",
   "        return getattr(module, self.attr)\n",
   "        for fullname in fullnames:\n",
   "            self.known_modules[self.name + \".\" + fullname] = mod\n",
   "        return self.known_modules[self.name + \".\" + fullname]\n",
   "        if fullname in self.known_modules:\n",
   "            return self.known_modules[fullname]\n",
   "            raise ImportError(\"This loader does not know module \" + fullname)\n",
   "            return sys.modules[fullname]\n",
   "        mod = self.__get_module(fullname)\n",
   "        if isinstance(mod, MovedModule):\n",
   "            mod = mod._resolve()\n",
   "            mod.__loader__ = self\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "        return hasattr(self.__get_module(fullname), \"__path__\")\n",
   "        self.__get_module(fullname)  # eventually raises ImportError\n",
   "    setattr(_MovedItems, attr.name, attr)\n",
   "    if isinstance(attr, MovedModule):\n",
   "        _importer._add_module(attr, \"moves.\" + attr.name)\n",
   "    setattr(Module_six_moves_urllib_parse, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_error, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_request, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_response, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\n",
   "        delattr(_MovedItems, name)\n",
   "            del moves.__dict__[name]\n",
   "            raise AttributeError(\"no such move, %r\" % (name,))\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "            if value is None:\n",
   "            if value.__traceback__ is not tb:\n",
   "                raise value.with_traceback(tb)\n",
   "            raise value\n",
   "        raise value\n",
   "            return meta(name, resolved_bases, d)\n",
   "            return meta.__prepare__(name, bases)\n",
   "    elif PY3 and isinstance(s, binary_type):\n"
  ]
 },
 "182": {
  "name": "get_source",
  "type": "function",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/module_utils/six/__init__.py",
  "lineno": "234",
  "column": "4",
  "context": "tually raises ImportError\n        return None\n    get_source = get_code  # same as get_code\n\n\n_importer = _SixMetaPathImporter(__name__)\n\n\ncla",
  "context_lines": "        \"\"\"Return None\n\n        Required, if is_package is implemented\"\"\"\n        self.__get_module(fullname)  # eventually raises ImportError\n        return None\n    get_source = get_code  # same as get_code\n\n\n_importer = _SixMetaPathImporter(__name__)\n\n\nclass _MovedItems(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects\"\"\"\n",
  "slicing": [
   "PY3 = sys.version_info[0] == 3\n",
   "if PY3:\n",
   "def _import_module(name):\n",
   "        result = self._resolve()\n",
   "        setattr(obj, self.name, result)  # Invokes __set__.\n",
   "        return result\n",
   "        if PY3:\n",
   "                new = name\n",
   "            self.mod = new\n",
   "        _module = self._resolve()\n",
   "        value = getattr(_module, attr)\n",
   "        setattr(self, attr, value)\n",
   "        return value\n",
   "        super(_LazyModule, self).__init__(name)\n",
   "        attrs = [\"__doc__\", \"__name__\"]\n",
   "        attrs += [attr.name for attr in self._moved_attributes]\n",
   "        return attrs\n",
   "        super(MovedAttribute, self).__init__(name)\n",
   "        if PY3:\n",
   "                new_mod = name\n",
   "            self.mod = new_mod\n",
   "                    new_attr = name\n",
   "                    new_attr = old_attr\n",
   "            self.attr = new_attr\n",
   "                old_attr = name\n",
   "            self.attr = old_attr\n",
   "        module = _import_module(self.mod)\n",
   "        return getattr(module, self.attr)\n",
   "        for fullname in fullnames:\n",
   "            self.known_modules[self.name + \".\" + fullname] = mod\n",
   "        return self.known_modules[self.name + \".\" + fullname]\n",
   "        if fullname in self.known_modules:\n",
   "            return self.known_modules[fullname]\n",
   "            raise ImportError(\"This loader does not know module \" + fullname)\n",
   "            return sys.modules[fullname]\n",
   "        mod = self.__get_module(fullname)\n",
   "        if isinstance(mod, MovedModule):\n",
   "            mod = mod._resolve()\n",
   "            mod.__loader__ = self\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "        return hasattr(self.__get_module(fullname), \"__path__\")\n",
   "        self.__get_module(fullname)  # eventually raises ImportError\n",
   "    get_source = get_code  # same as get_code\n",
   "    setattr(_MovedItems, attr.name, attr)\n",
   "    if isinstance(attr, MovedModule):\n",
   "        _importer._add_module(attr, \"moves.\" + attr.name)\n",
   "    setattr(Module_six_moves_urllib_parse, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_error, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_request, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_response, attr.name, attr)\n",
   "    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\n",
   "        delattr(_MovedItems, name)\n",
   "            del moves.__dict__[name]\n",
   "            raise AttributeError(\"no such move, %r\" % (name,))\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "if PY3:\n",
   "            if value is None:\n",
   "            if value.__traceback__ is not tb:\n",
   "                raise value.with_traceback(tb)\n",
   "            raise value\n",
   "        raise value\n",
   "            return meta(name, resolved_bases, d)\n",
   "            return meta.__prepare__(name, bases)\n",
   "    elif PY3 and isinstance(s, binary_type):\n"
  ]
 },
 "183": {
  "name": "parse",
  "type": "Module_six_moves_urllib_parse",
  "class": "customized",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/six/__init__.py",
  "lineno": "492",
  "column": "4",
  "context": "space\"\"\"\n    __path__ = []  # mark as package\n    parse = _importer._get_module(\"moves.urllib_parse\")\n    error = _importer._get_module(\"moves.urllib_er",
  "context_lines": "                      \"moves.urllib_robotparser\", \"moves.urllib.robotparser\")\n\n\nclass Module_six_moves_urllib(types.ModuleType):\n\n    \"\"\"Create a six.moves.urllib namespace that resembles the Python 3 namespace\"\"\"\n    __path__ = []  # mark as package\n    parse = _importer._get_module(\"moves.urllib_parse\")\n    error = _importer._get_module(\"moves.urllib_error\")\n    request = _importer._get_module(\"moves.urllib_request\")\n    response = _importer._get_module(\"moves.urllib_response\")\n    robotparser = _importer._get_module(\"moves.urllib_robotparser\")\n\n",
  "slicing": [
   "PY2 = sys.version_info[0] == 2\n",
   "PY3 = sys.version_info[0] == 3\n",
   "PY34 = sys.version_info[0:2] >= (3, 4)\n",
   "if PY3:\n",
   "    text_type = str\n",
   "    binary_type = bytes\n",
   "    text_type = unicode\n",
   "    binary_type = str\n",
   "def _add_doc(func, doc):\n",
   "    func.__doc__ = doc\n",
   "def _import_module(name):\n",
   "        result = self._resolve()\n",
   "        setattr(obj, self.name, result)  # Invokes __set__.\n",
   "        return result\n",
   "        if PY3:\n",
   "                new = name\n",
   "            self.mod = new\n",
   "        _module = self._resolve()\n",
   "        value = getattr(_module, attr)\n",
   "        setattr(self, attr, value)\n",
   "        return value\n",
   "        super(_LazyModule, self).__init__(name)\n",
   "        attrs = [\"__doc__\", \"__name__\"]\n",
   "        attrs += [attr.name for attr in self._moved_attributes]\n",
   "        return attrs\n",
   "    _moved_attributes = []\n",
   "        super(MovedAttribute, self).__init__(name)\n",
   "        if PY3:\n",
   "                new_mod = name\n",
   "            self.mod = new_mod\n",
   "                    new_attr = name\n",
   "                    new_attr = old_attr\n",
   "            self.attr = new_attr\n",
   "                old_attr = name\n",
   "            self.attr = old_attr\n",
   "        module = _import_module(self.mod)\n",
   "        return getattr(module, self.attr)\n",
   "        for fullname in fullnames:\n",
   "            self.known_modules[self.name + \".\" + fullname] = mod\n",
   "        return self.known_modules[self.name + \".\" + fullname]\n",
   "        if fullname in self.known_modules:\n",
   "            return self.known_modules[fullname]\n",
   "            raise ImportError(\"This loader does not know module \" + fullname)\n",
   "            return sys.modules[fullname]\n",
   "        mod = self.__get_module(fullname)\n",
   "        if isinstance(mod, MovedModule):\n",
   "            mod = mod._resolve()\n",
   "            mod.__loader__ = self\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "        return hasattr(self.__get_module(fullname), \"__path__\")\n",
   "        self.__get_module(fullname)  # eventually raises ImportError\n",
   "_importer = _SixMetaPathImporter(__name__)\n",
   "_moved_attributes = [\n",
   "    MovedAttribute(\"reload_module\", \"__builtin__\", \"importlib\" if PY34 else \"imp\", \"reload\"),\n",
   "    _moved_attributes += [\n",
   "for attr in _moved_attributes:\n",
   "    setattr(_MovedItems, attr.name, attr)\n",
   "    if isinstance(attr, MovedModule):\n",
   "        _importer._add_module(attr, \"moves.\" + attr.name)\n",
   "_MovedItems._moved_attributes = _moved_attributes\n",
   "moves = _MovedItems(__name__ + \".moves\")\n",
   "_importer._add_module(moves, \"moves\")\n",
   "_urllib_parse_moved_attributes = [\n",
   "for attr in _urllib_parse_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_parse, attr.name, attr)\n",
   "Module_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_parse(__name__ + \".moves.urllib_parse\"),\n",
   "_urllib_error_moved_attributes = [\n",
   "for attr in _urllib_error_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_error, attr.name, attr)\n",
   "Module_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_error(__name__ + \".moves.urllib.error\"),\n",
   "_urllib_request_moved_attributes = [\n",
   "for attr in _urllib_request_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_request, attr.name, attr)\n",
   "Module_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_request(__name__ + \".moves.urllib.request\"),\n",
   "_urllib_response_moved_attributes = [\n",
   "for attr in _urllib_response_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_response, attr.name, attr)\n",
   "Module_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_response(__name__ + \".moves.urllib.response\"),\n",
   "_urllib_robotparser_moved_attributes = [\n",
   "for attr in _urllib_robotparser_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\n",
   "Module_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + \".moves.urllib.robotparser\"),\n",
   "    parse = _importer._get_module(\"moves.urllib_parse\")\n",
   "    error = _importer._get_module(\"moves.urllib_error\")\n",
   "    request = _importer._get_module(\"moves.urllib_request\")\n",
   "    response = _importer._get_module(\"moves.urllib_response\")\n",
   "    robotparser = _importer._get_module(\"moves.urllib_robotparser\")\n",
   "_importer._add_module(Module_six_moves_urllib(__name__ + \".moves.urllib\"),\n",
   "        delattr(_MovedItems, name)\n",
   "            del moves.__dict__[name]\n",
   "            raise AttributeError(\"no such move, %r\" % (name,))\n",
   "if PY3:\n",
   "    _meth_func = \"__func__\"\n",
   "    _meth_self = \"__self__\"\n",
   "    _func_closure = \"__closure__\"\n",
   "    _func_code = \"__code__\"\n",
   "    _func_defaults = \"__defaults__\"\n",
   "    _func_globals = \"__globals__\"\n",
   "    _meth_func = \"im_func\"\n",
   "    _meth_self = \"im_self\"\n",
   "    _func_closure = \"func_closure\"\n",
   "    _func_code = \"func_code\"\n",
   "    _func_defaults = \"func_defaults\"\n",
   "    _func_globals = \"func_globals\"\n",
   "    advance_iterator = next\n",
   "next = advance_iterator\n",
   "    callable = callable\n",
   "        return any(\"__call__\" in klass.__dict__ for klass in type(obj).__mro__)\n",
   "if PY3:\n",
   "        return types.MethodType(func, None, cls)\n",
   "    callable = callable\n",
   "_add_doc(get_unbound_function,\n",
   "get_method_function = operator.attrgetter(_meth_func)\n",
   "get_method_self = operator.attrgetter(_meth_self)\n",
   "get_function_closure = operator.attrgetter(_func_closure)\n",
   "get_function_code = operator.attrgetter(_func_code)\n",
   "get_function_defaults = operator.attrgetter(_func_defaults)\n",
   "get_function_globals = operator.attrgetter(_func_globals)\n",
   "if PY3:\n",
   "_add_doc(iterkeys, \"Return an iterator over the keys of a dictionary.\")\n",
   "_add_doc(itervalues, \"Return an iterator over the values of a dictionary.\")\n",
   "_add_doc(iteritems,\n",
   "_add_doc(iterlists,\n",
   "if PY3:\n",
   "    unichr = chr\n",
   "    StringIO = io.StringIO\n",
   "    _assertCountEqual = \"assertCountEqual\"\n",
   "        _assertRaisesRegex = \"assertRaisesRegexp\"\n",
   "        _assertRegex = \"assertRegexpMatches\"\n",
   "        _assertRaisesRegex = \"assertRaisesRegex\"\n",
   "        _assertRegex = \"assertRegex\"\n",
   "    unichr = unichr\n",
   "    StringIO = BytesIO = StringIO.StringIO\n",
   "    _assertCountEqual = \"assertItemsEqual\"\n",
   "    _assertRaisesRegex = \"assertRaisesRegexp\"\n",
   "    _assertRegex = \"assertRegexpMatches\"\n",
   "_add_doc(b, \"\"\"Byte literal\"\"\")\n",
   "_add_doc(u, \"\"\"Text literal\"\"\")\n",
   "    return getattr(self, _assertCountEqual)(*args, **kwargs)\n",
   "    return getattr(self, _assertRaisesRegex)(*args, **kwargs)\n",
   "    return getattr(self, _assertRegex)(*args, **kwargs)\n",
   "if PY3:\n",
   "    exec_ = getattr(moves.builtins, \"exec\")\n",
   "            if value is None:\n",
   "                value = tp()\n",
   "            if value.__traceback__ is not tb:\n",
   "                raise value.with_traceback(tb)\n",
   "            raise value\n",
   "            value = None\n",
   "            tb = None\n",
   "            frame = sys._getframe(1)\n",
   "            _globs_ = frame.f_globals\n",
   "                _locs_ = frame.f_locals\n",
   "        elif _locs_ is None:\n",
   "            _locs_ = _globs_\n",
   "    exec_(\"\"\"def reraise(tp, value, tb=None):\n",
   "    exec_(\"\"\"def raise_from(value, from_value):\n",
   "    exec_(\"\"\"def raise_from(value, from_value):\n",
   "        raise value\n",
   "print_ = getattr(moves.builtins, \"print\", None)\n",
   "if print_ is None:\n",
   "        fp = kwargs.pop(\"file\", sys.stdout)\n",
   "        if fp is None:\n",
   "        def write(data):\n",
   "                data = str(data)\n",
   "            if (isinstance(fp, file) and\n",
   "                    isinstance(data, unicode) and\n",
   "                    fp.encoding is not None):\n",
   "                errors = getattr(fp, \"errors\", None)\n",
   "                if errors is None:\n",
   "                    errors = \"strict\"\n",
   "                data = data.encode(fp.encoding, errors)\n",
   "            fp.write(data)\n",
   "        want_unicode = False\n",
   "        sep = kwargs.pop(\"sep\", None)\n",
   "        if sep is not None:\n",
   "            if isinstance(sep, unicode):\n",
   "                want_unicode = True\n",
   "            elif not isinstance(sep, str):\n",
   "        end = kwargs.pop(\"end\", None)\n",
   "        if end is not None:\n",
   "            if isinstance(end, unicode):\n",
   "                want_unicode = True\n",
   "            elif not isinstance(end, str):\n",
   "        if not want_unicode:\n",
   "            for arg in args:\n",
   "                if isinstance(arg, unicode):\n",
   "                    want_unicode = True\n",
   "        if want_unicode:\n",
   "            newline = unicode(\"\\n\")\n",
   "            space = unicode(\" \")\n",
   "            newline = \"\\n\"\n",
   "            space = \" \"\n",
   "        if sep is None:\n",
   "            sep = space\n",
   "        if end is None:\n",
   "            end = newline\n",
   "        for i, arg in enumerate(args):\n",
   "            if i:\n",
   "                write(sep)\n",
   "            write(arg)\n",
   "        write(end)\n",
   "    _print = print_\n",
   "        fp = kwargs.get(\"file\", sys.stdout)\n",
   "        flush = kwargs.pop(\"flush\", False)\n",
   "        _print(*args, **kwargs)\n",
   "        if flush and fp is not None:\n",
   "            fp.flush()\n",
   "_add_doc(reraise, \"\"\"Reraise an exception.\"\"\")\n",
   "            f = functools.wraps(wrapped, assigned, updated)(f)\n",
   "            f.__wrapped__ = wrapped\n",
   "            return f\n",
   "                resolved_bases = types.resolve_bases(bases)\n",
   "                if resolved_bases is not bases:\n",
   "                resolved_bases = bases\n",
   "            return meta(name, resolved_bases, d)\n",
   "            return meta.__prepare__(name, bases)\n",
   "        orig_vars = cls.__dict__.copy()\n",
   "        slots = orig_vars.get('__slots__')\n",
   "        if slots is not None:\n",
   "            if isinstance(slots, str):\n",
   "                slots = [slots]\n",
   "            for slots_var in slots:\n",
   "                orig_vars.pop(slots_var)\n",
   "        orig_vars.pop('__dict__', None)\n",
   "        orig_vars.pop('__weakref__', None)\n",
   "            orig_vars['__qualname__'] = cls.__qualname__\n",
   "        return metaclass(cls.__name__, cls.__bases__, orig_vars)\n",
   "    if isinstance(s, text_type):\n",
   "        return s.encode(encoding, errors)\n",
   "    elif isinstance(s, binary_type):\n",
   "    if not isinstance(s, (text_type, binary_type)):\n",
   "    if PY2 and isinstance(s, text_type):\n",
   "        s = s.encode(encoding, errors)\n",
   "    elif PY3 and isinstance(s, binary_type):\n",
   "        s = s.decode(encoding, errors)\n",
   "    return s\n",
   "    if isinstance(s, binary_type):\n",
   "        return s.decode(encoding, errors)\n",
   "    elif isinstance(s, text_type):\n",
   "        return s\n",
   "        raise TypeError(\"not expecting type '%s'\" % type(s))\n",
   "    if PY2:\n",
   "        if '__str__' not in klass.__dict__:\n",
   "                             klass.__name__)\n",
   "        klass.__unicode__ = klass.__str__\n",
   "        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')\n",
   "    return klass\n",
   "            del sys.meta_path[i]\n",
   "sys.meta_path.append(_importer)\n"
  ]
 },
 "184": {
  "name": "error",
  "type": "Module_six_moves_urllib_error",
  "class": "customized",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/six/__init__.py",
  "lineno": "493",
  "column": "4",
  "context": "= _importer._get_module(\"moves.urllib_parse\")\n    error = _importer._get_module(\"moves.urllib_error\")\n    request = _importer._get_module(\"moves.urllib_",
  "context_lines": "class Module_six_moves_urllib(types.ModuleType):\n\n    \"\"\"Create a six.moves.urllib namespace that resembles the Python 3 namespace\"\"\"\n    __path__ = []  # mark as package\n    parse = _importer._get_module(\"moves.urllib_parse\")\n    error = _importer._get_module(\"moves.urllib_error\")\n    request = _importer._get_module(\"moves.urllib_request\")\n    response = _importer._get_module(\"moves.urllib_response\")\n    robotparser = _importer._get_module(\"moves.urllib_robotparser\")\n\n    def __dir__(self):\n",
  "slicing": [
   "PY2 = sys.version_info[0] == 2\n",
   "PY3 = sys.version_info[0] == 3\n",
   "PY34 = sys.version_info[0:2] >= (3, 4)\n",
   "if PY3:\n",
   "    text_type = str\n",
   "    binary_type = bytes\n",
   "    text_type = unicode\n",
   "    binary_type = str\n",
   "def _add_doc(func, doc):\n",
   "    func.__doc__ = doc\n",
   "def _import_module(name):\n",
   "        result = self._resolve()\n",
   "        setattr(obj, self.name, result)  # Invokes __set__.\n",
   "        return result\n",
   "        if PY3:\n",
   "                new = name\n",
   "            self.mod = new\n",
   "        _module = self._resolve()\n",
   "        value = getattr(_module, attr)\n",
   "        setattr(self, attr, value)\n",
   "        return value\n",
   "        super(_LazyModule, self).__init__(name)\n",
   "        attrs = [\"__doc__\", \"__name__\"]\n",
   "        attrs += [attr.name for attr in self._moved_attributes]\n",
   "        return attrs\n",
   "    _moved_attributes = []\n",
   "        super(MovedAttribute, self).__init__(name)\n",
   "        if PY3:\n",
   "                new_mod = name\n",
   "            self.mod = new_mod\n",
   "                    new_attr = name\n",
   "                    new_attr = old_attr\n",
   "            self.attr = new_attr\n",
   "                old_attr = name\n",
   "            self.attr = old_attr\n",
   "        module = _import_module(self.mod)\n",
   "        return getattr(module, self.attr)\n",
   "        for fullname in fullnames:\n",
   "            self.known_modules[self.name + \".\" + fullname] = mod\n",
   "        return self.known_modules[self.name + \".\" + fullname]\n",
   "        if fullname in self.known_modules:\n",
   "            return self.known_modules[fullname]\n",
   "            raise ImportError(\"This loader does not know module \" + fullname)\n",
   "            return sys.modules[fullname]\n",
   "        mod = self.__get_module(fullname)\n",
   "        if isinstance(mod, MovedModule):\n",
   "            mod = mod._resolve()\n",
   "            mod.__loader__ = self\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "        return hasattr(self.__get_module(fullname), \"__path__\")\n",
   "        self.__get_module(fullname)  # eventually raises ImportError\n",
   "_importer = _SixMetaPathImporter(__name__)\n",
   "_moved_attributes = [\n",
   "    MovedAttribute(\"reload_module\", \"__builtin__\", \"importlib\" if PY34 else \"imp\", \"reload\"),\n",
   "    _moved_attributes += [\n",
   "for attr in _moved_attributes:\n",
   "    setattr(_MovedItems, attr.name, attr)\n",
   "    if isinstance(attr, MovedModule):\n",
   "        _importer._add_module(attr, \"moves.\" + attr.name)\n",
   "_MovedItems._moved_attributes = _moved_attributes\n",
   "moves = _MovedItems(__name__ + \".moves\")\n",
   "_importer._add_module(moves, \"moves\")\n",
   "_urllib_parse_moved_attributes = [\n",
   "for attr in _urllib_parse_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_parse, attr.name, attr)\n",
   "Module_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_parse(__name__ + \".moves.urllib_parse\"),\n",
   "_urllib_error_moved_attributes = [\n",
   "for attr in _urllib_error_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_error, attr.name, attr)\n",
   "Module_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_error(__name__ + \".moves.urllib.error\"),\n",
   "_urllib_request_moved_attributes = [\n",
   "for attr in _urllib_request_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_request, attr.name, attr)\n",
   "Module_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_request(__name__ + \".moves.urllib.request\"),\n",
   "_urllib_response_moved_attributes = [\n",
   "for attr in _urllib_response_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_response, attr.name, attr)\n",
   "Module_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_response(__name__ + \".moves.urllib.response\"),\n",
   "_urllib_robotparser_moved_attributes = [\n",
   "for attr in _urllib_robotparser_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\n",
   "Module_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + \".moves.urllib.robotparser\"),\n",
   "    parse = _importer._get_module(\"moves.urllib_parse\")\n",
   "    error = _importer._get_module(\"moves.urllib_error\")\n",
   "    request = _importer._get_module(\"moves.urllib_request\")\n",
   "    response = _importer._get_module(\"moves.urllib_response\")\n",
   "    robotparser = _importer._get_module(\"moves.urllib_robotparser\")\n",
   "_importer._add_module(Module_six_moves_urllib(__name__ + \".moves.urllib\"),\n",
   "        delattr(_MovedItems, name)\n",
   "            del moves.__dict__[name]\n",
   "            raise AttributeError(\"no such move, %r\" % (name,))\n",
   "if PY3:\n",
   "    _meth_func = \"__func__\"\n",
   "    _meth_self = \"__self__\"\n",
   "    _func_closure = \"__closure__\"\n",
   "    _func_code = \"__code__\"\n",
   "    _func_defaults = \"__defaults__\"\n",
   "    _func_globals = \"__globals__\"\n",
   "    _meth_func = \"im_func\"\n",
   "    _meth_self = \"im_self\"\n",
   "    _func_closure = \"func_closure\"\n",
   "    _func_code = \"func_code\"\n",
   "    _func_defaults = \"func_defaults\"\n",
   "    _func_globals = \"func_globals\"\n",
   "    advance_iterator = next\n",
   "next = advance_iterator\n",
   "    callable = callable\n",
   "        return any(\"__call__\" in klass.__dict__ for klass in type(obj).__mro__)\n",
   "if PY3:\n",
   "        return types.MethodType(func, None, cls)\n",
   "    callable = callable\n",
   "_add_doc(get_unbound_function,\n",
   "get_method_function = operator.attrgetter(_meth_func)\n",
   "get_method_self = operator.attrgetter(_meth_self)\n",
   "get_function_closure = operator.attrgetter(_func_closure)\n",
   "get_function_code = operator.attrgetter(_func_code)\n",
   "get_function_defaults = operator.attrgetter(_func_defaults)\n",
   "get_function_globals = operator.attrgetter(_func_globals)\n",
   "if PY3:\n",
   "_add_doc(iterkeys, \"Return an iterator over the keys of a dictionary.\")\n",
   "_add_doc(itervalues, \"Return an iterator over the values of a dictionary.\")\n",
   "_add_doc(iteritems,\n",
   "_add_doc(iterlists,\n",
   "if PY3:\n",
   "    unichr = chr\n",
   "    StringIO = io.StringIO\n",
   "    _assertCountEqual = \"assertCountEqual\"\n",
   "        _assertRaisesRegex = \"assertRaisesRegexp\"\n",
   "        _assertRegex = \"assertRegexpMatches\"\n",
   "        _assertRaisesRegex = \"assertRaisesRegex\"\n",
   "        _assertRegex = \"assertRegex\"\n",
   "    unichr = unichr\n",
   "    StringIO = BytesIO = StringIO.StringIO\n",
   "    _assertCountEqual = \"assertItemsEqual\"\n",
   "    _assertRaisesRegex = \"assertRaisesRegexp\"\n",
   "    _assertRegex = \"assertRegexpMatches\"\n",
   "_add_doc(b, \"\"\"Byte literal\"\"\")\n",
   "_add_doc(u, \"\"\"Text literal\"\"\")\n",
   "    return getattr(self, _assertCountEqual)(*args, **kwargs)\n",
   "    return getattr(self, _assertRaisesRegex)(*args, **kwargs)\n",
   "    return getattr(self, _assertRegex)(*args, **kwargs)\n",
   "if PY3:\n",
   "    exec_ = getattr(moves.builtins, \"exec\")\n",
   "            if value is None:\n",
   "                value = tp()\n",
   "            if value.__traceback__ is not tb:\n",
   "                raise value.with_traceback(tb)\n",
   "            raise value\n",
   "            value = None\n",
   "            tb = None\n",
   "            frame = sys._getframe(1)\n",
   "            _globs_ = frame.f_globals\n",
   "                _locs_ = frame.f_locals\n",
   "        elif _locs_ is None:\n",
   "            _locs_ = _globs_\n",
   "    exec_(\"\"\"def reraise(tp, value, tb=None):\n",
   "    exec_(\"\"\"def raise_from(value, from_value):\n",
   "    exec_(\"\"\"def raise_from(value, from_value):\n",
   "        raise value\n",
   "print_ = getattr(moves.builtins, \"print\", None)\n",
   "if print_ is None:\n",
   "        fp = kwargs.pop(\"file\", sys.stdout)\n",
   "        if fp is None:\n",
   "        def write(data):\n",
   "                data = str(data)\n",
   "            if (isinstance(fp, file) and\n",
   "                    isinstance(data, unicode) and\n",
   "                    fp.encoding is not None):\n",
   "                errors = getattr(fp, \"errors\", None)\n",
   "                if errors is None:\n",
   "                    errors = \"strict\"\n",
   "                data = data.encode(fp.encoding, errors)\n",
   "            fp.write(data)\n",
   "        want_unicode = False\n",
   "        sep = kwargs.pop(\"sep\", None)\n",
   "        if sep is not None:\n",
   "            if isinstance(sep, unicode):\n",
   "                want_unicode = True\n",
   "            elif not isinstance(sep, str):\n",
   "        end = kwargs.pop(\"end\", None)\n",
   "        if end is not None:\n",
   "            if isinstance(end, unicode):\n",
   "                want_unicode = True\n",
   "            elif not isinstance(end, str):\n",
   "        if not want_unicode:\n",
   "            for arg in args:\n",
   "                if isinstance(arg, unicode):\n",
   "                    want_unicode = True\n",
   "        if want_unicode:\n",
   "            newline = unicode(\"\\n\")\n",
   "            space = unicode(\" \")\n",
   "            newline = \"\\n\"\n",
   "            space = \" \"\n",
   "        if sep is None:\n",
   "            sep = space\n",
   "        if end is None:\n",
   "            end = newline\n",
   "        for i, arg in enumerate(args):\n",
   "            if i:\n",
   "                write(sep)\n",
   "            write(arg)\n",
   "        write(end)\n",
   "    _print = print_\n",
   "        fp = kwargs.get(\"file\", sys.stdout)\n",
   "        flush = kwargs.pop(\"flush\", False)\n",
   "        _print(*args, **kwargs)\n",
   "        if flush and fp is not None:\n",
   "            fp.flush()\n",
   "_add_doc(reraise, \"\"\"Reraise an exception.\"\"\")\n",
   "            f = functools.wraps(wrapped, assigned, updated)(f)\n",
   "            f.__wrapped__ = wrapped\n",
   "            return f\n",
   "                resolved_bases = types.resolve_bases(bases)\n",
   "                if resolved_bases is not bases:\n",
   "                resolved_bases = bases\n",
   "            return meta(name, resolved_bases, d)\n",
   "            return meta.__prepare__(name, bases)\n",
   "        orig_vars = cls.__dict__.copy()\n",
   "        slots = orig_vars.get('__slots__')\n",
   "        if slots is not None:\n",
   "            if isinstance(slots, str):\n",
   "                slots = [slots]\n",
   "            for slots_var in slots:\n",
   "                orig_vars.pop(slots_var)\n",
   "        orig_vars.pop('__dict__', None)\n",
   "        orig_vars.pop('__weakref__', None)\n",
   "            orig_vars['__qualname__'] = cls.__qualname__\n",
   "        return metaclass(cls.__name__, cls.__bases__, orig_vars)\n",
   "    if isinstance(s, text_type):\n",
   "        return s.encode(encoding, errors)\n",
   "    elif isinstance(s, binary_type):\n",
   "    if not isinstance(s, (text_type, binary_type)):\n",
   "    if PY2 and isinstance(s, text_type):\n",
   "        s = s.encode(encoding, errors)\n",
   "    elif PY3 and isinstance(s, binary_type):\n",
   "        s = s.decode(encoding, errors)\n",
   "    return s\n",
   "    if isinstance(s, binary_type):\n",
   "        return s.decode(encoding, errors)\n",
   "    elif isinstance(s, text_type):\n",
   "        return s\n",
   "        raise TypeError(\"not expecting type '%s'\" % type(s))\n",
   "    if PY2:\n",
   "        if '__str__' not in klass.__dict__:\n",
   "                             klass.__name__)\n",
   "        klass.__unicode__ = klass.__str__\n",
   "        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')\n",
   "    return klass\n",
   "            del sys.meta_path[i]\n",
   "sys.meta_path.append(_importer)\n"
  ]
 },
 "185": {
  "name": "request",
  "type": "Module_six_moves_urllib_request",
  "class": "customized",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/six/__init__.py",
  "lineno": "494",
  "column": "4",
  "context": "= _importer._get_module(\"moves.urllib_error\")\n    request = _importer._get_module(\"moves.urllib_request\")\n    response = _importer._get_module(\"moves.urllib",
  "context_lines": "    \"\"\"Create a six.moves.urllib namespace that resembles the Python 3 namespace\"\"\"\n    __path__ = []  # mark as package\n    parse = _importer._get_module(\"moves.urllib_parse\")\n    error = _importer._get_module(\"moves.urllib_error\")\n    request = _importer._get_module(\"moves.urllib_request\")\n    response = _importer._get_module(\"moves.urllib_response\")\n    robotparser = _importer._get_module(\"moves.urllib_robotparser\")\n\n    def __dir__(self):\n        return ['parse', 'error', 'request', 'response', 'robotparser']\n\n\n",
  "slicing": [
   "PY2 = sys.version_info[0] == 2\n",
   "PY3 = sys.version_info[0] == 3\n",
   "PY34 = sys.version_info[0:2] >= (3, 4)\n",
   "if PY3:\n",
   "    text_type = str\n",
   "    binary_type = bytes\n",
   "    text_type = unicode\n",
   "    binary_type = str\n",
   "def _add_doc(func, doc):\n",
   "    func.__doc__ = doc\n",
   "def _import_module(name):\n",
   "        result = self._resolve()\n",
   "        setattr(obj, self.name, result)  # Invokes __set__.\n",
   "        return result\n",
   "        if PY3:\n",
   "                new = name\n",
   "            self.mod = new\n",
   "        _module = self._resolve()\n",
   "        value = getattr(_module, attr)\n",
   "        setattr(self, attr, value)\n",
   "        return value\n",
   "        super(_LazyModule, self).__init__(name)\n",
   "        attrs = [\"__doc__\", \"__name__\"]\n",
   "        attrs += [attr.name for attr in self._moved_attributes]\n",
   "        return attrs\n",
   "    _moved_attributes = []\n",
   "        super(MovedAttribute, self).__init__(name)\n",
   "        if PY3:\n",
   "                new_mod = name\n",
   "            self.mod = new_mod\n",
   "                    new_attr = name\n",
   "                    new_attr = old_attr\n",
   "            self.attr = new_attr\n",
   "                old_attr = name\n",
   "            self.attr = old_attr\n",
   "        module = _import_module(self.mod)\n",
   "        return getattr(module, self.attr)\n",
   "        for fullname in fullnames:\n",
   "            self.known_modules[self.name + \".\" + fullname] = mod\n",
   "        return self.known_modules[self.name + \".\" + fullname]\n",
   "        if fullname in self.known_modules:\n",
   "            return self.known_modules[fullname]\n",
   "            raise ImportError(\"This loader does not know module \" + fullname)\n",
   "            return sys.modules[fullname]\n",
   "        mod = self.__get_module(fullname)\n",
   "        if isinstance(mod, MovedModule):\n",
   "            mod = mod._resolve()\n",
   "            mod.__loader__ = self\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "        return hasattr(self.__get_module(fullname), \"__path__\")\n",
   "        self.__get_module(fullname)  # eventually raises ImportError\n",
   "_importer = _SixMetaPathImporter(__name__)\n",
   "_moved_attributes = [\n",
   "    MovedAttribute(\"reload_module\", \"__builtin__\", \"importlib\" if PY34 else \"imp\", \"reload\"),\n",
   "    _moved_attributes += [\n",
   "for attr in _moved_attributes:\n",
   "    setattr(_MovedItems, attr.name, attr)\n",
   "    if isinstance(attr, MovedModule):\n",
   "        _importer._add_module(attr, \"moves.\" + attr.name)\n",
   "_MovedItems._moved_attributes = _moved_attributes\n",
   "moves = _MovedItems(__name__ + \".moves\")\n",
   "_importer._add_module(moves, \"moves\")\n",
   "_urllib_parse_moved_attributes = [\n",
   "for attr in _urllib_parse_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_parse, attr.name, attr)\n",
   "Module_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_parse(__name__ + \".moves.urllib_parse\"),\n",
   "_urllib_error_moved_attributes = [\n",
   "for attr in _urllib_error_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_error, attr.name, attr)\n",
   "Module_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_error(__name__ + \".moves.urllib.error\"),\n",
   "_urllib_request_moved_attributes = [\n",
   "for attr in _urllib_request_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_request, attr.name, attr)\n",
   "Module_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_request(__name__ + \".moves.urllib.request\"),\n",
   "_urllib_response_moved_attributes = [\n",
   "for attr in _urllib_response_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_response, attr.name, attr)\n",
   "Module_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_response(__name__ + \".moves.urllib.response\"),\n",
   "_urllib_robotparser_moved_attributes = [\n",
   "for attr in _urllib_robotparser_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\n",
   "Module_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + \".moves.urllib.robotparser\"),\n",
   "    parse = _importer._get_module(\"moves.urllib_parse\")\n",
   "    error = _importer._get_module(\"moves.urllib_error\")\n",
   "    request = _importer._get_module(\"moves.urllib_request\")\n",
   "    response = _importer._get_module(\"moves.urllib_response\")\n",
   "    robotparser = _importer._get_module(\"moves.urllib_robotparser\")\n",
   "_importer._add_module(Module_six_moves_urllib(__name__ + \".moves.urllib\"),\n",
   "        delattr(_MovedItems, name)\n",
   "            del moves.__dict__[name]\n",
   "            raise AttributeError(\"no such move, %r\" % (name,))\n",
   "if PY3:\n",
   "    _meth_func = \"__func__\"\n",
   "    _meth_self = \"__self__\"\n",
   "    _func_closure = \"__closure__\"\n",
   "    _func_code = \"__code__\"\n",
   "    _func_defaults = \"__defaults__\"\n",
   "    _func_globals = \"__globals__\"\n",
   "    _meth_func = \"im_func\"\n",
   "    _meth_self = \"im_self\"\n",
   "    _func_closure = \"func_closure\"\n",
   "    _func_code = \"func_code\"\n",
   "    _func_defaults = \"func_defaults\"\n",
   "    _func_globals = \"func_globals\"\n",
   "    advance_iterator = next\n",
   "next = advance_iterator\n",
   "    callable = callable\n",
   "        return any(\"__call__\" in klass.__dict__ for klass in type(obj).__mro__)\n",
   "if PY3:\n",
   "        return types.MethodType(func, None, cls)\n",
   "    callable = callable\n",
   "_add_doc(get_unbound_function,\n",
   "get_method_function = operator.attrgetter(_meth_func)\n",
   "get_method_self = operator.attrgetter(_meth_self)\n",
   "get_function_closure = operator.attrgetter(_func_closure)\n",
   "get_function_code = operator.attrgetter(_func_code)\n",
   "get_function_defaults = operator.attrgetter(_func_defaults)\n",
   "get_function_globals = operator.attrgetter(_func_globals)\n",
   "if PY3:\n",
   "_add_doc(iterkeys, \"Return an iterator over the keys of a dictionary.\")\n",
   "_add_doc(itervalues, \"Return an iterator over the values of a dictionary.\")\n",
   "_add_doc(iteritems,\n",
   "_add_doc(iterlists,\n",
   "if PY3:\n",
   "    unichr = chr\n",
   "    StringIO = io.StringIO\n",
   "    _assertCountEqual = \"assertCountEqual\"\n",
   "        _assertRaisesRegex = \"assertRaisesRegexp\"\n",
   "        _assertRegex = \"assertRegexpMatches\"\n",
   "        _assertRaisesRegex = \"assertRaisesRegex\"\n",
   "        _assertRegex = \"assertRegex\"\n",
   "    unichr = unichr\n",
   "    StringIO = BytesIO = StringIO.StringIO\n",
   "    _assertCountEqual = \"assertItemsEqual\"\n",
   "    _assertRaisesRegex = \"assertRaisesRegexp\"\n",
   "    _assertRegex = \"assertRegexpMatches\"\n",
   "_add_doc(b, \"\"\"Byte literal\"\"\")\n",
   "_add_doc(u, \"\"\"Text literal\"\"\")\n",
   "    return getattr(self, _assertCountEqual)(*args, **kwargs)\n",
   "    return getattr(self, _assertRaisesRegex)(*args, **kwargs)\n",
   "    return getattr(self, _assertRegex)(*args, **kwargs)\n",
   "if PY3:\n",
   "    exec_ = getattr(moves.builtins, \"exec\")\n",
   "            if value is None:\n",
   "                value = tp()\n",
   "            if value.__traceback__ is not tb:\n",
   "                raise value.with_traceback(tb)\n",
   "            raise value\n",
   "            value = None\n",
   "            tb = None\n",
   "            frame = sys._getframe(1)\n",
   "            _globs_ = frame.f_globals\n",
   "                _locs_ = frame.f_locals\n",
   "        elif _locs_ is None:\n",
   "            _locs_ = _globs_\n",
   "    exec_(\"\"\"def reraise(tp, value, tb=None):\n",
   "    exec_(\"\"\"def raise_from(value, from_value):\n",
   "    exec_(\"\"\"def raise_from(value, from_value):\n",
   "        raise value\n",
   "print_ = getattr(moves.builtins, \"print\", None)\n",
   "if print_ is None:\n",
   "        fp = kwargs.pop(\"file\", sys.stdout)\n",
   "        if fp is None:\n",
   "        def write(data):\n",
   "                data = str(data)\n",
   "            if (isinstance(fp, file) and\n",
   "                    isinstance(data, unicode) and\n",
   "                    fp.encoding is not None):\n",
   "                errors = getattr(fp, \"errors\", None)\n",
   "                if errors is None:\n",
   "                    errors = \"strict\"\n",
   "                data = data.encode(fp.encoding, errors)\n",
   "            fp.write(data)\n",
   "        want_unicode = False\n",
   "        sep = kwargs.pop(\"sep\", None)\n",
   "        if sep is not None:\n",
   "            if isinstance(sep, unicode):\n",
   "                want_unicode = True\n",
   "            elif not isinstance(sep, str):\n",
   "        end = kwargs.pop(\"end\", None)\n",
   "        if end is not None:\n",
   "            if isinstance(end, unicode):\n",
   "                want_unicode = True\n",
   "            elif not isinstance(end, str):\n",
   "        if not want_unicode:\n",
   "            for arg in args:\n",
   "                if isinstance(arg, unicode):\n",
   "                    want_unicode = True\n",
   "        if want_unicode:\n",
   "            newline = unicode(\"\\n\")\n",
   "            space = unicode(\" \")\n",
   "            newline = \"\\n\"\n",
   "            space = \" \"\n",
   "        if sep is None:\n",
   "            sep = space\n",
   "        if end is None:\n",
   "            end = newline\n",
   "        for i, arg in enumerate(args):\n",
   "            if i:\n",
   "                write(sep)\n",
   "            write(arg)\n",
   "        write(end)\n",
   "    _print = print_\n",
   "        fp = kwargs.get(\"file\", sys.stdout)\n",
   "        flush = kwargs.pop(\"flush\", False)\n",
   "        _print(*args, **kwargs)\n",
   "        if flush and fp is not None:\n",
   "            fp.flush()\n",
   "_add_doc(reraise, \"\"\"Reraise an exception.\"\"\")\n",
   "            f = functools.wraps(wrapped, assigned, updated)(f)\n",
   "            f.__wrapped__ = wrapped\n",
   "            return f\n",
   "                resolved_bases = types.resolve_bases(bases)\n",
   "                if resolved_bases is not bases:\n",
   "                resolved_bases = bases\n",
   "            return meta(name, resolved_bases, d)\n",
   "            return meta.__prepare__(name, bases)\n",
   "        orig_vars = cls.__dict__.copy()\n",
   "        slots = orig_vars.get('__slots__')\n",
   "        if slots is not None:\n",
   "            if isinstance(slots, str):\n",
   "                slots = [slots]\n",
   "            for slots_var in slots:\n",
   "                orig_vars.pop(slots_var)\n",
   "        orig_vars.pop('__dict__', None)\n",
   "        orig_vars.pop('__weakref__', None)\n",
   "            orig_vars['__qualname__'] = cls.__qualname__\n",
   "        return metaclass(cls.__name__, cls.__bases__, orig_vars)\n",
   "    if isinstance(s, text_type):\n",
   "        return s.encode(encoding, errors)\n",
   "    elif isinstance(s, binary_type):\n",
   "    if not isinstance(s, (text_type, binary_type)):\n",
   "    if PY2 and isinstance(s, text_type):\n",
   "        s = s.encode(encoding, errors)\n",
   "    elif PY3 and isinstance(s, binary_type):\n",
   "        s = s.decode(encoding, errors)\n",
   "    return s\n",
   "    if isinstance(s, binary_type):\n",
   "        return s.decode(encoding, errors)\n",
   "    elif isinstance(s, text_type):\n",
   "        return s\n",
   "        raise TypeError(\"not expecting type '%s'\" % type(s))\n",
   "    if PY2:\n",
   "        if '__str__' not in klass.__dict__:\n",
   "                             klass.__name__)\n",
   "        klass.__unicode__ = klass.__str__\n",
   "        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')\n",
   "    return klass\n",
   "            del sys.meta_path[i]\n",
   "sys.meta_path.append(_importer)\n"
  ]
 },
 "186": {
  "name": "response",
  "type": "Module_six_moves_urllib_response",
  "class": "customized",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/six/__init__.py",
  "lineno": "495",
  "column": "4",
  "context": "_importer._get_module(\"moves.urllib_request\")\n    response = _importer._get_module(\"moves.urllib_response\")\n    robotparser = _importer._get_module(\"moves.url",
  "context_lines": "    __path__ = []  # mark as package\n    parse = _importer._get_module(\"moves.urllib_parse\")\n    error = _importer._get_module(\"moves.urllib_error\")\n    request = _importer._get_module(\"moves.urllib_request\")\n    response = _importer._get_module(\"moves.urllib_response\")\n    robotparser = _importer._get_module(\"moves.urllib_robotparser\")\n\n    def __dir__(self):\n        return ['parse', 'error', 'request', 'response', 'robotparser']\n\n\n_importer._add_module(Module_six_moves_urllib(__name__ + \".moves.urllib\"),\n",
  "slicing": [
   "PY2 = sys.version_info[0] == 2\n",
   "PY3 = sys.version_info[0] == 3\n",
   "PY34 = sys.version_info[0:2] >= (3, 4)\n",
   "if PY3:\n",
   "    text_type = str\n",
   "    binary_type = bytes\n",
   "    text_type = unicode\n",
   "    binary_type = str\n",
   "def _add_doc(func, doc):\n",
   "    func.__doc__ = doc\n",
   "def _import_module(name):\n",
   "        result = self._resolve()\n",
   "        setattr(obj, self.name, result)  # Invokes __set__.\n",
   "        return result\n",
   "        if PY3:\n",
   "                new = name\n",
   "            self.mod = new\n",
   "        _module = self._resolve()\n",
   "        value = getattr(_module, attr)\n",
   "        setattr(self, attr, value)\n",
   "        return value\n",
   "        super(_LazyModule, self).__init__(name)\n",
   "        attrs = [\"__doc__\", \"__name__\"]\n",
   "        attrs += [attr.name for attr in self._moved_attributes]\n",
   "        return attrs\n",
   "    _moved_attributes = []\n",
   "        super(MovedAttribute, self).__init__(name)\n",
   "        if PY3:\n",
   "                new_mod = name\n",
   "            self.mod = new_mod\n",
   "                    new_attr = name\n",
   "                    new_attr = old_attr\n",
   "            self.attr = new_attr\n",
   "                old_attr = name\n",
   "            self.attr = old_attr\n",
   "        module = _import_module(self.mod)\n",
   "        return getattr(module, self.attr)\n",
   "        for fullname in fullnames:\n",
   "            self.known_modules[self.name + \".\" + fullname] = mod\n",
   "        return self.known_modules[self.name + \".\" + fullname]\n",
   "        if fullname in self.known_modules:\n",
   "            return self.known_modules[fullname]\n",
   "            raise ImportError(\"This loader does not know module \" + fullname)\n",
   "            return sys.modules[fullname]\n",
   "        mod = self.__get_module(fullname)\n",
   "        if isinstance(mod, MovedModule):\n",
   "            mod = mod._resolve()\n",
   "            mod.__loader__ = self\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "        return hasattr(self.__get_module(fullname), \"__path__\")\n",
   "        self.__get_module(fullname)  # eventually raises ImportError\n",
   "_importer = _SixMetaPathImporter(__name__)\n",
   "_moved_attributes = [\n",
   "    MovedAttribute(\"reload_module\", \"__builtin__\", \"importlib\" if PY34 else \"imp\", \"reload\"),\n",
   "    _moved_attributes += [\n",
   "for attr in _moved_attributes:\n",
   "    setattr(_MovedItems, attr.name, attr)\n",
   "    if isinstance(attr, MovedModule):\n",
   "        _importer._add_module(attr, \"moves.\" + attr.name)\n",
   "_MovedItems._moved_attributes = _moved_attributes\n",
   "moves = _MovedItems(__name__ + \".moves\")\n",
   "_importer._add_module(moves, \"moves\")\n",
   "_urllib_parse_moved_attributes = [\n",
   "for attr in _urllib_parse_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_parse, attr.name, attr)\n",
   "Module_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_parse(__name__ + \".moves.urllib_parse\"),\n",
   "_urllib_error_moved_attributes = [\n",
   "for attr in _urllib_error_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_error, attr.name, attr)\n",
   "Module_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_error(__name__ + \".moves.urllib.error\"),\n",
   "_urllib_request_moved_attributes = [\n",
   "for attr in _urllib_request_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_request, attr.name, attr)\n",
   "Module_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_request(__name__ + \".moves.urllib.request\"),\n",
   "_urllib_response_moved_attributes = [\n",
   "for attr in _urllib_response_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_response, attr.name, attr)\n",
   "Module_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_response(__name__ + \".moves.urllib.response\"),\n",
   "_urllib_robotparser_moved_attributes = [\n",
   "for attr in _urllib_robotparser_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\n",
   "Module_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + \".moves.urllib.robotparser\"),\n",
   "    parse = _importer._get_module(\"moves.urllib_parse\")\n",
   "    error = _importer._get_module(\"moves.urllib_error\")\n",
   "    request = _importer._get_module(\"moves.urllib_request\")\n",
   "    response = _importer._get_module(\"moves.urllib_response\")\n",
   "    robotparser = _importer._get_module(\"moves.urllib_robotparser\")\n",
   "_importer._add_module(Module_six_moves_urllib(__name__ + \".moves.urllib\"),\n",
   "        delattr(_MovedItems, name)\n",
   "            del moves.__dict__[name]\n",
   "            raise AttributeError(\"no such move, %r\" % (name,))\n",
   "if PY3:\n",
   "    _meth_func = \"__func__\"\n",
   "    _meth_self = \"__self__\"\n",
   "    _func_closure = \"__closure__\"\n",
   "    _func_code = \"__code__\"\n",
   "    _func_defaults = \"__defaults__\"\n",
   "    _func_globals = \"__globals__\"\n",
   "    _meth_func = \"im_func\"\n",
   "    _meth_self = \"im_self\"\n",
   "    _func_closure = \"func_closure\"\n",
   "    _func_code = \"func_code\"\n",
   "    _func_defaults = \"func_defaults\"\n",
   "    _func_globals = \"func_globals\"\n",
   "    advance_iterator = next\n",
   "next = advance_iterator\n",
   "    callable = callable\n",
   "        return any(\"__call__\" in klass.__dict__ for klass in type(obj).__mro__)\n",
   "if PY3:\n",
   "        return types.MethodType(func, None, cls)\n",
   "    callable = callable\n",
   "_add_doc(get_unbound_function,\n",
   "get_method_function = operator.attrgetter(_meth_func)\n",
   "get_method_self = operator.attrgetter(_meth_self)\n",
   "get_function_closure = operator.attrgetter(_func_closure)\n",
   "get_function_code = operator.attrgetter(_func_code)\n",
   "get_function_defaults = operator.attrgetter(_func_defaults)\n",
   "get_function_globals = operator.attrgetter(_func_globals)\n",
   "if PY3:\n",
   "_add_doc(iterkeys, \"Return an iterator over the keys of a dictionary.\")\n",
   "_add_doc(itervalues, \"Return an iterator over the values of a dictionary.\")\n",
   "_add_doc(iteritems,\n",
   "_add_doc(iterlists,\n",
   "if PY3:\n",
   "    unichr = chr\n",
   "    StringIO = io.StringIO\n",
   "    _assertCountEqual = \"assertCountEqual\"\n",
   "        _assertRaisesRegex = \"assertRaisesRegexp\"\n",
   "        _assertRegex = \"assertRegexpMatches\"\n",
   "        _assertRaisesRegex = \"assertRaisesRegex\"\n",
   "        _assertRegex = \"assertRegex\"\n",
   "    unichr = unichr\n",
   "    StringIO = BytesIO = StringIO.StringIO\n",
   "    _assertCountEqual = \"assertItemsEqual\"\n",
   "    _assertRaisesRegex = \"assertRaisesRegexp\"\n",
   "    _assertRegex = \"assertRegexpMatches\"\n",
   "_add_doc(b, \"\"\"Byte literal\"\"\")\n",
   "_add_doc(u, \"\"\"Text literal\"\"\")\n",
   "    return getattr(self, _assertCountEqual)(*args, **kwargs)\n",
   "    return getattr(self, _assertRaisesRegex)(*args, **kwargs)\n",
   "    return getattr(self, _assertRegex)(*args, **kwargs)\n",
   "if PY3:\n",
   "    exec_ = getattr(moves.builtins, \"exec\")\n",
   "            if value is None:\n",
   "                value = tp()\n",
   "            if value.__traceback__ is not tb:\n",
   "                raise value.with_traceback(tb)\n",
   "            raise value\n",
   "            value = None\n",
   "            tb = None\n",
   "            frame = sys._getframe(1)\n",
   "            _globs_ = frame.f_globals\n",
   "                _locs_ = frame.f_locals\n",
   "        elif _locs_ is None:\n",
   "            _locs_ = _globs_\n",
   "    exec_(\"\"\"def reraise(tp, value, tb=None):\n",
   "    exec_(\"\"\"def raise_from(value, from_value):\n",
   "    exec_(\"\"\"def raise_from(value, from_value):\n",
   "        raise value\n",
   "print_ = getattr(moves.builtins, \"print\", None)\n",
   "if print_ is None:\n",
   "        fp = kwargs.pop(\"file\", sys.stdout)\n",
   "        if fp is None:\n",
   "        def write(data):\n",
   "                data = str(data)\n",
   "            if (isinstance(fp, file) and\n",
   "                    isinstance(data, unicode) and\n",
   "                    fp.encoding is not None):\n",
   "                errors = getattr(fp, \"errors\", None)\n",
   "                if errors is None:\n",
   "                    errors = \"strict\"\n",
   "                data = data.encode(fp.encoding, errors)\n",
   "            fp.write(data)\n",
   "        want_unicode = False\n",
   "        sep = kwargs.pop(\"sep\", None)\n",
   "        if sep is not None:\n",
   "            if isinstance(sep, unicode):\n",
   "                want_unicode = True\n",
   "            elif not isinstance(sep, str):\n",
   "        end = kwargs.pop(\"end\", None)\n",
   "        if end is not None:\n",
   "            if isinstance(end, unicode):\n",
   "                want_unicode = True\n",
   "            elif not isinstance(end, str):\n",
   "        if not want_unicode:\n",
   "            for arg in args:\n",
   "                if isinstance(arg, unicode):\n",
   "                    want_unicode = True\n",
   "        if want_unicode:\n",
   "            newline = unicode(\"\\n\")\n",
   "            space = unicode(\" \")\n",
   "            newline = \"\\n\"\n",
   "            space = \" \"\n",
   "        if sep is None:\n",
   "            sep = space\n",
   "        if end is None:\n",
   "            end = newline\n",
   "        for i, arg in enumerate(args):\n",
   "            if i:\n",
   "                write(sep)\n",
   "            write(arg)\n",
   "        write(end)\n",
   "    _print = print_\n",
   "        fp = kwargs.get(\"file\", sys.stdout)\n",
   "        flush = kwargs.pop(\"flush\", False)\n",
   "        _print(*args, **kwargs)\n",
   "        if flush and fp is not None:\n",
   "            fp.flush()\n",
   "_add_doc(reraise, \"\"\"Reraise an exception.\"\"\")\n",
   "            f = functools.wraps(wrapped, assigned, updated)(f)\n",
   "            f.__wrapped__ = wrapped\n",
   "            return f\n",
   "                resolved_bases = types.resolve_bases(bases)\n",
   "                if resolved_bases is not bases:\n",
   "                resolved_bases = bases\n",
   "            return meta(name, resolved_bases, d)\n",
   "            return meta.__prepare__(name, bases)\n",
   "        orig_vars = cls.__dict__.copy()\n",
   "        slots = orig_vars.get('__slots__')\n",
   "        if slots is not None:\n",
   "            if isinstance(slots, str):\n",
   "                slots = [slots]\n",
   "            for slots_var in slots:\n",
   "                orig_vars.pop(slots_var)\n",
   "        orig_vars.pop('__dict__', None)\n",
   "        orig_vars.pop('__weakref__', None)\n",
   "            orig_vars['__qualname__'] = cls.__qualname__\n",
   "        return metaclass(cls.__name__, cls.__bases__, orig_vars)\n",
   "    if isinstance(s, text_type):\n",
   "        return s.encode(encoding, errors)\n",
   "    elif isinstance(s, binary_type):\n",
   "    if not isinstance(s, (text_type, binary_type)):\n",
   "    if PY2 and isinstance(s, text_type):\n",
   "        s = s.encode(encoding, errors)\n",
   "    elif PY3 and isinstance(s, binary_type):\n",
   "        s = s.decode(encoding, errors)\n",
   "    return s\n",
   "    if isinstance(s, binary_type):\n",
   "        return s.decode(encoding, errors)\n",
   "    elif isinstance(s, text_type):\n",
   "        return s\n",
   "        raise TypeError(\"not expecting type '%s'\" % type(s))\n",
   "    if PY2:\n",
   "        if '__str__' not in klass.__dict__:\n",
   "                             klass.__name__)\n",
   "        klass.__unicode__ = klass.__str__\n",
   "        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')\n",
   "    return klass\n",
   "            del sys.meta_path[i]\n",
   "sys.meta_path.append(_importer)\n"
  ]
 },
 "187": {
  "name": "robotparser",
  "type": "Module_six_moves_urllib_robotparser",
  "class": "customized",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/six/__init__.py",
  "lineno": "496",
  "column": "4",
  "context": "importer._get_module(\"moves.urllib_response\")\n    robotparser = _importer._get_module(\"moves.urllib_robotparser\")\n\n    def __dir__(self):\n        return ['parse', '",
  "context_lines": "    parse = _importer._get_module(\"moves.urllib_parse\")\n    error = _importer._get_module(\"moves.urllib_error\")\n    request = _importer._get_module(\"moves.urllib_request\")\n    response = _importer._get_module(\"moves.urllib_response\")\n    robotparser = _importer._get_module(\"moves.urllib_robotparser\")\n\n    def __dir__(self):\n        return ['parse', 'error', 'request', 'response', 'robotparser']\n\n\n_importer._add_module(Module_six_moves_urllib(__name__ + \".moves.urllib\"),\n",
  "slicing": [
   "PY2 = sys.version_info[0] == 2\n",
   "PY3 = sys.version_info[0] == 3\n",
   "PY34 = sys.version_info[0:2] >= (3, 4)\n",
   "if PY3:\n",
   "    text_type = str\n",
   "    binary_type = bytes\n",
   "    text_type = unicode\n",
   "    binary_type = str\n",
   "def _add_doc(func, doc):\n",
   "    func.__doc__ = doc\n",
   "def _import_module(name):\n",
   "        result = self._resolve()\n",
   "        setattr(obj, self.name, result)  # Invokes __set__.\n",
   "        return result\n",
   "        if PY3:\n",
   "                new = name\n",
   "            self.mod = new\n",
   "        _module = self._resolve()\n",
   "        value = getattr(_module, attr)\n",
   "        setattr(self, attr, value)\n",
   "        return value\n",
   "        super(_LazyModule, self).__init__(name)\n",
   "        attrs = [\"__doc__\", \"__name__\"]\n",
   "        attrs += [attr.name for attr in self._moved_attributes]\n",
   "        return attrs\n",
   "    _moved_attributes = []\n",
   "        super(MovedAttribute, self).__init__(name)\n",
   "        if PY3:\n",
   "                new_mod = name\n",
   "            self.mod = new_mod\n",
   "                    new_attr = name\n",
   "                    new_attr = old_attr\n",
   "            self.attr = new_attr\n",
   "                old_attr = name\n",
   "            self.attr = old_attr\n",
   "        module = _import_module(self.mod)\n",
   "        return getattr(module, self.attr)\n",
   "        for fullname in fullnames:\n",
   "            self.known_modules[self.name + \".\" + fullname] = mod\n",
   "        return self.known_modules[self.name + \".\" + fullname]\n",
   "        if fullname in self.known_modules:\n",
   "            return self.known_modules[fullname]\n",
   "            raise ImportError(\"This loader does not know module \" + fullname)\n",
   "            return sys.modules[fullname]\n",
   "        mod = self.__get_module(fullname)\n",
   "        if isinstance(mod, MovedModule):\n",
   "            mod = mod._resolve()\n",
   "            mod.__loader__ = self\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "        return hasattr(self.__get_module(fullname), \"__path__\")\n",
   "        self.__get_module(fullname)  # eventually raises ImportError\n",
   "_importer = _SixMetaPathImporter(__name__)\n",
   "_moved_attributes = [\n",
   "    MovedAttribute(\"reload_module\", \"__builtin__\", \"importlib\" if PY34 else \"imp\", \"reload\"),\n",
   "    _moved_attributes += [\n",
   "for attr in _moved_attributes:\n",
   "    setattr(_MovedItems, attr.name, attr)\n",
   "    if isinstance(attr, MovedModule):\n",
   "        _importer._add_module(attr, \"moves.\" + attr.name)\n",
   "_MovedItems._moved_attributes = _moved_attributes\n",
   "moves = _MovedItems(__name__ + \".moves\")\n",
   "_importer._add_module(moves, \"moves\")\n",
   "_urllib_parse_moved_attributes = [\n",
   "for attr in _urllib_parse_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_parse, attr.name, attr)\n",
   "Module_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_parse(__name__ + \".moves.urllib_parse\"),\n",
   "_urllib_error_moved_attributes = [\n",
   "for attr in _urllib_error_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_error, attr.name, attr)\n",
   "Module_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_error(__name__ + \".moves.urllib.error\"),\n",
   "_urllib_request_moved_attributes = [\n",
   "for attr in _urllib_request_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_request, attr.name, attr)\n",
   "Module_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_request(__name__ + \".moves.urllib.request\"),\n",
   "_urllib_response_moved_attributes = [\n",
   "for attr in _urllib_response_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_response, attr.name, attr)\n",
   "Module_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_response(__name__ + \".moves.urllib.response\"),\n",
   "_urllib_robotparser_moved_attributes = [\n",
   "for attr in _urllib_robotparser_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\n",
   "Module_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + \".moves.urllib.robotparser\"),\n",
   "    parse = _importer._get_module(\"moves.urllib_parse\")\n",
   "    error = _importer._get_module(\"moves.urllib_error\")\n",
   "    request = _importer._get_module(\"moves.urllib_request\")\n",
   "    response = _importer._get_module(\"moves.urllib_response\")\n",
   "    robotparser = _importer._get_module(\"moves.urllib_robotparser\")\n",
   "_importer._add_module(Module_six_moves_urllib(__name__ + \".moves.urllib\"),\n",
   "        delattr(_MovedItems, name)\n",
   "            del moves.__dict__[name]\n",
   "            raise AttributeError(\"no such move, %r\" % (name,))\n",
   "if PY3:\n",
   "    _meth_func = \"__func__\"\n",
   "    _meth_self = \"__self__\"\n",
   "    _func_closure = \"__closure__\"\n",
   "    _func_code = \"__code__\"\n",
   "    _func_defaults = \"__defaults__\"\n",
   "    _func_globals = \"__globals__\"\n",
   "    _meth_func = \"im_func\"\n",
   "    _meth_self = \"im_self\"\n",
   "    _func_closure = \"func_closure\"\n",
   "    _func_code = \"func_code\"\n",
   "    _func_defaults = \"func_defaults\"\n",
   "    _func_globals = \"func_globals\"\n",
   "    advance_iterator = next\n",
   "next = advance_iterator\n",
   "    callable = callable\n",
   "        return any(\"__call__\" in klass.__dict__ for klass in type(obj).__mro__)\n",
   "if PY3:\n",
   "        return types.MethodType(func, None, cls)\n",
   "    callable = callable\n",
   "_add_doc(get_unbound_function,\n",
   "get_method_function = operator.attrgetter(_meth_func)\n",
   "get_method_self = operator.attrgetter(_meth_self)\n",
   "get_function_closure = operator.attrgetter(_func_closure)\n",
   "get_function_code = operator.attrgetter(_func_code)\n",
   "get_function_defaults = operator.attrgetter(_func_defaults)\n",
   "get_function_globals = operator.attrgetter(_func_globals)\n",
   "if PY3:\n",
   "_add_doc(iterkeys, \"Return an iterator over the keys of a dictionary.\")\n",
   "_add_doc(itervalues, \"Return an iterator over the values of a dictionary.\")\n",
   "_add_doc(iteritems,\n",
   "_add_doc(iterlists,\n",
   "if PY3:\n",
   "    unichr = chr\n",
   "    StringIO = io.StringIO\n",
   "    _assertCountEqual = \"assertCountEqual\"\n",
   "        _assertRaisesRegex = \"assertRaisesRegexp\"\n",
   "        _assertRegex = \"assertRegexpMatches\"\n",
   "        _assertRaisesRegex = \"assertRaisesRegex\"\n",
   "        _assertRegex = \"assertRegex\"\n",
   "    unichr = unichr\n",
   "    StringIO = BytesIO = StringIO.StringIO\n",
   "    _assertCountEqual = \"assertItemsEqual\"\n",
   "    _assertRaisesRegex = \"assertRaisesRegexp\"\n",
   "    _assertRegex = \"assertRegexpMatches\"\n",
   "_add_doc(b, \"\"\"Byte literal\"\"\")\n",
   "_add_doc(u, \"\"\"Text literal\"\"\")\n",
   "    return getattr(self, _assertCountEqual)(*args, **kwargs)\n",
   "    return getattr(self, _assertRaisesRegex)(*args, **kwargs)\n",
   "    return getattr(self, _assertRegex)(*args, **kwargs)\n",
   "if PY3:\n",
   "    exec_ = getattr(moves.builtins, \"exec\")\n",
   "            if value is None:\n",
   "                value = tp()\n",
   "            if value.__traceback__ is not tb:\n",
   "                raise value.with_traceback(tb)\n",
   "            raise value\n",
   "            value = None\n",
   "            tb = None\n",
   "            frame = sys._getframe(1)\n",
   "            _globs_ = frame.f_globals\n",
   "                _locs_ = frame.f_locals\n",
   "        elif _locs_ is None:\n",
   "            _locs_ = _globs_\n",
   "    exec_(\"\"\"def reraise(tp, value, tb=None):\n",
   "    exec_(\"\"\"def raise_from(value, from_value):\n",
   "    exec_(\"\"\"def raise_from(value, from_value):\n",
   "        raise value\n",
   "print_ = getattr(moves.builtins, \"print\", None)\n",
   "if print_ is None:\n",
   "        fp = kwargs.pop(\"file\", sys.stdout)\n",
   "        if fp is None:\n",
   "        def write(data):\n",
   "                data = str(data)\n",
   "            if (isinstance(fp, file) and\n",
   "                    isinstance(data, unicode) and\n",
   "                    fp.encoding is not None):\n",
   "                errors = getattr(fp, \"errors\", None)\n",
   "                if errors is None:\n",
   "                    errors = \"strict\"\n",
   "                data = data.encode(fp.encoding, errors)\n",
   "            fp.write(data)\n",
   "        want_unicode = False\n",
   "        sep = kwargs.pop(\"sep\", None)\n",
   "        if sep is not None:\n",
   "            if isinstance(sep, unicode):\n",
   "                want_unicode = True\n",
   "            elif not isinstance(sep, str):\n",
   "        end = kwargs.pop(\"end\", None)\n",
   "        if end is not None:\n",
   "            if isinstance(end, unicode):\n",
   "                want_unicode = True\n",
   "            elif not isinstance(end, str):\n",
   "        if not want_unicode:\n",
   "            for arg in args:\n",
   "                if isinstance(arg, unicode):\n",
   "                    want_unicode = True\n",
   "        if want_unicode:\n",
   "            newline = unicode(\"\\n\")\n",
   "            space = unicode(\" \")\n",
   "            newline = \"\\n\"\n",
   "            space = \" \"\n",
   "        if sep is None:\n",
   "            sep = space\n",
   "        if end is None:\n",
   "            end = newline\n",
   "        for i, arg in enumerate(args):\n",
   "            if i:\n",
   "                write(sep)\n",
   "            write(arg)\n",
   "        write(end)\n",
   "    _print = print_\n",
   "        fp = kwargs.get(\"file\", sys.stdout)\n",
   "        flush = kwargs.pop(\"flush\", False)\n",
   "        _print(*args, **kwargs)\n",
   "        if flush and fp is not None:\n",
   "            fp.flush()\n",
   "_add_doc(reraise, \"\"\"Reraise an exception.\"\"\")\n",
   "            f = functools.wraps(wrapped, assigned, updated)(f)\n",
   "            f.__wrapped__ = wrapped\n",
   "            return f\n",
   "                resolved_bases = types.resolve_bases(bases)\n",
   "                if resolved_bases is not bases:\n",
   "                resolved_bases = bases\n",
   "            return meta(name, resolved_bases, d)\n",
   "            return meta.__prepare__(name, bases)\n",
   "        orig_vars = cls.__dict__.copy()\n",
   "        slots = orig_vars.get('__slots__')\n",
   "        if slots is not None:\n",
   "            if isinstance(slots, str):\n",
   "                slots = [slots]\n",
   "            for slots_var in slots:\n",
   "                orig_vars.pop(slots_var)\n",
   "        orig_vars.pop('__dict__', None)\n",
   "        orig_vars.pop('__weakref__', None)\n",
   "            orig_vars['__qualname__'] = cls.__qualname__\n",
   "        return metaclass(cls.__name__, cls.__bases__, orig_vars)\n",
   "    if isinstance(s, text_type):\n",
   "        return s.encode(encoding, errors)\n",
   "    elif isinstance(s, binary_type):\n",
   "    if not isinstance(s, (text_type, binary_type)):\n",
   "    if PY2 and isinstance(s, text_type):\n",
   "        s = s.encode(encoding, errors)\n",
   "    elif PY3 and isinstance(s, binary_type):\n",
   "        s = s.decode(encoding, errors)\n",
   "    return s\n",
   "    if isinstance(s, binary_type):\n",
   "        return s.decode(encoding, errors)\n",
   "    elif isinstance(s, text_type):\n",
   "        return s\n",
   "        raise TypeError(\"not expecting type '%s'\" % type(s))\n",
   "    if PY2:\n",
   "        if '__str__' not in klass.__dict__:\n",
   "                             klass.__name__)\n",
   "        klass.__unicode__ = klass.__str__\n",
   "        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')\n",
   "    return klass\n",
   "            del sys.meta_path[i]\n",
   "sys.meta_path.append(_importer)\n"
  ]
 },
 "188": {
  "name": "resolved_bases",
  "type": "tuple",
  "class": "build-in",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/module_utils/six/__init__.py",
  "lineno": "849",
  "column": "16",
  "context": "ses__'] = bases\n            else:\n                resolved_bases = bases\n            return meta(name, resolved_bases, d)\n\n",
  "context_lines": "                resolved_bases = types.resolve_bases(bases)\n                if resolved_bases is not bases:\n                    d['__orig_bases__'] = bases\n            else:\n                resolved_bases = bases\n            return meta(name, resolved_bases, d)\n\n        @classmethod\n        def __prepare__(cls, name, this_bases):\n            return meta.__prepare__(name, bases)\n",
  "slicing": [
   "PY2 = sys.version_info[0] == 2\n",
   "PY3 = sys.version_info[0] == 3\n",
   "PY34 = sys.version_info[0:2] >= (3, 4)\n",
   "if PY3:\n",
   "    text_type = str\n",
   "    binary_type = bytes\n",
   "    text_type = unicode\n",
   "    binary_type = str\n",
   "def _add_doc(func, doc):\n",
   "    func.__doc__ = doc\n",
   "def _import_module(name):\n",
   "        result = self._resolve()\n",
   "        setattr(obj, self.name, result)  # Invokes __set__.\n",
   "        return result\n",
   "        if PY3:\n",
   "                new = name\n",
   "            self.mod = new\n",
   "        _module = self._resolve()\n",
   "        value = getattr(_module, attr)\n",
   "        setattr(self, attr, value)\n",
   "        return value\n",
   "        super(_LazyModule, self).__init__(name)\n",
   "        attrs = [\"__doc__\", \"__name__\"]\n",
   "        attrs += [attr.name for attr in self._moved_attributes]\n",
   "        return attrs\n",
   "    _moved_attributes = []\n",
   "        super(MovedAttribute, self).__init__(name)\n",
   "        if PY3:\n",
   "                new_mod = name\n",
   "            self.mod = new_mod\n",
   "                    new_attr = name\n",
   "                    new_attr = old_attr\n",
   "            self.attr = new_attr\n",
   "                old_attr = name\n",
   "            self.attr = old_attr\n",
   "        module = _import_module(self.mod)\n",
   "        return getattr(module, self.attr)\n",
   "        for fullname in fullnames:\n",
   "            self.known_modules[self.name + \".\" + fullname] = mod\n",
   "        return self.known_modules[self.name + \".\" + fullname]\n",
   "        if fullname in self.known_modules:\n",
   "            return self.known_modules[fullname]\n",
   "            raise ImportError(\"This loader does not know module \" + fullname)\n",
   "            return sys.modules[fullname]\n",
   "        mod = self.__get_module(fullname)\n",
   "        if isinstance(mod, MovedModule):\n",
   "            mod = mod._resolve()\n",
   "            mod.__loader__ = self\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "        return hasattr(self.__get_module(fullname), \"__path__\")\n",
   "        self.__get_module(fullname)  # eventually raises ImportError\n",
   "_importer = _SixMetaPathImporter(__name__)\n",
   "_moved_attributes = [\n",
   "    MovedAttribute(\"reload_module\", \"__builtin__\", \"importlib\" if PY34 else \"imp\", \"reload\"),\n",
   "    _moved_attributes += [\n",
   "for attr in _moved_attributes:\n",
   "    setattr(_MovedItems, attr.name, attr)\n",
   "    if isinstance(attr, MovedModule):\n",
   "        _importer._add_module(attr, \"moves.\" + attr.name)\n",
   "_MovedItems._moved_attributes = _moved_attributes\n",
   "moves = _MovedItems(__name__ + \".moves\")\n",
   "_importer._add_module(moves, \"moves\")\n",
   "_urllib_parse_moved_attributes = [\n",
   "for attr in _urllib_parse_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_parse, attr.name, attr)\n",
   "Module_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_parse(__name__ + \".moves.urllib_parse\"),\n",
   "_urllib_error_moved_attributes = [\n",
   "for attr in _urllib_error_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_error, attr.name, attr)\n",
   "Module_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_error(__name__ + \".moves.urllib.error\"),\n",
   "_urllib_request_moved_attributes = [\n",
   "for attr in _urllib_request_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_request, attr.name, attr)\n",
   "Module_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_request(__name__ + \".moves.urllib.request\"),\n",
   "_urllib_response_moved_attributes = [\n",
   "for attr in _urllib_response_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_response, attr.name, attr)\n",
   "Module_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_response(__name__ + \".moves.urllib.response\"),\n",
   "_urllib_robotparser_moved_attributes = [\n",
   "for attr in _urllib_robotparser_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\n",
   "Module_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + \".moves.urllib.robotparser\"),\n",
   "    parse = _importer._get_module(\"moves.urllib_parse\")\n",
   "    error = _importer._get_module(\"moves.urllib_error\")\n",
   "    request = _importer._get_module(\"moves.urllib_request\")\n",
   "    response = _importer._get_module(\"moves.urllib_response\")\n",
   "    robotparser = _importer._get_module(\"moves.urllib_robotparser\")\n",
   "_importer._add_module(Module_six_moves_urllib(__name__ + \".moves.urllib\"),\n",
   "        delattr(_MovedItems, name)\n",
   "            del moves.__dict__[name]\n",
   "            raise AttributeError(\"no such move, %r\" % (name,))\n",
   "if PY3:\n",
   "    _meth_func = \"__func__\"\n",
   "    _meth_self = \"__self__\"\n",
   "    _func_closure = \"__closure__\"\n",
   "    _func_code = \"__code__\"\n",
   "    _func_defaults = \"__defaults__\"\n",
   "    _func_globals = \"__globals__\"\n",
   "    _meth_func = \"im_func\"\n",
   "    _meth_self = \"im_self\"\n",
   "    _func_closure = \"func_closure\"\n",
   "    _func_code = \"func_code\"\n",
   "    _func_defaults = \"func_defaults\"\n",
   "    _func_globals = \"func_globals\"\n",
   "    advance_iterator = next\n",
   "next = advance_iterator\n",
   "    callable = callable\n",
   "        return any(\"__call__\" in klass.__dict__ for klass in type(obj).__mro__)\n",
   "if PY3:\n",
   "        return types.MethodType(func, None, cls)\n",
   "    callable = callable\n",
   "_add_doc(get_unbound_function,\n",
   "get_method_function = operator.attrgetter(_meth_func)\n",
   "get_method_self = operator.attrgetter(_meth_self)\n",
   "get_function_closure = operator.attrgetter(_func_closure)\n",
   "get_function_code = operator.attrgetter(_func_code)\n",
   "get_function_defaults = operator.attrgetter(_func_defaults)\n",
   "get_function_globals = operator.attrgetter(_func_globals)\n",
   "if PY3:\n",
   "_add_doc(iterkeys, \"Return an iterator over the keys of a dictionary.\")\n",
   "_add_doc(itervalues, \"Return an iterator over the values of a dictionary.\")\n",
   "_add_doc(iteritems,\n",
   "_add_doc(iterlists,\n",
   "if PY3:\n",
   "    unichr = chr\n",
   "    StringIO = io.StringIO\n",
   "    _assertCountEqual = \"assertCountEqual\"\n",
   "        _assertRaisesRegex = \"assertRaisesRegexp\"\n",
   "        _assertRegex = \"assertRegexpMatches\"\n",
   "        _assertRaisesRegex = \"assertRaisesRegex\"\n",
   "        _assertRegex = \"assertRegex\"\n",
   "    unichr = unichr\n",
   "    StringIO = BytesIO = StringIO.StringIO\n",
   "    _assertCountEqual = \"assertItemsEqual\"\n",
   "    _assertRaisesRegex = \"assertRaisesRegexp\"\n",
   "    _assertRegex = \"assertRegexpMatches\"\n",
   "_add_doc(b, \"\"\"Byte literal\"\"\")\n",
   "_add_doc(u, \"\"\"Text literal\"\"\")\n",
   "    return getattr(self, _assertCountEqual)(*args, **kwargs)\n",
   "    return getattr(self, _assertRaisesRegex)(*args, **kwargs)\n",
   "    return getattr(self, _assertRegex)(*args, **kwargs)\n",
   "if PY3:\n",
   "    exec_ = getattr(moves.builtins, \"exec\")\n",
   "            if value is None:\n",
   "                value = tp()\n",
   "            if value.__traceback__ is not tb:\n",
   "                raise value.with_traceback(tb)\n",
   "            raise value\n",
   "            value = None\n",
   "            tb = None\n",
   "            frame = sys._getframe(1)\n",
   "            _globs_ = frame.f_globals\n",
   "                _locs_ = frame.f_locals\n",
   "        elif _locs_ is None:\n",
   "            _locs_ = _globs_\n",
   "    exec_(\"\"\"def reraise(tp, value, tb=None):\n",
   "    exec_(\"\"\"def raise_from(value, from_value):\n",
   "    exec_(\"\"\"def raise_from(value, from_value):\n",
   "        raise value\n",
   "print_ = getattr(moves.builtins, \"print\", None)\n",
   "if print_ is None:\n",
   "        fp = kwargs.pop(\"file\", sys.stdout)\n",
   "        if fp is None:\n",
   "        def write(data):\n",
   "                data = str(data)\n",
   "            if (isinstance(fp, file) and\n",
   "                    isinstance(data, unicode) and\n",
   "                    fp.encoding is not None):\n",
   "                errors = getattr(fp, \"errors\", None)\n",
   "                if errors is None:\n",
   "                    errors = \"strict\"\n",
   "                data = data.encode(fp.encoding, errors)\n",
   "            fp.write(data)\n",
   "        want_unicode = False\n",
   "        sep = kwargs.pop(\"sep\", None)\n",
   "        if sep is not None:\n",
   "            if isinstance(sep, unicode):\n",
   "                want_unicode = True\n",
   "            elif not isinstance(sep, str):\n",
   "        end = kwargs.pop(\"end\", None)\n",
   "        if end is not None:\n",
   "            if isinstance(end, unicode):\n",
   "                want_unicode = True\n",
   "            elif not isinstance(end, str):\n",
   "        if not want_unicode:\n",
   "            for arg in args:\n",
   "                if isinstance(arg, unicode):\n",
   "                    want_unicode = True\n",
   "        if want_unicode:\n",
   "            newline = unicode(\"\\n\")\n",
   "            space = unicode(\" \")\n",
   "            newline = \"\\n\"\n",
   "            space = \" \"\n",
   "        if sep is None:\n",
   "            sep = space\n",
   "        if end is None:\n",
   "            end = newline\n",
   "        for i, arg in enumerate(args):\n",
   "            if i:\n",
   "                write(sep)\n",
   "            write(arg)\n",
   "        write(end)\n",
   "    _print = print_\n",
   "        fp = kwargs.get(\"file\", sys.stdout)\n",
   "        flush = kwargs.pop(\"flush\", False)\n",
   "        _print(*args, **kwargs)\n",
   "        if flush and fp is not None:\n",
   "            fp.flush()\n",
   "_add_doc(reraise, \"\"\"Reraise an exception.\"\"\")\n",
   "            f = functools.wraps(wrapped, assigned, updated)(f)\n",
   "            f.__wrapped__ = wrapped\n",
   "            return f\n",
   "                resolved_bases = types.resolve_bases(bases)\n",
   "                if resolved_bases is not bases:\n",
   "                resolved_bases = bases\n",
   "            return meta(name, resolved_bases, d)\n",
   "            return meta.__prepare__(name, bases)\n",
   "        orig_vars = cls.__dict__.copy()\n",
   "        slots = orig_vars.get('__slots__')\n",
   "        if slots is not None:\n",
   "            if isinstance(slots, str):\n",
   "                slots = [slots]\n",
   "            for slots_var in slots:\n",
   "                orig_vars.pop(slots_var)\n",
   "        orig_vars.pop('__dict__', None)\n",
   "        orig_vars.pop('__weakref__', None)\n",
   "            orig_vars['__qualname__'] = cls.__qualname__\n",
   "        return metaclass(cls.__name__, cls.__bases__, orig_vars)\n",
   "    if isinstance(s, text_type):\n",
   "        return s.encode(encoding, errors)\n",
   "    elif isinstance(s, binary_type):\n",
   "    if not isinstance(s, (text_type, binary_type)):\n",
   "    if PY2 and isinstance(s, text_type):\n",
   "        s = s.encode(encoding, errors)\n",
   "    elif PY3 and isinstance(s, binary_type):\n",
   "        s = s.decode(encoding, errors)\n",
   "    return s\n",
   "    if isinstance(s, binary_type):\n",
   "        return s.decode(encoding, errors)\n",
   "    elif isinstance(s, text_type):\n",
   "        return s\n",
   "        raise TypeError(\"not expecting type '%s'\" % type(s))\n",
   "    if PY2:\n",
   "        if '__str__' not in klass.__dict__:\n",
   "                             klass.__name__)\n",
   "        klass.__unicode__ = klass.__str__\n",
   "        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')\n",
   "    return klass\n",
   "            del sys.meta_path[i]\n",
   "sys.meta_path.append(_importer)\n"
  ]
 },
 "189": {
  "name": "orig_vars",
  "type": "dict",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/six/__init__.py",
  "lineno": "861",
  "column": "8",
  "context": "ith a metaclass.\"\"\"\n    def wrapper(cls):\n        orig_vars = cls.__dict__.copy()\n        slots = orig_vars.get('__slots__')\n       ",
  "context_lines": "    return type.__new__(metaclass, 'temporary_class', (), {})\n\n\ndef add_metaclass(metaclass):\n    \"\"\"Class decorator for creating a class with a metaclass.\"\"\"\n    def wrapper(cls):\n        orig_vars = cls.__dict__.copy()\n        slots = orig_vars.get('__slots__')\n        if slots is not None:\n            if isinstance(slots, str):\n                slots = [slots]\n",
  "slicing": [
   "PY2 = sys.version_info[0] == 2\n",
   "PY3 = sys.version_info[0] == 3\n",
   "PY34 = sys.version_info[0:2] >= (3, 4)\n",
   "if PY3:\n",
   "    text_type = str\n",
   "    binary_type = bytes\n",
   "    text_type = unicode\n",
   "    binary_type = str\n",
   "def _add_doc(func, doc):\n",
   "    func.__doc__ = doc\n",
   "def _import_module(name):\n",
   "        result = self._resolve()\n",
   "        setattr(obj, self.name, result)  # Invokes __set__.\n",
   "        return result\n",
   "        if PY3:\n",
   "                new = name\n",
   "            self.mod = new\n",
   "        _module = self._resolve()\n",
   "        value = getattr(_module, attr)\n",
   "        setattr(self, attr, value)\n",
   "        return value\n",
   "        super(_LazyModule, self).__init__(name)\n",
   "        attrs = [\"__doc__\", \"__name__\"]\n",
   "        attrs += [attr.name for attr in self._moved_attributes]\n",
   "        return attrs\n",
   "    _moved_attributes = []\n",
   "        super(MovedAttribute, self).__init__(name)\n",
   "        if PY3:\n",
   "                new_mod = name\n",
   "            self.mod = new_mod\n",
   "                    new_attr = name\n",
   "                    new_attr = old_attr\n",
   "            self.attr = new_attr\n",
   "                old_attr = name\n",
   "            self.attr = old_attr\n",
   "        module = _import_module(self.mod)\n",
   "        return getattr(module, self.attr)\n",
   "        for fullname in fullnames:\n",
   "            self.known_modules[self.name + \".\" + fullname] = mod\n",
   "        return self.known_modules[self.name + \".\" + fullname]\n",
   "        if fullname in self.known_modules:\n",
   "            return self.known_modules[fullname]\n",
   "            raise ImportError(\"This loader does not know module \" + fullname)\n",
   "            return sys.modules[fullname]\n",
   "        mod = self.__get_module(fullname)\n",
   "        if isinstance(mod, MovedModule):\n",
   "            mod = mod._resolve()\n",
   "            mod.__loader__ = self\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "        return hasattr(self.__get_module(fullname), \"__path__\")\n",
   "        self.__get_module(fullname)  # eventually raises ImportError\n",
   "_importer = _SixMetaPathImporter(__name__)\n",
   "_moved_attributes = [\n",
   "    MovedAttribute(\"reload_module\", \"__builtin__\", \"importlib\" if PY34 else \"imp\", \"reload\"),\n",
   "    _moved_attributes += [\n",
   "for attr in _moved_attributes:\n",
   "    setattr(_MovedItems, attr.name, attr)\n",
   "    if isinstance(attr, MovedModule):\n",
   "        _importer._add_module(attr, \"moves.\" + attr.name)\n",
   "_MovedItems._moved_attributes = _moved_attributes\n",
   "moves = _MovedItems(__name__ + \".moves\")\n",
   "_importer._add_module(moves, \"moves\")\n",
   "_urllib_parse_moved_attributes = [\n",
   "for attr in _urllib_parse_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_parse, attr.name, attr)\n",
   "Module_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_parse(__name__ + \".moves.urllib_parse\"),\n",
   "_urllib_error_moved_attributes = [\n",
   "for attr in _urllib_error_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_error, attr.name, attr)\n",
   "Module_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_error(__name__ + \".moves.urllib.error\"),\n",
   "_urllib_request_moved_attributes = [\n",
   "for attr in _urllib_request_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_request, attr.name, attr)\n",
   "Module_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_request(__name__ + \".moves.urllib.request\"),\n",
   "_urllib_response_moved_attributes = [\n",
   "for attr in _urllib_response_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_response, attr.name, attr)\n",
   "Module_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_response(__name__ + \".moves.urllib.response\"),\n",
   "_urllib_robotparser_moved_attributes = [\n",
   "for attr in _urllib_robotparser_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\n",
   "Module_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + \".moves.urllib.robotparser\"),\n",
   "    parse = _importer._get_module(\"moves.urllib_parse\")\n",
   "    error = _importer._get_module(\"moves.urllib_error\")\n",
   "    request = _importer._get_module(\"moves.urllib_request\")\n",
   "    response = _importer._get_module(\"moves.urllib_response\")\n",
   "    robotparser = _importer._get_module(\"moves.urllib_robotparser\")\n",
   "_importer._add_module(Module_six_moves_urllib(__name__ + \".moves.urllib\"),\n",
   "        delattr(_MovedItems, name)\n",
   "            del moves.__dict__[name]\n",
   "            raise AttributeError(\"no such move, %r\" % (name,))\n",
   "if PY3:\n",
   "    _meth_func = \"__func__\"\n",
   "    _meth_self = \"__self__\"\n",
   "    _func_closure = \"__closure__\"\n",
   "    _func_code = \"__code__\"\n",
   "    _func_defaults = \"__defaults__\"\n",
   "    _func_globals = \"__globals__\"\n",
   "    _meth_func = \"im_func\"\n",
   "    _meth_self = \"im_self\"\n",
   "    _func_closure = \"func_closure\"\n",
   "    _func_code = \"func_code\"\n",
   "    _func_defaults = \"func_defaults\"\n",
   "    _func_globals = \"func_globals\"\n",
   "    advance_iterator = next\n",
   "next = advance_iterator\n",
   "    callable = callable\n",
   "        return any(\"__call__\" in klass.__dict__ for klass in type(obj).__mro__)\n",
   "if PY3:\n",
   "        return types.MethodType(func, None, cls)\n",
   "    callable = callable\n",
   "_add_doc(get_unbound_function,\n",
   "get_method_function = operator.attrgetter(_meth_func)\n",
   "get_method_self = operator.attrgetter(_meth_self)\n",
   "get_function_closure = operator.attrgetter(_func_closure)\n",
   "get_function_code = operator.attrgetter(_func_code)\n",
   "get_function_defaults = operator.attrgetter(_func_defaults)\n",
   "get_function_globals = operator.attrgetter(_func_globals)\n",
   "if PY3:\n",
   "_add_doc(iterkeys, \"Return an iterator over the keys of a dictionary.\")\n",
   "_add_doc(itervalues, \"Return an iterator over the values of a dictionary.\")\n",
   "_add_doc(iteritems,\n",
   "_add_doc(iterlists,\n",
   "if PY3:\n",
   "    unichr = chr\n",
   "    StringIO = io.StringIO\n",
   "    _assertCountEqual = \"assertCountEqual\"\n",
   "        _assertRaisesRegex = \"assertRaisesRegexp\"\n",
   "        _assertRegex = \"assertRegexpMatches\"\n",
   "        _assertRaisesRegex = \"assertRaisesRegex\"\n",
   "        _assertRegex = \"assertRegex\"\n",
   "    unichr = unichr\n",
   "    StringIO = BytesIO = StringIO.StringIO\n",
   "    _assertCountEqual = \"assertItemsEqual\"\n",
   "    _assertRaisesRegex = \"assertRaisesRegexp\"\n",
   "    _assertRegex = \"assertRegexpMatches\"\n",
   "_add_doc(b, \"\"\"Byte literal\"\"\")\n",
   "_add_doc(u, \"\"\"Text literal\"\"\")\n",
   "    return getattr(self, _assertCountEqual)(*args, **kwargs)\n",
   "    return getattr(self, _assertRaisesRegex)(*args, **kwargs)\n",
   "    return getattr(self, _assertRegex)(*args, **kwargs)\n",
   "if PY3:\n",
   "    exec_ = getattr(moves.builtins, \"exec\")\n",
   "            if value is None:\n",
   "                value = tp()\n",
   "            if value.__traceback__ is not tb:\n",
   "                raise value.with_traceback(tb)\n",
   "            raise value\n",
   "            value = None\n",
   "            tb = None\n",
   "            frame = sys._getframe(1)\n",
   "            _globs_ = frame.f_globals\n",
   "                _locs_ = frame.f_locals\n",
   "        elif _locs_ is None:\n",
   "            _locs_ = _globs_\n",
   "    exec_(\"\"\"def reraise(tp, value, tb=None):\n",
   "    exec_(\"\"\"def raise_from(value, from_value):\n",
   "    exec_(\"\"\"def raise_from(value, from_value):\n",
   "        raise value\n",
   "print_ = getattr(moves.builtins, \"print\", None)\n",
   "if print_ is None:\n",
   "        fp = kwargs.pop(\"file\", sys.stdout)\n",
   "        if fp is None:\n",
   "        def write(data):\n",
   "                data = str(data)\n",
   "            if (isinstance(fp, file) and\n",
   "                    isinstance(data, unicode) and\n",
   "                    fp.encoding is not None):\n",
   "                errors = getattr(fp, \"errors\", None)\n",
   "                if errors is None:\n",
   "                    errors = \"strict\"\n",
   "                data = data.encode(fp.encoding, errors)\n",
   "            fp.write(data)\n",
   "        want_unicode = False\n",
   "        sep = kwargs.pop(\"sep\", None)\n",
   "        if sep is not None:\n",
   "            if isinstance(sep, unicode):\n",
   "                want_unicode = True\n",
   "            elif not isinstance(sep, str):\n",
   "        end = kwargs.pop(\"end\", None)\n",
   "        if end is not None:\n",
   "            if isinstance(end, unicode):\n",
   "                want_unicode = True\n",
   "            elif not isinstance(end, str):\n",
   "        if not want_unicode:\n",
   "            for arg in args:\n",
   "                if isinstance(arg, unicode):\n",
   "                    want_unicode = True\n",
   "        if want_unicode:\n",
   "            newline = unicode(\"\\n\")\n",
   "            space = unicode(\" \")\n",
   "            newline = \"\\n\"\n",
   "            space = \" \"\n",
   "        if sep is None:\n",
   "            sep = space\n",
   "        if end is None:\n",
   "            end = newline\n",
   "        for i, arg in enumerate(args):\n",
   "            if i:\n",
   "                write(sep)\n",
   "            write(arg)\n",
   "        write(end)\n",
   "    _print = print_\n",
   "        fp = kwargs.get(\"file\", sys.stdout)\n",
   "        flush = kwargs.pop(\"flush\", False)\n",
   "        _print(*args, **kwargs)\n",
   "        if flush and fp is not None:\n",
   "            fp.flush()\n",
   "_add_doc(reraise, \"\"\"Reraise an exception.\"\"\")\n",
   "            f = functools.wraps(wrapped, assigned, updated)(f)\n",
   "            f.__wrapped__ = wrapped\n",
   "            return f\n",
   "                resolved_bases = types.resolve_bases(bases)\n",
   "                if resolved_bases is not bases:\n",
   "                resolved_bases = bases\n",
   "            return meta(name, resolved_bases, d)\n",
   "            return meta.__prepare__(name, bases)\n",
   "        orig_vars = cls.__dict__.copy()\n",
   "        slots = orig_vars.get('__slots__')\n",
   "        if slots is not None:\n",
   "            if isinstance(slots, str):\n",
   "                slots = [slots]\n",
   "            for slots_var in slots:\n",
   "                orig_vars.pop(slots_var)\n",
   "        orig_vars.pop('__dict__', None)\n",
   "        orig_vars.pop('__weakref__', None)\n",
   "            orig_vars['__qualname__'] = cls.__qualname__\n",
   "        return metaclass(cls.__name__, cls.__bases__, orig_vars)\n",
   "    if isinstance(s, text_type):\n",
   "        return s.encode(encoding, errors)\n",
   "    elif isinstance(s, binary_type):\n",
   "    if not isinstance(s, (text_type, binary_type)):\n",
   "    if PY2 and isinstance(s, text_type):\n",
   "        s = s.encode(encoding, errors)\n",
   "    elif PY3 and isinstance(s, binary_type):\n",
   "        s = s.decode(encoding, errors)\n",
   "    return s\n",
   "    if isinstance(s, binary_type):\n",
   "        return s.decode(encoding, errors)\n",
   "    elif isinstance(s, text_type):\n",
   "        return s\n",
   "        raise TypeError(\"not expecting type '%s'\" % type(s))\n",
   "    if PY2:\n",
   "        if '__str__' not in klass.__dict__:\n",
   "                             klass.__name__)\n",
   "        klass.__unicode__ = klass.__str__\n",
   "        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')\n",
   "    return klass\n",
   "            del sys.meta_path[i]\n",
   "sys.meta_path.append(_importer)\n"
  ]
 },
 "190": {
  "name": "slots",
  "type": "NoneType",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/six/__init__.py",
  "lineno": "862",
  "column": "8",
  "context": ":\n        orig_vars = cls.__dict__.copy()\n        slots = orig_vars.get('__slots__')\n        if slots is not None:\n            if isins",
  "context_lines": "def add_metaclass(metaclass):\n    \"\"\"Class decorator for creating a class with a metaclass.\"\"\"\n    def wrapper(cls):\n        orig_vars = cls.__dict__.copy()\n        slots = orig_vars.get('__slots__')\n        if slots is not None:\n            if isinstance(slots, str):\n                slots = [slots]\n            for slots_var in slots:\n",
  "slicing": [
   "PY2 = sys.version_info[0] == 2\n",
   "PY3 = sys.version_info[0] == 3\n",
   "PY34 = sys.version_info[0:2] >= (3, 4)\n",
   "if PY3:\n",
   "    text_type = str\n",
   "    binary_type = bytes\n",
   "    text_type = unicode\n",
   "    binary_type = str\n",
   "def _add_doc(func, doc):\n",
   "    func.__doc__ = doc\n",
   "def _import_module(name):\n",
   "        result = self._resolve()\n",
   "        setattr(obj, self.name, result)  # Invokes __set__.\n",
   "        return result\n",
   "        if PY3:\n",
   "                new = name\n",
   "            self.mod = new\n",
   "        _module = self._resolve()\n",
   "        value = getattr(_module, attr)\n",
   "        setattr(self, attr, value)\n",
   "        return value\n",
   "        super(_LazyModule, self).__init__(name)\n",
   "        attrs = [\"__doc__\", \"__name__\"]\n",
   "        attrs += [attr.name for attr in self._moved_attributes]\n",
   "        return attrs\n",
   "    _moved_attributes = []\n",
   "        super(MovedAttribute, self).__init__(name)\n",
   "        if PY3:\n",
   "                new_mod = name\n",
   "            self.mod = new_mod\n",
   "                    new_attr = name\n",
   "                    new_attr = old_attr\n",
   "            self.attr = new_attr\n",
   "                old_attr = name\n",
   "            self.attr = old_attr\n",
   "        module = _import_module(self.mod)\n",
   "        return getattr(module, self.attr)\n",
   "        for fullname in fullnames:\n",
   "            self.known_modules[self.name + \".\" + fullname] = mod\n",
   "        return self.known_modules[self.name + \".\" + fullname]\n",
   "        if fullname in self.known_modules:\n",
   "            return self.known_modules[fullname]\n",
   "            raise ImportError(\"This loader does not know module \" + fullname)\n",
   "            return sys.modules[fullname]\n",
   "        mod = self.__get_module(fullname)\n",
   "        if isinstance(mod, MovedModule):\n",
   "            mod = mod._resolve()\n",
   "            mod.__loader__ = self\n",
   "        sys.modules[fullname] = mod\n",
   "        return mod\n",
   "        return hasattr(self.__get_module(fullname), \"__path__\")\n",
   "        self.__get_module(fullname)  # eventually raises ImportError\n",
   "_importer = _SixMetaPathImporter(__name__)\n",
   "_moved_attributes = [\n",
   "    MovedAttribute(\"reload_module\", \"__builtin__\", \"importlib\" if PY34 else \"imp\", \"reload\"),\n",
   "    _moved_attributes += [\n",
   "for attr in _moved_attributes:\n",
   "    setattr(_MovedItems, attr.name, attr)\n",
   "    if isinstance(attr, MovedModule):\n",
   "        _importer._add_module(attr, \"moves.\" + attr.name)\n",
   "_MovedItems._moved_attributes = _moved_attributes\n",
   "moves = _MovedItems(__name__ + \".moves\")\n",
   "_importer._add_module(moves, \"moves\")\n",
   "_urllib_parse_moved_attributes = [\n",
   "for attr in _urllib_parse_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_parse, attr.name, attr)\n",
   "Module_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_parse(__name__ + \".moves.urllib_parse\"),\n",
   "_urllib_error_moved_attributes = [\n",
   "for attr in _urllib_error_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_error, attr.name, attr)\n",
   "Module_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_error(__name__ + \".moves.urllib.error\"),\n",
   "_urllib_request_moved_attributes = [\n",
   "for attr in _urllib_request_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_request, attr.name, attr)\n",
   "Module_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_request(__name__ + \".moves.urllib.request\"),\n",
   "_urllib_response_moved_attributes = [\n",
   "for attr in _urllib_response_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_response, attr.name, attr)\n",
   "Module_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_response(__name__ + \".moves.urllib.response\"),\n",
   "_urllib_robotparser_moved_attributes = [\n",
   "for attr in _urllib_robotparser_moved_attributes:\n",
   "    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\n",
   "Module_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes\n",
   "_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + \".moves.urllib.robotparser\"),\n",
   "    parse = _importer._get_module(\"moves.urllib_parse\")\n",
   "    error = _importer._get_module(\"moves.urllib_error\")\n",
   "    request = _importer._get_module(\"moves.urllib_request\")\n",
   "    response = _importer._get_module(\"moves.urllib_response\")\n",
   "    robotparser = _importer._get_module(\"moves.urllib_robotparser\")\n",
   "_importer._add_module(Module_six_moves_urllib(__name__ + \".moves.urllib\"),\n",
   "        delattr(_MovedItems, name)\n",
   "            del moves.__dict__[name]\n",
   "            raise AttributeError(\"no such move, %r\" % (name,))\n",
   "if PY3:\n",
   "    _meth_func = \"__func__\"\n",
   "    _meth_self = \"__self__\"\n",
   "    _func_closure = \"__closure__\"\n",
   "    _func_code = \"__code__\"\n",
   "    _func_defaults = \"__defaults__\"\n",
   "    _func_globals = \"__globals__\"\n",
   "    _meth_func = \"im_func\"\n",
   "    _meth_self = \"im_self\"\n",
   "    _func_closure = \"func_closure\"\n",
   "    _func_code = \"func_code\"\n",
   "    _func_defaults = \"func_defaults\"\n",
   "    _func_globals = \"func_globals\"\n",
   "    advance_iterator = next\n",
   "next = advance_iterator\n",
   "    callable = callable\n",
   "        return any(\"__call__\" in klass.__dict__ for klass in type(obj).__mro__)\n",
   "if PY3:\n",
   "        return types.MethodType(func, None, cls)\n",
   "    callable = callable\n",
   "_add_doc(get_unbound_function,\n",
   "get_method_function = operator.attrgetter(_meth_func)\n",
   "get_method_self = operator.attrgetter(_meth_self)\n",
   "get_function_closure = operator.attrgetter(_func_closure)\n",
   "get_function_code = operator.attrgetter(_func_code)\n",
   "get_function_defaults = operator.attrgetter(_func_defaults)\n",
   "get_function_globals = operator.attrgetter(_func_globals)\n",
   "if PY3:\n",
   "_add_doc(iterkeys, \"Return an iterator over the keys of a dictionary.\")\n",
   "_add_doc(itervalues, \"Return an iterator over the values of a dictionary.\")\n",
   "_add_doc(iteritems,\n",
   "_add_doc(iterlists,\n",
   "if PY3:\n",
   "    unichr = chr\n",
   "    StringIO = io.StringIO\n",
   "    _assertCountEqual = \"assertCountEqual\"\n",
   "        _assertRaisesRegex = \"assertRaisesRegexp\"\n",
   "        _assertRegex = \"assertRegexpMatches\"\n",
   "        _assertRaisesRegex = \"assertRaisesRegex\"\n",
   "        _assertRegex = \"assertRegex\"\n",
   "    unichr = unichr\n",
   "    StringIO = BytesIO = StringIO.StringIO\n",
   "    _assertCountEqual = \"assertItemsEqual\"\n",
   "    _assertRaisesRegex = \"assertRaisesRegexp\"\n",
   "    _assertRegex = \"assertRegexpMatches\"\n",
   "_add_doc(b, \"\"\"Byte literal\"\"\")\n",
   "_add_doc(u, \"\"\"Text literal\"\"\")\n",
   "    return getattr(self, _assertCountEqual)(*args, **kwargs)\n",
   "    return getattr(self, _assertRaisesRegex)(*args, **kwargs)\n",
   "    return getattr(self, _assertRegex)(*args, **kwargs)\n",
   "if PY3:\n",
   "    exec_ = getattr(moves.builtins, \"exec\")\n",
   "            if value is None:\n",
   "                value = tp()\n",
   "            if value.__traceback__ is not tb:\n",
   "                raise value.with_traceback(tb)\n",
   "            raise value\n",
   "            value = None\n",
   "            tb = None\n",
   "            frame = sys._getframe(1)\n",
   "            _globs_ = frame.f_globals\n",
   "                _locs_ = frame.f_locals\n",
   "        elif _locs_ is None:\n",
   "            _locs_ = _globs_\n",
   "    exec_(\"\"\"def reraise(tp, value, tb=None):\n",
   "    exec_(\"\"\"def raise_from(value, from_value):\n",
   "    exec_(\"\"\"def raise_from(value, from_value):\n",
   "        raise value\n",
   "print_ = getattr(moves.builtins, \"print\", None)\n",
   "if print_ is None:\n",
   "        fp = kwargs.pop(\"file\", sys.stdout)\n",
   "        if fp is None:\n",
   "        def write(data):\n",
   "                data = str(data)\n",
   "            if (isinstance(fp, file) and\n",
   "                    isinstance(data, unicode) and\n",
   "                    fp.encoding is not None):\n",
   "                errors = getattr(fp, \"errors\", None)\n",
   "                if errors is None:\n",
   "                    errors = \"strict\"\n",
   "                data = data.encode(fp.encoding, errors)\n",
   "            fp.write(data)\n",
   "        want_unicode = False\n",
   "        sep = kwargs.pop(\"sep\", None)\n",
   "        if sep is not None:\n",
   "            if isinstance(sep, unicode):\n",
   "                want_unicode = True\n",
   "            elif not isinstance(sep, str):\n",
   "        end = kwargs.pop(\"end\", None)\n",
   "        if end is not None:\n",
   "            if isinstance(end, unicode):\n",
   "                want_unicode = True\n",
   "            elif not isinstance(end, str):\n",
   "        if not want_unicode:\n",
   "            for arg in args:\n",
   "                if isinstance(arg, unicode):\n",
   "                    want_unicode = True\n",
   "        if want_unicode:\n",
   "            newline = unicode(\"\\n\")\n",
   "            space = unicode(\" \")\n",
   "            newline = \"\\n\"\n",
   "            space = \" \"\n",
   "        if sep is None:\n",
   "            sep = space\n",
   "        if end is None:\n",
   "            end = newline\n",
   "        for i, arg in enumerate(args):\n",
   "            if i:\n",
   "                write(sep)\n",
   "            write(arg)\n",
   "        write(end)\n",
   "    _print = print_\n",
   "        fp = kwargs.get(\"file\", sys.stdout)\n",
   "        flush = kwargs.pop(\"flush\", False)\n",
   "        _print(*args, **kwargs)\n",
   "        if flush and fp is not None:\n",
   "            fp.flush()\n",
   "_add_doc(reraise, \"\"\"Reraise an exception.\"\"\")\n",
   "            f = functools.wraps(wrapped, assigned, updated)(f)\n",
   "            f.__wrapped__ = wrapped\n",
   "            return f\n",
   "                resolved_bases = types.resolve_bases(bases)\n",
   "                if resolved_bases is not bases:\n",
   "                resolved_bases = bases\n",
   "            return meta(name, resolved_bases, d)\n",
   "            return meta.__prepare__(name, bases)\n",
   "        orig_vars = cls.__dict__.copy()\n",
   "        slots = orig_vars.get('__slots__')\n",
   "        if slots is not None:\n",
   "            if isinstance(slots, str):\n",
   "                slots = [slots]\n",
   "            for slots_var in slots:\n",
   "                orig_vars.pop(slots_var)\n",
   "        orig_vars.pop('__dict__', None)\n",
   "        orig_vars.pop('__weakref__', None)\n",
   "            orig_vars['__qualname__'] = cls.__qualname__\n",
   "        return metaclass(cls.__name__, cls.__bases__, orig_vars)\n",
   "    if isinstance(s, text_type):\n",
   "        return s.encode(encoding, errors)\n",
   "    elif isinstance(s, binary_type):\n",
   "    if not isinstance(s, (text_type, binary_type)):\n",
   "    if PY2 and isinstance(s, text_type):\n",
   "        s = s.encode(encoding, errors)\n",
   "    elif PY3 and isinstance(s, binary_type):\n",
   "        s = s.decode(encoding, errors)\n",
   "    return s\n",
   "    if isinstance(s, binary_type):\n",
   "        return s.decode(encoding, errors)\n",
   "    elif isinstance(s, text_type):\n",
   "        return s\n",
   "        raise TypeError(\"not expecting type '%s'\" % type(s))\n",
   "    if PY2:\n",
   "        if '__str__' not in klass.__dict__:\n",
   "                             klass.__name__)\n",
   "        klass.__unicode__ = klass.__str__\n",
   "        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')\n",
   "    return klass\n",
   "            del sys.meta_path[i]\n",
   "sys.meta_path.append(_importer)\n"
  ]
 },
 "191": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/ansible_collector.py",
  "lineno": "92",
  "column": "4",
  "context": " a facts with the gather_subset metadata.'''\n\n    name = 'gather_subset'\n    _fact_ids = set([])\n\n    def __init__(self, co",
  "context_lines": "            facts_dict.update(self._filter(info_dict, self.filter_spec))\n\n        return facts_dict\n\n\nclass CollectorMetaDataCollector(collector.BaseFactCollector):\n    '''Collector that provides a facts with the gather_subset metadata.'''\n\n    name = 'gather_subset'\n    _fact_ids = set([])\n\n    def __init__(self, collectors=None, namespace=None, gather_subset=None, module_setup=None):\n        super(CollectorMetaDataCollector, self).__init__(collectors, namespace)\n        self.gather_subset = gather_subset\n",
  "slicing": "    name = 'gather_subset'\n"
 },
 "192": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/collector.py",
  "lineno": "61",
  "column": "4",
  "context": "ass BaseFactCollector:\n    _fact_ids = set()\n\n    _platform = 'Generic'\n    name = None\n    required_facts = set()\n\n    de",
  "context_lines": "class CollectorNotFoundError(KeyError):\n    pass\n\n\nclass BaseFactCollector:\n    _fact_ids = set()\n\n    _platform = 'Generic'\n    name = None\n    required_facts = set()\n\n    def __init__(self, collectors=None, namespace=None):\n        '''Base class for things that collect facts.\n\n",
  "slicing": "    _platform = 'Generic'\n"
 },
 "193": {
  "name": "name",
  "type": "NoneType",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/module_utils/facts/collector.py",
  "lineno": "62",
  "column": "4",
  "context": " _fact_ids = set()\n\n    _platform = 'Generic'\n    name = None\n    required_facts = set()\n\n    def __init__(self,",
  "context_lines": "    pass\n\n\nclass BaseFactCollector:\n    _fact_ids = set()\n\n    _platform = 'Generic'\n    name = None\n    required_facts = set()\n\n    def __init__(self, collectors=None, namespace=None):\n        '''Base class for things that collect facts.\n\n        'collectors' is an optional list of other FactCollectors for composing.'''\n",
  "slicing": "    name = None\n"
 },
 "194": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/virtual/openbsd.py",
  "lineno": "33",
  "column": "4",
  "context": "zation_type\n    - virtualization_role\n    \"\"\"\n    platform = 'OpenBSD'\n    DMESG_BOOT = '/var/run/dmesg.boot'\n\n    def ge",
  "context_lines": "    This is a OpenBSD-specific subclass of Virtual.  It defines\n    - virtualization_type\n    - virtualization_role\n    \"\"\"\n    platform = 'OpenBSD'\n    DMESG_BOOT = '/var/run/dmesg.boot'\n\n    def get_virtual_facts(self):\n        virtual_facts = {}\n\n        # Set empty values as default\n",
  "slicing": "    platform = 'OpenBSD'\n"
 },
 "195": {
  "name": "DMESG_BOOT",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/virtual/openbsd.py",
  "lineno": "34",
  "column": "4",
  "context": "ization_role\n    \"\"\"\n    platform = 'OpenBSD'\n    DMESG_BOOT = '/var/run/dmesg.boot'\n\n    def get_virtual_facts(self):\n        virtual_",
  "context_lines": "    - virtualization_type\n    - virtualization_role\n    \"\"\"\n    platform = 'OpenBSD'\n    DMESG_BOOT = '/var/run/dmesg.boot'\n\n    def get_virtual_facts(self):\n        virtual_facts = {}\n\n        # Set empty values as default\n",
  "slicing": "    DMESG_BOOT = '/var/run/dmesg.boot'\n"
 },
 "196": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/virtual/openbsd.py",
  "lineno": "64",
  "column": "4",
  "context": "lCollector):\n    _fact_class = OpenBSDVirtual\n    _platform = 'OpenBSD'\n",
  "context_lines": "                virtual_facts['virtualization_role'] = 'host'\n\n        return virtual_facts\n\n\nclass OpenBSDVirtualCollector(VirtualCollector):\n    _fact_class = OpenBSDVirtual\n    _platform = 'OpenBSD'\n",
  "slicing": "    _platform = 'OpenBSD'\n"
 },
 "197": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/virtual/hpux.py",
  "lineno": "31",
  "column": "4",
  "context": "zation_type\n    - virtualization_role\n    \"\"\"\n    platform = 'HP-UX'\n\n    def get_virtual_facts(self):\n        virtual_",
  "context_lines": "    This is a HP-UX specific subclass of Virtual. It defines\n    - virtualization_type\n    - virtualization_role\n    \"\"\"\n    platform = 'HP-UX'\n\n    def get_virtual_facts(self):\n        virtual_facts = {}\n        if os.path.exists('/usr/sbin/vecheck'):\n",
  "slicing": "    platform = 'HP-UX'\n"
 },
 "198": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/virtual/hpux.py",
  "lineno": "62",
  "column": "4",
  "context": "tualCollector):\n    _fact_class = HPUXVirtual\n    _platform = 'HP-UX'\n",
  "context_lines": "                virtual_facts['virtualization_role'] = 'HP nPar'\n\n        return virtual_facts\n\n\nclass HPUXVirtualCollector(VirtualCollector):\n    _fact_class = HPUXVirtual\n    _platform = 'HP-UX'\n",
  "slicing": "    _platform = 'HP-UX'\n"
 },
 "199": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/virtual/sunos.py",
  "lineno": "31",
  "column": "4",
  "context": "- virtualization_role\n    - container\n    \"\"\"\n    platform = 'SunOS'\n\n    def get_virtual_facts(self):\n        virtual_",
  "context_lines": "    - virtualization_type\n    - virtualization_role\n    - container\n    \"\"\"\n    platform = 'SunOS'\n\n    def get_virtual_facts(self):\n        virtual_facts = {}\n        # Check if it's a zone\n\n",
  "slicing": "    platform = 'SunOS'\n"
 },
 "200": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/virtual/sunos.py",
  "lineno": "120",
  "column": "4",
  "context": "ualCollector):\n    _fact_class = SunOSVirtual\n    _platform = 'SunOS'\n",
  "context_lines": "                        virtual_facts['virtualization_role'] = 'guest'\n\n        return virtual_facts\n\n\nclass SunOSVirtualCollector(VirtualCollector):\n    _fact_class = SunOSVirtual\n    _platform = 'SunOS'\n",
  "slicing": "    _platform = 'SunOS'\n"
 },
 "201": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/virtual/dragonfly.py",
  "lineno": "25",
  "column": "4",
  "context": "Virtual impl\n    _fact_class = FreeBSDVirtual\n    _platform = 'DragonFly'\n",
  "context_lines": "from ansible.module_utils.facts.virtual.freebsd import FreeBSDVirtual, VirtualCollector\n\n\nclass DragonFlyVirtualCollector(VirtualCollector):\n    # Note the _fact_class impl is actually the FreeBSDVirtual impl\n    _fact_class = FreeBSDVirtual\n    _platform = 'DragonFly'\n",
  "slicing": "    _platform = 'DragonFly'\n"
 },
 "202": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/virtual/freebsd.py",
  "lineno": "31",
  "column": "4",
  "context": "zation_type\n    - virtualization_role\n    \"\"\"\n    platform = 'FreeBSD'\n\n    def get_virtual_facts(self):\n        virtual_",
  "context_lines": "    This is a FreeBSD-specific subclass of Virtual.  It defines\n    - virtualization_type\n    - virtualization_role\n    \"\"\"\n    platform = 'FreeBSD'\n\n    def get_virtual_facts(self):\n        virtual_facts = {}\n        # Set empty values as default\n",
  "slicing": "    platform = 'FreeBSD'\n"
 },
 "203": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/virtual/freebsd.py",
  "lineno": "57",
  "column": "4",
  "context": "lCollector):\n    _fact_class = FreeBSDVirtual\n    _platform = 'FreeBSD'\n",
  "context_lines": "            virtual_facts.update(virtual_vendor_facts)\n\n        return virtual_facts\n\n\nclass FreeBSDVirtualCollector(VirtualCollector):\n    _fact_class = FreeBSDVirtual\n    _platform = 'FreeBSD'\n",
  "slicing": "    _platform = 'FreeBSD'\n"
 },
 "204": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/virtual/linux.py",
  "lineno": "33",
  "column": "4",
  "context": "zation_type\n    - virtualization_role\n    \"\"\"\n    platform = 'Linux'\n\n    # For more information, check: http://people.",
  "context_lines": "    This is a Linux-specific subclass of Virtual.  It defines\n    - virtualization_type\n    - virtualization_role\n    \"\"\"\n    platform = 'Linux'\n\n    # For more information, check: http://people.redhat.com/~rjones/virt-what/\n    def get_virtual_facts(self):\n        virtual_facts = {}\n",
  "slicing": "    platform = 'Linux'\n"
 },
 "205": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/virtual/linux.py",
  "lineno": "256",
  "column": "4",
  "context": "ualCollector):\n    _fact_class = LinuxVirtual\n    _platform = 'Linux'\n",
  "context_lines": "        virtual_facts['virtualization_role'] = 'NA'\n\n        return virtual_facts\n\n\nclass LinuxVirtualCollector(VirtualCollector):\n    _fact_class = LinuxVirtual\n    _platform = 'Linux'\n",
  "slicing": "    _platform = 'Linux'\n"
 },
 "206": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/virtual/base.py",
  "lineno": "36",
  "column": "4",
  "context": " All subclasses MUST define platform.\n    \"\"\"\n    platform = 'Generic'\n\n    # FIXME: remove load_on_init if we can\n    de",
  "context_lines": "    - virtualization_role\n    - container (e.g. solaris zones, freebsd jails, linux containers)\n\n    All subclasses MUST define platform.\n    \"\"\"\n    platform = 'Generic'\n\n    # FIXME: remove load_on_init if we can\n    def __init__(self, module, load_on_init=False):\n        self.module = module\n\n",
  "slicing": "    platform = 'Generic'\n"
 },
 "207": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/virtual/base.py",
  "lineno": "55",
  "column": "4",
  "context": "\n\n\nclass VirtualCollector(BaseFactCollector):\n    name = 'virtual'\n    _fact_class = Virtual\n    _fact_ids = set(['vi",
  "context_lines": "        virtual_facts = {'virtualization_type': '',\n                         'virtualization_role': ''}\n        return virtual_facts\n\n\nclass VirtualCollector(BaseFactCollector):\n    name = 'virtual'\n    _fact_class = Virtual\n    _fact_ids = set(['virtualization_type',\n                     'virtualization_role'])\n\n    def collect(self, module=None, collected_facts=None):\n",
  "slicing": "    name = 'virtual'\n"
 },
 "208": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/virtual/netbsd.py",
  "lineno": "26",
  "column": "4",
  "context": "irtual(Virtual, VirtualSysctlDetectionMixin):\n    platform = 'NetBSD'\n\n    def get_virtual_facts(self):\n        virtual_",
  "context_lines": "import os\n\nfrom ansible.module_utils.facts.virtual.base import Virtual, VirtualCollector\nfrom ansible.module_utils.facts.virtual.sysctl import VirtualSysctlDetectionMixin\n\n\nclass NetBSDVirtual(Virtual, VirtualSysctlDetectionMixin):\n    platform = 'NetBSD'\n\n    def get_virtual_facts(self):\n        virtual_facts = {}\n        # Set empty values as default\n",
  "slicing": "    platform = 'NetBSD'\n"
 },
 "209": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/virtual/netbsd.py",
  "lineno": "58",
  "column": "4",
  "context": "alCollector):\n    _fact_class = NetBSDVirtual\n    _platform = 'NetBSD'\n",
  "context_lines": "            virtual_facts['virtualization_role'] = 'guest'\n\n        return virtual_facts\n\n\nclass NetBSDVirtualCollector(VirtualCollector):\n    _fact_class = NetBSDVirtual\n    _platform = 'NetBSD'\n",
  "slicing": "    _platform = 'NetBSD'\n"
 },
 "210": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/hurd.py",
  "lineno": "31",
  "column": "4",
  "context": "e interface of\n    the Linux kernel.\n    \"\"\"\n\n    platform = 'GNU'\n\n    def populate(self, collected_facts=None):\n   ",
  "context_lines": "    GNU Hurd specific subclass of Hardware. Define memory and mount facts\n    based on procfs compatibility translator mimicking the interface of\n    the Linux kernel.\n    \"\"\"\n\n    platform = 'GNU'\n\n    def populate(self, collected_facts=None):\n        hardware_facts = {}\n        uptime_facts = self.get_uptime_facts()\n",
  "slicing": "    platform = 'GNU'\n"
 },
 "211": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/hurd.py",
  "lineno": "53",
  "column": "4",
  "context": "areCollector):\n    _fact_class = HurdHardware\n    _platform = 'GNU'\n",
  "context_lines": "        hardware_facts.update(mount_facts)\n\n        return hardware_facts\n\n\nclass HurdHardwareCollector(HardwareCollector):\n    _fact_class = HurdHardware\n    _platform = 'GNU'\n",
  "slicing": "    _platform = 'GNU'\n"
 },
 "212": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/openbsd.py",
  "lineno": "44",
  "column": "4",
  "context": "number of DMI facts and device facts.\n    \"\"\"\n    platform = 'OpenBSD'\n\n    def populate(self, collected_facts=None):\n   ",
  "context_lines": "    - processor_count\n    - processor_speed\n\n    In addition, it also defines number of DMI facts and device facts.\n    \"\"\"\n    platform = 'OpenBSD'\n\n    def populate(self, collected_facts=None):\n        hardware_facts = {}\n        self.sysctl = get_sysctl(self.module, ['hw'])\n\n",
  "slicing": "    platform = 'OpenBSD'\n"
 },
 "213": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/openbsd.py",
  "lineno": "170",
  "column": "4",
  "context": "Collector):\n    _fact_class = OpenBSDHardware\n    _platform = 'OpenBSD'\n",
  "context_lines": "                dmi_facts[sysctl_to_dmi[mib]] = self.sysctl[mib]\n\n        return dmi_facts\n\n\nclass OpenBSDHardwareCollector(HardwareCollector):\n    _fact_class = OpenBSDHardware\n    _platform = 'OpenBSD'\n",
  "slicing": "    _platform = 'OpenBSD'\n"
 },
 "214": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/darwin.py",
  "lineno": "36",
  "column": "4",
  "context": "odel\n    - osversion\n    - osrevision\n    \"\"\"\n    platform = 'Darwin'\n\n    def populate(self, collected_facts=None):\n   ",
  "context_lines": "    - model\n    - osversion\n    - osrevision\n    \"\"\"\n    platform = 'Darwin'\n\n    def populate(self, collected_facts=None):\n        hardware_facts = {}\n\n        self.sysctl = get_sysctl(self.module, ['hw', 'machdep', 'kern'])\n",
  "slicing": "    platform = 'Darwin'\n"
 },
 "215": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/darwin.py",
  "lineno": "131",
  "column": "4",
  "context": "eCollector):\n    _fact_class = DarwinHardware\n    _platform = 'Darwin'\n",
  "context_lines": "            memory_facts['memfree_mb'] = memory_facts['memtotal_mb'] - (total_used // 1024 // 1024)\n\n        return memory_facts\n\n\nclass DarwinHardwareCollector(HardwareCollector):\n    _fact_class = DarwinHardware\n    _platform = 'Darwin'\n",
  "slicing": "    _platform = 'Darwin'\n"
 },
 "216": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/hpux.py",
  "lineno": "39",
  "column": "4",
  "context": "sor_count\n    - model\n    - firmware\n    \"\"\"\n\n    platform = 'HP-UX'\n\n    def populate(self, collected_facts=None):\n   ",
  "context_lines": "    - processor_count\n    - model\n    - firmware\n    \"\"\"\n\n    platform = 'HP-UX'\n\n    def populate(self, collected_facts=None):\n        hardware_facts = {}\n\n        cpu_facts = self.get_cpu_facts(collected_facts=collected_facts)\n",
  "slicing": "    platform = 'HP-UX'\n"
 },
 "217": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/hpux.py",
  "lineno": "163",
  "column": "4",
  "context": "areCollector):\n    _fact_class = HPUXHardware\n    _platform = 'HP-UX'\n\n    required_facts = set(['platform', 'distributi",
  "context_lines": "                hw_facts['product_serial'] = out.split(separator)[1].strip()\n\n        return hw_facts\n\n\nclass HPUXHardwareCollector(HardwareCollector):\n    _fact_class = HPUXHardware\n    _platform = 'HP-UX'\n\n",
  "slicing": "    _platform = 'HP-UX'\n"
 },
 "218": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/sunos.py",
  "lineno": "37",
  "column": "4",
  "context": "_mb that is available from *swap -s*.\n    \"\"\"\n    platform = 'SunOS'\n\n    def populate(self, collected_facts=None):\n   ",
  "context_lines": "    \"\"\"\n    In addition to the generic memory and cpu facts, this also sets\n    swap_reserved_mb and swap_allocated_mb that is available from *swap -s*.\n    \"\"\"\n    platform = 'SunOS'\n\n    def populate(self, collected_facts=None):\n        hardware_facts = {}\n\n        # FIXME: could pass to run_command(environ_update), but it also tweaks the env\n",
  "slicing": [
   "    platform = 'SunOS'\n",
   "        hardware_facts = {}\n",
   "        cpu_facts = self.get_cpu_facts()\n",
   "        memory_facts = self.get_memory_facts()\n",
   "        dmi_facts = self.get_dmi_facts()\n",
   "        device_facts = self.get_device_facts()\n",
   "        uptime_facts = self.get_uptime_facts()\n",
   "        mount_facts = {}\n",
   "            mount_facts = self.get_mount_facts()\n",
   "        hardware_facts.update(cpu_facts)\n",
   "        hardware_facts.update(memory_facts)\n",
   "        hardware_facts.update(dmi_facts)\n",
   "        hardware_facts.update(device_facts)\n",
   "        hardware_facts.update(uptime_facts)\n",
   "        hardware_facts.update(mount_facts)\n",
   "        return hardware_facts\n",
   "        physid = 0\n",
   "        sockets = {}\n",
   "        cpu_facts = {}\n",
   "        collected_facts = collected_facts or {}\n",
   "        rc, out, err = self.module.run_command(\"/usr/bin/kstat cpu_info\")\n",
   "        cpu_facts['processor'] = []\n",
   "        for line in out.splitlines():\n",
   "            if len(line) < 1:\n",
   "            data = line.split(None, 1)\n",
   "            key = data[0].strip()\n",
   "            if key == 'module:':\n",
   "                brand = ''\n",
   "            elif key == 'brand':\n",
   "                brand = data[1].strip()\n",
   "            elif key == 'clock_MHz':\n",
   "                clock_mhz = data[1].strip()\n",
   "            elif key == 'implementation':\n",
   "                processor = brand or data[1].strip()\n",
   "                if collected_facts.get('ansible_machine') != 'i86pc':\n",
   "                    processor += \" @ \" + clock_mhz + \"MHz\"\n",
   "                if 'ansible_processor' not in collected_facts:\n",
   "                    cpu_facts['processor'] = []\n",
   "                cpu_facts['processor'].append(processor)\n",
   "            elif key == 'chip_id':\n",
   "                physid = data[1].strip()\n",
   "                if physid not in sockets:\n",
   "                    sockets[physid] = 1\n",
   "                    sockets[physid] += 1\n",
   "        if len(sockets) > 0:\n",
   "            cpu_facts['processor_count'] = len(sockets)\n",
   "            cpu_facts['processor_cores'] = reduce(lambda x, y: x + y, sockets.values())\n",
   "            cpu_facts['processor_cores'] = 'NA'\n",
   "            cpu_facts['processor_count'] = len(cpu_facts['processor'])\n",
   "        return cpu_facts\n",
   "        memory_facts = {}\n",
   "        rc, out, err = self.module.run_command([\"/usr/sbin/prtconf\"])\n",
   "        for line in out.splitlines():\n",
   "            if 'Memory size' in line:\n",
   "                memory_facts['memtotal_mb'] = int(line.split()[2])\n",
   "        rc, out, err = self.module.run_command(\"/usr/sbin/swap -s\")\n",
   "        allocated = int(out.split()[1][:-1])\n",
   "        reserved = int(out.split()[5][:-1])\n",
   "        used = int(out.split()[8][:-1])\n",
   "        free = int(out.split()[10][:-1])\n",
   "        memory_facts['swapfree_mb'] = free // 1024\n",
   "        memory_facts['swaptotal_mb'] = (free + used) // 1024\n",
   "        memory_facts['swap_allocated_mb'] = allocated // 1024\n",
   "        memory_facts['swap_reserved_mb'] = reserved // 1024\n",
   "        return memory_facts\n",
   "        mount_facts = {}\n",
   "        mount_facts['mounts'] = []\n",
   "        fstab = get_file_content('/etc/mnttab')\n",
   "        if fstab:\n",
   "            for line in fstab.splitlines():\n",
   "                fields = line.split('\\t')\n",
   "                mount_statvfs_info = get_mount_size(fields[1])\n",
   "                mount_info = {'mount': fields[1],\n",
   "                              'device': fields[0],\n",
   "                              'fstype': fields[2],\n",
   "                              'options': fields[3],\n",
   "                              'time': fields[4]}\n",
   "                mount_info.update(mount_statvfs_info)\n",
   "                mount_facts['mounts'].append(mount_info)\n",
   "        return mount_facts\n",
   "        dmi_facts = {}\n",
   "        rc, platform, err = self.module.run_command('/usr/bin/uname -i')\n",
   "        platform_sbin = '/usr/platform/' + platform.rstrip() + '/sbin'\n",
   "        prtdiag_path = self.module.get_bin_path(\"prtdiag\", opt_dirs=[platform_sbin])\n",
   "        rc, out, err = self.module.run_command(prtdiag_path)\n",
   "        if out:\n",
   "            system_conf = out.split('\\n')[0]\n",
   "            vendors = [\n",
   "            vendor_regexp = \"|\".join(map(re.escape, vendors))\n",
   "            system_conf_regexp = (r'System Configuration:\\s+'\n",
   "                                  + r'(' + vendor_regexp + r')\\s+'\n",
   "            found = re.match(system_conf_regexp, system_conf)\n",
   "            if found:\n",
   "                dmi_facts['system_vendor'] = found.group(1)\n",
   "                dmi_facts['product_name'] = found.group(2)\n",
   "        return dmi_facts\n",
   "        device_facts = {}\n",
   "        device_facts['devices'] = {}\n",
   "        disk_stats = {\n",
   "        cmd = ['/usr/bin/kstat', '-p']\n",
   "        for ds in disk_stats:\n",
   "            cmd.append('sderr:::%s' % ds)\n",
   "        d = {}\n",
   "        rc, out, err = self.module.run_command(cmd)\n",
   "        if rc != 0:\n",
   "            return device_facts\n",
   "        sd_instances = frozenset(line.split(':')[1] for line in out.split('\\n') if line.startswith('sderr'))\n",
   "        for instance in sd_instances:\n",
   "            lines = (line for line in out.split('\\n') if ':' in line and line.split(':')[1] == instance)\n",
   "            for line in lines:\n",
   "                text, value = line.split('\\t')\n",
   "                stat = text.split(':')[3]\n",
   "                if stat == 'Size':\n",
   "                    d[disk_stats.get(stat)] = bytes_to_human(float(value))\n",
   "                    d[disk_stats.get(stat)] = value.rstrip()\n",
   "            diskname = 'sd' + instance\n",
   "            device_facts['devices'][diskname] = d\n",
   "        return device_facts\n",
   "        if rc != 0:\n",
   "        uptime_facts['uptime_seconds'] = int(time.time() - int(out.split('\\t')[1]))\n",
   "        return uptime_facts\n"
  ]
 },
 "219": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/sunos.py",
  "lineno": "285",
  "column": "4",
  "context": "reCollector):\n    _fact_class = SunOSHardware\n    _platform = 'SunOS'\n\n    required_facts = set(['platform'])\n",
  "context_lines": "        uptime_facts['uptime_seconds'] = int(time.time() - int(out.split('\\t')[1]))\n\n        return uptime_facts\n\n\nclass SunOSHardwareCollector(HardwareCollector):\n    _fact_class = SunOSHardware\n    _platform = 'SunOS'\n\n",
  "slicing": "    _platform = 'SunOS'\n"
 },
 "220": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/aix.py",
  "lineno": "36",
  "column": "4",
  "context": "processor_cores\n    - processor_count\n    \"\"\"\n    platform = 'AIX'\n\n    def populate(self, collected_facts=None):\n   ",
  "context_lines": "    - processor (a list)\n    - processor_cores\n    - processor_count\n    \"\"\"\n    platform = 'AIX'\n\n    def populate(self, collected_facts=None):\n        hardware_facts = {}\n\n        cpu_facts = self.get_cpu_facts()\n",
  "slicing": "    platform = 'AIX'\n"
 },
 "221": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/aix.py",
  "lineno": "251",
  "column": "4",
  "context": "lass AIXHardwareCollector(HardwareCollector):\n    _platform = 'AIX'\n    _fact_class = AIXHardware\n",
  "context_lines": "                'attributes': device_attrs\n            }\n\n        return device_facts\n\n\nclass AIXHardwareCollector(HardwareCollector):\n    _platform = 'AIX'\n",
  "slicing": "    _platform = 'AIX'\n"
 },
 "222": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/dragonfly.py",
  "lineno": "26",
  "column": "4",
  "context": " fact class\n    _fact_class = FreeBSDHardware\n    _platform = 'DragonFly'\n",
  "context_lines": "from ansible.module_utils.facts.hardware.freebsd import FreeBSDHardware\n\n\nclass DragonFlyHardwareCollector(HardwareCollector):\n    # Note: This uses the freebsd fact class, there is no dragonfly hardware fact class\n    _fact_class = FreeBSDHardware\n    _platform = 'DragonFly'\n",
  "slicing": "    _platform = 'DragonFly'\n"
 },
 "223": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/freebsd.py",
  "lineno": "41",
  "column": "4",
  "context": "s\n    - processor_count\n    - devices\n    \"\"\"\n    platform = 'FreeBSD'\n    DMESG_BOOT = '/var/run/dmesg.boot'\n\n    def po",
  "context_lines": "    - processor_cores\n    - processor_count\n    - devices\n    \"\"\"\n    platform = 'FreeBSD'\n    DMESG_BOOT = '/var/run/dmesg.boot'\n\n    def populate(self, collected_facts=None):\n        hardware_facts = {}\n\n        cpu_facts = self.get_cpu_facts()\n",
  "slicing": "    platform = 'FreeBSD'\n"
 },
 "224": {
  "name": "DMESG_BOOT",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/freebsd.py",
  "lineno": "42",
  "column": "4",
  "context": "   - devices\n    \"\"\"\n    platform = 'FreeBSD'\n    DMESG_BOOT = '/var/run/dmesg.boot'\n\n    def populate(self, collected_facts=None):\n   ",
  "context_lines": "    - processor_count\n    - devices\n    \"\"\"\n    platform = 'FreeBSD'\n    DMESG_BOOT = '/var/run/dmesg.boot'\n\n    def populate(self, collected_facts=None):\n        hardware_facts = {}\n\n        cpu_facts = self.get_cpu_facts()\n",
  "slicing": "    DMESG_BOOT = '/var/run/dmesg.boot'\n"
 },
 "225": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/freebsd.py",
  "lineno": "214",
  "column": "4",
  "context": "Collector):\n    _fact_class = FreeBSDHardware\n    _platform = 'FreeBSD'\n",
  "context_lines": "                dmi_facts[k] = 'NA'\n\n        return dmi_facts\n\n\nclass FreeBSDHardwareCollector(HardwareCollector):\n    _fact_class = FreeBSDHardware\n    _platform = 'FreeBSD'\n",
  "slicing": "    _platform = 'FreeBSD'\n"
 },
 "226": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/linux.py",
  "lineno": "70",
  "column": "4",
  "context": "umber of DMI facts and device facts.\n    \"\"\"\n\n    platform = 'Linux'\n\n    # Originally only had these four as toplevelf",
  "context_lines": "    - processor_cores\n    - processor_count\n\n    In addition, it also defines number of DMI facts and device facts.\n    \"\"\"\n\n    platform = 'Linux'\n\n    # Originally only had these four as toplevelfacts\n    ORIGINAL_MEMORY_FACTS = frozenset(('MemTotal', 'SwapTotal', 'MemFree', 'SwapFree'))\n    # Now we have all of these in a dict structure\n",
  "slicing": "    platform = 'Linux'\n"
 },
 "227": {
  "name": "ORIGINAL_MEMORY_FACTS",
  "type": "frozenset",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/linux.py",
  "lineno": "73",
  "column": "4",
  "context": "iginally only had these four as toplevelfacts\n    ORIGINAL_MEMORY_FACTS = frozenset(('MemTotal', 'SwapTotal', 'MemFree', 'SwapFree'))\n    # Now we have all of these in a dict structure",
  "context_lines": "    In addition, it also defines number of DMI facts and device facts.\n    \"\"\"\n\n    platform = 'Linux'\n\n    # Originally only had these four as toplevelfacts\n    ORIGINAL_MEMORY_FACTS = frozenset(('MemTotal', 'SwapTotal', 'MemFree', 'SwapFree'))\n    # Now we have all of these in a dict structure\n    MEMORY_FACTS = ORIGINAL_MEMORY_FACTS.union(('Buffers', 'Cached', 'SwapCached'))\n\n    # regex used against findmnt output to detect bind mounts\n    BIND_MOUNT_RE = re.compile(r'.*\\]')\n\n",
  "slicing": [
   "def get_partition_uuid(partname):\n",
   "        uuids = os.listdir(\"/dev/disk/by-uuid\")\n",
   "    for uuid in uuids:\n",
   "        dev = os.path.realpath(\"/dev/disk/by-uuid/\" + uuid)\n",
   "        if dev == (\"/dev/\" + partname):\n",
   "            return uuid\n",
   "    ORIGINAL_MEMORY_FACTS = frozenset(('MemTotal', 'SwapTotal', 'MemFree', 'SwapFree'))\n",
   "    MEMORY_FACTS = ORIGINAL_MEMORY_FACTS.union(('Buffers', 'Cached', 'SwapCached'))\n",
   "        hardware_facts = {}\n",
   "        cpu_facts = self.get_cpu_facts(collected_facts=collected_facts)\n",
   "        memory_facts = self.get_memory_facts()\n",
   "        dmi_facts = self.get_dmi_facts()\n",
   "        device_facts = self.get_device_facts()\n",
   "        uptime_facts = self.get_uptime_facts()\n",
   "        lvm_facts = self.get_lvm_facts()\n",
   "        mount_facts = {}\n",
   "            mount_facts = self.get_mount_facts()\n",
   "        hardware_facts.update(cpu_facts)\n",
   "        hardware_facts.update(memory_facts)\n",
   "        hardware_facts.update(dmi_facts)\n",
   "        hardware_facts.update(device_facts)\n",
   "        hardware_facts.update(uptime_facts)\n",
   "        hardware_facts.update(lvm_facts)\n",
   "        hardware_facts.update(mount_facts)\n",
   "        return hardware_facts\n",
   "        memory_facts = {}\n",
   "            return memory_facts\n",
   "        memstats = {}\n",
   "        for line in get_file_lines(\"/proc/meminfo\"):\n",
   "            data = line.split(\":\", 1)\n",
   "            key = data[0]\n",
   "            if key in self.ORIGINAL_MEMORY_FACTS:\n",
   "                val = data[1].strip().split(' ')[0]\n",
   "                memory_facts[\"%s_mb\" % key.lower()] = int(val) // 1024\n",
   "            if key in self.MEMORY_FACTS:\n",
   "                val = data[1].strip().split(' ')[0]\n",
   "                memstats[key.lower()] = int(val) // 1024\n",
   "        if None not in (memstats.get('memtotal'), memstats.get('memfree')):\n",
   "            memstats['real:used'] = memstats['memtotal'] - memstats['memfree']\n",
   "        if None not in (memstats.get('cached'), memstats.get('memfree'), memstats.get('buffers')):\n",
   "            memstats['nocache:free'] = memstats['cached'] + memstats['memfree'] + memstats['buffers']\n",
   "        if None not in (memstats.get('memtotal'), memstats.get('nocache:free')):\n",
   "            memstats['nocache:used'] = memstats['memtotal'] - memstats['nocache:free']\n",
   "        if None not in (memstats.get('swaptotal'), memstats.get('swapfree')):\n",
   "            memstats['swap:used'] = memstats['swaptotal'] - memstats['swapfree']\n",
   "        memory_facts['memory_mb'] = {\n",
   "                'total': memstats.get('memtotal'),\n",
   "                'used': memstats.get('real:used'),\n",
   "                'free': memstats.get('memfree'),\n",
   "                'free': memstats.get('nocache:free'),\n",
   "                'used': memstats.get('nocache:used'),\n",
   "                'total': memstats.get('swaptotal'),\n",
   "                'free': memstats.get('swapfree'),\n",
   "                'used': memstats.get('swap:used'),\n",
   "                'cached': memstats.get('swapcached'),\n",
   "        return memory_facts\n",
   "        cpu_facts = {}\n",
   "        collected_facts = collected_facts or {}\n",
   "        i = 0\n",
   "        vendor_id_occurrence = 0\n",
   "        model_name_occurrence = 0\n",
   "        processor_occurence = 0\n",
   "        physid = 0\n",
   "        coreid = 0\n",
   "        sockets = {}\n",
   "        cores = {}\n",
   "        xen = False\n",
   "        xen_paravirt = False\n",
   "                xen = True\n",
   "                for line in get_file_lines('/sys/hypervisor/type'):\n",
   "                    if line.strip() == 'xen':\n",
   "                        xen = True\n",
   "            return cpu_facts\n",
   "        cpu_facts['processor'] = []\n",
   "        for line in get_file_lines('/proc/cpuinfo'):\n",
   "            data = line.split(\":\", 1)\n",
   "            key = data[0].strip()\n",
   "                val = data[1].strip()\n",
   "                val = \"\"\n",
   "            if xen:\n",
   "                if key == 'flags':\n",
   "                    if 'vme' not in val:\n",
   "                        xen_paravirt = True\n",
   "            if key in ['model name', 'Processor', 'vendor_id', 'cpu', 'Vendor', 'processor']:\n",
   "                if 'processor' not in cpu_facts:\n",
   "                    cpu_facts['processor'] = []\n",
   "                cpu_facts['processor'].append(val)\n",
   "                if key == 'vendor_id':\n",
   "                    vendor_id_occurrence += 1\n",
   "                if key == 'model name':\n",
   "                    model_name_occurrence += 1\n",
   "                if key == 'processor':\n",
   "                    processor_occurence += 1\n",
   "                i += 1\n",
   "            elif key == 'physical id':\n",
   "                physid = val\n",
   "                if physid not in sockets:\n",
   "                    sockets[physid] = 1\n",
   "            elif key == 'core id':\n",
   "                coreid = val\n",
   "                if coreid not in sockets:\n",
   "                    cores[coreid] = 1\n",
   "            elif key == 'cpu cores':\n",
   "                sockets[physid] = int(val)\n",
   "            elif key == 'siblings':\n",
   "                cores[coreid] = int(val)\n",
   "            elif key == '# processors':\n",
   "                cpu_facts['processor_cores'] = int(val)\n",
   "            elif key == 'ncpus active':\n",
   "                i = int(val)\n",
   "        if vendor_id_occurrence > 0:\n",
   "            if vendor_id_occurrence == model_name_occurrence:\n",
   "                i = vendor_id_occurrence\n",
   "        if collected_facts.get('ansible_architecture', '').startswith(('armv', 'aarch', 'ppc')):\n",
   "            i = processor_occurence\n",
   "        if collected_facts.get('ansible_architecture') != 's390x':\n",
   "            if xen_paravirt:\n",
   "                cpu_facts['processor_count'] = i\n",
   "                cpu_facts['processor_cores'] = i\n",
   "                cpu_facts['processor_threads_per_core'] = 1\n",
   "                cpu_facts['processor_vcpus'] = i\n",
   "                if sockets:\n",
   "                    cpu_facts['processor_count'] = len(sockets)\n",
   "                    cpu_facts['processor_count'] = i\n",
   "                socket_values = list(sockets.values())\n",
   "                if socket_values and socket_values[0]:\n",
   "                    cpu_facts['processor_cores'] = socket_values[0]\n",
   "                    cpu_facts['processor_cores'] = 1\n",
   "                core_values = list(cores.values())\n",
   "                if core_values:\n",
   "                    cpu_facts['processor_threads_per_core'] = core_values[0] // cpu_facts['processor_cores']\n",
   "                    cpu_facts['processor_threads_per_core'] = 1 // cpu_facts['processor_cores']\n",
   "                cpu_facts['processor_vcpus'] = (cpu_facts['processor_threads_per_core'] *\n",
   "                                                cpu_facts['processor_count'] * cpu_facts['processor_cores'])\n",
   "                cpu_facts['processor_nproc'] = processor_occurence\n",
   "                    cpu_facts['processor_nproc'] = len(\n",
   "                        cmd = get_bin_path('nproc')\n",
   "                        rc, out, _err = self.module.run_command(cmd)\n",
   "                        if rc == 0:\n",
   "                            cpu_facts['processor_nproc'] = int(out)\n",
   "        return cpu_facts\n",
   "        dmi_facts = {}\n",
   "            FORM_FACTOR = [\"Unknown\", \"Other\", \"Unknown\", \"Desktop\",\n",
   "            DMI_DICT = {\n",
   "            for (key, path) in DMI_DICT.items():\n",
   "                data = get_file_content(path)\n",
   "                if data is not None:\n",
   "                    if key == 'form_factor':\n",
   "                            dmi_facts['form_factor'] = FORM_FACTOR[int(data)]\n",
   "                            dmi_facts['form_factor'] = 'unknown (%s)' % data\n",
   "                        dmi_facts[key] = data\n",
   "                    dmi_facts[key] = 'NA'\n",
   "            dmi_bin = self.module.get_bin_path('dmidecode')\n",
   "            DMI_DICT = {\n",
   "            for (k, v) in DMI_DICT.items():\n",
   "                if dmi_bin is not None:\n",
   "                    (rc, out, err) = self.module.run_command('%s -s %s' % (dmi_bin, v))\n",
   "                    if rc == 0:\n",
   "                        thisvalue = ''.join([line for line in out.splitlines() if not line.startswith('#')])\n",
   "                            json.dumps(thisvalue)\n",
   "                            thisvalue = \"NA\"\n",
   "                        dmi_facts[k] = thisvalue\n",
   "                        dmi_facts[k] = 'NA'\n",
   "                    dmi_facts[k] = 'NA'\n",
   "        return dmi_facts\n",
   "        args = ['--list', '--noheadings', '--paths', '--output', 'NAME,UUID', '--exclude', '2']\n",
   "        cmd = [lsblk_path] + args\n",
   "        rc, out, err = self.module.run_command(cmd)\n",
   "        return rc, out, err\n",
   "        uuids = {}\n",
   "        lsblk_path = self.module.get_bin_path(\"lsblk\")\n",
   "        if not lsblk_path:\n",
   "            return uuids\n",
   "        rc, out, err = self._run_lsblk(lsblk_path)\n",
   "        if rc != 0:\n",
   "            return uuids\n",
   "        for lsblk_line in out.splitlines():\n",
   "            if not lsblk_line:\n",
   "            line = lsblk_line.strip()\n",
   "            fields = line.rsplit(None, 1)\n",
   "            if len(fields) < 2:\n",
   "            device_name, uuid = fields[0].strip(), fields[1].strip()\n",
   "            if device_name in uuids:\n",
   "            uuids[device_name] = uuid\n",
   "        return uuids\n",
   "        uuid = 'N/A'\n",
   "        udevadm_path = self.module.get_bin_path('udevadm')\n",
   "        if not udevadm_path:\n",
   "            return uuid\n",
   "        cmd = [udevadm_path, 'info', '--query', 'property', '--name', device]\n",
   "        rc, out, err = self.module.run_command(cmd)\n",
   "        if rc != 0:\n",
   "            return uuid\n",
   "        m = re.search('ID_FS_UUID=(.*)\\n', out)\n",
   "        if m:\n",
   "            uuid = m.group(1)\n",
   "        return uuid\n",
   "        args = ['--list', '--noheadings', '--notruncate']\n",
   "        cmd = [findmnt_path] + args\n",
   "        rc, out, err = self.module.run_command(cmd, errors='surrogate_then_replace')\n",
   "        return rc, out, err\n",
   "        bind_mounts = set()\n",
   "        findmnt_path = self.module.get_bin_path(\"findmnt\")\n",
   "        if not findmnt_path:\n",
   "            return bind_mounts\n",
   "        rc, out, err = self._run_findmnt(findmnt_path)\n",
   "        if rc != 0:\n",
   "            return bind_mounts\n",
   "        for line in out.splitlines():\n",
   "            fields = line.split()\n",
   "            if len(fields) < 2:\n",
   "            if self.BIND_MOUNT_RE.match(fields[1]):\n",
   "                bind_mounts.add(fields[0])\n",
   "        return bind_mounts\n",
   "        mtab_file = '/etc/mtab'\n",
   "        if not os.path.exists(mtab_file):\n",
   "            mtab_file = '/proc/mounts'\n",
   "        mtab = get_file_content(mtab_file, '')\n",
   "        mtab_entries = []\n",
   "        for line in mtab.splitlines():\n",
   "            fields = line.split()\n",
   "            if len(fields) < 4:\n",
   "            mtab_entries.append(fields)\n",
   "        return mtab_entries\n",
   "        mount_size = get_mount_size(mount)\n",
   "        uuid = uuids.get(device, self._udevadm_uuid(device))\n",
   "        return mount_size, uuid\n",
   "        mounts = []\n",
   "        bind_mounts = self._find_bind_mounts()\n",
   "        uuids = self._lsblk_uuid()\n",
   "        mtab_entries = self._mtab_entries()\n",
   "        results = {}\n",
   "        pool = ThreadPool(processes=min(len(mtab_entries), cpu_count()))\n",
   "        maxtime = globals().get('GATHER_TIMEOUT') or timeout.DEFAULT_GATHER_TIMEOUT\n",
   "        for fields in mtab_entries:\n",
   "            fields = [self._replace_octal_escapes(field) for field in fields]\n",
   "            device, mount, fstype, options = fields[0], fields[1], fields[2], fields[3]\n",
   "            if not device.startswith(('/', '\\\\')) and ':/' not in device or fstype == 'none':\n",
   "            mount_info = {'mount': mount,\n",
   "                          'device': device,\n",
   "                          'fstype': fstype,\n",
   "                          'options': options}\n",
   "            if mount in bind_mounts:\n",
   "                if not self.MTAB_BIND_MOUNT_RE.match(options):\n",
   "                    mount_info['options'] += \",bind\"\n",
   "            results[mount] = {'info': mount_info,\n",
   "                              'extra': pool.apply_async(self.get_mount_info, (mount, device, uuids)),\n",
   "                              'timelimit': time.time() + maxtime}\n",
   "        pool.close()  # done with new workers, start gc\n",
   "        while results:\n",
   "            for mount in results:\n",
   "                res = results[mount]['extra']\n",
   "                if res.ready():\n",
   "                    if res.successful():\n",
   "                        mount_size, uuid = res.get()\n",
   "                        if mount_size:\n",
   "                            results[mount]['info'].update(mount_size)\n",
   "                        results[mount]['info']['uuid'] = uuid or 'N/A'\n",
   "                        errmsg = to_text(res.get())\n",
   "                        self.module.warn(\"Error prevented getting extra info for mount %s: %s.\" % (mount, errmsg))\n",
   "                        results[mount]['info']['note'] = 'Could not get extra information: %s.' % (errmsg)\n",
   "                    mounts.append(results[mount]['info'])\n",
   "                    del results[mount]\n",
   "                elif time.time() > results[mount]['timelimit']:\n",
   "                    results[mount]['info']['note'] = 'Timed out while attempting to get extra information.'\n",
   "                    mounts.append(results[mount]['info'])\n",
   "                    del results[mount]\n",
   "        return {'mounts': mounts}\n",
   "            retval = collections.defaultdict(set)\n",
   "            for entry in os.listdir(link_dir):\n",
   "                    target = os.path.basename(os.readlink(os.path.join(link_dir, entry)))\n",
   "                    retval[target].add(entry)\n",
   "            return dict((k, list(sorted(v))) for (k, v) in iteritems(retval))\n",
   "            retval = collections.defaultdict(set)\n",
   "            for path in glob.glob('/sys/block/*/slaves/*'):\n",
   "                elements = path.split('/')\n",
   "                device = elements[3]\n",
   "                target = elements[5]\n",
   "                retval[target].add(device)\n",
   "            return dict((k, list(sorted(v))) for (k, v) in iteritems(retval))\n",
   "            for folder in os.listdir(sysdir + \"/holders\"):\n",
   "                if not folder.startswith(\"dm-\"):\n",
   "                name = get_file_content(sysdir + \"/holders/\" + folder + \"/dm/name\")\n",
   "                if name:\n",
   "                    block_dev_dict['holders'].append(name)\n",
   "                    block_dev_dict['holders'].append(folder)\n",
   "        device_facts = {}\n",
   "        device_facts['devices'] = {}\n",
   "        lspci = self.module.get_bin_path('lspci')\n",
   "        if lspci:\n",
   "            rc, pcidata, err = self.module.run_command([lspci, '-D'], errors='surrogate_then_replace')\n",
   "            pcidata = None\n",
   "            block_devs = os.listdir(\"/sys/block\")\n",
   "            return device_facts\n",
   "        devs_wwn = {}\n",
   "            devs_by_id = os.listdir(\"/dev/disk/by-id\")\n",
   "            for link_name in devs_by_id:\n",
   "                if link_name.startswith(\"wwn-\"):\n",
   "                        wwn_link = os.readlink(os.path.join(\"/dev/disk/by-id\", link_name))\n",
   "                    devs_wwn[os.path.basename(wwn_link)] = link_name[4:]\n",
   "        links = self.get_all_device_links()\n",
   "        device_facts['device_links'] = links\n",
   "        for block in block_devs:\n",
   "            virtual = 1\n",
   "            sysfs_no_links = 0\n",
   "                path = os.readlink(os.path.join(\"/sys/block/\", block))\n",
   "                e = sys.exc_info()[1]\n",
   "                if e.errno == errno.EINVAL:\n",
   "                    path = block\n",
   "                    sysfs_no_links = 1\n",
   "            sysdir = os.path.join(\"/sys/block\", path)\n",
   "            if sysfs_no_links == 1:\n",
   "                for folder in os.listdir(sysdir):\n",
   "                    if \"device\" in folder:\n",
   "                        virtual = 0\n",
   "            d = {}\n",
   "            d['virtual'] = virtual\n",
   "            d['links'] = {}\n",
   "            for (link_type, link_values) in iteritems(links):\n",
   "                d['links'][link_type] = link_values.get(block, [])\n",
   "            diskname = os.path.basename(sysdir)\n",
   "            for key in ['vendor', 'model', 'sas_address', 'sas_device_handle']:\n",
   "                d[key] = get_file_content(sysdir + \"/device/\" + key)\n",
   "            sg_inq = self.module.get_bin_path('sg_inq')\n",
   "            serial_path = \"/sys/block/%s/device/serial\" % (block)\n",
   "            if sg_inq:\n",
   "                device = \"/dev/%s\" % (block)\n",
   "                rc, drivedata, err = self.module.run_command([sg_inq, device])\n",
   "                if rc == 0:\n",
   "                    serial = re.search(r\"Unit serial number:\\s+(\\w+)\", drivedata)\n",
   "                    if serial:\n",
   "                        d['serial'] = serial.group(1)\n",
   "                serial = get_file_content(serial_path)\n",
   "                if serial:\n",
   "                    d['serial'] = serial\n",
   "            for key, test in [('removable', '/removable'),\n",
   "                d[key] = get_file_content(sysdir + test)\n",
   "            if diskname in devs_wwn:\n",
   "                d['wwn'] = devs_wwn[diskname]\n",
   "            d['partitions'] = {}\n",
   "            for folder in os.listdir(sysdir):\n",
   "                m = re.search(\"(\" + diskname + r\"[p]?\\d+)\", folder)\n",
   "                if m:\n",
   "                    part = {}\n",
   "                    partname = m.group(1)\n",
   "                    part_sysdir = sysdir + \"/\" + partname\n",
   "                    part['links'] = {}\n",
   "                    for (link_type, link_values) in iteritems(links):\n",
   "                        part['links'][link_type] = link_values.get(partname, [])\n",
   "                    part['start'] = get_file_content(part_sysdir + \"/start\", 0)\n",
   "                    part['sectors'] = get_file_content(part_sysdir + \"/size\", 0)\n",
   "                    part['sectorsize'] = get_file_content(part_sysdir + \"/queue/logical_block_size\")\n",
   "                    if not part['sectorsize']:\n",
   "                        part['sectorsize'] = get_file_content(part_sysdir + \"/queue/hw_sector_size\", 512)\n",
   "                    part['size'] = bytes_to_human((float(part['sectors']) * 512.0))\n",
   "                    part['uuid'] = get_partition_uuid(partname)\n",
   "                    self.get_holders(part, part_sysdir)\n",
   "                    d['partitions'][partname] = part\n",
   "            d['rotational'] = get_file_content(sysdir + \"/queue/rotational\")\n",
   "            d['scheduler_mode'] = \"\"\n",
   "            scheduler = get_file_content(sysdir + \"/queue/scheduler\")\n",
   "            if scheduler is not None:\n",
   "                m = re.match(r\".*?(\\[(.*)\\])\", scheduler)\n",
   "                if m:\n",
   "                    d['scheduler_mode'] = m.group(2)\n",
   "            d['sectors'] = get_file_content(sysdir + \"/size\")\n",
   "            if not d['sectors']:\n",
   "                d['sectors'] = 0\n",
   "            d['sectorsize'] = get_file_content(sysdir + \"/queue/logical_block_size\")\n",
   "            if not d['sectorsize']:\n",
   "                d['sectorsize'] = get_file_content(sysdir + \"/queue/hw_sector_size\", 512)\n",
   "            d['size'] = bytes_to_human(float(d['sectors']) * 512.0)\n",
   "            d['host'] = \"\"\n",
   "            m = re.match(r\".+/([a-f0-9]{4}:[a-f0-9]{2}:[0|1][a-f0-9]\\.[0-7])/\", sysdir)\n",
   "            if m and pcidata:\n",
   "                pciid = m.group(1)\n",
   "                did = re.escape(pciid)\n",
   "                m = re.search(\"^\" + did + r\"\\s(.*)$\", pcidata, re.MULTILINE)\n",
   "                if m:\n",
   "                    d['host'] = m.group(1)\n",
   "            self.get_holders(d, sysdir)\n",
   "            device_facts['devices'][diskname] = d\n",
   "        return device_facts\n",
   "            uptime_facts['uptime_seconds'] = int(float(uptime_seconds_string))\n",
   "        return uptime_facts\n",
   "            if rc == 0:\n",
   "            lvm_facts['lvm'] = {'lvs': lvs, 'vgs': vgs, 'pvs': pvs}\n",
   "        return lvm_facts\n"
  ]
 },
 "228": {
  "name": "MEMORY_FACTS",
  "type": "frozenset",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/linux.py",
  "lineno": "75",
  "column": "4",
  "context": " Now we have all of these in a dict structure\n    MEMORY_FACTS = ORIGINAL_MEMORY_FACTS.union(('Buffers', 'Cached', 'SwapCached'))\n\n    # regex used against findmnt output to detect",
  "context_lines": "    platform = 'Linux'\n\n    # Originally only had these four as toplevelfacts\n    ORIGINAL_MEMORY_FACTS = frozenset(('MemTotal', 'SwapTotal', 'MemFree', 'SwapFree'))\n    # Now we have all of these in a dict structure\n    MEMORY_FACTS = ORIGINAL_MEMORY_FACTS.union(('Buffers', 'Cached', 'SwapCached'))\n\n    # regex used against findmnt output to detect bind mounts\n    BIND_MOUNT_RE = re.compile(r'.*\\]')\n\n    # regex used against mtab content to find entries that are bind mounts\n",
  "slicing": [
   "def get_partition_uuid(partname):\n",
   "        uuids = os.listdir(\"/dev/disk/by-uuid\")\n",
   "    for uuid in uuids:\n",
   "        dev = os.path.realpath(\"/dev/disk/by-uuid/\" + uuid)\n",
   "        if dev == (\"/dev/\" + partname):\n",
   "            return uuid\n",
   "    ORIGINAL_MEMORY_FACTS = frozenset(('MemTotal', 'SwapTotal', 'MemFree', 'SwapFree'))\n",
   "    MEMORY_FACTS = ORIGINAL_MEMORY_FACTS.union(('Buffers', 'Cached', 'SwapCached'))\n",
   "        hardware_facts = {}\n",
   "        cpu_facts = self.get_cpu_facts(collected_facts=collected_facts)\n",
   "        memory_facts = self.get_memory_facts()\n",
   "        dmi_facts = self.get_dmi_facts()\n",
   "        device_facts = self.get_device_facts()\n",
   "        uptime_facts = self.get_uptime_facts()\n",
   "        lvm_facts = self.get_lvm_facts()\n",
   "        mount_facts = {}\n",
   "            mount_facts = self.get_mount_facts()\n",
   "        hardware_facts.update(cpu_facts)\n",
   "        hardware_facts.update(memory_facts)\n",
   "        hardware_facts.update(dmi_facts)\n",
   "        hardware_facts.update(device_facts)\n",
   "        hardware_facts.update(uptime_facts)\n",
   "        hardware_facts.update(lvm_facts)\n",
   "        hardware_facts.update(mount_facts)\n",
   "        return hardware_facts\n",
   "        memory_facts = {}\n",
   "            return memory_facts\n",
   "        memstats = {}\n",
   "        for line in get_file_lines(\"/proc/meminfo\"):\n",
   "            data = line.split(\":\", 1)\n",
   "            key = data[0]\n",
   "            if key in self.ORIGINAL_MEMORY_FACTS:\n",
   "                val = data[1].strip().split(' ')[0]\n",
   "                memory_facts[\"%s_mb\" % key.lower()] = int(val) // 1024\n",
   "            if key in self.MEMORY_FACTS:\n",
   "                val = data[1].strip().split(' ')[0]\n",
   "                memstats[key.lower()] = int(val) // 1024\n",
   "        if None not in (memstats.get('memtotal'), memstats.get('memfree')):\n",
   "            memstats['real:used'] = memstats['memtotal'] - memstats['memfree']\n",
   "        if None not in (memstats.get('cached'), memstats.get('memfree'), memstats.get('buffers')):\n",
   "            memstats['nocache:free'] = memstats['cached'] + memstats['memfree'] + memstats['buffers']\n",
   "        if None not in (memstats.get('memtotal'), memstats.get('nocache:free')):\n",
   "            memstats['nocache:used'] = memstats['memtotal'] - memstats['nocache:free']\n",
   "        if None not in (memstats.get('swaptotal'), memstats.get('swapfree')):\n",
   "            memstats['swap:used'] = memstats['swaptotal'] - memstats['swapfree']\n",
   "        memory_facts['memory_mb'] = {\n",
   "                'total': memstats.get('memtotal'),\n",
   "                'used': memstats.get('real:used'),\n",
   "                'free': memstats.get('memfree'),\n",
   "                'free': memstats.get('nocache:free'),\n",
   "                'used': memstats.get('nocache:used'),\n",
   "                'total': memstats.get('swaptotal'),\n",
   "                'free': memstats.get('swapfree'),\n",
   "                'used': memstats.get('swap:used'),\n",
   "                'cached': memstats.get('swapcached'),\n",
   "        return memory_facts\n",
   "        cpu_facts = {}\n",
   "        collected_facts = collected_facts or {}\n",
   "        i = 0\n",
   "        vendor_id_occurrence = 0\n",
   "        model_name_occurrence = 0\n",
   "        processor_occurence = 0\n",
   "        physid = 0\n",
   "        coreid = 0\n",
   "        sockets = {}\n",
   "        cores = {}\n",
   "        xen = False\n",
   "        xen_paravirt = False\n",
   "                xen = True\n",
   "                for line in get_file_lines('/sys/hypervisor/type'):\n",
   "                    if line.strip() == 'xen':\n",
   "                        xen = True\n",
   "            return cpu_facts\n",
   "        cpu_facts['processor'] = []\n",
   "        for line in get_file_lines('/proc/cpuinfo'):\n",
   "            data = line.split(\":\", 1)\n",
   "            key = data[0].strip()\n",
   "                val = data[1].strip()\n",
   "                val = \"\"\n",
   "            if xen:\n",
   "                if key == 'flags':\n",
   "                    if 'vme' not in val:\n",
   "                        xen_paravirt = True\n",
   "            if key in ['model name', 'Processor', 'vendor_id', 'cpu', 'Vendor', 'processor']:\n",
   "                if 'processor' not in cpu_facts:\n",
   "                    cpu_facts['processor'] = []\n",
   "                cpu_facts['processor'].append(val)\n",
   "                if key == 'vendor_id':\n",
   "                    vendor_id_occurrence += 1\n",
   "                if key == 'model name':\n",
   "                    model_name_occurrence += 1\n",
   "                if key == 'processor':\n",
   "                    processor_occurence += 1\n",
   "                i += 1\n",
   "            elif key == 'physical id':\n",
   "                physid = val\n",
   "                if physid not in sockets:\n",
   "                    sockets[physid] = 1\n",
   "            elif key == 'core id':\n",
   "                coreid = val\n",
   "                if coreid not in sockets:\n",
   "                    cores[coreid] = 1\n",
   "            elif key == 'cpu cores':\n",
   "                sockets[physid] = int(val)\n",
   "            elif key == 'siblings':\n",
   "                cores[coreid] = int(val)\n",
   "            elif key == '# processors':\n",
   "                cpu_facts['processor_cores'] = int(val)\n",
   "            elif key == 'ncpus active':\n",
   "                i = int(val)\n",
   "        if vendor_id_occurrence > 0:\n",
   "            if vendor_id_occurrence == model_name_occurrence:\n",
   "                i = vendor_id_occurrence\n",
   "        if collected_facts.get('ansible_architecture', '').startswith(('armv', 'aarch', 'ppc')):\n",
   "            i = processor_occurence\n",
   "        if collected_facts.get('ansible_architecture') != 's390x':\n",
   "            if xen_paravirt:\n",
   "                cpu_facts['processor_count'] = i\n",
   "                cpu_facts['processor_cores'] = i\n",
   "                cpu_facts['processor_threads_per_core'] = 1\n",
   "                cpu_facts['processor_vcpus'] = i\n",
   "                if sockets:\n",
   "                    cpu_facts['processor_count'] = len(sockets)\n",
   "                    cpu_facts['processor_count'] = i\n",
   "                socket_values = list(sockets.values())\n",
   "                if socket_values and socket_values[0]:\n",
   "                    cpu_facts['processor_cores'] = socket_values[0]\n",
   "                    cpu_facts['processor_cores'] = 1\n",
   "                core_values = list(cores.values())\n",
   "                if core_values:\n",
   "                    cpu_facts['processor_threads_per_core'] = core_values[0] // cpu_facts['processor_cores']\n",
   "                    cpu_facts['processor_threads_per_core'] = 1 // cpu_facts['processor_cores']\n",
   "                cpu_facts['processor_vcpus'] = (cpu_facts['processor_threads_per_core'] *\n",
   "                                                cpu_facts['processor_count'] * cpu_facts['processor_cores'])\n",
   "                cpu_facts['processor_nproc'] = processor_occurence\n",
   "                    cpu_facts['processor_nproc'] = len(\n",
   "                        cmd = get_bin_path('nproc')\n",
   "                        rc, out, _err = self.module.run_command(cmd)\n",
   "                        if rc == 0:\n",
   "                            cpu_facts['processor_nproc'] = int(out)\n",
   "        return cpu_facts\n",
   "        dmi_facts = {}\n",
   "            FORM_FACTOR = [\"Unknown\", \"Other\", \"Unknown\", \"Desktop\",\n",
   "            DMI_DICT = {\n",
   "            for (key, path) in DMI_DICT.items():\n",
   "                data = get_file_content(path)\n",
   "                if data is not None:\n",
   "                    if key == 'form_factor':\n",
   "                            dmi_facts['form_factor'] = FORM_FACTOR[int(data)]\n",
   "                            dmi_facts['form_factor'] = 'unknown (%s)' % data\n",
   "                        dmi_facts[key] = data\n",
   "                    dmi_facts[key] = 'NA'\n",
   "            dmi_bin = self.module.get_bin_path('dmidecode')\n",
   "            DMI_DICT = {\n",
   "            for (k, v) in DMI_DICT.items():\n",
   "                if dmi_bin is not None:\n",
   "                    (rc, out, err) = self.module.run_command('%s -s %s' % (dmi_bin, v))\n",
   "                    if rc == 0:\n",
   "                        thisvalue = ''.join([line for line in out.splitlines() if not line.startswith('#')])\n",
   "                            json.dumps(thisvalue)\n",
   "                            thisvalue = \"NA\"\n",
   "                        dmi_facts[k] = thisvalue\n",
   "                        dmi_facts[k] = 'NA'\n",
   "                    dmi_facts[k] = 'NA'\n",
   "        return dmi_facts\n",
   "        args = ['--list', '--noheadings', '--paths', '--output', 'NAME,UUID', '--exclude', '2']\n",
   "        cmd = [lsblk_path] + args\n",
   "        rc, out, err = self.module.run_command(cmd)\n",
   "        return rc, out, err\n",
   "        uuids = {}\n",
   "        lsblk_path = self.module.get_bin_path(\"lsblk\")\n",
   "        if not lsblk_path:\n",
   "            return uuids\n",
   "        rc, out, err = self._run_lsblk(lsblk_path)\n",
   "        if rc != 0:\n",
   "            return uuids\n",
   "        for lsblk_line in out.splitlines():\n",
   "            if not lsblk_line:\n",
   "            line = lsblk_line.strip()\n",
   "            fields = line.rsplit(None, 1)\n",
   "            if len(fields) < 2:\n",
   "            device_name, uuid = fields[0].strip(), fields[1].strip()\n",
   "            if device_name in uuids:\n",
   "            uuids[device_name] = uuid\n",
   "        return uuids\n",
   "        uuid = 'N/A'\n",
   "        udevadm_path = self.module.get_bin_path('udevadm')\n",
   "        if not udevadm_path:\n",
   "            return uuid\n",
   "        cmd = [udevadm_path, 'info', '--query', 'property', '--name', device]\n",
   "        rc, out, err = self.module.run_command(cmd)\n",
   "        if rc != 0:\n",
   "            return uuid\n",
   "        m = re.search('ID_FS_UUID=(.*)\\n', out)\n",
   "        if m:\n",
   "            uuid = m.group(1)\n",
   "        return uuid\n",
   "        args = ['--list', '--noheadings', '--notruncate']\n",
   "        cmd = [findmnt_path] + args\n",
   "        rc, out, err = self.module.run_command(cmd, errors='surrogate_then_replace')\n",
   "        return rc, out, err\n",
   "        bind_mounts = set()\n",
   "        findmnt_path = self.module.get_bin_path(\"findmnt\")\n",
   "        if not findmnt_path:\n",
   "            return bind_mounts\n",
   "        rc, out, err = self._run_findmnt(findmnt_path)\n",
   "        if rc != 0:\n",
   "            return bind_mounts\n",
   "        for line in out.splitlines():\n",
   "            fields = line.split()\n",
   "            if len(fields) < 2:\n",
   "            if self.BIND_MOUNT_RE.match(fields[1]):\n",
   "                bind_mounts.add(fields[0])\n",
   "        return bind_mounts\n",
   "        mtab_file = '/etc/mtab'\n",
   "        if not os.path.exists(mtab_file):\n",
   "            mtab_file = '/proc/mounts'\n",
   "        mtab = get_file_content(mtab_file, '')\n",
   "        mtab_entries = []\n",
   "        for line in mtab.splitlines():\n",
   "            fields = line.split()\n",
   "            if len(fields) < 4:\n",
   "            mtab_entries.append(fields)\n",
   "        return mtab_entries\n",
   "        mount_size = get_mount_size(mount)\n",
   "        uuid = uuids.get(device, self._udevadm_uuid(device))\n",
   "        return mount_size, uuid\n",
   "        mounts = []\n",
   "        bind_mounts = self._find_bind_mounts()\n",
   "        uuids = self._lsblk_uuid()\n",
   "        mtab_entries = self._mtab_entries()\n",
   "        results = {}\n",
   "        pool = ThreadPool(processes=min(len(mtab_entries), cpu_count()))\n",
   "        maxtime = globals().get('GATHER_TIMEOUT') or timeout.DEFAULT_GATHER_TIMEOUT\n",
   "        for fields in mtab_entries:\n",
   "            fields = [self._replace_octal_escapes(field) for field in fields]\n",
   "            device, mount, fstype, options = fields[0], fields[1], fields[2], fields[3]\n",
   "            if not device.startswith(('/', '\\\\')) and ':/' not in device or fstype == 'none':\n",
   "            mount_info = {'mount': mount,\n",
   "                          'device': device,\n",
   "                          'fstype': fstype,\n",
   "                          'options': options}\n",
   "            if mount in bind_mounts:\n",
   "                if not self.MTAB_BIND_MOUNT_RE.match(options):\n",
   "                    mount_info['options'] += \",bind\"\n",
   "            results[mount] = {'info': mount_info,\n",
   "                              'extra': pool.apply_async(self.get_mount_info, (mount, device, uuids)),\n",
   "                              'timelimit': time.time() + maxtime}\n",
   "        pool.close()  # done with new workers, start gc\n",
   "        while results:\n",
   "            for mount in results:\n",
   "                res = results[mount]['extra']\n",
   "                if res.ready():\n",
   "                    if res.successful():\n",
   "                        mount_size, uuid = res.get()\n",
   "                        if mount_size:\n",
   "                            results[mount]['info'].update(mount_size)\n",
   "                        results[mount]['info']['uuid'] = uuid or 'N/A'\n",
   "                        errmsg = to_text(res.get())\n",
   "                        self.module.warn(\"Error prevented getting extra info for mount %s: %s.\" % (mount, errmsg))\n",
   "                        results[mount]['info']['note'] = 'Could not get extra information: %s.' % (errmsg)\n",
   "                    mounts.append(results[mount]['info'])\n",
   "                    del results[mount]\n",
   "                elif time.time() > results[mount]['timelimit']:\n",
   "                    results[mount]['info']['note'] = 'Timed out while attempting to get extra information.'\n",
   "                    mounts.append(results[mount]['info'])\n",
   "                    del results[mount]\n",
   "        return {'mounts': mounts}\n",
   "            retval = collections.defaultdict(set)\n",
   "            for entry in os.listdir(link_dir):\n",
   "                    target = os.path.basename(os.readlink(os.path.join(link_dir, entry)))\n",
   "                    retval[target].add(entry)\n",
   "            return dict((k, list(sorted(v))) for (k, v) in iteritems(retval))\n",
   "            retval = collections.defaultdict(set)\n",
   "            for path in glob.glob('/sys/block/*/slaves/*'):\n",
   "                elements = path.split('/')\n",
   "                device = elements[3]\n",
   "                target = elements[5]\n",
   "                retval[target].add(device)\n",
   "            return dict((k, list(sorted(v))) for (k, v) in iteritems(retval))\n",
   "            for folder in os.listdir(sysdir + \"/holders\"):\n",
   "                if not folder.startswith(\"dm-\"):\n",
   "                name = get_file_content(sysdir + \"/holders/\" + folder + \"/dm/name\")\n",
   "                if name:\n",
   "                    block_dev_dict['holders'].append(name)\n",
   "                    block_dev_dict['holders'].append(folder)\n",
   "        device_facts = {}\n",
   "        device_facts['devices'] = {}\n",
   "        lspci = self.module.get_bin_path('lspci')\n",
   "        if lspci:\n",
   "            rc, pcidata, err = self.module.run_command([lspci, '-D'], errors='surrogate_then_replace')\n",
   "            pcidata = None\n",
   "            block_devs = os.listdir(\"/sys/block\")\n",
   "            return device_facts\n",
   "        devs_wwn = {}\n",
   "            devs_by_id = os.listdir(\"/dev/disk/by-id\")\n",
   "            for link_name in devs_by_id:\n",
   "                if link_name.startswith(\"wwn-\"):\n",
   "                        wwn_link = os.readlink(os.path.join(\"/dev/disk/by-id\", link_name))\n",
   "                    devs_wwn[os.path.basename(wwn_link)] = link_name[4:]\n",
   "        links = self.get_all_device_links()\n",
   "        device_facts['device_links'] = links\n",
   "        for block in block_devs:\n",
   "            virtual = 1\n",
   "            sysfs_no_links = 0\n",
   "                path = os.readlink(os.path.join(\"/sys/block/\", block))\n",
   "                e = sys.exc_info()[1]\n",
   "                if e.errno == errno.EINVAL:\n",
   "                    path = block\n",
   "                    sysfs_no_links = 1\n",
   "            sysdir = os.path.join(\"/sys/block\", path)\n",
   "            if sysfs_no_links == 1:\n",
   "                for folder in os.listdir(sysdir):\n",
   "                    if \"device\" in folder:\n",
   "                        virtual = 0\n",
   "            d = {}\n",
   "            d['virtual'] = virtual\n",
   "            d['links'] = {}\n",
   "            for (link_type, link_values) in iteritems(links):\n",
   "                d['links'][link_type] = link_values.get(block, [])\n",
   "            diskname = os.path.basename(sysdir)\n",
   "            for key in ['vendor', 'model', 'sas_address', 'sas_device_handle']:\n",
   "                d[key] = get_file_content(sysdir + \"/device/\" + key)\n",
   "            sg_inq = self.module.get_bin_path('sg_inq')\n",
   "            serial_path = \"/sys/block/%s/device/serial\" % (block)\n",
   "            if sg_inq:\n",
   "                device = \"/dev/%s\" % (block)\n",
   "                rc, drivedata, err = self.module.run_command([sg_inq, device])\n",
   "                if rc == 0:\n",
   "                    serial = re.search(r\"Unit serial number:\\s+(\\w+)\", drivedata)\n",
   "                    if serial:\n",
   "                        d['serial'] = serial.group(1)\n",
   "                serial = get_file_content(serial_path)\n",
   "                if serial:\n",
   "                    d['serial'] = serial\n",
   "            for key, test in [('removable', '/removable'),\n",
   "                d[key] = get_file_content(sysdir + test)\n",
   "            if diskname in devs_wwn:\n",
   "                d['wwn'] = devs_wwn[diskname]\n",
   "            d['partitions'] = {}\n",
   "            for folder in os.listdir(sysdir):\n",
   "                m = re.search(\"(\" + diskname + r\"[p]?\\d+)\", folder)\n",
   "                if m:\n",
   "                    part = {}\n",
   "                    partname = m.group(1)\n",
   "                    part_sysdir = sysdir + \"/\" + partname\n",
   "                    part['links'] = {}\n",
   "                    for (link_type, link_values) in iteritems(links):\n",
   "                        part['links'][link_type] = link_values.get(partname, [])\n",
   "                    part['start'] = get_file_content(part_sysdir + \"/start\", 0)\n",
   "                    part['sectors'] = get_file_content(part_sysdir + \"/size\", 0)\n",
   "                    part['sectorsize'] = get_file_content(part_sysdir + \"/queue/logical_block_size\")\n",
   "                    if not part['sectorsize']:\n",
   "                        part['sectorsize'] = get_file_content(part_sysdir + \"/queue/hw_sector_size\", 512)\n",
   "                    part['size'] = bytes_to_human((float(part['sectors']) * 512.0))\n",
   "                    part['uuid'] = get_partition_uuid(partname)\n",
   "                    self.get_holders(part, part_sysdir)\n",
   "                    d['partitions'][partname] = part\n",
   "            d['rotational'] = get_file_content(sysdir + \"/queue/rotational\")\n",
   "            d['scheduler_mode'] = \"\"\n",
   "            scheduler = get_file_content(sysdir + \"/queue/scheduler\")\n",
   "            if scheduler is not None:\n",
   "                m = re.match(r\".*?(\\[(.*)\\])\", scheduler)\n",
   "                if m:\n",
   "                    d['scheduler_mode'] = m.group(2)\n",
   "            d['sectors'] = get_file_content(sysdir + \"/size\")\n",
   "            if not d['sectors']:\n",
   "                d['sectors'] = 0\n",
   "            d['sectorsize'] = get_file_content(sysdir + \"/queue/logical_block_size\")\n",
   "            if not d['sectorsize']:\n",
   "                d['sectorsize'] = get_file_content(sysdir + \"/queue/hw_sector_size\", 512)\n",
   "            d['size'] = bytes_to_human(float(d['sectors']) * 512.0)\n",
   "            d['host'] = \"\"\n",
   "            m = re.match(r\".+/([a-f0-9]{4}:[a-f0-9]{2}:[0|1][a-f0-9]\\.[0-7])/\", sysdir)\n",
   "            if m and pcidata:\n",
   "                pciid = m.group(1)\n",
   "                did = re.escape(pciid)\n",
   "                m = re.search(\"^\" + did + r\"\\s(.*)$\", pcidata, re.MULTILINE)\n",
   "                if m:\n",
   "                    d['host'] = m.group(1)\n",
   "            self.get_holders(d, sysdir)\n",
   "            device_facts['devices'][diskname] = d\n",
   "        return device_facts\n",
   "            uptime_facts['uptime_seconds'] = int(float(uptime_seconds_string))\n",
   "        return uptime_facts\n",
   "            if rc == 0:\n",
   "            lvm_facts['lvm'] = {'lvs': lvs, 'vgs': vgs, 'pvs': pvs}\n",
   "        return lvm_facts\n"
  ]
 },
 "229": {
  "name": "BIND_MOUNT_RE",
  "type": "_sre.SRE_Pattern",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/linux.py",
  "lineno": "78",
  "column": "4",
  "context": " against findmnt output to detect bind mounts\n    BIND_MOUNT_RE = re.compile(r'.*\\]')\n\n    # regex used against mtab content to find ent",
  "context_lines": "    ORIGINAL_MEMORY_FACTS = frozenset(('MemTotal', 'SwapTotal', 'MemFree', 'SwapFree'))\n    # Now we have all of these in a dict structure\n    MEMORY_FACTS = ORIGINAL_MEMORY_FACTS.union(('Buffers', 'Cached', 'SwapCached'))\n\n    # regex used against findmnt output to detect bind mounts\n    BIND_MOUNT_RE = re.compile(r'.*\\]')\n\n    # regex used against mtab content to find entries that are bind mounts\n    MTAB_BIND_MOUNT_RE = re.compile(r'.*bind.*\"')\n\n    # regex used for replacing octal escape sequences\n",
  "slicing": [
   "def get_partition_uuid(partname):\n",
   "        uuids = os.listdir(\"/dev/disk/by-uuid\")\n",
   "    for uuid in uuids:\n",
   "        dev = os.path.realpath(\"/dev/disk/by-uuid/\" + uuid)\n",
   "        if dev == (\"/dev/\" + partname):\n",
   "            return uuid\n",
   "    ORIGINAL_MEMORY_FACTS = frozenset(('MemTotal', 'SwapTotal', 'MemFree', 'SwapFree'))\n",
   "    MEMORY_FACTS = ORIGINAL_MEMORY_FACTS.union(('Buffers', 'Cached', 'SwapCached'))\n",
   "    BIND_MOUNT_RE = re.compile(r'.*\\]')\n",
   "        hardware_facts = {}\n",
   "        cpu_facts = self.get_cpu_facts(collected_facts=collected_facts)\n",
   "        memory_facts = self.get_memory_facts()\n",
   "        dmi_facts = self.get_dmi_facts()\n",
   "        device_facts = self.get_device_facts()\n",
   "        uptime_facts = self.get_uptime_facts()\n",
   "        lvm_facts = self.get_lvm_facts()\n",
   "        mount_facts = {}\n",
   "            mount_facts = self.get_mount_facts()\n",
   "        hardware_facts.update(cpu_facts)\n",
   "        hardware_facts.update(memory_facts)\n",
   "        hardware_facts.update(dmi_facts)\n",
   "        hardware_facts.update(device_facts)\n",
   "        hardware_facts.update(uptime_facts)\n",
   "        hardware_facts.update(lvm_facts)\n",
   "        hardware_facts.update(mount_facts)\n",
   "        return hardware_facts\n",
   "        memory_facts = {}\n",
   "            return memory_facts\n",
   "        memstats = {}\n",
   "        for line in get_file_lines(\"/proc/meminfo\"):\n",
   "            data = line.split(\":\", 1)\n",
   "            key = data[0]\n",
   "            if key in self.ORIGINAL_MEMORY_FACTS:\n",
   "                val = data[1].strip().split(' ')[0]\n",
   "                memory_facts[\"%s_mb\" % key.lower()] = int(val) // 1024\n",
   "            if key in self.MEMORY_FACTS:\n",
   "                val = data[1].strip().split(' ')[0]\n",
   "                memstats[key.lower()] = int(val) // 1024\n",
   "        if None not in (memstats.get('memtotal'), memstats.get('memfree')):\n",
   "            memstats['real:used'] = memstats['memtotal'] - memstats['memfree']\n",
   "        if None not in (memstats.get('cached'), memstats.get('memfree'), memstats.get('buffers')):\n",
   "            memstats['nocache:free'] = memstats['cached'] + memstats['memfree'] + memstats['buffers']\n",
   "        if None not in (memstats.get('memtotal'), memstats.get('nocache:free')):\n",
   "            memstats['nocache:used'] = memstats['memtotal'] - memstats['nocache:free']\n",
   "        if None not in (memstats.get('swaptotal'), memstats.get('swapfree')):\n",
   "            memstats['swap:used'] = memstats['swaptotal'] - memstats['swapfree']\n",
   "        memory_facts['memory_mb'] = {\n",
   "                'total': memstats.get('memtotal'),\n",
   "                'used': memstats.get('real:used'),\n",
   "                'free': memstats.get('memfree'),\n",
   "                'free': memstats.get('nocache:free'),\n",
   "                'used': memstats.get('nocache:used'),\n",
   "                'total': memstats.get('swaptotal'),\n",
   "                'free': memstats.get('swapfree'),\n",
   "                'used': memstats.get('swap:used'),\n",
   "                'cached': memstats.get('swapcached'),\n",
   "        return memory_facts\n",
   "        cpu_facts = {}\n",
   "        collected_facts = collected_facts or {}\n",
   "        i = 0\n",
   "        vendor_id_occurrence = 0\n",
   "        model_name_occurrence = 0\n",
   "        processor_occurence = 0\n",
   "        physid = 0\n",
   "        coreid = 0\n",
   "        sockets = {}\n",
   "        cores = {}\n",
   "        xen = False\n",
   "        xen_paravirt = False\n",
   "                xen = True\n",
   "                for line in get_file_lines('/sys/hypervisor/type'):\n",
   "                    if line.strip() == 'xen':\n",
   "                        xen = True\n",
   "            return cpu_facts\n",
   "        cpu_facts['processor'] = []\n",
   "        for line in get_file_lines('/proc/cpuinfo'):\n",
   "            data = line.split(\":\", 1)\n",
   "            key = data[0].strip()\n",
   "                val = data[1].strip()\n",
   "                val = \"\"\n",
   "            if xen:\n",
   "                if key == 'flags':\n",
   "                    if 'vme' not in val:\n",
   "                        xen_paravirt = True\n",
   "            if key in ['model name', 'Processor', 'vendor_id', 'cpu', 'Vendor', 'processor']:\n",
   "                if 'processor' not in cpu_facts:\n",
   "                    cpu_facts['processor'] = []\n",
   "                cpu_facts['processor'].append(val)\n",
   "                if key == 'vendor_id':\n",
   "                    vendor_id_occurrence += 1\n",
   "                if key == 'model name':\n",
   "                    model_name_occurrence += 1\n",
   "                if key == 'processor':\n",
   "                    processor_occurence += 1\n",
   "                i += 1\n",
   "            elif key == 'physical id':\n",
   "                physid = val\n",
   "                if physid not in sockets:\n",
   "                    sockets[physid] = 1\n",
   "            elif key == 'core id':\n",
   "                coreid = val\n",
   "                if coreid not in sockets:\n",
   "                    cores[coreid] = 1\n",
   "            elif key == 'cpu cores':\n",
   "                sockets[physid] = int(val)\n",
   "            elif key == 'siblings':\n",
   "                cores[coreid] = int(val)\n",
   "            elif key == '# processors':\n",
   "                cpu_facts['processor_cores'] = int(val)\n",
   "            elif key == 'ncpus active':\n",
   "                i = int(val)\n",
   "        if vendor_id_occurrence > 0:\n",
   "            if vendor_id_occurrence == model_name_occurrence:\n",
   "                i = vendor_id_occurrence\n",
   "        if collected_facts.get('ansible_architecture', '').startswith(('armv', 'aarch', 'ppc')):\n",
   "            i = processor_occurence\n",
   "        if collected_facts.get('ansible_architecture') != 's390x':\n",
   "            if xen_paravirt:\n",
   "                cpu_facts['processor_count'] = i\n",
   "                cpu_facts['processor_cores'] = i\n",
   "                cpu_facts['processor_threads_per_core'] = 1\n",
   "                cpu_facts['processor_vcpus'] = i\n",
   "                if sockets:\n",
   "                    cpu_facts['processor_count'] = len(sockets)\n",
   "                    cpu_facts['processor_count'] = i\n",
   "                socket_values = list(sockets.values())\n",
   "                if socket_values and socket_values[0]:\n",
   "                    cpu_facts['processor_cores'] = socket_values[0]\n",
   "                    cpu_facts['processor_cores'] = 1\n",
   "                core_values = list(cores.values())\n",
   "                if core_values:\n",
   "                    cpu_facts['processor_threads_per_core'] = core_values[0] // cpu_facts['processor_cores']\n",
   "                    cpu_facts['processor_threads_per_core'] = 1 // cpu_facts['processor_cores']\n",
   "                cpu_facts['processor_vcpus'] = (cpu_facts['processor_threads_per_core'] *\n",
   "                                                cpu_facts['processor_count'] * cpu_facts['processor_cores'])\n",
   "                cpu_facts['processor_nproc'] = processor_occurence\n",
   "                    cpu_facts['processor_nproc'] = len(\n",
   "                        cmd = get_bin_path('nproc')\n",
   "                        rc, out, _err = self.module.run_command(cmd)\n",
   "                        if rc == 0:\n",
   "                            cpu_facts['processor_nproc'] = int(out)\n",
   "        return cpu_facts\n",
   "        dmi_facts = {}\n",
   "            FORM_FACTOR = [\"Unknown\", \"Other\", \"Unknown\", \"Desktop\",\n",
   "            DMI_DICT = {\n",
   "            for (key, path) in DMI_DICT.items():\n",
   "                data = get_file_content(path)\n",
   "                if data is not None:\n",
   "                    if key == 'form_factor':\n",
   "                            dmi_facts['form_factor'] = FORM_FACTOR[int(data)]\n",
   "                            dmi_facts['form_factor'] = 'unknown (%s)' % data\n",
   "                        dmi_facts[key] = data\n",
   "                    dmi_facts[key] = 'NA'\n",
   "            dmi_bin = self.module.get_bin_path('dmidecode')\n",
   "            DMI_DICT = {\n",
   "            for (k, v) in DMI_DICT.items():\n",
   "                if dmi_bin is not None:\n",
   "                    (rc, out, err) = self.module.run_command('%s -s %s' % (dmi_bin, v))\n",
   "                    if rc == 0:\n",
   "                        thisvalue = ''.join([line for line in out.splitlines() if not line.startswith('#')])\n",
   "                            json.dumps(thisvalue)\n",
   "                            thisvalue = \"NA\"\n",
   "                        dmi_facts[k] = thisvalue\n",
   "                        dmi_facts[k] = 'NA'\n",
   "                    dmi_facts[k] = 'NA'\n",
   "        return dmi_facts\n",
   "        args = ['--list', '--noheadings', '--paths', '--output', 'NAME,UUID', '--exclude', '2']\n",
   "        cmd = [lsblk_path] + args\n",
   "        rc, out, err = self.module.run_command(cmd)\n",
   "        return rc, out, err\n",
   "        uuids = {}\n",
   "        lsblk_path = self.module.get_bin_path(\"lsblk\")\n",
   "        if not lsblk_path:\n",
   "            return uuids\n",
   "        rc, out, err = self._run_lsblk(lsblk_path)\n",
   "        if rc != 0:\n",
   "            return uuids\n",
   "        for lsblk_line in out.splitlines():\n",
   "            if not lsblk_line:\n",
   "            line = lsblk_line.strip()\n",
   "            fields = line.rsplit(None, 1)\n",
   "            if len(fields) < 2:\n",
   "            device_name, uuid = fields[0].strip(), fields[1].strip()\n",
   "            if device_name in uuids:\n",
   "            uuids[device_name] = uuid\n",
   "        return uuids\n",
   "        uuid = 'N/A'\n",
   "        udevadm_path = self.module.get_bin_path('udevadm')\n",
   "        if not udevadm_path:\n",
   "            return uuid\n",
   "        cmd = [udevadm_path, 'info', '--query', 'property', '--name', device]\n",
   "        rc, out, err = self.module.run_command(cmd)\n",
   "        if rc != 0:\n",
   "            return uuid\n",
   "        m = re.search('ID_FS_UUID=(.*)\\n', out)\n",
   "        if m:\n",
   "            uuid = m.group(1)\n",
   "        return uuid\n",
   "        args = ['--list', '--noheadings', '--notruncate']\n",
   "        cmd = [findmnt_path] + args\n",
   "        rc, out, err = self.module.run_command(cmd, errors='surrogate_then_replace')\n",
   "        return rc, out, err\n",
   "        bind_mounts = set()\n",
   "        findmnt_path = self.module.get_bin_path(\"findmnt\")\n",
   "        if not findmnt_path:\n",
   "            return bind_mounts\n",
   "        rc, out, err = self._run_findmnt(findmnt_path)\n",
   "        if rc != 0:\n",
   "            return bind_mounts\n",
   "        for line in out.splitlines():\n",
   "            fields = line.split()\n",
   "            if len(fields) < 2:\n",
   "            if self.BIND_MOUNT_RE.match(fields[1]):\n",
   "                bind_mounts.add(fields[0])\n",
   "        return bind_mounts\n",
   "        mtab_file = '/etc/mtab'\n",
   "        if not os.path.exists(mtab_file):\n",
   "            mtab_file = '/proc/mounts'\n",
   "        mtab = get_file_content(mtab_file, '')\n",
   "        mtab_entries = []\n",
   "        for line in mtab.splitlines():\n",
   "            fields = line.split()\n",
   "            if len(fields) < 4:\n",
   "            mtab_entries.append(fields)\n",
   "        return mtab_entries\n",
   "        mount_size = get_mount_size(mount)\n",
   "        uuid = uuids.get(device, self._udevadm_uuid(device))\n",
   "        return mount_size, uuid\n",
   "        mounts = []\n",
   "        bind_mounts = self._find_bind_mounts()\n",
   "        uuids = self._lsblk_uuid()\n",
   "        mtab_entries = self._mtab_entries()\n",
   "        results = {}\n",
   "        pool = ThreadPool(processes=min(len(mtab_entries), cpu_count()))\n",
   "        maxtime = globals().get('GATHER_TIMEOUT') or timeout.DEFAULT_GATHER_TIMEOUT\n",
   "        for fields in mtab_entries:\n",
   "            fields = [self._replace_octal_escapes(field) for field in fields]\n",
   "            device, mount, fstype, options = fields[0], fields[1], fields[2], fields[3]\n",
   "            if not device.startswith(('/', '\\\\')) and ':/' not in device or fstype == 'none':\n",
   "            mount_info = {'mount': mount,\n",
   "                          'device': device,\n",
   "                          'fstype': fstype,\n",
   "                          'options': options}\n",
   "            if mount in bind_mounts:\n",
   "                if not self.MTAB_BIND_MOUNT_RE.match(options):\n",
   "                    mount_info['options'] += \",bind\"\n",
   "            results[mount] = {'info': mount_info,\n",
   "                              'extra': pool.apply_async(self.get_mount_info, (mount, device, uuids)),\n",
   "                              'timelimit': time.time() + maxtime}\n",
   "        pool.close()  # done with new workers, start gc\n",
   "        while results:\n",
   "            for mount in results:\n",
   "                res = results[mount]['extra']\n",
   "                if res.ready():\n",
   "                    if res.successful():\n",
   "                        mount_size, uuid = res.get()\n",
   "                        if mount_size:\n",
   "                            results[mount]['info'].update(mount_size)\n",
   "                        results[mount]['info']['uuid'] = uuid or 'N/A'\n",
   "                        errmsg = to_text(res.get())\n",
   "                        self.module.warn(\"Error prevented getting extra info for mount %s: %s.\" % (mount, errmsg))\n",
   "                        results[mount]['info']['note'] = 'Could not get extra information: %s.' % (errmsg)\n",
   "                    mounts.append(results[mount]['info'])\n",
   "                    del results[mount]\n",
   "                elif time.time() > results[mount]['timelimit']:\n",
   "                    results[mount]['info']['note'] = 'Timed out while attempting to get extra information.'\n",
   "                    mounts.append(results[mount]['info'])\n",
   "                    del results[mount]\n",
   "        return {'mounts': mounts}\n",
   "            retval = collections.defaultdict(set)\n",
   "            for entry in os.listdir(link_dir):\n",
   "                    target = os.path.basename(os.readlink(os.path.join(link_dir, entry)))\n",
   "                    retval[target].add(entry)\n",
   "            return dict((k, list(sorted(v))) for (k, v) in iteritems(retval))\n",
   "            retval = collections.defaultdict(set)\n",
   "            for path in glob.glob('/sys/block/*/slaves/*'):\n",
   "                elements = path.split('/')\n",
   "                device = elements[3]\n",
   "                target = elements[5]\n",
   "                retval[target].add(device)\n",
   "            return dict((k, list(sorted(v))) for (k, v) in iteritems(retval))\n",
   "            for folder in os.listdir(sysdir + \"/holders\"):\n",
   "                if not folder.startswith(\"dm-\"):\n",
   "                name = get_file_content(sysdir + \"/holders/\" + folder + \"/dm/name\")\n",
   "                if name:\n",
   "                    block_dev_dict['holders'].append(name)\n",
   "                    block_dev_dict['holders'].append(folder)\n",
   "        device_facts = {}\n",
   "        device_facts['devices'] = {}\n",
   "        lspci = self.module.get_bin_path('lspci')\n",
   "        if lspci:\n",
   "            rc, pcidata, err = self.module.run_command([lspci, '-D'], errors='surrogate_then_replace')\n",
   "            pcidata = None\n",
   "            block_devs = os.listdir(\"/sys/block\")\n",
   "            return device_facts\n",
   "        devs_wwn = {}\n",
   "            devs_by_id = os.listdir(\"/dev/disk/by-id\")\n",
   "            for link_name in devs_by_id:\n",
   "                if link_name.startswith(\"wwn-\"):\n",
   "                        wwn_link = os.readlink(os.path.join(\"/dev/disk/by-id\", link_name))\n",
   "                    devs_wwn[os.path.basename(wwn_link)] = link_name[4:]\n",
   "        links = self.get_all_device_links()\n",
   "        device_facts['device_links'] = links\n",
   "        for block in block_devs:\n",
   "            virtual = 1\n",
   "            sysfs_no_links = 0\n",
   "                path = os.readlink(os.path.join(\"/sys/block/\", block))\n",
   "                e = sys.exc_info()[1]\n",
   "                if e.errno == errno.EINVAL:\n",
   "                    path = block\n",
   "                    sysfs_no_links = 1\n",
   "            sysdir = os.path.join(\"/sys/block\", path)\n",
   "            if sysfs_no_links == 1:\n",
   "                for folder in os.listdir(sysdir):\n",
   "                    if \"device\" in folder:\n",
   "                        virtual = 0\n",
   "            d = {}\n",
   "            d['virtual'] = virtual\n",
   "            d['links'] = {}\n",
   "            for (link_type, link_values) in iteritems(links):\n",
   "                d['links'][link_type] = link_values.get(block, [])\n",
   "            diskname = os.path.basename(sysdir)\n",
   "            for key in ['vendor', 'model', 'sas_address', 'sas_device_handle']:\n",
   "                d[key] = get_file_content(sysdir + \"/device/\" + key)\n",
   "            sg_inq = self.module.get_bin_path('sg_inq')\n",
   "            serial_path = \"/sys/block/%s/device/serial\" % (block)\n",
   "            if sg_inq:\n",
   "                device = \"/dev/%s\" % (block)\n",
   "                rc, drivedata, err = self.module.run_command([sg_inq, device])\n",
   "                if rc == 0:\n",
   "                    serial = re.search(r\"Unit serial number:\\s+(\\w+)\", drivedata)\n",
   "                    if serial:\n",
   "                        d['serial'] = serial.group(1)\n",
   "                serial = get_file_content(serial_path)\n",
   "                if serial:\n",
   "                    d['serial'] = serial\n",
   "            for key, test in [('removable', '/removable'),\n",
   "                d[key] = get_file_content(sysdir + test)\n",
   "            if diskname in devs_wwn:\n",
   "                d['wwn'] = devs_wwn[diskname]\n",
   "            d['partitions'] = {}\n",
   "            for folder in os.listdir(sysdir):\n",
   "                m = re.search(\"(\" + diskname + r\"[p]?\\d+)\", folder)\n",
   "                if m:\n",
   "                    part = {}\n",
   "                    partname = m.group(1)\n",
   "                    part_sysdir = sysdir + \"/\" + partname\n",
   "                    part['links'] = {}\n",
   "                    for (link_type, link_values) in iteritems(links):\n",
   "                        part['links'][link_type] = link_values.get(partname, [])\n",
   "                    part['start'] = get_file_content(part_sysdir + \"/start\", 0)\n",
   "                    part['sectors'] = get_file_content(part_sysdir + \"/size\", 0)\n",
   "                    part['sectorsize'] = get_file_content(part_sysdir + \"/queue/logical_block_size\")\n",
   "                    if not part['sectorsize']:\n",
   "                        part['sectorsize'] = get_file_content(part_sysdir + \"/queue/hw_sector_size\", 512)\n",
   "                    part['size'] = bytes_to_human((float(part['sectors']) * 512.0))\n",
   "                    part['uuid'] = get_partition_uuid(partname)\n",
   "                    self.get_holders(part, part_sysdir)\n",
   "                    d['partitions'][partname] = part\n",
   "            d['rotational'] = get_file_content(sysdir + \"/queue/rotational\")\n",
   "            d['scheduler_mode'] = \"\"\n",
   "            scheduler = get_file_content(sysdir + \"/queue/scheduler\")\n",
   "            if scheduler is not None:\n",
   "                m = re.match(r\".*?(\\[(.*)\\])\", scheduler)\n",
   "                if m:\n",
   "                    d['scheduler_mode'] = m.group(2)\n",
   "            d['sectors'] = get_file_content(sysdir + \"/size\")\n",
   "            if not d['sectors']:\n",
   "                d['sectors'] = 0\n",
   "            d['sectorsize'] = get_file_content(sysdir + \"/queue/logical_block_size\")\n",
   "            if not d['sectorsize']:\n",
   "                d['sectorsize'] = get_file_content(sysdir + \"/queue/hw_sector_size\", 512)\n",
   "            d['size'] = bytes_to_human(float(d['sectors']) * 512.0)\n",
   "            d['host'] = \"\"\n",
   "            m = re.match(r\".+/([a-f0-9]{4}:[a-f0-9]{2}:[0|1][a-f0-9]\\.[0-7])/\", sysdir)\n",
   "            if m and pcidata:\n",
   "                pciid = m.group(1)\n",
   "                did = re.escape(pciid)\n",
   "                m = re.search(\"^\" + did + r\"\\s(.*)$\", pcidata, re.MULTILINE)\n",
   "                if m:\n",
   "                    d['host'] = m.group(1)\n",
   "            self.get_holders(d, sysdir)\n",
   "            device_facts['devices'][diskname] = d\n",
   "        return device_facts\n",
   "            uptime_facts['uptime_seconds'] = int(float(uptime_seconds_string))\n",
   "        return uptime_facts\n",
   "            if rc == 0:\n",
   "            lvm_facts['lvm'] = {'lvs': lvs, 'vgs': vgs, 'pvs': pvs}\n",
   "        return lvm_facts\n"
  ]
 },
 "230": {
  "name": "MTAB_BIND_MOUNT_RE",
  "type": "_sre.SRE_Pattern",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/linux.py",
  "lineno": "81",
  "column": "4",
  "context": " content to find entries that are bind mounts\n    MTAB_BIND_MOUNT_RE = re.compile(r'.*bind.*\"')\n\n    # regex used for replacing octal escape seque",
  "context_lines": "    MEMORY_FACTS = ORIGINAL_MEMORY_FACTS.union(('Buffers', 'Cached', 'SwapCached'))\n\n    # regex used against findmnt output to detect bind mounts\n    BIND_MOUNT_RE = re.compile(r'.*\\]')\n\n    # regex used against mtab content to find entries that are bind mounts\n    MTAB_BIND_MOUNT_RE = re.compile(r'.*bind.*\"')\n\n    # regex used for replacing octal escape sequences\n    OCTAL_ESCAPE_RE = re.compile(r'\\\\[0-9]{3}')\n\n    def populate(self, collected_facts=None):\n",
  "slicing": [
   "def get_partition_uuid(partname):\n",
   "        uuids = os.listdir(\"/dev/disk/by-uuid\")\n",
   "    for uuid in uuids:\n",
   "        dev = os.path.realpath(\"/dev/disk/by-uuid/\" + uuid)\n",
   "        if dev == (\"/dev/\" + partname):\n",
   "            return uuid\n",
   "    ORIGINAL_MEMORY_FACTS = frozenset(('MemTotal', 'SwapTotal', 'MemFree', 'SwapFree'))\n",
   "    MEMORY_FACTS = ORIGINAL_MEMORY_FACTS.union(('Buffers', 'Cached', 'SwapCached'))\n",
   "    MTAB_BIND_MOUNT_RE = re.compile(r'.*bind.*\"')\n",
   "        hardware_facts = {}\n",
   "        cpu_facts = self.get_cpu_facts(collected_facts=collected_facts)\n",
   "        memory_facts = self.get_memory_facts()\n",
   "        dmi_facts = self.get_dmi_facts()\n",
   "        device_facts = self.get_device_facts()\n",
   "        uptime_facts = self.get_uptime_facts()\n",
   "        lvm_facts = self.get_lvm_facts()\n",
   "        mount_facts = {}\n",
   "            mount_facts = self.get_mount_facts()\n",
   "        hardware_facts.update(cpu_facts)\n",
   "        hardware_facts.update(memory_facts)\n",
   "        hardware_facts.update(dmi_facts)\n",
   "        hardware_facts.update(device_facts)\n",
   "        hardware_facts.update(uptime_facts)\n",
   "        hardware_facts.update(lvm_facts)\n",
   "        hardware_facts.update(mount_facts)\n",
   "        return hardware_facts\n",
   "        memory_facts = {}\n",
   "            return memory_facts\n",
   "        memstats = {}\n",
   "        for line in get_file_lines(\"/proc/meminfo\"):\n",
   "            data = line.split(\":\", 1)\n",
   "            key = data[0]\n",
   "            if key in self.ORIGINAL_MEMORY_FACTS:\n",
   "                val = data[1].strip().split(' ')[0]\n",
   "                memory_facts[\"%s_mb\" % key.lower()] = int(val) // 1024\n",
   "            if key in self.MEMORY_FACTS:\n",
   "                val = data[1].strip().split(' ')[0]\n",
   "                memstats[key.lower()] = int(val) // 1024\n",
   "        if None not in (memstats.get('memtotal'), memstats.get('memfree')):\n",
   "            memstats['real:used'] = memstats['memtotal'] - memstats['memfree']\n",
   "        if None not in (memstats.get('cached'), memstats.get('memfree'), memstats.get('buffers')):\n",
   "            memstats['nocache:free'] = memstats['cached'] + memstats['memfree'] + memstats['buffers']\n",
   "        if None not in (memstats.get('memtotal'), memstats.get('nocache:free')):\n",
   "            memstats['nocache:used'] = memstats['memtotal'] - memstats['nocache:free']\n",
   "        if None not in (memstats.get('swaptotal'), memstats.get('swapfree')):\n",
   "            memstats['swap:used'] = memstats['swaptotal'] - memstats['swapfree']\n",
   "        memory_facts['memory_mb'] = {\n",
   "                'total': memstats.get('memtotal'),\n",
   "                'used': memstats.get('real:used'),\n",
   "                'free': memstats.get('memfree'),\n",
   "                'free': memstats.get('nocache:free'),\n",
   "                'used': memstats.get('nocache:used'),\n",
   "                'total': memstats.get('swaptotal'),\n",
   "                'free': memstats.get('swapfree'),\n",
   "                'used': memstats.get('swap:used'),\n",
   "                'cached': memstats.get('swapcached'),\n",
   "        return memory_facts\n",
   "        cpu_facts = {}\n",
   "        collected_facts = collected_facts or {}\n",
   "        i = 0\n",
   "        vendor_id_occurrence = 0\n",
   "        model_name_occurrence = 0\n",
   "        processor_occurence = 0\n",
   "        physid = 0\n",
   "        coreid = 0\n",
   "        sockets = {}\n",
   "        cores = {}\n",
   "        xen = False\n",
   "        xen_paravirt = False\n",
   "                xen = True\n",
   "                for line in get_file_lines('/sys/hypervisor/type'):\n",
   "                    if line.strip() == 'xen':\n",
   "                        xen = True\n",
   "            return cpu_facts\n",
   "        cpu_facts['processor'] = []\n",
   "        for line in get_file_lines('/proc/cpuinfo'):\n",
   "            data = line.split(\":\", 1)\n",
   "            key = data[0].strip()\n",
   "                val = data[1].strip()\n",
   "                val = \"\"\n",
   "            if xen:\n",
   "                if key == 'flags':\n",
   "                    if 'vme' not in val:\n",
   "                        xen_paravirt = True\n",
   "            if key in ['model name', 'Processor', 'vendor_id', 'cpu', 'Vendor', 'processor']:\n",
   "                if 'processor' not in cpu_facts:\n",
   "                    cpu_facts['processor'] = []\n",
   "                cpu_facts['processor'].append(val)\n",
   "                if key == 'vendor_id':\n",
   "                    vendor_id_occurrence += 1\n",
   "                if key == 'model name':\n",
   "                    model_name_occurrence += 1\n",
   "                if key == 'processor':\n",
   "                    processor_occurence += 1\n",
   "                i += 1\n",
   "            elif key == 'physical id':\n",
   "                physid = val\n",
   "                if physid not in sockets:\n",
   "                    sockets[physid] = 1\n",
   "            elif key == 'core id':\n",
   "                coreid = val\n",
   "                if coreid not in sockets:\n",
   "                    cores[coreid] = 1\n",
   "            elif key == 'cpu cores':\n",
   "                sockets[physid] = int(val)\n",
   "            elif key == 'siblings':\n",
   "                cores[coreid] = int(val)\n",
   "            elif key == '# processors':\n",
   "                cpu_facts['processor_cores'] = int(val)\n",
   "            elif key == 'ncpus active':\n",
   "                i = int(val)\n",
   "        if vendor_id_occurrence > 0:\n",
   "            if vendor_id_occurrence == model_name_occurrence:\n",
   "                i = vendor_id_occurrence\n",
   "        if collected_facts.get('ansible_architecture', '').startswith(('armv', 'aarch', 'ppc')):\n",
   "            i = processor_occurence\n",
   "        if collected_facts.get('ansible_architecture') != 's390x':\n",
   "            if xen_paravirt:\n",
   "                cpu_facts['processor_count'] = i\n",
   "                cpu_facts['processor_cores'] = i\n",
   "                cpu_facts['processor_threads_per_core'] = 1\n",
   "                cpu_facts['processor_vcpus'] = i\n",
   "                if sockets:\n",
   "                    cpu_facts['processor_count'] = len(sockets)\n",
   "                    cpu_facts['processor_count'] = i\n",
   "                socket_values = list(sockets.values())\n",
   "                if socket_values and socket_values[0]:\n",
   "                    cpu_facts['processor_cores'] = socket_values[0]\n",
   "                    cpu_facts['processor_cores'] = 1\n",
   "                core_values = list(cores.values())\n",
   "                if core_values:\n",
   "                    cpu_facts['processor_threads_per_core'] = core_values[0] // cpu_facts['processor_cores']\n",
   "                    cpu_facts['processor_threads_per_core'] = 1 // cpu_facts['processor_cores']\n",
   "                cpu_facts['processor_vcpus'] = (cpu_facts['processor_threads_per_core'] *\n",
   "                                                cpu_facts['processor_count'] * cpu_facts['processor_cores'])\n",
   "                cpu_facts['processor_nproc'] = processor_occurence\n",
   "                    cpu_facts['processor_nproc'] = len(\n",
   "                        cmd = get_bin_path('nproc')\n",
   "                        rc, out, _err = self.module.run_command(cmd)\n",
   "                        if rc == 0:\n",
   "                            cpu_facts['processor_nproc'] = int(out)\n",
   "        return cpu_facts\n",
   "        dmi_facts = {}\n",
   "            FORM_FACTOR = [\"Unknown\", \"Other\", \"Unknown\", \"Desktop\",\n",
   "            DMI_DICT = {\n",
   "            for (key, path) in DMI_DICT.items():\n",
   "                data = get_file_content(path)\n",
   "                if data is not None:\n",
   "                    if key == 'form_factor':\n",
   "                            dmi_facts['form_factor'] = FORM_FACTOR[int(data)]\n",
   "                            dmi_facts['form_factor'] = 'unknown (%s)' % data\n",
   "                        dmi_facts[key] = data\n",
   "                    dmi_facts[key] = 'NA'\n",
   "            dmi_bin = self.module.get_bin_path('dmidecode')\n",
   "            DMI_DICT = {\n",
   "            for (k, v) in DMI_DICT.items():\n",
   "                if dmi_bin is not None:\n",
   "                    (rc, out, err) = self.module.run_command('%s -s %s' % (dmi_bin, v))\n",
   "                    if rc == 0:\n",
   "                        thisvalue = ''.join([line for line in out.splitlines() if not line.startswith('#')])\n",
   "                            json.dumps(thisvalue)\n",
   "                            thisvalue = \"NA\"\n",
   "                        dmi_facts[k] = thisvalue\n",
   "                        dmi_facts[k] = 'NA'\n",
   "                    dmi_facts[k] = 'NA'\n",
   "        return dmi_facts\n",
   "        args = ['--list', '--noheadings', '--paths', '--output', 'NAME,UUID', '--exclude', '2']\n",
   "        cmd = [lsblk_path] + args\n",
   "        rc, out, err = self.module.run_command(cmd)\n",
   "        return rc, out, err\n",
   "        uuids = {}\n",
   "        lsblk_path = self.module.get_bin_path(\"lsblk\")\n",
   "        if not lsblk_path:\n",
   "            return uuids\n",
   "        rc, out, err = self._run_lsblk(lsblk_path)\n",
   "        if rc != 0:\n",
   "            return uuids\n",
   "        for lsblk_line in out.splitlines():\n",
   "            if not lsblk_line:\n",
   "            line = lsblk_line.strip()\n",
   "            fields = line.rsplit(None, 1)\n",
   "            if len(fields) < 2:\n",
   "            device_name, uuid = fields[0].strip(), fields[1].strip()\n",
   "            if device_name in uuids:\n",
   "            uuids[device_name] = uuid\n",
   "        return uuids\n",
   "        uuid = 'N/A'\n",
   "        udevadm_path = self.module.get_bin_path('udevadm')\n",
   "        if not udevadm_path:\n",
   "            return uuid\n",
   "        cmd = [udevadm_path, 'info', '--query', 'property', '--name', device]\n",
   "        rc, out, err = self.module.run_command(cmd)\n",
   "        if rc != 0:\n",
   "            return uuid\n",
   "        m = re.search('ID_FS_UUID=(.*)\\n', out)\n",
   "        if m:\n",
   "            uuid = m.group(1)\n",
   "        return uuid\n",
   "        args = ['--list', '--noheadings', '--notruncate']\n",
   "        cmd = [findmnt_path] + args\n",
   "        rc, out, err = self.module.run_command(cmd, errors='surrogate_then_replace')\n",
   "        return rc, out, err\n",
   "        bind_mounts = set()\n",
   "        findmnt_path = self.module.get_bin_path(\"findmnt\")\n",
   "        if not findmnt_path:\n",
   "            return bind_mounts\n",
   "        rc, out, err = self._run_findmnt(findmnt_path)\n",
   "        if rc != 0:\n",
   "            return bind_mounts\n",
   "        for line in out.splitlines():\n",
   "            fields = line.split()\n",
   "            if len(fields) < 2:\n",
   "            if self.BIND_MOUNT_RE.match(fields[1]):\n",
   "                bind_mounts.add(fields[0])\n",
   "        return bind_mounts\n",
   "        mtab_file = '/etc/mtab'\n",
   "        if not os.path.exists(mtab_file):\n",
   "            mtab_file = '/proc/mounts'\n",
   "        mtab = get_file_content(mtab_file, '')\n",
   "        mtab_entries = []\n",
   "        for line in mtab.splitlines():\n",
   "            fields = line.split()\n",
   "            if len(fields) < 4:\n",
   "            mtab_entries.append(fields)\n",
   "        return mtab_entries\n",
   "        mount_size = get_mount_size(mount)\n",
   "        uuid = uuids.get(device, self._udevadm_uuid(device))\n",
   "        return mount_size, uuid\n",
   "        mounts = []\n",
   "        bind_mounts = self._find_bind_mounts()\n",
   "        uuids = self._lsblk_uuid()\n",
   "        mtab_entries = self._mtab_entries()\n",
   "        results = {}\n",
   "        pool = ThreadPool(processes=min(len(mtab_entries), cpu_count()))\n",
   "        maxtime = globals().get('GATHER_TIMEOUT') or timeout.DEFAULT_GATHER_TIMEOUT\n",
   "        for fields in mtab_entries:\n",
   "            fields = [self._replace_octal_escapes(field) for field in fields]\n",
   "            device, mount, fstype, options = fields[0], fields[1], fields[2], fields[3]\n",
   "            if not device.startswith(('/', '\\\\')) and ':/' not in device or fstype == 'none':\n",
   "            mount_info = {'mount': mount,\n",
   "                          'device': device,\n",
   "                          'fstype': fstype,\n",
   "                          'options': options}\n",
   "            if mount in bind_mounts:\n",
   "                if not self.MTAB_BIND_MOUNT_RE.match(options):\n",
   "                    mount_info['options'] += \",bind\"\n",
   "            results[mount] = {'info': mount_info,\n",
   "                              'extra': pool.apply_async(self.get_mount_info, (mount, device, uuids)),\n",
   "                              'timelimit': time.time() + maxtime}\n",
   "        pool.close()  # done with new workers, start gc\n",
   "        while results:\n",
   "            for mount in results:\n",
   "                res = results[mount]['extra']\n",
   "                if res.ready():\n",
   "                    if res.successful():\n",
   "                        mount_size, uuid = res.get()\n",
   "                        if mount_size:\n",
   "                            results[mount]['info'].update(mount_size)\n",
   "                        results[mount]['info']['uuid'] = uuid or 'N/A'\n",
   "                        errmsg = to_text(res.get())\n",
   "                        self.module.warn(\"Error prevented getting extra info for mount %s: %s.\" % (mount, errmsg))\n",
   "                        results[mount]['info']['note'] = 'Could not get extra information: %s.' % (errmsg)\n",
   "                    mounts.append(results[mount]['info'])\n",
   "                    del results[mount]\n",
   "                elif time.time() > results[mount]['timelimit']:\n",
   "                    results[mount]['info']['note'] = 'Timed out while attempting to get extra information.'\n",
   "                    mounts.append(results[mount]['info'])\n",
   "                    del results[mount]\n",
   "        return {'mounts': mounts}\n",
   "            retval = collections.defaultdict(set)\n",
   "            for entry in os.listdir(link_dir):\n",
   "                    target = os.path.basename(os.readlink(os.path.join(link_dir, entry)))\n",
   "                    retval[target].add(entry)\n",
   "            return dict((k, list(sorted(v))) for (k, v) in iteritems(retval))\n",
   "            retval = collections.defaultdict(set)\n",
   "            for path in glob.glob('/sys/block/*/slaves/*'):\n",
   "                elements = path.split('/')\n",
   "                device = elements[3]\n",
   "                target = elements[5]\n",
   "                retval[target].add(device)\n",
   "            return dict((k, list(sorted(v))) for (k, v) in iteritems(retval))\n",
   "            for folder in os.listdir(sysdir + \"/holders\"):\n",
   "                if not folder.startswith(\"dm-\"):\n",
   "                name = get_file_content(sysdir + \"/holders/\" + folder + \"/dm/name\")\n",
   "                if name:\n",
   "                    block_dev_dict['holders'].append(name)\n",
   "                    block_dev_dict['holders'].append(folder)\n",
   "        device_facts = {}\n",
   "        device_facts['devices'] = {}\n",
   "        lspci = self.module.get_bin_path('lspci')\n",
   "        if lspci:\n",
   "            rc, pcidata, err = self.module.run_command([lspci, '-D'], errors='surrogate_then_replace')\n",
   "            pcidata = None\n",
   "            block_devs = os.listdir(\"/sys/block\")\n",
   "            return device_facts\n",
   "        devs_wwn = {}\n",
   "            devs_by_id = os.listdir(\"/dev/disk/by-id\")\n",
   "            for link_name in devs_by_id:\n",
   "                if link_name.startswith(\"wwn-\"):\n",
   "                        wwn_link = os.readlink(os.path.join(\"/dev/disk/by-id\", link_name))\n",
   "                    devs_wwn[os.path.basename(wwn_link)] = link_name[4:]\n",
   "        links = self.get_all_device_links()\n",
   "        device_facts['device_links'] = links\n",
   "        for block in block_devs:\n",
   "            virtual = 1\n",
   "            sysfs_no_links = 0\n",
   "                path = os.readlink(os.path.join(\"/sys/block/\", block))\n",
   "                e = sys.exc_info()[1]\n",
   "                if e.errno == errno.EINVAL:\n",
   "                    path = block\n",
   "                    sysfs_no_links = 1\n",
   "            sysdir = os.path.join(\"/sys/block\", path)\n",
   "            if sysfs_no_links == 1:\n",
   "                for folder in os.listdir(sysdir):\n",
   "                    if \"device\" in folder:\n",
   "                        virtual = 0\n",
   "            d = {}\n",
   "            d['virtual'] = virtual\n",
   "            d['links'] = {}\n",
   "            for (link_type, link_values) in iteritems(links):\n",
   "                d['links'][link_type] = link_values.get(block, [])\n",
   "            diskname = os.path.basename(sysdir)\n",
   "            for key in ['vendor', 'model', 'sas_address', 'sas_device_handle']:\n",
   "                d[key] = get_file_content(sysdir + \"/device/\" + key)\n",
   "            sg_inq = self.module.get_bin_path('sg_inq')\n",
   "            serial_path = \"/sys/block/%s/device/serial\" % (block)\n",
   "            if sg_inq:\n",
   "                device = \"/dev/%s\" % (block)\n",
   "                rc, drivedata, err = self.module.run_command([sg_inq, device])\n",
   "                if rc == 0:\n",
   "                    serial = re.search(r\"Unit serial number:\\s+(\\w+)\", drivedata)\n",
   "                    if serial:\n",
   "                        d['serial'] = serial.group(1)\n",
   "                serial = get_file_content(serial_path)\n",
   "                if serial:\n",
   "                    d['serial'] = serial\n",
   "            for key, test in [('removable', '/removable'),\n",
   "                d[key] = get_file_content(sysdir + test)\n",
   "            if diskname in devs_wwn:\n",
   "                d['wwn'] = devs_wwn[diskname]\n",
   "            d['partitions'] = {}\n",
   "            for folder in os.listdir(sysdir):\n",
   "                m = re.search(\"(\" + diskname + r\"[p]?\\d+)\", folder)\n",
   "                if m:\n",
   "                    part = {}\n",
   "                    partname = m.group(1)\n",
   "                    part_sysdir = sysdir + \"/\" + partname\n",
   "                    part['links'] = {}\n",
   "                    for (link_type, link_values) in iteritems(links):\n",
   "                        part['links'][link_type] = link_values.get(partname, [])\n",
   "                    part['start'] = get_file_content(part_sysdir + \"/start\", 0)\n",
   "                    part['sectors'] = get_file_content(part_sysdir + \"/size\", 0)\n",
   "                    part['sectorsize'] = get_file_content(part_sysdir + \"/queue/logical_block_size\")\n",
   "                    if not part['sectorsize']:\n",
   "                        part['sectorsize'] = get_file_content(part_sysdir + \"/queue/hw_sector_size\", 512)\n",
   "                    part['size'] = bytes_to_human((float(part['sectors']) * 512.0))\n",
   "                    part['uuid'] = get_partition_uuid(partname)\n",
   "                    self.get_holders(part, part_sysdir)\n",
   "                    d['partitions'][partname] = part\n",
   "            d['rotational'] = get_file_content(sysdir + \"/queue/rotational\")\n",
   "            d['scheduler_mode'] = \"\"\n",
   "            scheduler = get_file_content(sysdir + \"/queue/scheduler\")\n",
   "            if scheduler is not None:\n",
   "                m = re.match(r\".*?(\\[(.*)\\])\", scheduler)\n",
   "                if m:\n",
   "                    d['scheduler_mode'] = m.group(2)\n",
   "            d['sectors'] = get_file_content(sysdir + \"/size\")\n",
   "            if not d['sectors']:\n",
   "                d['sectors'] = 0\n",
   "            d['sectorsize'] = get_file_content(sysdir + \"/queue/logical_block_size\")\n",
   "            if not d['sectorsize']:\n",
   "                d['sectorsize'] = get_file_content(sysdir + \"/queue/hw_sector_size\", 512)\n",
   "            d['size'] = bytes_to_human(float(d['sectors']) * 512.0)\n",
   "            d['host'] = \"\"\n",
   "            m = re.match(r\".+/([a-f0-9]{4}:[a-f0-9]{2}:[0|1][a-f0-9]\\.[0-7])/\", sysdir)\n",
   "            if m and pcidata:\n",
   "                pciid = m.group(1)\n",
   "                did = re.escape(pciid)\n",
   "                m = re.search(\"^\" + did + r\"\\s(.*)$\", pcidata, re.MULTILINE)\n",
   "                if m:\n",
   "                    d['host'] = m.group(1)\n",
   "            self.get_holders(d, sysdir)\n",
   "            device_facts['devices'][diskname] = d\n",
   "        return device_facts\n",
   "            uptime_facts['uptime_seconds'] = int(float(uptime_seconds_string))\n",
   "        return uptime_facts\n",
   "            if rc == 0:\n",
   "            lvm_facts['lvm'] = {'lvs': lvs, 'vgs': vgs, 'pvs': pvs}\n",
   "        return lvm_facts\n"
  ]
 },
 "231": {
  "name": "OCTAL_ESCAPE_RE",
  "type": "_sre.SRE_Pattern",
  "class": "unknown",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/linux.py",
  "lineno": "84",
  "column": "4",
  "context": "gex used for replacing octal escape sequences\n    OCTAL_ESCAPE_RE = re.compile(r'\\\\[0-9]{3}')\n\n    def populate(self, collected_facts=None):\n   ",
  "context_lines": "    BIND_MOUNT_RE = re.compile(r'.*\\]')\n\n    # regex used against mtab content to find entries that are bind mounts\n    MTAB_BIND_MOUNT_RE = re.compile(r'.*bind.*\"')\n\n    # regex used for replacing octal escape sequences\n    OCTAL_ESCAPE_RE = re.compile(r'\\\\[0-9]{3}')\n\n    def populate(self, collected_facts=None):\n        hardware_facts = {}\n        self.module.run_command_environ_update = {'LANG': 'C', 'LC_ALL': 'C', 'LC_NUMERIC': 'C'}\n\n",
  "slicing": [
   "def get_partition_uuid(partname):\n",
   "        uuids = os.listdir(\"/dev/disk/by-uuid\")\n",
   "    for uuid in uuids:\n",
   "        dev = os.path.realpath(\"/dev/disk/by-uuid/\" + uuid)\n",
   "        if dev == (\"/dev/\" + partname):\n",
   "            return uuid\n",
   "    ORIGINAL_MEMORY_FACTS = frozenset(('MemTotal', 'SwapTotal', 'MemFree', 'SwapFree'))\n",
   "    MEMORY_FACTS = ORIGINAL_MEMORY_FACTS.union(('Buffers', 'Cached', 'SwapCached'))\n",
   "    OCTAL_ESCAPE_RE = re.compile(r'\\\\[0-9]{3}')\n",
   "        hardware_facts = {}\n",
   "        cpu_facts = self.get_cpu_facts(collected_facts=collected_facts)\n",
   "        memory_facts = self.get_memory_facts()\n",
   "        dmi_facts = self.get_dmi_facts()\n",
   "        device_facts = self.get_device_facts()\n",
   "        uptime_facts = self.get_uptime_facts()\n",
   "        lvm_facts = self.get_lvm_facts()\n",
   "        mount_facts = {}\n",
   "            mount_facts = self.get_mount_facts()\n",
   "        hardware_facts.update(cpu_facts)\n",
   "        hardware_facts.update(memory_facts)\n",
   "        hardware_facts.update(dmi_facts)\n",
   "        hardware_facts.update(device_facts)\n",
   "        hardware_facts.update(uptime_facts)\n",
   "        hardware_facts.update(lvm_facts)\n",
   "        hardware_facts.update(mount_facts)\n",
   "        return hardware_facts\n",
   "        memory_facts = {}\n",
   "            return memory_facts\n",
   "        memstats = {}\n",
   "        for line in get_file_lines(\"/proc/meminfo\"):\n",
   "            data = line.split(\":\", 1)\n",
   "            key = data[0]\n",
   "            if key in self.ORIGINAL_MEMORY_FACTS:\n",
   "                val = data[1].strip().split(' ')[0]\n",
   "                memory_facts[\"%s_mb\" % key.lower()] = int(val) // 1024\n",
   "            if key in self.MEMORY_FACTS:\n",
   "                val = data[1].strip().split(' ')[0]\n",
   "                memstats[key.lower()] = int(val) // 1024\n",
   "        if None not in (memstats.get('memtotal'), memstats.get('memfree')):\n",
   "            memstats['real:used'] = memstats['memtotal'] - memstats['memfree']\n",
   "        if None not in (memstats.get('cached'), memstats.get('memfree'), memstats.get('buffers')):\n",
   "            memstats['nocache:free'] = memstats['cached'] + memstats['memfree'] + memstats['buffers']\n",
   "        if None not in (memstats.get('memtotal'), memstats.get('nocache:free')):\n",
   "            memstats['nocache:used'] = memstats['memtotal'] - memstats['nocache:free']\n",
   "        if None not in (memstats.get('swaptotal'), memstats.get('swapfree')):\n",
   "            memstats['swap:used'] = memstats['swaptotal'] - memstats['swapfree']\n",
   "        memory_facts['memory_mb'] = {\n",
   "                'total': memstats.get('memtotal'),\n",
   "                'used': memstats.get('real:used'),\n",
   "                'free': memstats.get('memfree'),\n",
   "                'free': memstats.get('nocache:free'),\n",
   "                'used': memstats.get('nocache:used'),\n",
   "                'total': memstats.get('swaptotal'),\n",
   "                'free': memstats.get('swapfree'),\n",
   "                'used': memstats.get('swap:used'),\n",
   "                'cached': memstats.get('swapcached'),\n",
   "        return memory_facts\n",
   "        cpu_facts = {}\n",
   "        collected_facts = collected_facts or {}\n",
   "        i = 0\n",
   "        vendor_id_occurrence = 0\n",
   "        model_name_occurrence = 0\n",
   "        processor_occurence = 0\n",
   "        physid = 0\n",
   "        coreid = 0\n",
   "        sockets = {}\n",
   "        cores = {}\n",
   "        xen = False\n",
   "        xen_paravirt = False\n",
   "                xen = True\n",
   "                for line in get_file_lines('/sys/hypervisor/type'):\n",
   "                    if line.strip() == 'xen':\n",
   "                        xen = True\n",
   "            return cpu_facts\n",
   "        cpu_facts['processor'] = []\n",
   "        for line in get_file_lines('/proc/cpuinfo'):\n",
   "            data = line.split(\":\", 1)\n",
   "            key = data[0].strip()\n",
   "                val = data[1].strip()\n",
   "                val = \"\"\n",
   "            if xen:\n",
   "                if key == 'flags':\n",
   "                    if 'vme' not in val:\n",
   "                        xen_paravirt = True\n",
   "            if key in ['model name', 'Processor', 'vendor_id', 'cpu', 'Vendor', 'processor']:\n",
   "                if 'processor' not in cpu_facts:\n",
   "                    cpu_facts['processor'] = []\n",
   "                cpu_facts['processor'].append(val)\n",
   "                if key == 'vendor_id':\n",
   "                    vendor_id_occurrence += 1\n",
   "                if key == 'model name':\n",
   "                    model_name_occurrence += 1\n",
   "                if key == 'processor':\n",
   "                    processor_occurence += 1\n",
   "                i += 1\n",
   "            elif key == 'physical id':\n",
   "                physid = val\n",
   "                if physid not in sockets:\n",
   "                    sockets[physid] = 1\n",
   "            elif key == 'core id':\n",
   "                coreid = val\n",
   "                if coreid not in sockets:\n",
   "                    cores[coreid] = 1\n",
   "            elif key == 'cpu cores':\n",
   "                sockets[physid] = int(val)\n",
   "            elif key == 'siblings':\n",
   "                cores[coreid] = int(val)\n",
   "            elif key == '# processors':\n",
   "                cpu_facts['processor_cores'] = int(val)\n",
   "            elif key == 'ncpus active':\n",
   "                i = int(val)\n",
   "        if vendor_id_occurrence > 0:\n",
   "            if vendor_id_occurrence == model_name_occurrence:\n",
   "                i = vendor_id_occurrence\n",
   "        if collected_facts.get('ansible_architecture', '').startswith(('armv', 'aarch', 'ppc')):\n",
   "            i = processor_occurence\n",
   "        if collected_facts.get('ansible_architecture') != 's390x':\n",
   "            if xen_paravirt:\n",
   "                cpu_facts['processor_count'] = i\n",
   "                cpu_facts['processor_cores'] = i\n",
   "                cpu_facts['processor_threads_per_core'] = 1\n",
   "                cpu_facts['processor_vcpus'] = i\n",
   "                if sockets:\n",
   "                    cpu_facts['processor_count'] = len(sockets)\n",
   "                    cpu_facts['processor_count'] = i\n",
   "                socket_values = list(sockets.values())\n",
   "                if socket_values and socket_values[0]:\n",
   "                    cpu_facts['processor_cores'] = socket_values[0]\n",
   "                    cpu_facts['processor_cores'] = 1\n",
   "                core_values = list(cores.values())\n",
   "                if core_values:\n",
   "                    cpu_facts['processor_threads_per_core'] = core_values[0] // cpu_facts['processor_cores']\n",
   "                    cpu_facts['processor_threads_per_core'] = 1 // cpu_facts['processor_cores']\n",
   "                cpu_facts['processor_vcpus'] = (cpu_facts['processor_threads_per_core'] *\n",
   "                                                cpu_facts['processor_count'] * cpu_facts['processor_cores'])\n",
   "                cpu_facts['processor_nproc'] = processor_occurence\n",
   "                    cpu_facts['processor_nproc'] = len(\n",
   "                        cmd = get_bin_path('nproc')\n",
   "                        rc, out, _err = self.module.run_command(cmd)\n",
   "                        if rc == 0:\n",
   "                            cpu_facts['processor_nproc'] = int(out)\n",
   "        return cpu_facts\n",
   "        dmi_facts = {}\n",
   "            FORM_FACTOR = [\"Unknown\", \"Other\", \"Unknown\", \"Desktop\",\n",
   "            DMI_DICT = {\n",
   "            for (key, path) in DMI_DICT.items():\n",
   "                data = get_file_content(path)\n",
   "                if data is not None:\n",
   "                    if key == 'form_factor':\n",
   "                            dmi_facts['form_factor'] = FORM_FACTOR[int(data)]\n",
   "                            dmi_facts['form_factor'] = 'unknown (%s)' % data\n",
   "                        dmi_facts[key] = data\n",
   "                    dmi_facts[key] = 'NA'\n",
   "            dmi_bin = self.module.get_bin_path('dmidecode')\n",
   "            DMI_DICT = {\n",
   "            for (k, v) in DMI_DICT.items():\n",
   "                if dmi_bin is not None:\n",
   "                    (rc, out, err) = self.module.run_command('%s -s %s' % (dmi_bin, v))\n",
   "                    if rc == 0:\n",
   "                        thisvalue = ''.join([line for line in out.splitlines() if not line.startswith('#')])\n",
   "                            json.dumps(thisvalue)\n",
   "                            thisvalue = \"NA\"\n",
   "                        dmi_facts[k] = thisvalue\n",
   "                        dmi_facts[k] = 'NA'\n",
   "                    dmi_facts[k] = 'NA'\n",
   "        return dmi_facts\n",
   "        args = ['--list', '--noheadings', '--paths', '--output', 'NAME,UUID', '--exclude', '2']\n",
   "        cmd = [lsblk_path] + args\n",
   "        rc, out, err = self.module.run_command(cmd)\n",
   "        return rc, out, err\n",
   "        uuids = {}\n",
   "        lsblk_path = self.module.get_bin_path(\"lsblk\")\n",
   "        if not lsblk_path:\n",
   "            return uuids\n",
   "        rc, out, err = self._run_lsblk(lsblk_path)\n",
   "        if rc != 0:\n",
   "            return uuids\n",
   "        for lsblk_line in out.splitlines():\n",
   "            if not lsblk_line:\n",
   "            line = lsblk_line.strip()\n",
   "            fields = line.rsplit(None, 1)\n",
   "            if len(fields) < 2:\n",
   "            device_name, uuid = fields[0].strip(), fields[1].strip()\n",
   "            if device_name in uuids:\n",
   "            uuids[device_name] = uuid\n",
   "        return uuids\n",
   "        uuid = 'N/A'\n",
   "        udevadm_path = self.module.get_bin_path('udevadm')\n",
   "        if not udevadm_path:\n",
   "            return uuid\n",
   "        cmd = [udevadm_path, 'info', '--query', 'property', '--name', device]\n",
   "        rc, out, err = self.module.run_command(cmd)\n",
   "        if rc != 0:\n",
   "            return uuid\n",
   "        m = re.search('ID_FS_UUID=(.*)\\n', out)\n",
   "        if m:\n",
   "            uuid = m.group(1)\n",
   "        return uuid\n",
   "        args = ['--list', '--noheadings', '--notruncate']\n",
   "        cmd = [findmnt_path] + args\n",
   "        rc, out, err = self.module.run_command(cmd, errors='surrogate_then_replace')\n",
   "        return rc, out, err\n",
   "        bind_mounts = set()\n",
   "        findmnt_path = self.module.get_bin_path(\"findmnt\")\n",
   "        if not findmnt_path:\n",
   "            return bind_mounts\n",
   "        rc, out, err = self._run_findmnt(findmnt_path)\n",
   "        if rc != 0:\n",
   "            return bind_mounts\n",
   "        for line in out.splitlines():\n",
   "            fields = line.split()\n",
   "            if len(fields) < 2:\n",
   "            if self.BIND_MOUNT_RE.match(fields[1]):\n",
   "                bind_mounts.add(fields[0])\n",
   "        return bind_mounts\n",
   "        mtab_file = '/etc/mtab'\n",
   "        if not os.path.exists(mtab_file):\n",
   "            mtab_file = '/proc/mounts'\n",
   "        mtab = get_file_content(mtab_file, '')\n",
   "        mtab_entries = []\n",
   "        for line in mtab.splitlines():\n",
   "            fields = line.split()\n",
   "            if len(fields) < 4:\n",
   "            mtab_entries.append(fields)\n",
   "        return mtab_entries\n",
   "        mount_size = get_mount_size(mount)\n",
   "        uuid = uuids.get(device, self._udevadm_uuid(device))\n",
   "        return mount_size, uuid\n",
   "        mounts = []\n",
   "        bind_mounts = self._find_bind_mounts()\n",
   "        uuids = self._lsblk_uuid()\n",
   "        mtab_entries = self._mtab_entries()\n",
   "        results = {}\n",
   "        pool = ThreadPool(processes=min(len(mtab_entries), cpu_count()))\n",
   "        maxtime = globals().get('GATHER_TIMEOUT') or timeout.DEFAULT_GATHER_TIMEOUT\n",
   "        for fields in mtab_entries:\n",
   "            fields = [self._replace_octal_escapes(field) for field in fields]\n",
   "            device, mount, fstype, options = fields[0], fields[1], fields[2], fields[3]\n",
   "            if not device.startswith(('/', '\\\\')) and ':/' not in device or fstype == 'none':\n",
   "            mount_info = {'mount': mount,\n",
   "                          'device': device,\n",
   "                          'fstype': fstype,\n",
   "                          'options': options}\n",
   "            if mount in bind_mounts:\n",
   "                if not self.MTAB_BIND_MOUNT_RE.match(options):\n",
   "                    mount_info['options'] += \",bind\"\n",
   "            results[mount] = {'info': mount_info,\n",
   "                              'extra': pool.apply_async(self.get_mount_info, (mount, device, uuids)),\n",
   "                              'timelimit': time.time() + maxtime}\n",
   "        pool.close()  # done with new workers, start gc\n",
   "        while results:\n",
   "            for mount in results:\n",
   "                res = results[mount]['extra']\n",
   "                if res.ready():\n",
   "                    if res.successful():\n",
   "                        mount_size, uuid = res.get()\n",
   "                        if mount_size:\n",
   "                            results[mount]['info'].update(mount_size)\n",
   "                        results[mount]['info']['uuid'] = uuid or 'N/A'\n",
   "                        errmsg = to_text(res.get())\n",
   "                        self.module.warn(\"Error prevented getting extra info for mount %s: %s.\" % (mount, errmsg))\n",
   "                        results[mount]['info']['note'] = 'Could not get extra information: %s.' % (errmsg)\n",
   "                    mounts.append(results[mount]['info'])\n",
   "                    del results[mount]\n",
   "                elif time.time() > results[mount]['timelimit']:\n",
   "                    results[mount]['info']['note'] = 'Timed out while attempting to get extra information.'\n",
   "                    mounts.append(results[mount]['info'])\n",
   "                    del results[mount]\n",
   "        return {'mounts': mounts}\n",
   "            retval = collections.defaultdict(set)\n",
   "            for entry in os.listdir(link_dir):\n",
   "                    target = os.path.basename(os.readlink(os.path.join(link_dir, entry)))\n",
   "                    retval[target].add(entry)\n",
   "            return dict((k, list(sorted(v))) for (k, v) in iteritems(retval))\n",
   "            retval = collections.defaultdict(set)\n",
   "            for path in glob.glob('/sys/block/*/slaves/*'):\n",
   "                elements = path.split('/')\n",
   "                device = elements[3]\n",
   "                target = elements[5]\n",
   "                retval[target].add(device)\n",
   "            return dict((k, list(sorted(v))) for (k, v) in iteritems(retval))\n",
   "            for folder in os.listdir(sysdir + \"/holders\"):\n",
   "                if not folder.startswith(\"dm-\"):\n",
   "                name = get_file_content(sysdir + \"/holders/\" + folder + \"/dm/name\")\n",
   "                if name:\n",
   "                    block_dev_dict['holders'].append(name)\n",
   "                    block_dev_dict['holders'].append(folder)\n",
   "        device_facts = {}\n",
   "        device_facts['devices'] = {}\n",
   "        lspci = self.module.get_bin_path('lspci')\n",
   "        if lspci:\n",
   "            rc, pcidata, err = self.module.run_command([lspci, '-D'], errors='surrogate_then_replace')\n",
   "            pcidata = None\n",
   "            block_devs = os.listdir(\"/sys/block\")\n",
   "            return device_facts\n",
   "        devs_wwn = {}\n",
   "            devs_by_id = os.listdir(\"/dev/disk/by-id\")\n",
   "            for link_name in devs_by_id:\n",
   "                if link_name.startswith(\"wwn-\"):\n",
   "                        wwn_link = os.readlink(os.path.join(\"/dev/disk/by-id\", link_name))\n",
   "                    devs_wwn[os.path.basename(wwn_link)] = link_name[4:]\n",
   "        links = self.get_all_device_links()\n",
   "        device_facts['device_links'] = links\n",
   "        for block in block_devs:\n",
   "            virtual = 1\n",
   "            sysfs_no_links = 0\n",
   "                path = os.readlink(os.path.join(\"/sys/block/\", block))\n",
   "                e = sys.exc_info()[1]\n",
   "                if e.errno == errno.EINVAL:\n",
   "                    path = block\n",
   "                    sysfs_no_links = 1\n",
   "            sysdir = os.path.join(\"/sys/block\", path)\n",
   "            if sysfs_no_links == 1:\n",
   "                for folder in os.listdir(sysdir):\n",
   "                    if \"device\" in folder:\n",
   "                        virtual = 0\n",
   "            d = {}\n",
   "            d['virtual'] = virtual\n",
   "            d['links'] = {}\n",
   "            for (link_type, link_values) in iteritems(links):\n",
   "                d['links'][link_type] = link_values.get(block, [])\n",
   "            diskname = os.path.basename(sysdir)\n",
   "            for key in ['vendor', 'model', 'sas_address', 'sas_device_handle']:\n",
   "                d[key] = get_file_content(sysdir + \"/device/\" + key)\n",
   "            sg_inq = self.module.get_bin_path('sg_inq')\n",
   "            serial_path = \"/sys/block/%s/device/serial\" % (block)\n",
   "            if sg_inq:\n",
   "                device = \"/dev/%s\" % (block)\n",
   "                rc, drivedata, err = self.module.run_command([sg_inq, device])\n",
   "                if rc == 0:\n",
   "                    serial = re.search(r\"Unit serial number:\\s+(\\w+)\", drivedata)\n",
   "                    if serial:\n",
   "                        d['serial'] = serial.group(1)\n",
   "                serial = get_file_content(serial_path)\n",
   "                if serial:\n",
   "                    d['serial'] = serial\n",
   "            for key, test in [('removable', '/removable'),\n",
   "                d[key] = get_file_content(sysdir + test)\n",
   "            if diskname in devs_wwn:\n",
   "                d['wwn'] = devs_wwn[diskname]\n",
   "            d['partitions'] = {}\n",
   "            for folder in os.listdir(sysdir):\n",
   "                m = re.search(\"(\" + diskname + r\"[p]?\\d+)\", folder)\n",
   "                if m:\n",
   "                    part = {}\n",
   "                    partname = m.group(1)\n",
   "                    part_sysdir = sysdir + \"/\" + partname\n",
   "                    part['links'] = {}\n",
   "                    for (link_type, link_values) in iteritems(links):\n",
   "                        part['links'][link_type] = link_values.get(partname, [])\n",
   "                    part['start'] = get_file_content(part_sysdir + \"/start\", 0)\n",
   "                    part['sectors'] = get_file_content(part_sysdir + \"/size\", 0)\n",
   "                    part['sectorsize'] = get_file_content(part_sysdir + \"/queue/logical_block_size\")\n",
   "                    if not part['sectorsize']:\n",
   "                        part['sectorsize'] = get_file_content(part_sysdir + \"/queue/hw_sector_size\", 512)\n",
   "                    part['size'] = bytes_to_human((float(part['sectors']) * 512.0))\n",
   "                    part['uuid'] = get_partition_uuid(partname)\n",
   "                    self.get_holders(part, part_sysdir)\n",
   "                    d['partitions'][partname] = part\n",
   "            d['rotational'] = get_file_content(sysdir + \"/queue/rotational\")\n",
   "            d['scheduler_mode'] = \"\"\n",
   "            scheduler = get_file_content(sysdir + \"/queue/scheduler\")\n",
   "            if scheduler is not None:\n",
   "                m = re.match(r\".*?(\\[(.*)\\])\", scheduler)\n",
   "                if m:\n",
   "                    d['scheduler_mode'] = m.group(2)\n",
   "            d['sectors'] = get_file_content(sysdir + \"/size\")\n",
   "            if not d['sectors']:\n",
   "                d['sectors'] = 0\n",
   "            d['sectorsize'] = get_file_content(sysdir + \"/queue/logical_block_size\")\n",
   "            if not d['sectorsize']:\n",
   "                d['sectorsize'] = get_file_content(sysdir + \"/queue/hw_sector_size\", 512)\n",
   "            d['size'] = bytes_to_human(float(d['sectors']) * 512.0)\n",
   "            d['host'] = \"\"\n",
   "            m = re.match(r\".+/([a-f0-9]{4}:[a-f0-9]{2}:[0|1][a-f0-9]\\.[0-7])/\", sysdir)\n",
   "            if m and pcidata:\n",
   "                pciid = m.group(1)\n",
   "                did = re.escape(pciid)\n",
   "                m = re.search(\"^\" + did + r\"\\s(.*)$\", pcidata, re.MULTILINE)\n",
   "                if m:\n",
   "                    d['host'] = m.group(1)\n",
   "            self.get_holders(d, sysdir)\n",
   "            device_facts['devices'][diskname] = d\n",
   "        return device_facts\n",
   "            uptime_facts['uptime_seconds'] = int(float(uptime_seconds_string))\n",
   "        return uptime_facts\n",
   "            if rc == 0:\n",
   "            lvm_facts['lvm'] = {'lvs': lvs, 'vgs': vgs, 'pvs': pvs}\n",
   "        return lvm_facts\n"
  ]
 },
 "232": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/linux.py",
  "lineno": "851",
  "column": "4",
  "context": "ss LinuxHardwareCollector(HardwareCollector):\n    _platform = 'Linux'\n    _fact_class = LinuxHardware\n\n    required_fact",
  "context_lines": "                        'vg': items[1]}\n\n            lvm_facts['lvm'] = {'lvs': lvs, 'vgs': vgs, 'pvs': pvs}\n\n        return lvm_facts\n\n\nclass LinuxHardwareCollector(HardwareCollector):\n    _platform = 'Linux'\n    _fact_class = LinuxHardware\n\n",
  "slicing": "    _platform = 'Linux'\n"
 },
 "233": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/base.py",
  "lineno": "36",
  "column": "4",
  "context": "or import BaseFactCollector\n\n\nclass Hardware:\n    platform = 'Generic'\n\n    # FIXME: remove load_on_init when we can\n    ",
  "context_lines": "from __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\nfrom ansible.module_utils.facts.collector import BaseFactCollector\n\n\nclass Hardware:\n    platform = 'Generic'\n\n    # FIXME: remove load_on_init when we can\n    def __init__(self, module, load_on_init=False):\n        self.module = module\n\n",
  "slicing": "    platform = 'Generic'\n"
 },
 "234": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/base.py",
  "lineno": "47",
  "column": "4",
  "context": "\n\nclass HardwareCollector(BaseFactCollector):\n    name = 'hardware'\n    _fact_ids = set(['processor',\n                ",
  "context_lines": "        self.module = module\n\n    def populate(self, collected_facts=None):\n        return {}\n\n\nclass HardwareCollector(BaseFactCollector):\n    name = 'hardware'\n    _fact_ids = set(['processor',\n                     'processor_cores',\n                     'processor_count',\n                     # TODO: mounts isnt exactly hardware\n",
  "slicing": "    name = 'hardware'\n"
 },
 "235": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/netbsd.py",
  "lineno": "43",
  "column": "4",
  "context": "s\n    - processor_count\n    - devices\n    \"\"\"\n    platform = 'NetBSD'\n    MEMORY_FACTS = ['MemTotal', 'SwapTotal', 'MemF",
  "context_lines": "    - processor_cores\n    - processor_count\n    - devices\n    \"\"\"\n    platform = 'NetBSD'\n    MEMORY_FACTS = ['MemTotal', 'SwapTotal', 'MemFree', 'SwapFree']\n\n    def populate(self, collected_facts=None):\n        hardware_facts = {}\n        self.sysctl = get_sysctl(self.module, ['machdep'])\n",
  "slicing": "    platform = 'NetBSD'\n"
 },
 "236": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/hardware/netbsd.py",
  "lineno": "162",
  "column": "4",
  "context": "eCollector):\n    _fact_class = NetBSDHardware\n    _platform = 'NetBSD'\n",
  "context_lines": "                dmi_facts[sysctl_to_dmi[mib]] = self.sysctl[mib]\n\n        return dmi_facts\n\n\nclass NetBSDHardwareCollector(HardwareCollector):\n    _fact_class = NetBSDHardware\n    _platform = 'NetBSD'\n",
  "slicing": "    _platform = 'NetBSD'\n"
 },
 "237": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/other/facter.py",
  "lineno": "27",
  "column": "4",
  "context": "class FacterFactCollector(BaseFactCollector):\n    name = 'facter'\n    _fact_ids = set(['facter'])\n\n    def __init__(",
  "context_lines": "import json\n\nfrom ansible.module_utils.facts.namespace import PrefixFactNamespace\n\nfrom ansible.module_utils.facts.collector import BaseFactCollector\n\n\nclass FacterFactCollector(BaseFactCollector):\n    name = 'facter'\n    _fact_ids = set(['facter'])\n\n    def __init__(self, collectors=None, namespace=None):\n        namespace = PrefixFactNamespace(namespace_name='facter',\n                                        prefix='facter_')\n",
  "slicing": "    name = 'facter'\n"
 },
 "238": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/other/ohai.py",
  "lineno": "28",
  "column": "4",
  "context": " including information gathered from Ohai.'''\n    name = 'ohai'\n    _fact_ids = set()\n\n    def __init__(self, coll",
  "context_lines": "from ansible.module_utils.facts.namespace import PrefixFactNamespace\n\nfrom ansible.module_utils.facts.collector import BaseFactCollector\n\n\nclass OhaiFactCollector(BaseFactCollector):\n    '''This is a subclass of Facts for including information gathered from Ohai.'''\n    name = 'ohai'\n    _fact_ids = set()\n\n    def __init__(self, collectors=None, namespace=None):\n        namespace = PrefixFactNamespace(namespace_name='ohai',\n                                        prefix='ohai_')\n",
  "slicing": "    name = 'ohai'\n"
 },
 "239": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/hurd.py",
  "lineno": "29",
  "column": "4",
  "context": "e ip address and support only pfinet.\n    \"\"\"\n    platform = 'GNU'\n    _socket_dir = '/servers/socket/'\n\n    def assi",
  "context_lines": "    \"\"\"\n    This is a GNU Hurd specific subclass of Network. It use fsysopts to\n    get the ip address and support only pfinet.\n    \"\"\"\n    platform = 'GNU'\n    _socket_dir = '/servers/socket/'\n\n    def assign_network_facts(self, network_facts, fsysopts_path, socket_path):\n        rc, out, err = self.module.run_command([fsysopts_path, '-L', socket_path])\n        # FIXME: build up a interfaces datastructure, then assign into network_facts\n",
  "slicing": "    platform = 'GNU'\n"
 },
 "240": {
  "name": "_socket_dir",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/hurd.py",
  "lineno": "30",
  "column": "4",
  "context": "ort only pfinet.\n    \"\"\"\n    platform = 'GNU'\n    _socket_dir = '/servers/socket/'\n\n    def assign_network_facts(self, network_facts,",
  "context_lines": "    This is a GNU Hurd specific subclass of Network. It use fsysopts to\n    get the ip address and support only pfinet.\n    \"\"\"\n    platform = 'GNU'\n    _socket_dir = '/servers/socket/'\n\n    def assign_network_facts(self, network_facts, fsysopts_path, socket_path):\n        rc, out, err = self.module.run_command([fsysopts_path, '-L', socket_path])\n        # FIXME: build up a interfaces datastructure, then assign into network_facts\n",
  "slicing": "    _socket_dir = '/servers/socket/'\n"
 },
 "241": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/hurd.py",
  "lineno": "86",
  "column": "4",
  "context": "class HurdNetworkCollector(NetworkCollector):\n    _platform = 'GNU'\n    _fact_class = HurdPfinetNetwork\n",
  "context_lines": "        if socket_path is None:\n            return network_facts\n\n        return self.assign_network_facts(network_facts, fsysopts_path, socket_path)\n\n\nclass HurdNetworkCollector(NetworkCollector):\n    _platform = 'GNU'\n",
  "slicing": "    _platform = 'GNU'\n"
 },
 "242": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/openbsd.py",
  "lineno": "28",
  "column": "4",
  "context": "t uses the GenericBsdIfconfigNetwork.\n    \"\"\"\n    platform = 'OpenBSD'\n\n    # OpenBSD 'ifconfig -a' does not have informa",
  "context_lines": "    \"\"\"\n    This is the OpenBSD Network Class.\n    It uses the GenericBsdIfconfigNetwork.\n    \"\"\"\n    platform = 'OpenBSD'\n\n    # OpenBSD 'ifconfig -a' does not have information about aliases\n    def get_interfaces_info(self, ifconfig_path, ifconfig_options='-aA'):\n        return super(OpenBSDNetwork, self).get_interfaces_info(ifconfig_path, ifconfig_options)\n\n",
  "slicing": "    platform = 'OpenBSD'\n"
 },
 "243": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/openbsd.py",
  "lineno": "42",
  "column": "4",
  "context": "kCollector):\n    _fact_class = OpenBSDNetwork\n    _platform = 'OpenBSD'\n",
  "context_lines": "        current_if['macaddress'] = words[1]\n        current_if['type'] = 'ether'\n\n\nclass OpenBSDNetworkCollector(NetworkCollector):\n    _fact_class = OpenBSDNetwork\n    _platform = 'OpenBSD'\n",
  "slicing": "    _platform = 'OpenBSD'\n"
 },
 "244": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/generic_bsd.py",
  "lineno": "34",
  "column": "4",
  "context": "s: lists of all configured addresses.\n    \"\"\"\n    platform = 'Generic_BSD_Ifconfig'\n\n    def populate(self, collected_facts=None):\n   ",
  "context_lines": "    - interfaces (a list of interface names)\n    - interface_<name> dictionary of ipv4, ipv6, and mac address information.\n    - all_ipv4_addresses and all_ipv6_addresses: lists of all configured addresses.\n    \"\"\"\n    platform = 'Generic_BSD_Ifconfig'\n\n    def populate(self, collected_facts=None):\n        network_facts = {}\n        ifconfig_path = self.module.get_bin_path('ifconfig')\n\n",
  "slicing": "    platform = 'Generic_BSD_Ifconfig'\n"
 },
 "245": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/darwin.py",
  "lineno": "28",
  "column": "4",
  "context": "e GenericBsdIfconfigNetwork unchanged\n    \"\"\"\n    platform = 'Darwin'\n\n    # media line is different to the default Free",
  "context_lines": "    \"\"\"\n    This is the Mac macOS Darwin Network Class.\n    It uses the GenericBsdIfconfigNetwork unchanged\n    \"\"\"\n    platform = 'Darwin'\n\n    # media line is different to the default FreeBSD one\n    def parse_media_line(self, words, current_if, ips):\n        # not sure if this is useful - we also drop information\n",
  "slicing": "    platform = 'Darwin'\n"
 },
 "246": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/darwin.py",
  "lineno": "49",
  "column": "4",
  "context": "rkCollector):\n    _fact_class = DarwinNetwork\n    _platform = 'Darwin'\n",
  "context_lines": "        if len(words) > 3:\n            current_if['media_options'] = self.get_options(words[3])\n\n\nclass DarwinNetworkCollector(NetworkCollector):\n    _fact_class = DarwinNetwork\n    _platform = 'Darwin'\n",
  "slicing": "    _platform = 'Darwin'\n"
 },
 "247": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/iscsi.py",
  "lineno": "30",
  "column": "4",
  "context": "iInitiatorNetworkCollector(NetworkCollector):\n    name = 'iscsi'\n    _fact_ids = set()\n\n    def collect(self, modul",
  "context_lines": "from ansible.module_utils.common.process import get_bin_path\nfrom ansible.module_utils.facts.utils import get_file_content\nfrom ansible.module_utils.facts.network.base import NetworkCollector\n\n\nclass IscsiInitiatorNetworkCollector(NetworkCollector):\n    name = 'iscsi'\n    _fact_ids = set()\n\n    def collect(self, module=None, collected_facts=None):\n        \"\"\"\n        Example of contents of /etc/iscsi/initiatorname.iscsi:\n\n",
  "slicing": "    name = 'iscsi'\n"
 },
 "248": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/fc_wwn.py",
  "lineno": "29",
  "column": "4",
  "context": "WwnInitiatorFactCollector(BaseFactCollector):\n    name = 'fibre_channel_wwn'\n    _fact_ids = set()\n\n    def collect(self, modul",
  "context_lines": "import glob\n\nfrom ansible.module_utils.facts.utils import get_file_lines\nfrom ansible.module_utils.facts.collector import BaseFactCollector\n\n\nclass FcWwnInitiatorFactCollector(BaseFactCollector):\n    name = 'fibre_channel_wwn'\n    _fact_ids = set()\n\n    def collect(self, module=None, collected_facts=None):\n        \"\"\"\n        Example contents /sys/class/fc_host/*/port_name:\n\n",
  "slicing": "    name = 'fibre_channel_wwn'\n"
 },
 "249": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/hpux.py",
  "lineno": "29",
  "column": "4",
  "context": "ctionary of ipv4 address information.\n    \"\"\"\n    platform = 'HP-UX'\n\n    def populate(self, collected_facts=None):\n   ",
  "context_lines": "    - default_interface\n    - interfaces (a list of interface names)\n    - interface_<name> dictionary of ipv4 address information.\n    \"\"\"\n    platform = 'HP-UX'\n\n    def populate(self, collected_facts=None):\n        network_facts = {}\n        netstat_path = self.module.get_bin_path('netstat')\n\n",
  "slicing": "    platform = 'HP-UX'\n"
 },
 "250": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/hpux.py",
  "lineno": "82",
  "column": "4",
  "context": "workCollector):\n    _fact_class = HPUXNetwork\n    _platform = 'HP-UX'\n",
  "context_lines": "                                                  'address': address}\n        return interfaces\n\n\nclass HPUXNetworkCollector(NetworkCollector):\n    _fact_class = HPUXNetwork\n    _platform = 'HP-UX'\n",
  "slicing": "    _platform = 'HP-UX'\n"
 },
 "251": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/sunos.py",
  "lineno": "33",
  "column": "4",
  "context": "d inside the 'ipv4' and 'ipv6' lists.\n    \"\"\"\n    platform = 'SunOS'\n\n    # Solaris 'ifconfig -a' will print interfaces",
  "context_lines": "    It uses the GenericBsdIfconfigNetwork.\n\n    Solaris can have different FLAGS and MTU for IPv4 and IPv6 on the same interface\n    so these facts have been moved inside the 'ipv4' and 'ipv6' lists.\n    \"\"\"\n    platform = 'SunOS'\n\n    # Solaris 'ifconfig -a' will print interfaces twice, once for IPv4 and again for IPv6.\n    # MTU and FLAGS also may differ between IPv4 and IPv6 on the same interface.\n    # 'parse_interface_line()' checks for previously seen interfaces before defining\n",
  "slicing": "    platform = 'SunOS'\n"
 },
 "252": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/sunos.py",
  "lineno": "116",
  "column": "4",
  "context": "orkCollector):\n    _fact_class = SunOSNetwork\n    _platform = 'SunOS'\n",
  "context_lines": "            macaddress += (octet + ':')\n        current_if['macaddress'] = macaddress[0:-1]\n\n\nclass SunOSNetworkCollector(NetworkCollector):\n    _fact_class = SunOSNetwork\n    _platform = 'SunOS'\n",
  "slicing": "    _platform = 'SunOS'\n"
 },
 "253": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/nvme.py",
  "lineno": "29",
  "column": "4",
  "context": "eInitiatorNetworkCollector(NetworkCollector):\n    name = 'nvme'\n    _fact_ids = set()\n\n    def collect(self, modul",
  "context_lines": "import subprocess\n\nfrom ansible.module_utils.facts.utils import get_file_content\nfrom ansible.module_utils.facts.network.base import NetworkCollector\n\n\nclass NvmeInitiatorNetworkCollector(NetworkCollector):\n    name = 'nvme'\n    _fact_ids = set()\n\n    def collect(self, module=None, collected_facts=None):\n        \"\"\"\n        Currently NVMe is only supported in some Linux distributions.\n",
  "slicing": "    name = 'nvme'\n"
 },
 "254": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/aix.py",
  "lineno": "30",
  "column": "4",
  "context": " GenericBsdIfconfigNetwork unchanged.\n    \"\"\"\n    platform = 'AIX'\n\n    def get_default_interfaces(self, route_path):",
  "context_lines": "    \"\"\"\n    This is the AIX Network Class.\n    It uses the GenericBsdIfconfigNetwork unchanged.\n    \"\"\"\n    platform = 'AIX'\n\n    def get_default_interfaces(self, route_path):\n        netstat_path = self.module.get_bin_path('netstat')\n\n        rc, out, err = self.module.run_command([netstat_path, '-nr'])\n\n",
  "slicing": "    platform = 'AIX'\n"
 },
 "255": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/aix.py",
  "lineno": "144",
  "column": "4",
  "context": "tworkCollector):\n    _fact_class = AIXNetwork\n    _platform = 'AIX'\n",
  "context_lines": "        current_if['macaddress'] = 'unknown'    # will be overwritten later\n        return current_if\n\n\nclass AIXNetworkCollector(NetworkCollector):\n    _fact_class = AIXNetwork\n    _platform = 'AIX'\n",
  "slicing": "    _platform = 'AIX'\n"
 },
 "256": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/dragonfly.py",
  "lineno": "28",
  "column": "4",
  "context": " GenericBsdIfconfigNetwork unchanged.\n    \"\"\"\n    platform = 'DragonFly'\n\n\nclass DragonFlyNetworkCollector(NetworkCollector",
  "context_lines": "    \"\"\"\n    This is the DragonFly Network Class.\n    It uses the GenericBsdIfconfigNetwork unchanged.\n    \"\"\"\n    platform = 'DragonFly'\n\n\nclass DragonFlyNetworkCollector(NetworkCollector):\n    _fact_class = DragonFlyNetwork\n",
  "slicing": "    platform = 'DragonFly'\n"
 },
 "257": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/dragonfly.py",
  "lineno": "33",
  "column": "4",
  "context": "ollector):\n    _fact_class = DragonFlyNetwork\n    _platform = 'DragonFly'\n",
  "context_lines": "    \"\"\"\n    platform = 'DragonFly'\n\n\nclass DragonFlyNetworkCollector(NetworkCollector):\n    _fact_class = DragonFlyNetwork\n    _platform = 'DragonFly'\n",
  "slicing": "    _platform = 'DragonFly'\n"
 },
 "258": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/freebsd.py",
  "lineno": "28",
  "column": "4",
  "context": " GenericBsdIfconfigNetwork unchanged.\n    \"\"\"\n    platform = 'FreeBSD'\n\n\nclass FreeBSDNetworkCollector(NetworkCollector):",
  "context_lines": "    \"\"\"\n    This is the FreeBSD Network Class.\n    It uses the GenericBsdIfconfigNetwork unchanged.\n    \"\"\"\n    platform = 'FreeBSD'\n\n\nclass FreeBSDNetworkCollector(NetworkCollector):\n    _fact_class = FreeBSDNetwork\n",
  "slicing": "    platform = 'FreeBSD'\n"
 },
 "259": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/freebsd.py",
  "lineno": "33",
  "column": "4",
  "context": "kCollector):\n    _fact_class = FreeBSDNetwork\n    _platform = 'FreeBSD'\n",
  "context_lines": "    \"\"\"\n    platform = 'FreeBSD'\n\n\nclass FreeBSDNetworkCollector(NetworkCollector):\n    _fact_class = FreeBSDNetwork\n    _platform = 'FreeBSD'\n",
  "slicing": "    _platform = 'FreeBSD'\n"
 },
 "260": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/linux.py",
  "lineno": "38",
  "column": "4",
  "context": "st non-local address for each family.\n    \"\"\"\n    platform = 'Linux'\n    INTERFACE_TYPE = {\n        '1': 'ether',\n     ",
  "context_lines": "    - interface_<name> dictionary of ipv4, ipv6, and mac address information.\n    - all_ipv4_addresses and all_ipv6_addresses: lists of all configured addresses.\n    - ipv4_address and ipv6_address: the first non-local address for each family.\n    \"\"\"\n    platform = 'Linux'\n    INTERFACE_TYPE = {\n        '1': 'ether',\n        '32': 'infiniband',\n        '512': 'ppp',\n",
  "slicing": "    platform = 'Linux'\n"
 },
 "261": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/linux.py",
  "lineno": "320",
  "column": "4",
  "context": "lass LinuxNetworkCollector(NetworkCollector):\n    _platform = 'Linux'\n    _fact_class = LinuxNetwork\n    required_facts ",
  "context_lines": "                if m:\n                    data['phc_index'] = int(m.groups()[0])\n\n        return data\n\n\nclass LinuxNetworkCollector(NetworkCollector):\n    _platform = 'Linux'\n    _fact_class = LinuxNetwork\n",
  "slicing": "    _platform = 'Linux'\n"
 },
 "262": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/base.py",
  "lineno": "32",
  "column": "4",
  "context": " All subclasses MUST define platform.\n    \"\"\"\n    platform = 'Generic'\n\n    # FIXME: remove load_on_init when we can\n    ",
  "context_lines": "    - interfaces (a list of interface names)\n    - interface_<name> dictionary of ipv4, ipv6, and mac address information.\n\n    All subclasses MUST define platform.\n    \"\"\"\n    platform = 'Generic'\n\n    # FIXME: remove load_on_init when we can\n    def __init__(self, module, load_on_init=False):\n        self.module = module\n\n",
  "slicing": "    platform = 'Generic'\n"
 },
 "263": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/base.py",
  "lineno": "45",
  "column": "4",
  "context": "ecific implementation of Network() or its kin\n    name = 'network'\n    _fact_class = Network\n    _fact_ids = set(['in",
  "context_lines": "    def populate(self, collected_facts=None):\n        return {}\n\n\nclass NetworkCollector(BaseFactCollector):\n    # MAYBE: we could try to build this based on the arch specific implementation of Network() or its kin\n    name = 'network'\n    _fact_class = Network\n    _fact_ids = set(['interfaces',\n                     'default_ipv4',\n                     'default_ipv6',\n",
  "slicing": "    name = 'network'\n"
 },
 "264": {
  "name": "platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/netbsd.py",
  "lineno": "28",
  "column": "4",
  "context": "It uses the GenericBsdIfconfigNetwork\n    \"\"\"\n    platform = 'NetBSD'\n\n    def parse_media_line(self, words, current_if,",
  "context_lines": "    \"\"\"\n    This is the NetBSD Network Class.\n    It uses the GenericBsdIfconfigNetwork\n    \"\"\"\n    platform = 'NetBSD'\n\n    def parse_media_line(self, words, current_if, ips):\n        # example of line:\n        # $ ifconfig\n",
  "slicing": "    platform = 'NetBSD'\n"
 },
 "265": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/network/netbsd.py",
  "lineno": "48",
  "column": "4",
  "context": "rkCollector):\n    _fact_class = NetBSDNetwork\n    _platform = 'NetBSD'\n",
  "context_lines": "        if len(words) > 3:\n            current_if['media_options'] = words[3].split(',')\n\n\nclass NetBSDNetworkCollector(NetworkCollector):\n    _fact_class = NetBSDNetwork\n    _platform = 'NetBSD'\n",
  "slicing": "    _platform = 'NetBSD'\n"
 },
 "266": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/system/date_time.py",
  "lineno": "28",
  "column": "4",
  "context": "ass DateTimeFactCollector(BaseFactCollector):\n    name = 'date_time'\n    _fact_ids = set()\n\n    def collect(self, modul",
  "context_lines": "import datetime\nimport time\n\nfrom ansible.module_utils.facts.collector import BaseFactCollector\n\n\nclass DateTimeFactCollector(BaseFactCollector):\n    name = 'date_time'\n    _fact_ids = set()\n\n    def collect(self, module=None, collected_facts=None):\n        facts_dict = {}\n        date_time_facts = {}\n\n",
  "slicing": "    name = 'date_time'\n"
 },
 "267": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/system/dns.py",
  "lineno": "26",
  "column": "4",
  "context": "\n\n\nclass DnsFactCollector(BaseFactCollector):\n    name = 'dns'\n    _fact_ids = set()\n\n    def collect(self, modul",
  "context_lines": "__metaclass__ = type\n\n\nfrom ansible.module_utils.facts.utils import get_file_content\n\nfrom ansible.module_utils.facts.collector import BaseFactCollector\n\n\nclass DnsFactCollector(BaseFactCollector):\n    name = 'dns'\n    _fact_ids = set()\n\n    def collect(self, module=None, collected_facts=None):\n        dns_facts = {}\n\n        # TODO: flatten\n",
  "slicing": "    name = 'dns'\n"
 },
 "268": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/system/cmdline.py",
  "lineno": "27",
  "column": "4",
  "context": "lass CmdLineFactCollector(BaseFactCollector):\n    name = 'cmdline'\n    _fact_ids = set()\n\n    def _get_proc_cmdline(s",
  "context_lines": "import shlex\n\nfrom ansible.module_utils.facts.utils import get_file_content\n\nfrom ansible.module_utils.facts.collector import BaseFactCollector\n\n\nclass CmdLineFactCollector(BaseFactCollector):\n    name = 'cmdline'\n    _fact_ids = set()\n\n    def _get_proc_cmdline(self):\n        return get_file_content('/proc/cmdline')\n\n    def _parse_proc_cmdline(self, data):\n",
  "slicing": "    name = 'cmdline'\n"
 },
 "269": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/system/env.py",
  "lineno": "27",
  "column": "4",
  "context": "\n\n\nclass EnvFactCollector(BaseFactCollector):\n    name = 'env'\n    _fact_ids = set()\n\n    def collect(self, modul",
  "context_lines": "import os\n\nfrom ansible.module_utils.six import iteritems\n\nfrom ansible.module_utils.facts.collector import BaseFactCollector\n\n\nclass EnvFactCollector(BaseFactCollector):\n    name = 'env'\n    _fact_ids = set()\n\n    def collect(self, module=None, collected_facts=None):\n        env_facts = {}\n        env_facts['env'] = {}\n\n",
  "slicing": "    name = 'env'\n"
 },
 "270": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/system/chroot.py",
  "lineno": "43",
  "column": "4",
  "context": "class ChrootFactCollector(BaseFactCollector):\n    name = 'chroot'\n    _fact_ids = set(['is_chroot'])\n\n    def collec",
  "context_lines": "                        fs_root_ino = 128\n\n            is_chroot = (my_root.st_ino != fs_root_ino)\n\n    return is_chroot\n\n\nclass ChrootFactCollector(BaseFactCollector):\n    name = 'chroot'\n    _fact_ids = set(['is_chroot'])\n\n    def collect(self, module=None, collected_facts=None):\n",
  "slicing": "    name = 'chroot'\n"
 },
 "271": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/system/apparmor.py",
  "lineno": "27",
  "column": "4",
  "context": "ass ApparmorFactCollector(BaseFactCollector):\n    name = 'apparmor'\n    _fact_ids = set()\n\n    def collect(self, modul",
  "context_lines": "__metaclass__ = type\n\nimport os\n\nfrom ansible.module_utils.facts.collector import BaseFactCollector\n\n\nclass ApparmorFactCollector(BaseFactCollector):\n    name = 'apparmor'\n    _fact_ids = set()\n\n    def collect(self, module=None, collected_facts=None):\n        facts_dict = {}\n        apparmor_facts = {}\n",
  "slicing": "    name = 'apparmor'\n"
 },
 "272": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/system/service_mgr.py",
  "lineno": "39",
  "column": "4",
  "context": "s ServiceMgrFactCollector(BaseFactCollector):\n    name = 'service_mgr'\n    _fact_ids = set()\n    required_facts = set(['p",
  "context_lines": "# depend on LooseVersion, do not import it on Solaris.\nif platform.system() != 'SunOS':\n    from distutils.version import LooseVersion\n\n\nclass ServiceMgrFactCollector(BaseFactCollector):\n    name = 'service_mgr'\n    _fact_ids = set()\n    required_facts = set(['platform', 'distribution'])\n\n    @staticmethod\n    def is_systemd_managed(module):\n",
  "slicing": "    name = 'service_mgr'\n"
 },
 "273": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/system/pkg_mgr.py",
  "lineno": "45",
  "column": "4",
  "context": "penBSDPkgMgrFactCollector(BaseFactCollector):\n    name = 'pkg_mgr'\n    _fact_ids = set()\n    _platform = 'OpenBSD'\n\n ",
  "context_lines": "            {'path': '/usr/bin/installp', 'name': 'installp'},\n            {'path': '/QOpenSys/pkgs/bin/yum', 'name': 'yum'},\n            ]\n\n\nclass OpenBSDPkgMgrFactCollector(BaseFactCollector):\n    name = 'pkg_mgr'\n    _fact_ids = set()\n    _platform = 'OpenBSD'\n\n    def collect(self, module=None, collected_facts=None):\n        facts_dict = {}\n\n",
  "slicing": "    name = 'pkg_mgr'\n"
 },
 "274": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/system/pkg_mgr.py",
  "lineno": "47",
  "column": "4",
  "context": "):\n    name = 'pkg_mgr'\n    _fact_ids = set()\n    _platform = 'OpenBSD'\n\n    def collect(self, module=None, collected_fact",
  "context_lines": "            ]\n\n\nclass OpenBSDPkgMgrFactCollector(BaseFactCollector):\n    name = 'pkg_mgr'\n    _fact_ids = set()\n    _platform = 'OpenBSD'\n\n    def collect(self, module=None, collected_facts=None):\n        facts_dict = {}\n\n        facts_dict['pkg_mgr'] = 'openbsd_pkg'\n",
  "slicing": "    _platform = 'OpenBSD'\n"
 },
 "275": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/system/pkg_mgr.py",
  "lineno": "58",
  "column": "4",
  "context": "class PkgMgrFactCollector(BaseFactCollector):\n    name = 'pkg_mgr'\n    _fact_ids = set()\n    _platform = 'Generic'\n  ",
  "context_lines": "        facts_dict['pkg_mgr'] = 'openbsd_pkg'\n        return facts_dict\n\n\n# the fact ends up being 'pkg_mgr' so stick with that naming/spelling\nclass PkgMgrFactCollector(BaseFactCollector):\n    name = 'pkg_mgr'\n    _fact_ids = set()\n    _platform = 'Generic'\n    required_facts = set(['distribution'])\n\n    def _check_rh_versions(self, pkg_mgr_name, collected_facts):\n",
  "slicing": "    name = 'pkg_mgr'\n"
 },
 "276": {
  "name": "_platform",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/system/pkg_mgr.py",
  "lineno": "60",
  "column": "4",
  "context": "):\n    name = 'pkg_mgr'\n    _fact_ids = set()\n    _platform = 'Generic'\n    required_facts = set(['distribution'])\n\n    de",
  "context_lines": "# the fact ends up being 'pkg_mgr' so stick with that naming/spelling\nclass PkgMgrFactCollector(BaseFactCollector):\n    name = 'pkg_mgr'\n    _fact_ids = set()\n    _platform = 'Generic'\n    required_facts = set(['distribution'])\n\n    def _check_rh_versions(self, pkg_mgr_name, collected_facts):\n        if collected_facts['ansible_distribution'] == 'Fedora':\n            if os.path.exists('/run/ostree-booted'):\n",
  "slicing": "    _platform = 'Generic'\n"
 },
 "277": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/system/ssh_pub_keys.py",
  "lineno": "25",
  "column": "4",
  "context": "ss SshPubKeyFactCollector(BaseFactCollector):\n    name = 'ssh_pub_keys'\n    _fact_ids = set(['ssh_host_pub_keys',\n        ",
  "context_lines": "__metaclass__ = type\n\nfrom ansible.module_utils.facts.utils import get_file_content\n\nfrom ansible.module_utils.facts.collector import BaseFactCollector\n\n\nclass SshPubKeyFactCollector(BaseFactCollector):\n    name = 'ssh_pub_keys'\n    _fact_ids = set(['ssh_host_pub_keys',\n                     'ssh_host_key_dsa_public',\n                     'ssh_host_key_rsa_public',\n                     'ssh_host_key_ecdsa_public',\n",
  "slicing": "    name = 'ssh_pub_keys'\n"
 },
 "278": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/system/python.py",
  "lineno": "34",
  "column": "4",
  "context": "class PythonFactCollector(BaseFactCollector):\n    name = 'python'\n    _fact_ids = set()\n\n    def collect(self, modul",
  "context_lines": "    HAS_SSLCONTEXT = True\nexcept ImportError:\n    HAS_SSLCONTEXT = False\n\n\nclass PythonFactCollector(BaseFactCollector):\n    name = 'python'\n    _fact_ids = set()\n\n    def collect(self, module=None, collected_facts=None):\n        python_facts = {}\n        python_facts['python'] = {\n",
  "slicing": "    name = 'python'\n"
 },
 "279": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/system/caps.py",
  "lineno": "26",
  "column": "4",
  "context": "CapabilitiesFactCollector(BaseFactCollector):\n    name = 'caps'\n    _fact_ids = set(['system_capabilities',\n      ",
  "context_lines": "from __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\n\nfrom ansible.module_utils.facts.collector import BaseFactCollector\n\n\nclass SystemCapabilitiesFactCollector(BaseFactCollector):\n    name = 'caps'\n    _fact_ids = set(['system_capabilities',\n                     'system_capabilities_enforced'])\n\n    def collect(self, module=None, collected_facts=None):\n        facts_dict = {}\n",
  "slicing": "    name = 'caps'\n"
 },
 "280": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/system/platform.py",
  "lineno": "33",
  "column": "4",
  "context": "ass PlatformFactCollector(BaseFactCollector):\n    name = 'platform'\n    _fact_ids = set(['system',\n                   ",
  "context_lines": "# i86pc is a Solaris and derivatives-ism\nSOLARIS_I86_RE_PATTERN = r'i([3456]86|86pc)'\nsolaris_i86_re = re.compile(SOLARIS_I86_RE_PATTERN)\n\n\nclass PlatformFactCollector(BaseFactCollector):\n    name = 'platform'\n    _fact_ids = set(['system',\n                     'kernel',\n                     'kernel_version',\n                     'machine',\n",
  "slicing": "    name = 'platform'\n"
 },
 "281": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/system/lsb.py",
  "lineno": "28",
  "column": "4",
  "context": "\n\n\nclass LSBFactCollector(BaseFactCollector):\n    name = 'lsb'\n    _fact_ids = set()\n    STRIP_QUOTES = r'\\'\\\"\\\\'",
  "context_lines": "import os\n\nfrom ansible.module_utils.facts.utils import get_file_lines\nfrom ansible.module_utils.facts.collector import BaseFactCollector\n\n\nclass LSBFactCollector(BaseFactCollector):\n    name = 'lsb'\n    _fact_ids = set()\n    STRIP_QUOTES = r'\\'\\\"\\\\'\n\n    def _lsb_release_bin(self, lsb_path, module):\n        lsb_facts = {}\n\n",
  "slicing": "    name = 'lsb'\n"
 },
 "282": {
  "name": "STRIP_QUOTES",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/system/lsb.py",
  "lineno": "30",
  "column": "4",
  "context": "ctor):\n    name = 'lsb'\n    _fact_ids = set()\n    STRIP_QUOTES = r'\\'\\\"\\\\'\n\n    def _lsb_release_bin(self, lsb_path, module):",
  "context_lines": "from ansible.module_utils.facts.collector import BaseFactCollector\n\n\nclass LSBFactCollector(BaseFactCollector):\n    name = 'lsb'\n    _fact_ids = set()\n    STRIP_QUOTES = r'\\'\\\"\\\\'\n\n    def _lsb_release_bin(self, lsb_path, module):\n        lsb_facts = {}\n\n        if not lsb_path:\n",
  "slicing": "    STRIP_QUOTES = r'\\'\\\"\\\\'\n"
 },
 "283": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/system/selinux.py",
  "lineno": "37",
  "column": "4",
  "context": "lass SelinuxFactCollector(BaseFactCollector):\n    name = 'selinux'\n    _fact_ids = set()\n\n    def collect(self, modul",
  "context_lines": "    0: 'permissive',\n    -1: 'disabled'\n}\n\n\nclass SelinuxFactCollector(BaseFactCollector):\n    name = 'selinux'\n    _fact_ids = set()\n\n    def collect(self, module=None, collected_facts=None):\n        facts_dict = {}\n        selinux_facts = {}\n\n",
  "slicing": "    name = 'selinux'\n"
 },
 "284": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/system/fips.py",
  "lineno": "27",
  "column": "4",
  "context": "\n\nclass FipsFactCollector(BaseFactCollector):\n    name = 'fips'\n    _fact_ids = set()\n\n    def collect(self, modul",
  "context_lines": "__metaclass__ = type\n\nfrom ansible.module_utils.facts.utils import get_file_content\n\nfrom ansible.module_utils.facts.collector import BaseFactCollector\n\n\nclass FipsFactCollector(BaseFactCollector):\n    name = 'fips'\n    _fact_ids = set()\n\n    def collect(self, module=None, collected_facts=None):\n        # NOTE: this is populated even if it is not set\n        fips_facts = {}\n",
  "slicing": "    name = 'fips'\n"
 },
 "285": {
  "name": "STRIP_QUOTES",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/system/distribution.py",
  "lineno": "92",
  "column": "4",
  "context": " = {\n        'Archlinux': 'Arch Linux'\n    }\n\n    STRIP_QUOTES = r'\\'\\\"\\\\'\n\n    def __init__(self, module):\n        self.modu",
  "context_lines": "    # as the name. For os-release, that is in form 'NAME=Arch'\n    OS_RELEASE_ALIAS = {\n        'Archlinux': 'Arch Linux'\n    }\n\n    STRIP_QUOTES = r'\\'\\\"\\\\'\n\n    def __init__(self, module):\n        self.module = module\n\n    def _get_file_content(self, path):\n",
  "slicing": "    STRIP_QUOTES = r'\\'\\\"\\\\'\n"
 },
 "286": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/system/distribution.py",
  "lineno": "678",
  "column": "4",
  "context": "DistributionFactCollector(BaseFactCollector):\n    name = 'distribution'\n    _fact_ids = set(['distribution_version',\n     ",
  "context_lines": "                sunos_facts['distribution_version'] = uname_v.splitlines()[0].strip()\n            return sunos_facts\n\n        return sunos_facts\n\n\nclass DistributionFactCollector(BaseFactCollector):\n    name = 'distribution'\n    _fact_ids = set(['distribution_version',\n                     'distribution_release',\n                     'distribution_major_version',\n                     'os_family'])\n\n",
  "slicing": "    name = 'distribution'\n"
 },
 "287": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/system/local.py",
  "lineno": "33",
  "column": "4",
  "context": "\nclass LocalFactCollector(BaseFactCollector):\n    name = 'local'\n    _fact_ids = set()\n\n    def collect(self, modul",
  "context_lines": "from ansible.module_utils.six.moves import StringIO\n\nfrom ansible.module_utils.facts.utils import get_file_content\n\nfrom ansible.module_utils.facts.collector import BaseFactCollector\n\n\nclass LocalFactCollector(BaseFactCollector):\n    name = 'local'\n    _fact_ids = set()\n\n    def collect(self, module=None, collected_facts=None):\n        local_facts = {}\n        local_facts['local'] = {}\n\n",
  "slicing": "    name = 'local'\n"
 },
 "288": {
  "name": "name",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/facts/system/user.py",
  "lineno": "27",
  "column": "4",
  "context": "\n\nclass UserFactCollector(BaseFactCollector):\n    name = 'user'\n    _fact_ids = set(['user_id', 'user_uid', 'user_",
  "context_lines": "import os\nimport pwd\n\nfrom ansible.module_utils.facts.collector import BaseFactCollector\n\n\nclass UserFactCollector(BaseFactCollector):\n    name = 'user'\n    _fact_ids = set(['user_id', 'user_uid', 'user_gid',\n                     'user_gecos', 'user_dir', 'user_shell',\n                     'real_user_id', 'effective_user_id',\n                     'effective_group_ids'])\n\n",
  "slicing": "    name = 'user'\n"
 },
 "289": {
  "name": "errors",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/common/text/converters.py",
  "lineno": "105",
  "column": "12",
  "context": "LERS:\n        if HAS_SURROGATEESCAPE:\n            errors = 'surrogateescape'\n        elif errors == 'surrogate_or_strict':\n    ",
  "context_lines": "    # If it has surrogates, we know because it will decode\n    original_errors = errors\n    if errors in _COMPOSED_ERROR_HANDLERS:\n        if HAS_SURROGATEESCAPE:\n            errors = 'surrogateescape'\n        elif errors == 'surrogate_or_strict':\n            errors = 'strict'\n        else:\n            errors = 'replace'\n\n",
  "slicing": [
   "            errors = 'surrogateescape'\n",
   "        elif errors == 'surrogate_or_strict':\n",
   "            return obj.encode(encoding, errors)\n",
   "    return to_bytes(value, encoding, errors)\n",
   "    if errors in _COMPOSED_ERROR_HANDLERS:\n",
   "        elif errors == 'surrogate_or_strict':\n",
   "        return obj.decode(encoding, errors)\n",
   "    return to_text(value, encoding, errors)\n",
   "        return to_bytes(d, encoding=encoding, errors=errors)\n",
   "        return dict(container_to_bytes(o, encoding, errors) for o in iteritems(d))\n",
   "        return [container_to_bytes(o, encoding, errors) for o in d]\n",
   "        return tuple(container_to_bytes(o, encoding, errors) for o in d)\n",
   "        return to_text(d, encoding=encoding, errors=errors)\n",
   "        return dict(container_to_text(o, encoding, errors) for o in iteritems(d))\n",
   "        return [container_to_text(o, encoding, errors) for o in d]\n",
   "        return tuple(container_to_text(o, encoding, errors) for o in d)\n"
  ]
 },
 "290": {
  "name": "errors",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/module_utils/common/text/converters.py",
  "lineno": "206",
  "column": "12",
  "context": "LERS:\n        if HAS_SURROGATEESCAPE:\n            errors = 'surrogateescape'\n        elif errors == 'surrogate_or_strict':\n    ",
  "context_lines": "    if isinstance(obj, text_type):\n        return obj\n\n    if errors in _COMPOSED_ERROR_HANDLERS:\n        if HAS_SURROGATEESCAPE:\n            errors = 'surrogateescape'\n        elif errors == 'surrogate_or_strict':\n            errors = 'strict'\n        else:\n            errors = 'replace'\n\n",
  "slicing": [
   "            errors = 'surrogateescape'\n",
   "        elif errors == 'surrogate_or_strict':\n",
   "        return obj.decode(encoding, errors)\n",
   "    return to_text(value, encoding, errors)\n",
   "        return to_bytes(d, encoding=encoding, errors=errors)\n",
   "        return dict(container_to_bytes(o, encoding, errors) for o in iteritems(d))\n",
   "        return [container_to_bytes(o, encoding, errors) for o in d]\n",
   "        return tuple(container_to_bytes(o, encoding, errors) for o in d)\n",
   "        return to_text(d, encoding=encoding, errors=errors)\n",
   "        return dict(container_to_text(o, encoding, errors) for o in iteritems(d))\n",
   "        return [container_to_text(o, encoding, errors) for o in d]\n",
   "        return tuple(container_to_text(o, encoding, errors) for o in d)\n"
  ]
 },
 "291": {
  "name": "GITHUB_AUTH",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/galaxy/login.py",
  "lineno": "43",
  "column": "4",
  "context": "y API prior to performing CUD operations '''\n\n    GITHUB_AUTH = 'https://api.github.com/authorizations'\n\n    def __init__(self, galaxy, github_token=None)",
  "context_lines": "from ansible.utils.display import Display\n\ndisplay = Display()\n\n\nclass GalaxyLogin(object):\n    ''' Class to handle authenticating user with Galaxy API prior to performing CUD operations '''\n\n    GITHUB_AUTH = 'https://api.github.com/authorizations'\n\n    def __init__(self, galaxy, github_token=None):\n        self.galaxy = galaxy\n        self.github_username = None\n",
  "slicing": "    GITHUB_AUTH = 'https://api.github.com/authorizations'\n"
 },
 "292": {
  "name": "token_type",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/galaxy/token.py",
  "lineno": "52",
  "column": "4",
  "context": "by cloud.redhat.com\n    ie Automation Hub'''\n\n    token_type = 'Bearer'\n\n    def __init__(self, access_token=None, auth_ur",
  "context_lines": "class KeycloakToken(object):\n    '''A token granted by a Keycloak server.\n\n    Like sso.redhat.com as used by cloud.redhat.com\n    ie Automation Hub'''\n\n    token_type = 'Bearer'\n\n    def __init__(self, access_token=None, auth_url=None, validate_certs=True):\n        self.access_token = access_token\n        self.auth_url = auth_url\n",
  "slicing": "    token_type = 'Bearer'\n"
 },
 "293": {
  "name": "token_type",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/galaxy/token.py",
  "lineno": "101",
  "column": "4",
  "context": "toring and retrieving local galaxy token '''\n\n    token_type = 'Token'\n\n    def __init__(self, token=None):\n        self.",
  "context_lines": "        headers['Authorization'] = '%s %s' % (self.token_type, self.get())\n        return headers\n\n\nclass GalaxyToken(object):\n    ''' Class to storing and retrieving local galaxy token '''\n\n    token_type = 'Token'\n\n    def __init__(self, token=None):\n        self.b_file = to_bytes(C.GALAXY_TOKEN_PATH, errors='surrogate_or_strict')\n        # Done so the config file is only opened when set/get/save is called\n",
  "slicing": "    token_type = 'Token'\n"
 },
 "294": {
  "name": "token_type",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/galaxy/token.py",
  "lineno": "155",
  "column": "4",
  "context": "eturn headers\n\n\nclass BasicAuthToken(object):\n    token_type = 'Basic'\n\n    def __init__(self, username, password=None):\n",
  "context_lines": "        if token:\n            headers['Authorization'] = '%s %s' % (self.token_type, self.get())\n        return headers\n\n\nclass BasicAuthToken(object):\n    token_type = 'Basic'\n\n    def __init__(self, username, password=None):\n        self.username = username\n        self.password = password\n",
  "slicing": "    token_type = 'Basic'\n"
 },
 "295": {
  "name": "_data_source",
  "type": "NoneType",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/parsing/yaml/objects.py",
  "lineno": "39",
  "column": "4",
  "context": "tributes to them during yaml parsing\n\n    '''\n    _data_source = None\n    _line_number = 0\n    _column_number = 0\n\n    d",
  "context_lines": "    '''\n    the base class used to sub-class python built-in objects\n    so that we can add attributes to them during yaml parsing\n\n    '''\n    _data_source = None\n    _line_number = 0\n    _column_number = 0\n\n    def _get_ansible_position(self):\n        return (self._data_source, self._line_number, self._column_number)\n\n",
  "slicing": "    _data_source = None\n"
 },
 "296": {
  "name": "_line_number",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/parsing/yaml/objects.py",
  "lineno": "40",
  "column": "4",
  "context": "yaml parsing\n\n    '''\n    _data_source = None\n    _line_number = 0\n    _column_number = 0\n\n    def _get_ansible_posit",
  "context_lines": "    the base class used to sub-class python built-in objects\n    so that we can add attributes to them during yaml parsing\n\n    '''\n    _data_source = None\n    _line_number = 0\n    _column_number = 0\n\n    def _get_ansible_position(self):\n        return (self._data_source, self._line_number, self._column_number)\n\n    def _set_ansible_position(self, obj):\n",
  "slicing": "    _line_number = 0\n"
 },
 "297": {
  "name": "_column_number",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/parsing/yaml/objects.py",
  "lineno": "41",
  "column": "4",
  "context": "\n    _data_source = None\n    _line_number = 0\n    _column_number = 0\n\n    def _get_ansible_position(self):\n        retu",
  "context_lines": "    so that we can add attributes to them during yaml parsing\n\n    '''\n    _data_source = None\n    _line_number = 0\n    _column_number = 0\n\n    def _get_ansible_position(self):\n        return (self._data_source, self._line_number, self._column_number)\n\n    def _set_ansible_position(self, obj):\n",
  "slicing": "    _column_number = 0\n"
 },
 "298": {
  "name": "ansible_pos",
  "type": "property",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/parsing/yaml/objects.py",
  "lineno": "58",
  "column": "4",
  "context": "ber = line\n        self._column_number = col\n\n    ansible_pos = property(_get_ansible_position, _set_ansible_position)\n\n\n# try to always use orderddict with yaml, after ",
  "context_lines": "            )\n        self._data_source = src\n        self._line_number = line\n        self._column_number = col\n\n    ansible_pos = property(_get_ansible_position, _set_ansible_position)\n\n\n# try to always use orderddict with yaml, after py3.6 the dict type already does this\nodict = dict\nif sys.version_info[:2] < (3, 7):\n",
  "slicing": [
   "            (src, line, col) = obj\n",
   "        self._data_source = src\n",
   "        self._line_number = line\n",
   "        self._column_number = col\n",
   "    ansible_pos = property(_get_ansible_position, _set_ansible_position)\n"
  ]
 },
 "299": {
  "name": "yaml_tag",
  "type": "str",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/parsing/yaml/objects.py",
  "lineno": "90",
  "column": "4",
  "context": "   __UNSAFE__ = True\n    __ENCRYPTED__ = True\n    yaml_tag = u'!vault'\n\n    @classmethod\n    def from_plaintext(cls, seq,",
  "context_lines": "class AnsibleVaultEncryptedUnicode(Sequence, AnsibleBaseYAMLObject):\n    '''Unicode like object that is not evaluated (decrypted) until it needs to be'''\n    __UNSAFE__ = True\n    __ENCRYPTED__ = True\n    yaml_tag = u'!vault'\n\n    @classmethod\n    def from_plaintext(cls, seq, vault, secret):\n        if not vault:\n",
  "slicing": "    yaml_tag = u'!vault'\n"
 },
 "300": {
  "name": "__rmul__",
  "type": "function",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/parsing/yaml/objects.py",
  "lineno": "226",
  "column": "4",
  "context": "mul__(self, n):\n        return self.data * n\n\n    __rmul__ = __mul__\n\n    def __mod__(self, args):\n        return self.",
  "context_lines": "            return other + self.data\n        return to_text(other) + self.data\n\n    def __mul__(self, n):\n        return self.data * n\n\n    __rmul__ = __mul__\n\n    def __mod__(self, args):\n        return self.data % args\n\n    def __rmod__(self, template):\n",
  "slicing": [
   "            (src, line, col) = obj\n",
   "        self._data_source = src\n",
   "        self._line_number = line\n",
   "        self._column_number = col\n",
   "odict = dict\n",
   "class AnsibleMapping(AnsibleBaseYAMLObject, odict):\n",
   "        ciphertext = vault.encrypt(seq, secret)\n",
   "        avu = cls(ciphertext)\n",
   "        avu.vault = vault\n",
   "        return avu\n",
   "        self._ciphertext = to_bytes(ciphertext)\n",
   "            char = char.data\n",
   "        return char in self.data\n",
   "        start = max(start, 0)\n",
   "        end = max(end, 0)\n",
   "        return self.data[start:end]\n",
   "    __rmul__ = __mul__\n",
   "        return self.data.count(sub, start, end)\n",
   "        return self.data.endswith(suffix, start, end)\n",
   "        return self.data.find(sub, start, end)\n",
   "        return self.data.index(sub, start, end)\n",
   "        return self.data.rfind(sub, start, end)\n",
   "        return self.data.rindex(sub, start, end)\n",
   "        return self.data.startswith(prefix, start, end)\n"
  ]
 },
 "301": {
  "name": "VAULT_ID_UNKNOWN_RC",
  "type": "int",
  "class": "build-in",
  "approach": "Pysonar2",
  "file_path": "ansible/lib/ansible/parsing/vault/__init__.py",
  "lineno": "490",
  "column": "4",
  "context": "s ClientScriptVaultSecret(ScriptVaultSecret):\n    VAULT_ID_UNKNOWN_RC = 2\n\n    def __init__(self, filename=None, encoding=No",
  "context_lines": "                               (self.filename, popen.returncode, stderr))\n\n    def _build_command(self):\n        return [self.filename]\n\n\nclass ClientScriptVaultSecret(ScriptVaultSecret):\n    VAULT_ID_UNKNOWN_RC = 2\n\n    def __init__(self, filename=None, encoding=None, loader=None, vault_id=None):\n        super(ClientScriptVaultSecret, self).__init__(filename=filename,\n                                                      encoding=encoding,\n",
  "slicing": "    VAULT_ID_UNKNOWN_RC = 2\n"
 },
 "302": {
  "name": "_do_template",
  "type": "function",
  "class": "unknown",
  "approach": "annotation",
  "file_path": "ansible/lib/ansible/template/__init__.py",
  "lineno": "1062",
  "column": "4",
  "context": "e anyone is using old private method directly\n    _do_template = do_template\n",
  "context_lines": "            else:\n                display.debug(\"Ignoring undefined failure: %s\" % to_text(e))\n                return data\n\n    # for backwards compatibility in case anyone is using old private method directly\n    _do_template = do_template\n",
  "slicing": [
   "display = Display()\n",
   "NON_TEMPLATED_TYPES = (bool, Number)\n",
   "JINJA2_OVERRIDE = '#jinja2:'\n",
   "USE_JINJA2_NATIVE = False\n",
   "        USE_JINJA2_NATIVE = True\n",
   "        display.warning(\n",
   "JINJA2_BEGIN_TOKENS = frozenset(('variable_begin', 'block_begin', 'comment_begin', 'raw_begin'))\n",
   "JINJA2_END_TOKENS = frozenset(('variable_end', 'block_end', 'comment_end', 'raw_end'))\n",
   "RANGE_TYPE = type(range(0))\n",
   "    b_path = to_bytes(path)\n",
   "        template_uid = pwd.getpwuid(os.stat(b_path).st_uid).pw_name\n",
   "        template_uid = os.stat(b_path).st_uid\n",
   "    temp_vars = {\n",
   "        'template_mtime': datetime.datetime.fromtimestamp(os.path.getmtime(b_path)),\n",
   "        'template_uid': to_text(template_uid),\n",
   "    managed_default = C.DEFAULT_MANAGED_STR\n",
   "    managed_str = managed_default.format(\n",
   "        host=temp_vars['template_host'],\n",
   "        uid=temp_vars['template_uid'],\n",
   "        file=temp_vars['template_path'],\n",
   "    temp_vars['ansible_managed'] = to_text(time.strftime(to_native(managed_str), time.localtime(os.path.getmtime(b_path))))\n",
   "    return temp_vars\n",
   "def _escape_backslashes(data, jinja_env):\n",
   "        new_data = []\n",
   "        d2 = jinja_env.preprocess(data)\n",
   "        in_var = False\n",
   "        for token in jinja_env.lex(d2):\n",
   "            if token[1] == 'variable_begin':\n",
   "                in_var = True\n",
   "                new_data.append(token[2])\n",
   "            elif token[1] == 'variable_end':\n",
   "                in_var = False\n",
   "                new_data.append(token[2])\n",
   "            elif in_var and token[1] == 'string':\n",
   "                new_data.append(token[2].replace('\\\\', '\\\\\\\\'))\n",
   "                new_data.append(token[2])\n",
   "        data = ''.join(new_data)\n",
   "    return data\n",
   "def is_template(data, jinja_env):\n",
   "    found = None\n",
   "    start = True\n",
   "    comment = False\n",
   "    d2 = jinja_env.preprocess(data)\n",
   "        for token in jinja_env.lex(d2):\n",
   "            if token[1] in JINJA2_BEGIN_TOKENS:\n",
   "                if start and token[1] == 'comment_begin':\n",
   "                    comment = True\n",
   "                start = False\n",
   "                found = token[1].split('_')[0]\n",
   "            elif token[1] in JINJA2_END_TOKENS:\n",
   "                if token[1].split('_')[0] == found:\n",
   "                elif comment:\n",
   "def _count_newlines_from_end(in_str):\n",
   "        i = len(in_str)\n",
   "        j = i - 1\n",
   "        while in_str[j] == '\\n':\n",
   "            j -= 1\n",
   "        return i - 1 - j\n",
   "        return i\n",
   "        for key in item:\n",
   "            recursive_check_defined(item[key])\n",
   "        for i in item:\n",
   "            recursive_check_defined(i)\n",
   "def _is_rolled(value):\n",
   "        isinstance(value, RANGE_TYPE)\n",
   "def _unroll_iterator(func):\n",
   "        ret = func(*args, **kwargs)\n",
   "        if _is_rolled(ret):\n",
   "            return list(ret)\n",
   "        return ret\n",
   "    for attr in ('__module__', '__name__', '__qualname__', '__doc__', '__annotations__'):\n",
   "            value = getattr(func, attr)\n",
   "            setattr(wrapper, attr, value)\n",
   "    for attr in ('__dict__',):\n",
   "        getattr(wrapper, attr).update(getattr(func, attr, {}))\n",
   "            for key in val.keys():\n",
   "                if self._is_unsafe(val[key]):\n",
   "            for item in val:\n",
   "                if self._is_unsafe(item):\n",
   "        val = super(AnsibleContext, self).resolve(key)\n",
   "        self._update_unsafe(val)\n",
   "        return val\n",
   "        val = super(AnsibleContext, self).resolve_or_missing(key)\n",
   "        self._update_unsafe(val)\n",
   "        return val\n",
   "            if not isinstance(key, string_types):\n",
   "            key = to_native(key)\n",
   "            if '.' not in key:  # might be a built-in or legacy, check the delegatee dict first, then try for a last-chance base redirect\n",
   "                func = self._delegatee.get(key)\n",
   "                if func:\n",
   "                    return func\n",
   "                leaf_key = key\n",
   "                key = 'ansible.builtin.' + key\n",
   "                leaf_key = key.split('.')[-1]\n",
   "            acr = AnsibleCollectionRef.try_parse_fqcr(key, self._dirname)\n",
   "            if not acr:\n",
   "                raise KeyError('invalid plugin name: {0}'.format(key))\n",
   "            ts = _get_collection_metadata(acr.collection)\n",
   "            routing_entry = ts.get('plugin_routing', {}).get(self._dirname, {}).get(leaf_key, {})\n",
   "            deprecation_entry = routing_entry.get('deprecation')\n",
   "            if deprecation_entry:\n",
   "                warning_text = deprecation_entry.get('warning_text')\n",
   "                removal_date = deprecation_entry.get('removal_date')\n",
   "                removal_version = deprecation_entry.get('removal_version')\n",
   "                if not warning_text:\n",
   "                    warning_text = '{0} \"{1}\" is deprecated'.format(self._dirname, key)\n",
   "                display.deprecated(warning_text, version=removal_version, date=removal_date, collection_name=acr.collection)\n",
   "            tombstone_entry = routing_entry.get('tombstone')\n",
   "            if tombstone_entry:\n",
   "                warning_text = tombstone_entry.get('warning_text')\n",
   "                removal_date = tombstone_entry.get('removal_date')\n",
   "                removal_version = tombstone_entry.get('removal_version')\n",
   "                if not warning_text:\n",
   "                    warning_text = '{0} \"{1}\" has been removed'.format(self._dirname, key)\n",
   "                exc_msg = display.get_deprecation_message(warning_text, version=removal_version, date=removal_date,\n",
   "                                                          collection_name=acr.collection, removed=True)\n",
   "                raise AnsiblePluginRemovedError(exc_msg)\n",
   "            redirect_fqcr = routing_entry.get('redirect', None)\n",
   "            if redirect_fqcr:\n",
   "                acr = AnsibleCollectionRef.from_fqcr(ref=redirect_fqcr, ref_type=self._dirname)\n",
   "                display.vvv('redirecting {0} {1} to {2}.{3}'.format(self._dirname, key, acr.collection, acr.resource))\n",
   "                key = redirect_fqcr\n",
   "            func = self._collection_jinja_func_cache.get(key)\n",
   "            if func:\n",
   "                return func\n",
   "                pkg = import_module(acr.n_python_package_name)\n",
   "            parent_prefix = acr.collection\n",
   "            if acr.subdirs:\n",
   "                parent_prefix = '{0}.{1}'.format(parent_prefix, acr.subdirs)\n",
   "            for dummy, module_name, ispkg in pkgutil.iter_modules(pkg.__path__, prefix=parent_prefix + '.'):\n",
   "                if ispkg:\n",
   "                    plugin_impl = self._pluginloader.get(module_name)\n",
   "                method_map = getattr(plugin_impl, self._method_map_name)\n",
   "                for f in iteritems(method_map()):\n",
   "                    fq_name = '.'.join((parent_prefix, f[0]))\n",
   "                    self._collection_jinja_func_cache[fq_name] = _unroll_iterator(f[1])\n",
   "            function_impl = self._collection_jinja_func_cache[key]\n",
   "            return function_impl\n",
   "            display.warning('an unexpected error occurred during Jinja2 environment setup: {0}'.format(to_native(ex)))\n",
   "            display.vvv('exception during Jinja2 environment setup: {0}'.format(format_exc()))\n",
   "        return self._delegatee.__setitem__(key, value)\n",
   "        variables = {} if variables is None else variables\n",
   "        self._available_variables = variables\n",
   "        for fp in self._filter_loader.all():\n",
   "            self._filters.update(fp.filters())\n",
   "        for fp in self._test_loader.all():\n",
   "            self._tests.update(fp.tests())\n",
   "        jinja_exts = []\n",
   "            jinja_exts = C.DEFAULT_JINJA2_EXTENSIONS.replace(\" \", \"\").split(',')\n",
   "        return jinja_exts\n",
   "        if not isinstance(variables, Mapping):\n",
   "            raise AnsibleAssertionError(\"the type of 'variables' should be a Mapping but was a %s\" % (type(variables)))\n",
   "        self._available_variables = variables\n",
   "        display.deprecated(\n",
   "        self.available_variables = variables\n",
   "        mapping = {\n",
   "        original = {}\n",
   "        for key, value in kwargs.items():\n",
   "            obj = mapping.get(key, self.environment)\n",
   "                original[key] = getattr(obj, key)\n",
   "                if value is not None:\n",
   "                    setattr(obj, key, value)\n",
   "        for key in original:\n",
   "            obj = mapping.get(key, self.environment)\n",
   "            setattr(obj, key, original[key])\n",
   "        static_vars = [''] if static_vars is None else static_vars\n",
   "            fail_on_undefined = self._fail_on_undefined_errors\n",
   "                variable = self._convert_bare_variable(variable)\n",
   "            if isinstance(variable, string_types):\n",
   "                result = variable\n",
   "                if self.is_possibly_template(variable):\n",
   "                    only_one = self.SINGLE_VAR.match(variable)\n",
   "                    if only_one:\n",
   "                        var_name = only_one.group(1)\n",
   "                        if var_name in self._available_variables:\n",
   "                            resolved_val = self._available_variables[var_name]\n",
   "                            if isinstance(resolved_val, NON_TEMPLATED_TYPES):\n",
   "                                return resolved_val\n",
   "                            elif resolved_val is None:\n",
   "                    sha1_hash = None\n",
   "                        variable_hash = sha1(text_type(variable).encode('utf-8'))\n",
   "                        options_hash = sha1(\n",
   "                                text_type(fail_on_undefined) +\n",
   "                        sha1_hash = variable_hash.hexdigest() + options_hash.hexdigest()\n",
   "                    if cache and sha1_hash in self._cached_result:\n",
   "                        result = self._cached_result[sha1_hash]\n",
   "                        result = self.do_template(\n",
   "                            variable,\n",
   "                            fail_on_undefined=fail_on_undefined,\n",
   "                        if not USE_JINJA2_NATIVE:\n",
   "                            unsafe = hasattr(result, '__UNSAFE__')\n",
   "                            if convert_data and not self._no_type_regex.match(variable):\n",
   "                                if (result.startswith(\"{\") and not result.startswith(self.environment.variable_start_string)) or \\\n",
   "                                        result.startswith(\"[\") or result in (\"True\", \"False\"):\n",
   "                                    eval_results = safe_eval(result, include_exceptions=True)\n",
   "                                    if eval_results[1] is None:\n",
   "                                        result = eval_results[0]\n",
   "                                        if unsafe:\n",
   "                                            result = wrap_var(result)\n",
   "                        if cache and only_one:\n",
   "                            self._cached_result[sha1_hash] = result\n",
   "                return result\n",
   "            elif is_sequence(variable):\n",
   "                    fail_on_undefined=fail_on_undefined,\n",
   "                ) for v in variable]\n",
   "            elif isinstance(variable, Mapping):\n",
   "                d = {}\n",
   "                for k in variable.keys():\n",
   "                    if k not in static_vars:\n",
   "                        d[k] = self.template(\n",
   "                            variable[k],\n",
   "                            fail_on_undefined=fail_on_undefined,\n",
   "                        d[k] = variable[k]\n",
   "                return d\n",
   "                return variable\n",
   "                return variable\n",
   "        if isinstance(data, string_types):\n",
   "            return is_template(data, self.environment)\n",
   "        elif isinstance(data, (list, tuple)):\n",
   "            for v in data:\n",
   "                if self.is_template(v):\n",
   "        elif isinstance(data, dict):\n",
   "            for k in data:\n",
   "                if self.is_template(k) or self.is_template(data[k]):\n",
   "        env = self.environment\n",
   "        if isinstance(data, string_types):\n",
   "            for marker in (env.block_start_string, env.variable_start_string, env.comment_start_string):\n",
   "                if marker in data:\n",
   "        if isinstance(variable, string_types):\n",
   "            contains_filters = \"|\" in variable\n",
   "            first_part = variable.split(\"|\")[0].split(\".\")[0].split(\"[\")[0]\n",
   "            if (contains_filters or first_part in self._available_variables) and self.environment.variable_start_string not in variable:\n",
   "                return \"%s%s%s\" % (self.environment.variable_start_string, variable, self.environment.variable_end_string)\n",
   "        return variable\n",
   "        if _is_rolled(thing):\n",
   "        if USE_JINJA2_NATIVE:\n",
   "            now = datetime.datetime.utcnow()\n",
   "            now = datetime.datetime.now()\n",
   "            return now.strftime(fmt)\n",
   "        return now\n",
   "        instance = self._lookup_loader.get(name, loader=self._loader, templar=self)\n",
   "        if instance is not None:\n",
   "            wantlist = kwargs.pop('wantlist', False)\n",
   "            allow_unsafe = kwargs.pop('allow_unsafe', C.DEFAULT_ALLOW_UNSAFE_LOOKUPS)\n",
   "            errors = kwargs.pop('errors', 'strict')\n",
   "            loop_terms = listify_lookup_plugin_terms(terms=args, templar=self, loader=self._loader, fail_on_undefined=True, convert_bare=False)\n",
   "                ran = instance.run(loop_terms, variables=self._available_variables, **kwargs)\n",
   "                    msg = u\"An unhandled exception occurred while running the lookup plugin '%s'. Error was a %s, original message: %s\" % \\\n",
   "                    if errors == 'warn':\n",
   "                        display.warning(msg)\n",
   "                    elif errors == 'ignore':\n",
   "                        display.display(msg, log_only=True)\n",
   "                        raise AnsibleError(to_native(msg))\n",
   "                ran = [] if wantlist else None\n",
   "            if ran and not allow_unsafe:\n",
   "                if wantlist:\n",
   "                    ran = wrap_var(ran)\n",
   "                        ran = wrap_var(\",\".join(ran))\n",
   "                        if not isinstance(ran, Sequence):\n",
   "                        if len(ran) == 1:\n",
   "                            ran = wrap_var(ran[0])\n",
   "                            ran = wrap_var(ran)\n",
   "            return ran\n",
   "        if USE_JINJA2_NATIVE and not isinstance(data, string_types):\n",
   "            return data\n",
   "        data_newlines = _count_newlines_from_end(data)\n",
   "        if fail_on_undefined is None:\n",
   "            fail_on_undefined = self._fail_on_undefined_errors\n",
   "                myenv = self.environment.overlay()\n",
   "                myenv = self.environment.overlay(overrides)\n",
   "            if hasattr(data, 'startswith') and data.startswith(JINJA2_OVERRIDE):\n",
   "                eol = data.find('\\n')\n",
   "                line = data[len(JINJA2_OVERRIDE):eol]\n",
   "                data = data[eol + 1:]\n",
   "                for pair in line.split(','):\n",
   "                    (key, val) = pair.split(':')\n",
   "                    key = key.strip()\n",
   "                    setattr(myenv, key, ast.literal_eval(val.strip()))\n",
   "            myenv.filters.update(self._get_filters())\n",
   "            for k in myenv.filters:\n",
   "                myenv.filters[k] = _unroll_iterator(myenv.filters[k])\n",
   "            myenv.tests.update(self._get_tests())\n",
   "                data = _escape_backslashes(data, myenv)\n",
   "                t = myenv.from_string(data)\n",
   "                raise AnsibleError(\"template error while templating string: %s. String: %s\" % (to_native(e), to_native(data)))\n",
   "                    raise AnsibleError(\"recursive loop detected in template string: %s\" % to_native(data))\n",
   "                    return data\n",
   "                t.globals['query'] = t.globals['q'] = t.globals['lookup'] = self._fail_lookup\n",
   "            jvars = AnsibleJ2Vars(self, t.globals)\n",
   "            self.cur_context = new_context = t.new_context(jvars, shared=True)\n",
   "            rf = t.root_render_func(new_context)\n",
   "                res = j2_concat(rf)\n",
   "                if getattr(new_context, 'unsafe', False):\n",
   "                    res = wrap_var(res)\n",
   "                    errmsg = \"Unable to look up a name or access an attribute in template string (%s).\\n\" % to_native(data)\n",
   "                    errmsg += \"Make sure your variable name does not contain invalid characters like '-': %s\" % to_native(te)\n",
   "                    raise AnsibleUndefinedVariable(errmsg)\n",
   "                    display.debug(\"failing because of a type error, template data is: %s\" % to_text(data))\n",
   "                    raise AnsibleError(\"Unexpected templating type error occurred on (%s): %s\" % (to_native(data), to_native(te)))\n",
   "            if USE_JINJA2_NATIVE and not isinstance(res, string_types):\n",
   "                return res\n",
   "                res_newlines = _count_newlines_from_end(res)\n",
   "                if data_newlines > res_newlines:\n",
   "                    res += self.environment.newline_sequence * (data_newlines - res_newlines)\n",
   "            return res\n",
   "            if fail_on_undefined:\n",
   "                display.debug(\"Ignoring undefined failure: %s\" % to_text(e))\n",
   "                return data\n",
   "    _do_template = do_template\n"
  ]
 }
}